nohup: ignoring input

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:38:53 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/18/2024 08:38:53 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:06 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:08 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:10 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:12 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:39:14 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ACCURACY: 0.7836
Train Loss: 0.6268
ID Validation ACCURACY: 0.7810
ID Validation Loss: 0.6189
ID Test ACCURACY: 0.7937
ID Test Loss: 0.6184
OOD Validation ACCURACY: 0.9207
OOD Validation Loss: 0.3550
OOD Test ACCURACY: 0.8803
OOD Test Loss: 0.4248

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.7018
Train Loss: 0.6760
ID Validation ACCURACY: 0.7110
ID Validation Loss: 0.6712
ID Test ACCURACY: 0.7193
ID Test Loss: 0.6608
OOD Validation ACCURACY: 0.9307
OOD Validation Loss: 0.4142
OOD Test ACCURACY: 0.5897
OOD Test Loss: 0.7743

[0m[1;37mINFO[0m: [1mChartInfo 0.7937 0.8803 0.7193 0.5897 0.7110 0.9307[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.416
WIoU for r=0.6 = 0.386
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 1.000
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.717
WIoU for r=0.6 = 0.674


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.78
Model XAI F1 of binarized graphs for r=0.6 =  0.4159075
Model XAI WIoU of binarized graphs for r=0.6 =  0.38573749999999996
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.574
SUFF++ for r=0.6 class 0 = 0.609 +- 0.211 (in-sample avg dev_std = 0.327)
SUFF++ for r=0.6 class 1 = 0.747 +- 0.211 (in-sample avg dev_std = 0.327)
SUFF++ for r=0.6 class 2 = 0.598 +- 0.211 (in-sample avg dev_std = 0.327)
SUFF++ for r=0.6 all KL = 0.767 +- 0.211 (in-sample avg dev_std = 0.327)
SUFF++ for r=0.6 all L1 = 0.652 +- 0.166 (in-sample avg dev_std = 0.327)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.928
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9999375
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.93
SUFF++ for r=0.6 class 0 = 0.9 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 class 1 = 0.916 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 class 2 = 0.893 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 all KL = 0.971 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 all L1 = 0.903 +- 0.073 (in-sample avg dev_std = 0.116)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.891
Model XAI F1 of binarized graphs for r=0.6 =  0.7171774999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.67429125
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.864
SUFF++ for r=0.6 class 0 = 0.795 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 class 1 = 0.94 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 class 2 = 0.86 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 all KL = 0.937 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 all L1 = 0.864 +- 0.108 (in-sample avg dev_std = 0.186)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.78
Model XAI F1 of binarized graphs for r=0.6 =  0.4159075
Model XAI WIoU of binarized graphs for r=0.6 =  0.38573749999999996
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.426
NEC for r=0.6 class 0 = 0.529 +- 0.340 (in-sample avg dev_std = 0.286)
NEC for r=0.6 class 1 = 0.295 +- 0.340 (in-sample avg dev_std = 0.286)
NEC for r=0.6 class 2 = 0.585 +- 0.340 (in-sample avg dev_std = 0.286)
NEC for r=0.6 all KL = 0.379 +- 0.340 (in-sample avg dev_std = 0.286)
NEC for r=0.6 all L1 = 0.47 +- 0.244 (in-sample avg dev_std = 0.286)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.928
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9999375
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.574
NEC for r=0.6 class 0 = 0.541 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 class 1 = 0.111 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 class 2 = 0.62 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 all KL = 0.446 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 all L1 = 0.425 +- 0.285 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.891
Model XAI F1 of binarized graphs for r=0.6 =  0.7171774999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.67429125
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.514
NEC for r=0.6 class 0 = 0.575 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 class 1 = 0.104 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 class 2 = 0.674 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 all KL = 0.461 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 all L1 = 0.457 +- 0.295 (in-sample avg dev_std = 0.491)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:40:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:20 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:33 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:35 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:37 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:39 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:40:40 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 2...
[0m[1;37mINFO[0m: [1mCheckpoint 2: 
-----------------------------------
Train ACCURACY: 0.7278
Train Loss: 0.7861
ID Validation ACCURACY: 0.7147
ID Validation Loss: 0.7972
ID Test ACCURACY: 0.7297
ID Test Loss: 0.8069
OOD Validation ACCURACY: 0.3787
OOD Validation Loss: 1.8551
OOD Test ACCURACY: 0.3293
OOD Test Loss: 4.7809

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 53...
[0m[1;37mINFO[0m: [1mCheckpoint 53: 
-----------------------------------
Train ACCURACY: 0.5174
Train Loss: 1.0790
ID Validation ACCURACY: 0.4963
ID Validation Loss: 1.1107
ID Test ACCURACY: 0.5230
ID Test Loss: 1.0871
OOD Validation ACCURACY: 0.8353
OOD Validation Loss: 0.7218
OOD Test ACCURACY: 0.4657
OOD Test Loss: 1.6445

[0m[1;37mINFO[0m: [1mChartInfo 0.7297 0.3293 0.5230 0.4657 0.4963 0.8353[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.283
WIoU for r=0.6 = 0.182
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.089
WIoU for r=0.6 = 0.036
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.202
WIoU for r=0.6 = 0.103


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.701
Model XAI F1 of binarized graphs for r=0.6 =  0.28333624999999996
Model XAI WIoU of binarized graphs for r=0.6 =  0.18214249999999998
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.428
SUFF++ for r=0.6 class 0 = 0.336 +- 0.281 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 class 1 = 0.578 +- 0.281 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 class 2 = 0.368 +- 0.281 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 all KL = 0.4 +- 0.281 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 all L1 = 0.428 +- 0.204 (in-sample avg dev_std = 0.448)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.376
Model XAI F1 of binarized graphs for r=0.6 =  0.08861624999999998
Model XAI WIoU of binarized graphs for r=0.6 =  0.03595875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.331
SUFF++ for r=0.6 class 0 = 0.802 +- 0.128 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.6 class 1 = 0.853 +- 0.128 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.6 class 2 = 0.776 +- 0.128 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.6 all KL = 0.898 +- 0.128 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.6 all L1 = 0.81 +- 0.171 (in-sample avg dev_std = 0.129)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.327
Model XAI F1 of binarized graphs for r=0.6 =  0.20160625
Model XAI WIoU of binarized graphs for r=0.6 =  0.1025225
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.303
SUFF++ for r=0.6 class 0 = 0.92 +- 0.198 (in-sample avg dev_std = 0.258)
SUFF++ for r=0.6 class 1 = 0.827 +- 0.198 (in-sample avg dev_std = 0.258)
SUFF++ for r=0.6 class 2 = 0.916 +- 0.198 (in-sample avg dev_std = 0.258)
SUFF++ for r=0.6 all KL = 0.856 +- 0.198 (in-sample avg dev_std = 0.258)
SUFF++ for r=0.6 all L1 = 0.888 +- 0.142 (in-sample avg dev_std = 0.258)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.701
Model XAI F1 of binarized graphs for r=0.6 =  0.28333624999999996
Model XAI WIoU of binarized graphs for r=0.6 =  0.18214249999999998
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.517
NEC for r=0.6 class 0 = 0.462 +- 0.245 (in-sample avg dev_std = 0.430)
NEC for r=0.6 class 1 = 0.424 +- 0.245 (in-sample avg dev_std = 0.430)
NEC for r=0.6 class 2 = 0.47 +- 0.245 (in-sample avg dev_std = 0.430)
NEC for r=0.6 all KL = 0.409 +- 0.245 (in-sample avg dev_std = 0.430)
NEC for r=0.6 all L1 = 0.452 +- 0.172 (in-sample avg dev_std = 0.430)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.376
Model XAI F1 of binarized graphs for r=0.6 =  0.08861624999999998
Model XAI WIoU of binarized graphs for r=0.6 =  0.03595875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.391
NEC for r=0.6 class 0 = 0.183 +- 0.094 (in-sample avg dev_std = 0.111)
NEC for r=0.6 class 1 = 0.149 +- 0.094 (in-sample avg dev_std = 0.111)
NEC for r=0.6 class 2 = 0.164 +- 0.094 (in-sample avg dev_std = 0.111)
NEC for r=0.6 all KL = 0.07 +- 0.094 (in-sample avg dev_std = 0.111)
NEC for r=0.6 all L1 = 0.165 +- 0.142 (in-sample avg dev_std = 0.111)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.327
Model XAI F1 of binarized graphs for r=0.6 =  0.20160625
Model XAI WIoU of binarized graphs for r=0.6 =  0.1025225
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.339
NEC for r=0.6 class 0 = 0.172 +- 0.255 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 1 = 0.188 +- 0.255 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 2 = 0.164 +- 0.255 (in-sample avg dev_std = 0.318)
NEC for r=0.6 all KL = 0.21 +- 0.255 (in-sample avg dev_std = 0.318)
NEC for r=0.6 all L1 = 0.175 +- 0.209 (in-sample avg dev_std = 0.318)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:41:42 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/18/2024 08:41:42 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 08:41:56 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 08:41:58 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:00 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:02 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:42:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 80...
[0m[1;37mINFO[0m: [1mCheckpoint 80: 
-----------------------------------
Train ACCURACY: 0.7508
Train Loss: 0.6072
ID Validation ACCURACY: 0.7547
ID Validation Loss: 0.6103
ID Test ACCURACY: 0.7617
ID Test Loss: 0.6033
OOD Validation ACCURACY: 0.9290
OOD Validation Loss: 0.3927
OOD Test ACCURACY: 0.7843
OOD Test Loss: 0.5272

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 24...
[0m[1;37mINFO[0m: [1mCheckpoint 24: 
-----------------------------------
Train ACCURACY: 0.7236
Train Loss: 0.6539
ID Validation ACCURACY: 0.7353
ID Validation Loss: 0.6573
ID Test ACCURACY: 0.7297
ID Test Loss: 0.6396
OOD Validation ACCURACY: 0.9310
OOD Validation Loss: 0.4148
OOD Test ACCURACY: 0.8723
OOD Test Loss: 0.4403

[0m[1;37mINFO[0m: [1mChartInfo 0.7617 0.7843 0.7297 0.8723 0.7353 0.9310[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.405
WIoU for r=0.6 = 0.374
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 0.998
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.711
WIoU for r=0.6 = 0.652


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.741
Model XAI F1 of binarized graphs for r=0.6 =  0.40491
Model XAI WIoU of binarized graphs for r=0.6 =  0.37449
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.545
SUFF++ for r=0.6 class 0 = 0.604 +- 0.225 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.6 class 1 = 0.702 +- 0.225 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.6 class 2 = 0.596 +- 0.225 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.6 all KL = 0.736 +- 0.225 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.6 all L1 = 0.634 +- 0.161 (in-sample avg dev_std = 0.395)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.93
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9978212500000001
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.927
SUFF++ for r=0.6 class 0 = 0.859 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 class 1 = 0.869 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 class 2 = 0.86 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 all KL = 0.951 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 all L1 = 0.863 +- 0.077 (in-sample avg dev_std = 0.166)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.794
Model XAI F1 of binarized graphs for r=0.6 =  0.7108775
Model XAI WIoU of binarized graphs for r=0.6 =  0.6517449999999999
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.805
SUFF++ for r=0.6 class 0 = 0.772 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 class 1 = 0.918 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 class 2 = 0.82 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 all KL = 0.914 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 all L1 = 0.835 +- 0.118 (in-sample avg dev_std = 0.234)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.741
Model XAI F1 of binarized graphs for r=0.6 =  0.40491
Model XAI WIoU of binarized graphs for r=0.6 =  0.37449
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.435
NEC for r=0.6 class 0 = 0.543 +- 0.343 (in-sample avg dev_std = 0.287)
NEC for r=0.6 class 1 = 0.278 +- 0.343 (in-sample avg dev_std = 0.287)
NEC for r=0.6 class 2 = 0.578 +- 0.343 (in-sample avg dev_std = 0.287)
NEC for r=0.6 all KL = 0.377 +- 0.343 (in-sample avg dev_std = 0.287)
NEC for r=0.6 all L1 = 0.467 +- 0.257 (in-sample avg dev_std = 0.287)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.93
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9978212500000001
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.616
NEC for r=0.6 class 0 = 0.534 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 class 1 = 0.063 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 class 2 = 0.514 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 all KL = 0.376 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 all L1 = 0.372 +- 0.277 (in-sample avg dev_std = 0.591)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.794
Model XAI F1 of binarized graphs for r=0.6 =  0.7108775
Model XAI WIoU of binarized graphs for r=0.6 =  0.6517449999999999
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.515
NEC for r=0.6 class 0 = 0.565 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 class 1 = 0.097 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 class 2 = 0.563 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 all KL = 0.37 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 all L1 = 0.414 +- 0.275 (in-sample avg dev_std = 0.460)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.767], 'all_L1': [0.652]}), defaultdict(<class 'list'>, {'all_KL': [0.4], 'all_L1': [0.428]}), defaultdict(<class 'list'>, {'all_KL': [0.736], 'all_L1': [0.634]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.47]}), defaultdict(<class 'list'>, {'all_KL': [0.409], 'all_L1': [0.452]}), defaultdict(<class 'list'>, {'all_KL': [0.377], 'all_L1': [0.467]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.971], 'all_L1': [0.903]}), defaultdict(<class 'list'>, {'all_KL': [0.898], 'all_L1': [0.81]}), defaultdict(<class 'list'>, {'all_KL': [0.951], 'all_L1': [0.863]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.446], 'all_L1': [0.425]}), defaultdict(<class 'list'>, {'all_KL': [0.07], 'all_L1': [0.165]}), defaultdict(<class 'list'>, {'all_KL': [0.376], 'all_L1': [0.372]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.937], 'all_L1': [0.864]}), defaultdict(<class 'list'>, {'all_KL': [0.856], 'all_L1': [0.888]}), defaultdict(<class 'list'>, {'all_KL': [0.914], 'all_L1': [0.835]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.461], 'all_L1': [0.457]}), defaultdict(<class 'list'>, {'all_KL': [0.21], 'all_L1': [0.175]}), defaultdict(<class 'list'>, {'all_KL': [0.37], 'all_L1': [0.414]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.571 +- 0.102
suff++ class all_KL  =  0.634 +- 0.166
suff++_acc_int  =  0.516 +- 0.063
nec class all_L1  =  0.463 +- 0.008
nec class all_KL  =  0.388 +- 0.015
nec_acc_int  =  0.460 +- 0.041

Eval split val
suff++ class all_L1  =  0.859 +- 0.038
suff++ class all_KL  =  0.940 +- 0.031
suff++_acc_int  =  0.729 +- 0.282
nec class all_L1  =  0.321 +- 0.112
nec class all_KL  =  0.297 +- 0.163
nec_acc_int  =  0.527 +- 0.098

Eval split test
suff++ class all_L1  =  0.862 +- 0.022
suff++ class all_KL  =  0.902 +- 0.034
suff++_acc_int  =  0.658 +- 0.252
nec class all_L1  =  0.349 +- 0.124
nec class all_KL  =  0.347 +- 0.104
nec_acc_int  =  0.456 +- 0.083


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.517 +- 0.055
Faith. Armon (L1)= 		  =  0.508 +- 0.048
Faith. GMean (L1)= 	  =  0.513 +- 0.052
Faith. Aritm (KL)= 		  =  0.511 +- 0.076
Faith. Armon (KL)= 		  =  0.470 +- 0.047
Faith. GMean (KL)= 	  =  0.490 +- 0.061

Eval split val
Faith. Aritm (L1)= 		  =  0.590 +- 0.075
Faith. Armon (L1)= 		  =  0.457 +- 0.132
Faith. GMean (L1)= 	  =  0.517 +- 0.109
Faith. Aritm (KL)= 		  =  0.619 +- 0.097
Faith. Armon (KL)= 		  =  0.427 +- 0.212
Faith. GMean (KL)= 	  =  0.502 +- 0.180

Eval split test
Faith. Aritm (L1)= 		  =  0.605 +- 0.054
Faith. Armon (L1)= 		  =  0.481 +- 0.135
Faith. GMean (L1)= 	  =  0.537 +- 0.102
Faith. Aritm (KL)= 		  =  0.625 +- 0.069
Faith. Armon (KL)= 		  =  0.494 +- 0.117
Faith. GMean (KL)= 	  =  0.554 +- 0.097
Computed for split load_split = id



Completed in  0:04:21.356589  for CIGAGIN GOODMotif/basis



DONE CIGA GOODMotif/basis

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:43:48 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/18/2024 08:43:48 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:01 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:03 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:05 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:09 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:44:15 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.6862
Train Loss: 0.7386
ID Validation ACCURACY: 0.6850
ID Validation Loss: 0.7486
ID Test ACCURACY: 0.7003
ID Test Loss: 0.7175
OOD Validation ACCURACY: 0.4753
OOD Validation Loss: 1.1049
OOD Test ACCURACY: 0.3357
OOD Test Loss: 1.3917

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 65...
[0m[1;37mINFO[0m: [1mCheckpoint 65: 
-----------------------------------
Train ACCURACY: 0.6780
Train Loss: 0.8487
ID Validation ACCURACY: 0.6703
ID Validation Loss: 0.8743
ID Test ACCURACY: 0.6780
ID Test Loss: 0.8433
OOD Validation ACCURACY: 0.4927
OOD Validation Loss: 1.1806
OOD Test ACCURACY: 0.3360
OOD Test Loss: 1.4773

[0m[1;37mINFO[0m: [1mChartInfo 0.7003 0.3357 0.6780 0.3360 0.6703 0.4927[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.436
WIoU for r=0.8 = 0.375
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.238
WIoU for r=0.8 = 0.370
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.118
WIoU for r=0.8 = 0.408


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  0.43615250000000005
Model XAI WIoU of binarized graphs for r=0.8 =  0.37512500000000004
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.561
SUFF++ for r=0.8 class 0 = 0.596 +- 0.232 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 class 1 = 0.687 +- 0.232 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 class 2 = 0.616 +- 0.232 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 all KL = 0.748 +- 0.232 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 all L1 = 0.632 +- 0.172 (in-sample avg dev_std = 0.314)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.514
Model XAI F1 of binarized graphs for r=0.8 =  0.23802625
Model XAI WIoU of binarized graphs for r=0.8 =  0.36979
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.483
SUFF++ for r=0.8 class 0 = 0.62 +- 0.194 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 class 1 = 0.706 +- 0.194 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 class 2 = 0.653 +- 0.194 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 all KL = 0.81 +- 0.194 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 all L1 = 0.66 +- 0.175 (in-sample avg dev_std = 0.242)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.338
Model XAI F1 of binarized graphs for r=0.8 =  0.11797125
Model XAI WIoU of binarized graphs for r=0.8 =  0.40825125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.343
SUFF++ for r=0.8 class 0 = 0.701 +- 0.116 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 class 1 = 0.74 +- 0.116 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 class 2 = 0.715 +- 0.116 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 all KL = 0.869 +- 0.116 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 all L1 = 0.719 +- 0.129 (in-sample avg dev_std = 0.243)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  0.43615250000000005
Model XAI WIoU of binarized graphs for r=0.8 =  0.37512500000000004
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.472
NEC for r=0.8 class 0 = 0.519 +- 0.286 (in-sample avg dev_std = 0.420)
NEC for r=0.8 class 1 = 0.373 +- 0.286 (in-sample avg dev_std = 0.420)
NEC for r=0.8 class 2 = 0.563 +- 0.286 (in-sample avg dev_std = 0.420)
NEC for r=0.8 all KL = 0.415 +- 0.286 (in-sample avg dev_std = 0.420)
NEC for r=0.8 all L1 = 0.486 +- 0.187 (in-sample avg dev_std = 0.420)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.514
Model XAI F1 of binarized graphs for r=0.8 =  0.23802625
Model XAI WIoU of binarized graphs for r=0.8 =  0.36979
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.412
NEC for r=0.8 class 0 = 0.384 +- 0.153 (in-sample avg dev_std = 0.269)
NEC for r=0.8 class 1 = 0.303 +- 0.153 (in-sample avg dev_std = 0.269)
NEC for r=0.8 class 2 = 0.389 +- 0.153 (in-sample avg dev_std = 0.269)
NEC for r=0.8 all KL = 0.19 +- 0.153 (in-sample avg dev_std = 0.269)
NEC for r=0.8 all L1 = 0.358 +- 0.139 (in-sample avg dev_std = 0.269)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.338
Model XAI F1 of binarized graphs for r=0.8 =  0.11797125
Model XAI WIoU of binarized graphs for r=0.8 =  0.40825125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.341
NEC for r=0.8 class 0 = 0.239 +- 0.090 (in-sample avg dev_std = 0.159)
NEC for r=0.8 class 1 = 0.229 +- 0.090 (in-sample avg dev_std = 0.159)
NEC for r=0.8 class 2 = 0.246 +- 0.090 (in-sample avg dev_std = 0.159)
NEC for r=0.8 all KL = 0.088 +- 0.090 (in-sample avg dev_std = 0.159)
NEC for r=0.8 all L1 = 0.238 +- 0.126 (in-sample avg dev_std = 0.159)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:46:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:14 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:26 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:28 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:30 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:33 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:46:40 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 82...
[0m[1;37mINFO[0m: [1mCheckpoint 82: 
-----------------------------------
Train ACCURACY: 0.7256
Train Loss: 0.7151
ID Validation ACCURACY: 0.7257
ID Validation Loss: 0.7191
ID Test ACCURACY: 0.7223
ID Test Loss: 0.7248
OOD Validation ACCURACY: 0.5067
OOD Validation Loss: 1.0661
OOD Test ACCURACY: 0.3497
OOD Test Loss: 8.1815

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 37...
[0m[1;37mINFO[0m: [1mCheckpoint 37: 
-----------------------------------
Train ACCURACY: 0.7199
Train Loss: 0.6887
ID Validation ACCURACY: 0.7147
ID Validation Loss: 0.6861
ID Test ACCURACY: 0.7243
ID Test Loss: 0.6787
OOD Validation ACCURACY: 0.5900
OOD Validation Loss: 0.9209
OOD Test ACCURACY: 0.3580
OOD Test Loss: 1.1206

[0m[1;37mINFO[0m: [1mChartInfo 0.7223 0.3497 0.7243 0.3580 0.7147 0.5900[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.422
WIoU for r=0.8 = 0.313
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.180
WIoU for r=0.8 = 0.194
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.065
WIoU for r=0.8 = 0.169


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.716
Model XAI F1 of binarized graphs for r=0.8 =  0.42215125
Model XAI WIoU of binarized graphs for r=0.8 =  0.3127175
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.505
SUFF++ for r=0.8 class 0 = 0.598 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 class 1 = 0.628 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 class 2 = 0.592 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 all KL = 0.737 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 all L1 = 0.605 +- 0.152 (in-sample avg dev_std = 0.326)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.553
Model XAI F1 of binarized graphs for r=0.8 =  0.17954874999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.19384999999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.417
SUFF++ for r=0.8 class 0 = 0.627 +- 0.146 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 class 1 = 0.695 +- 0.146 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 class 2 = 0.61 +- 0.146 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 all KL = 0.811 +- 0.146 (in-sample avg dev_std = 0.243)
SUFF++ for r=0.8 all L1 = 0.644 +- 0.133 (in-sample avg dev_std = 0.243)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.361
Model XAI F1 of binarized graphs for r=0.8 =  0.06461875
Model XAI WIoU of binarized graphs for r=0.8 =  0.16936500000000002
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.34
SUFF++ for r=0.8 class 0 = 0.693 +- 0.256 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.8 class 1 = 0.724 +- 0.256 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.8 class 2 = 0.692 +- 0.256 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.8 all KL = 0.808 +- 0.256 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.8 all L1 = 0.703 +- 0.246 (in-sample avg dev_std = 0.147)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.716
Model XAI F1 of binarized graphs for r=0.8 =  0.42215125
Model XAI WIoU of binarized graphs for r=0.8 =  0.3127175
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.437
NEC for r=0.8 class 0 = 0.487 +- 0.251 (in-sample avg dev_std = 0.407)
NEC for r=0.8 class 1 = 0.415 +- 0.251 (in-sample avg dev_std = 0.407)
NEC for r=0.8 class 2 = 0.528 +- 0.251 (in-sample avg dev_std = 0.407)
NEC for r=0.8 all KL = 0.389 +- 0.251 (in-sample avg dev_std = 0.407)
NEC for r=0.8 all L1 = 0.477 +- 0.158 (in-sample avg dev_std = 0.407)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.553
Model XAI F1 of binarized graphs for r=0.8 =  0.17954874999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.19384999999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.419
NEC for r=0.8 class 0 = 0.352 +- 0.132 (in-sample avg dev_std = 0.242)
NEC for r=0.8 class 1 = 0.287 +- 0.132 (in-sample avg dev_std = 0.242)
NEC for r=0.8 class 2 = 0.382 +- 0.132 (in-sample avg dev_std = 0.242)
NEC for r=0.8 all KL = 0.171 +- 0.132 (in-sample avg dev_std = 0.242)
NEC for r=0.8 all L1 = 0.34 +- 0.134 (in-sample avg dev_std = 0.242)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.361
Model XAI F1 of binarized graphs for r=0.8 =  0.06461875
Model XAI WIoU of binarized graphs for r=0.8 =  0.16936500000000002
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.357
NEC for r=0.8 class 0 = 0.288 +- 0.182 (in-sample avg dev_std = 0.196)
NEC for r=0.8 class 1 = 0.264 +- 0.182 (in-sample avg dev_std = 0.196)
NEC for r=0.8 class 2 = 0.271 +- 0.182 (in-sample avg dev_std = 0.196)
NEC for r=0.8 all KL = 0.141 +- 0.182 (in-sample avg dev_std = 0.196)
NEC for r=0.8 all L1 = 0.275 +- 0.212 (in-sample avg dev_std = 0.196)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:48:36 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/18/2024 08:48:36 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 08:48:48 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 08:48:50 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 08:48:52 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 08:48:55 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:49:02 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.6818
Train Loss: 0.8315
ID Validation ACCURACY: 0.6730
ID Validation Loss: 0.8527
ID Test ACCURACY: 0.6913
ID Test Loss: 0.8316
OOD Validation ACCURACY: 0.3920
OOD Validation Loss: 1.4612
OOD Test ACCURACY: 0.3440
OOD Test Loss: 1.8131

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 58...
[0m[1;37mINFO[0m: [1mCheckpoint 58: 
-----------------------------------
Train ACCURACY: 0.6716
Train Loss: 0.8047
ID Validation ACCURACY: 0.6600
ID Validation Loss: 0.8160
ID Test ACCURACY: 0.6783
ID Test Loss: 0.7950
OOD Validation ACCURACY: 0.4573
OOD Validation Loss: 1.2737
OOD Test ACCURACY: 0.3547
OOD Test Loss: 1.4224

[0m[1;37mINFO[0m: [1mChartInfo 0.6913 0.3440 0.6783 0.3547 0.6600 0.4573[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.385
WIoU for r=0.8 = 0.273
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.180
WIoU for r=0.8 = 0.148
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.093
WIoU for r=0.8 = 0.112


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.675
Model XAI F1 of binarized graphs for r=0.8 =  0.384975
Model XAI WIoU of binarized graphs for r=0.8 =  0.27252875
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.512
SUFF++ for r=0.8 class 0 = 0.645 +- 0.218 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 class 1 = 0.598 +- 0.218 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 class 2 = 0.585 +- 0.218 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 all KL = 0.731 +- 0.218 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 all L1 = 0.61 +- 0.154 (in-sample avg dev_std = 0.330)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.431
Model XAI F1 of binarized graphs for r=0.8 =  0.17992
Model XAI WIoU of binarized graphs for r=0.8 =  0.14758749999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.407
SUFF++ for r=0.8 class 0 = 0.699 +- 0.126 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 class 1 = 0.771 +- 0.126 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 class 2 = 0.709 +- 0.126 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 all KL = 0.872 +- 0.126 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 all L1 = 0.726 +- 0.132 (in-sample avg dev_std = 0.249)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.345
Model XAI F1 of binarized graphs for r=0.8 =  0.09335750000000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.11152625000000001
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.362
SUFF++ for r=0.8 class 0 = 0.754 +- 0.194 (in-sample avg dev_std = 0.204)
SUFF++ for r=0.8 class 1 = 0.764 +- 0.194 (in-sample avg dev_std = 0.204)
SUFF++ for r=0.8 class 2 = 0.765 +- 0.194 (in-sample avg dev_std = 0.204)
SUFF++ for r=0.8 all KL = 0.867 +- 0.194 (in-sample avg dev_std = 0.204)
SUFF++ for r=0.8 all L1 = 0.761 +- 0.191 (in-sample avg dev_std = 0.204)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.675
Model XAI F1 of binarized graphs for r=0.8 =  0.384975
Model XAI WIoU of binarized graphs for r=0.8 =  0.27252875
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.47
NEC for r=0.8 class 0 = 0.511 +- 0.246 (in-sample avg dev_std = 0.370)
NEC for r=0.8 class 1 = 0.423 +- 0.246 (in-sample avg dev_std = 0.370)
NEC for r=0.8 class 2 = 0.48 +- 0.246 (in-sample avg dev_std = 0.370)
NEC for r=0.8 all KL = 0.379 +- 0.246 (in-sample avg dev_std = 0.370)
NEC for r=0.8 all L1 = 0.472 +- 0.162 (in-sample avg dev_std = 0.370)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.43
Model XAI F1 of binarized graphs for r=0.8 =  0.17992
Model XAI WIoU of binarized graphs for r=0.8 =  0.14758749999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.426
NEC for r=0.8 class 0 = 0.369 +- 0.244 (in-sample avg dev_std = 0.320)
NEC for r=0.8 class 1 = 0.367 +- 0.244 (in-sample avg dev_std = 0.320)
NEC for r=0.8 class 2 = 0.361 +- 0.244 (in-sample avg dev_std = 0.320)
NEC for r=0.8 all KL = 0.252 +- 0.244 (in-sample avg dev_std = 0.320)
NEC for r=0.8 all L1 = 0.366 +- 0.167 (in-sample avg dev_std = 0.320)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.347
Model XAI F1 of binarized graphs for r=0.8 =  0.09335750000000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.11152625000000001
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.352
NEC for r=0.8 class 0 = 0.279 +- 0.148 (in-sample avg dev_std = 0.179)
NEC for r=0.8 class 1 = 0.249 +- 0.148 (in-sample avg dev_std = 0.179)
NEC for r=0.8 class 2 = 0.264 +- 0.148 (in-sample avg dev_std = 0.179)
NEC for r=0.8 all KL = 0.134 +- 0.148 (in-sample avg dev_std = 0.179)
NEC for r=0.8 all L1 = 0.264 +- 0.145 (in-sample avg dev_std = 0.179)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.748], 'all_L1': [0.632]}), defaultdict(<class 'list'>, {'all_KL': [0.737], 'all_L1': [0.605]}), defaultdict(<class 'list'>, {'all_KL': [0.731], 'all_L1': [0.61]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.415], 'all_L1': [0.486]}), defaultdict(<class 'list'>, {'all_KL': [0.389], 'all_L1': [0.477]}), defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.472]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.81], 'all_L1': [0.66]}), defaultdict(<class 'list'>, {'all_KL': [0.811], 'all_L1': [0.644]}), defaultdict(<class 'list'>, {'all_KL': [0.872], 'all_L1': [0.726]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.19], 'all_L1': [0.358]}), defaultdict(<class 'list'>, {'all_KL': [0.171], 'all_L1': [0.34]}), defaultdict(<class 'list'>, {'all_KL': [0.252], 'all_L1': [0.366]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.869], 'all_L1': [0.719]}), defaultdict(<class 'list'>, {'all_KL': [0.808], 'all_L1': [0.703]}), defaultdict(<class 'list'>, {'all_KL': [0.867], 'all_L1': [0.761]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.088], 'all_L1': [0.238]}), defaultdict(<class 'list'>, {'all_KL': [0.141], 'all_L1': [0.275]}), defaultdict(<class 'list'>, {'all_KL': [0.134], 'all_L1': [0.264]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.616 +- 0.012
suff++ class all_KL  =  0.739 +- 0.007
suff++_acc_int  =  0.526 +- 0.025
nec class all_L1  =  0.478 +- 0.006
nec class all_KL  =  0.394 +- 0.015
nec_acc_int  =  0.459 +- 0.016

Eval split val
suff++ class all_L1  =  0.677 +- 0.035
suff++ class all_KL  =  0.831 +- 0.029
suff++_acc_int  =  0.436 +- 0.034
nec class all_L1  =  0.355 +- 0.011
nec class all_KL  =  0.204 +- 0.035
nec_acc_int  =  0.419 +- 0.005

Eval split test
suff++ class all_L1  =  0.728 +- 0.024
suff++ class all_KL  =  0.848 +- 0.028
suff++_acc_int  =  0.348 +- 0.009
nec class all_L1  =  0.259 +- 0.016
nec class all_KL  =  0.121 +- 0.024
nec_acc_int  =  0.350 +- 0.007


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.547 +- 0.008
Faith. Armon (L1)= 		  =  0.538 +- 0.008
Faith. GMean (L1)= 	  =  0.543 +- 0.008
Faith. Aritm (KL)= 		  =  0.567 +- 0.011
Faith. Armon (KL)= 		  =  0.514 +- 0.015
Faith. GMean (KL)= 	  =  0.540 +- 0.013

Eval split val
Faith. Aritm (L1)= 		  =  0.516 +- 0.023
Faith. Armon (L1)= 		  =  0.465 +- 0.017
Faith. GMean (L1)= 	  =  0.490 +- 0.020
Faith. Aritm (KL)= 		  =  0.518 +- 0.032
Faith. Armon (KL)= 		  =  0.327 +- 0.046
Faith. GMean (KL)= 	  =  0.411 +- 0.042

Eval split test
Faith. Aritm (L1)= 		  =  0.493 +- 0.014
Faith. Armon (L1)= 		  =  0.382 +- 0.017
Faith. GMean (L1)= 	  =  0.434 +- 0.015
Faith. Aritm (KL)= 		  =  0.485 +- 0.011
Faith. Armon (KL)= 		  =  0.211 +- 0.036
Faith. GMean (KL)= 	  =  0.318 +- 0.030
Computed for split load_split = id



Completed in  0:07:17.295243  for CIGAGIN GOODMotif/size



DONE CIGA GOODMotif/size

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:51:38 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:51:38 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ACCURACY: 0.6192
Train Loss: 0.8793
ID Validation ACCURACY: 0.6427
ID Validation Loss: 0.8548
ID Test ACCURACY: 0.6190
ID Test Loss: 0.8785
OOD Validation ACCURACY: 0.4187
OOD Validation Loss: 1.1603
OOD Test ACCURACY: 0.4730
OOD Test Loss: 1.9920

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 55...
[0m[1;37mINFO[0m: [1mCheckpoint 55: 
-----------------------------------
Train ACCURACY: 0.4834
Train Loss: 1.3233
ID Validation ACCURACY: 0.4867
ID Validation Loss: 1.3383
ID Test ACCURACY: 0.4863
ID Test Loss: 1.2866
OOD Validation ACCURACY: 0.6313
OOD Validation Loss: 0.8829
OOD Test ACCURACY: 0.4813
OOD Test Loss: 1.8714

[0m[1;37mINFO[0m: [1mChartInfo 0.6190 0.4730 0.4863 0.4813 0.4867 0.6313[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.305
WIoU for r=0.8 = 0.158
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.090
WIoU for r=0.8 = 0.051
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.407
WIoU for r=0.8 = 0.410


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.655
Model XAI F1 of binarized graphs for r=0.8 =  0.305255
Model XAI WIoU of binarized graphs for r=0.8 =  0.15810625
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.361
SUFF++ for r=0.8 class 0 = 0.681 +- 0.264 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 class 1 = 0.437 +- 0.264 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 class 2 = 0.606 +- 0.264 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 all KL = 0.709 +- 0.264 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 all L1 = 0.574 +- 0.189 (in-sample avg dev_std = 0.295)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.412
Model XAI F1 of binarized graphs for r=0.8 =  0.08994999999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.050555
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.34
SUFF++ for r=0.8 class 0 = 0.793 +- 0.144 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 class 1 = 0.713 +- 0.144 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 class 2 = 0.712 +- 0.144 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 all KL = 0.891 +- 0.144 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 all L1 = 0.739 +- 0.147 (in-sample avg dev_std = 0.156)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.494
Model XAI F1 of binarized graphs for r=0.8 =  0.4068825
Model XAI WIoU of binarized graphs for r=0.8 =  0.4104425
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.38
SUFF++ for r=0.8 class 0 = 0.685 +- 0.240 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 class 1 = 0.693 +- 0.240 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 class 2 = 0.611 +- 0.240 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 all KL = 0.73 +- 0.240 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 all L1 = 0.663 +- 0.186 (in-sample avg dev_std = 0.312)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.655
Model XAI F1 of binarized graphs for r=0.8 =  0.305255
Model XAI WIoU of binarized graphs for r=0.8 =  0.15810625
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.408
NEC for r=0.8 class 0 = 0.404 +- 0.215 (in-sample avg dev_std = 0.347)
NEC for r=0.8 class 1 = 0.432 +- 0.215 (in-sample avg dev_std = 0.347)
NEC for r=0.8 class 2 = 0.399 +- 0.215 (in-sample avg dev_std = 0.347)
NEC for r=0.8 all KL = 0.27 +- 0.215 (in-sample avg dev_std = 0.347)
NEC for r=0.8 all L1 = 0.412 +- 0.160 (in-sample avg dev_std = 0.347)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.412
Model XAI F1 of binarized graphs for r=0.8 =  0.08994999999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.050555
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.41
NEC for r=0.8 class 0 = 0.234 +- 0.099 (in-sample avg dev_std = 0.176)
NEC for r=0.8 class 1 = 0.249 +- 0.099 (in-sample avg dev_std = 0.176)
NEC for r=0.8 class 2 = 0.294 +- 0.099 (in-sample avg dev_std = 0.176)
NEC for r=0.8 all KL = 0.098 +- 0.099 (in-sample avg dev_std = 0.176)
NEC for r=0.8 all L1 = 0.259 +- 0.109 (in-sample avg dev_std = 0.176)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.494
Model XAI F1 of binarized graphs for r=0.8 =  0.4068825
Model XAI WIoU of binarized graphs for r=0.8 =  0.4104425
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.406
NEC for r=0.8 class 0 = 0.415 +- 0.234 (in-sample avg dev_std = 0.351)
NEC for r=0.8 class 1 = 0.405 +- 0.234 (in-sample avg dev_std = 0.351)
NEC for r=0.8 class 2 = 0.436 +- 0.234 (in-sample avg dev_std = 0.351)
NEC for r=0.8 all KL = 0.396 +- 0.234 (in-sample avg dev_std = 0.351)
NEC for r=0.8 all L1 = 0.419 +- 0.183 (in-sample avg dev_std = 0.351)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:52:54 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:52:54 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 133...
[0m[1;37mINFO[0m: [1mCheckpoint 133: 
-----------------------------------
Train ACCURACY: 0.9237
Train Loss: 0.3588
ID Validation ACCURACY: 0.9287
ID Validation Loss: 0.3522
ID Test ACCURACY: 0.9200
ID Test Loss: 0.3948
OOD Validation ACCURACY: 0.9140
OOD Validation Loss: 0.4234
OOD Test ACCURACY: 0.6333
OOD Test Loss: 4.8335

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.9002
Train Loss: 0.4180
ID Validation ACCURACY: 0.9057
ID Validation Loss: 0.4083
ID Test ACCURACY: 0.8980
ID Test Loss: 0.4580
OOD Validation ACCURACY: 0.9200
OOD Validation Loss: 0.3940
OOD Test ACCURACY: 0.5167
OOD Test Loss: 2.2389

[0m[1;37mINFO[0m: [1mChartInfo 0.9200 0.6333 0.8980 0.5167 0.9057 0.9200[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.455
WIoU for r=0.8 = 0.303
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.314
WIoU for r=0.8 = 0.136
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.381
WIoU for r=0.8 = 0.312


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.934
Model XAI F1 of binarized graphs for r=0.8 =  0.455265
Model XAI WIoU of binarized graphs for r=0.8 =  0.30283875
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.594
SUFF++ for r=0.8 class 0 = 0.422 +- 0.356 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 class 1 = 0.795 +- 0.356 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 class 2 = 0.487 +- 0.356 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 all KL = 0.558 +- 0.356 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 all L1 = 0.569 +- 0.278 (in-sample avg dev_std = 0.379)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.914
Model XAI F1 of binarized graphs for r=0.8 =  0.3140975
Model XAI WIoU of binarized graphs for r=0.8 =  0.1356175
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.705
SUFF++ for r=0.8 class 0 = 0.558 +- 0.232 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.8 class 1 = 0.709 +- 0.232 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.8 class 2 = 0.62 +- 0.232 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.8 all KL = 0.681 +- 0.232 (in-sample avg dev_std = 0.395)
SUFF++ for r=0.8 all L1 = 0.628 +- 0.172 (in-sample avg dev_std = 0.395)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.649
Model XAI F1 of binarized graphs for r=0.8 =  0.38135749999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.31179124999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.482
SUFF++ for r=0.8 class 0 = 0.677 +- 0.339 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 class 1 = 0.926 +- 0.339 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 class 2 = 0.68 +- 0.339 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 all KL = 0.719 +- 0.339 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 all L1 = 0.763 +- 0.298 (in-sample avg dev_std = 0.303)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.934
Model XAI F1 of binarized graphs for r=0.8 =  0.455265
Model XAI WIoU of binarized graphs for r=0.8 =  0.30283875
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.535
NEC for r=0.8 class 0 = 0.606 +- 0.289 (in-sample avg dev_std = 0.595)
NEC for r=0.8 class 1 = 0.328 +- 0.289 (in-sample avg dev_std = 0.595)
NEC for r=0.8 class 2 = 0.646 +- 0.289 (in-sample avg dev_std = 0.595)
NEC for r=0.8 all KL = 0.578 +- 0.289 (in-sample avg dev_std = 0.595)
NEC for r=0.8 all L1 = 0.526 +- 0.210 (in-sample avg dev_std = 0.595)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.914
Model XAI F1 of binarized graphs for r=0.8 =  0.3140975
Model XAI WIoU of binarized graphs for r=0.8 =  0.1356175
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.506
NEC for r=0.8 class 0 = 0.6 +- 0.209 (in-sample avg dev_std = 0.569)
NEC for r=0.8 class 1 = 0.448 +- 0.209 (in-sample avg dev_std = 0.569)
NEC for r=0.8 class 2 = 0.579 +- 0.209 (in-sample avg dev_std = 0.569)
NEC for r=0.8 all KL = 0.559 +- 0.209 (in-sample avg dev_std = 0.569)
NEC for r=0.8 all L1 = 0.543 +- 0.137 (in-sample avg dev_std = 0.569)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.649
Model XAI F1 of binarized graphs for r=0.8 =  0.38135749999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.31179124999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.488
NEC for r=0.8 class 0 = 0.393 +- 0.348 (in-sample avg dev_std = 0.509)
NEC for r=0.8 class 1 = 0.21 +- 0.348 (in-sample avg dev_std = 0.509)
NEC for r=0.8 class 2 = 0.471 +- 0.348 (in-sample avg dev_std = 0.509)
NEC for r=0.8 all KL = 0.522 +- 0.348 (in-sample avg dev_std = 0.509)
NEC for r=0.8 all L1 = 0.356 +- 0.269 (in-sample avg dev_std = 0.509)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:54:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:54:08 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 79...
[0m[1;37mINFO[0m: [1mCheckpoint 79: 
-----------------------------------
Train ACCURACY: 0.8680
Train Loss: 0.4829
ID Validation ACCURACY: 0.8710
ID Validation Loss: 0.4693
ID Test ACCURACY: 0.8680
ID Test Loss: 0.4894
OOD Validation ACCURACY: 0.8327
OOD Validation Loss: 0.5758
OOD Test ACCURACY: 0.4990
OOD Test Loss: 3.3324

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 68...
[0m[1;37mINFO[0m: [1mCheckpoint 68: 
-----------------------------------
Train ACCURACY: 0.8033
Train Loss: 0.5322
ID Validation ACCURACY: 0.8120
ID Validation Loss: 0.5122
ID Test ACCURACY: 0.8153
ID Test Loss: 0.5128
OOD Validation ACCURACY: 0.9143
OOD Validation Loss: 0.5427
OOD Test ACCURACY: 0.5627
OOD Test Loss: 1.2968

[0m[1;37mINFO[0m: [1mChartInfo 0.8680 0.4990 0.8153 0.5627 0.8120 0.9143[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.413
WIoU for r=0.8 = 0.392
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.262
WIoU for r=0.8 = 0.367
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.323
WIoU for r=0.8 = 0.237


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  0.41340375
Model XAI WIoU of binarized graphs for r=0.8 =  0.39216375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.466
SUFF++ for r=0.8 class 0 = 0.257 +- 0.326 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 1 = 0.736 +- 0.326 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 2 = 0.451 +- 0.326 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all KL = 0.485 +- 0.326 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all L1 = 0.482 +- 0.268 (in-sample avg dev_std = 0.372)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.839
Model XAI F1 of binarized graphs for r=0.8 =  0.26192375
Model XAI WIoU of binarized graphs for r=0.8 =  0.36654375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.471
SUFF++ for r=0.8 class 0 = 0.407 +- 0.202 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 class 1 = 0.622 +- 0.202 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 class 2 = 0.512 +- 0.202 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 all KL = 0.618 +- 0.202 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 all L1 = 0.513 +- 0.158 (in-sample avg dev_std = 0.369)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.496
Model XAI F1 of binarized graphs for r=0.8 =  0.32278
Model XAI WIoU of binarized graphs for r=0.8 =  0.23695625
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.364
SUFF++ for r=0.8 class 0 = 0.618 +- 0.259 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 class 1 = 0.841 +- 0.259 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 class 2 = 0.681 +- 0.259 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 all KL = 0.741 +- 0.259 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 all L1 = 0.715 +- 0.262 (in-sample avg dev_std = 0.262)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  0.41340375
Model XAI WIoU of binarized graphs for r=0.8 =  0.39216375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.532
NEC for r=0.8 class 0 = 0.609 +- 0.255 (in-sample avg dev_std = 0.555)
NEC for r=0.8 class 1 = 0.339 +- 0.255 (in-sample avg dev_std = 0.555)
NEC for r=0.8 class 2 = 0.628 +- 0.255 (in-sample avg dev_std = 0.555)
NEC for r=0.8 all KL = 0.542 +- 0.255 (in-sample avg dev_std = 0.555)
NEC for r=0.8 all L1 = 0.525 +- 0.197 (in-sample avg dev_std = 0.555)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.839
Model XAI F1 of binarized graphs for r=0.8 =  0.26192375
Model XAI WIoU of binarized graphs for r=0.8 =  0.36654375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.504
NEC for r=0.8 class 0 = 0.554 +- 0.208 (in-sample avg dev_std = 0.446)
NEC for r=0.8 class 1 = 0.343 +- 0.208 (in-sample avg dev_std = 0.446)
NEC for r=0.8 class 2 = 0.558 +- 0.208 (in-sample avg dev_std = 0.446)
NEC for r=0.8 all KL = 0.393 +- 0.208 (in-sample avg dev_std = 0.446)
NEC for r=0.8 all L1 = 0.487 +- 0.162 (in-sample avg dev_std = 0.446)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.496
Model XAI F1 of binarized graphs for r=0.8 =  0.32278
Model XAI WIoU of binarized graphs for r=0.8 =  0.23695625
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.424
NEC for r=0.8 class 0 = 0.355 +- 0.223 (in-sample avg dev_std = 0.398)
NEC for r=0.8 class 1 = 0.254 +- 0.223 (in-sample avg dev_std = 0.398)
NEC for r=0.8 class 2 = 0.419 +- 0.223 (in-sample avg dev_std = 0.398)
NEC for r=0.8 all KL = 0.367 +- 0.223 (in-sample avg dev_std = 0.398)
NEC for r=0.8 all L1 = 0.341 +- 0.232 (in-sample avg dev_std = 0.398)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.709], 'all_L1': [0.574]}), defaultdict(<class 'list'>, {'all_KL': [0.558], 'all_L1': [0.569]}), defaultdict(<class 'list'>, {'all_KL': [0.485], 'all_L1': [0.482]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.27], 'all_L1': [0.412]}), defaultdict(<class 'list'>, {'all_KL': [0.578], 'all_L1': [0.526]}), defaultdict(<class 'list'>, {'all_KL': [0.542], 'all_L1': [0.525]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.891], 'all_L1': [0.739]}), defaultdict(<class 'list'>, {'all_KL': [0.681], 'all_L1': [0.628]}), defaultdict(<class 'list'>, {'all_KL': [0.618], 'all_L1': [0.513]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.259]}), defaultdict(<class 'list'>, {'all_KL': [0.559], 'all_L1': [0.543]}), defaultdict(<class 'list'>, {'all_KL': [0.393], 'all_L1': [0.487]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.73], 'all_L1': [0.663]}), defaultdict(<class 'list'>, {'all_KL': [0.719], 'all_L1': [0.763]}), defaultdict(<class 'list'>, {'all_KL': [0.741], 'all_L1': [0.715]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.396], 'all_L1': [0.419]}), defaultdict(<class 'list'>, {'all_KL': [0.522], 'all_L1': [0.356]}), defaultdict(<class 'list'>, {'all_KL': [0.367], 'all_L1': [0.341]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.542 +- 0.042
suff++ class all_KL  =  0.584 +- 0.093
suff++_acc_int  =  0.474 +- 0.096
nec class all_L1  =  0.488 +- 0.054
nec class all_KL  =  0.463 +- 0.137
nec_acc_int  =  0.492 +- 0.059

Eval split val
suff++ class all_L1  =  0.627 +- 0.092
suff++ class all_KL  =  0.730 +- 0.117
suff++_acc_int  =  0.505 +- 0.151
nec class all_L1  =  0.430 +- 0.123
nec class all_KL  =  0.350 +- 0.191
nec_acc_int  =  0.473 +- 0.045

Eval split test
suff++ class all_L1  =  0.714 +- 0.041
suff++ class all_KL  =  0.730 +- 0.009
suff++_acc_int  =  0.409 +- 0.052
nec class all_L1  =  0.372 +- 0.034
nec class all_KL  =  0.428 +- 0.067
nec_acc_int  =  0.439 +- 0.035


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.515 +- 0.024
Faith. Armon (L1)= 		  =  0.510 +- 0.028
Faith. GMean (L1)= 	  =  0.512 +- 0.026
Faith. Aritm (KL)= 		  =  0.524 +- 0.033
Faith. Armon (KL)= 		  =  0.490 +- 0.074
Faith. GMean (KL)= 	  =  0.506 +- 0.053

Eval split val
Faith. Aritm (L1)= 		  =  0.528 +- 0.041
Faith. Armon (L1)= 		  =  0.489 +- 0.082
Faith. GMean (L1)= 	  =  0.507 +- 0.060
Faith. Aritm (KL)= 		  =  0.540 +- 0.057
Faith. Armon (KL)= 		  =  0.424 +- 0.183
Faith. GMean (KL)= 	  =  0.468 +- 0.132

Eval split test
Faith. Aritm (L1)= 		  =  0.543 +- 0.013
Faith. Armon (L1)= 		  =  0.487 +- 0.021
Faith. GMean (L1)= 	  =  0.514 +- 0.015
Faith. Aritm (KL)= 		  =  0.579 +- 0.029
Faith. Armon (KL)= 		  =  0.536 +- 0.049
Faith. GMean (KL)= 	  =  0.557 +- 0.040
Computed for split load_split = id



Completed in  0:03:52.079877  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:56:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 08:56:23 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 146...
[0m[1;37mINFO[0m: [1mCheckpoint 146: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0762
ID Validation ACCURACY: 0.8615
ID Validation Loss: 0.8340
ID Test ACCURACY: 0.8593
ID Test Loss: 0.7822
OOD Validation ACCURACY: 0.8238
OOD Validation Loss: 2.7555
OOD Test ACCURACY: 0.6532
OOD Test Loss: 15.7757

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 100...
[0m[1;37mINFO[0m: [1mCheckpoint 100: 
-----------------------------------
Train ACCURACY: 0.9482
Train Loss: 0.0806
ID Validation ACCURACY: 0.8523
ID Validation Loss: 0.9562
ID Test ACCURACY: 0.8555
ID Test Loss: 0.8378
OOD Validation ACCURACY: 0.8527
OOD Validation Loss: 3.1390
OOD Test ACCURACY: 0.8097
OOD Test Loss: 9.3985

[0m[1;37mINFO[0m: [1mChartInfo 0.8593 0.6532 0.8555 0.8097 0.8523 0.8527[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/18/2024 08:56:26 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.856
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.825
SUFF++ for r=0.8 class 0.0 = 0.805 +- 0.322 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 class 1.0 = 0.922 +- 0.322 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 all KL = 0.776 +- 0.322 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 all L1 = 0.874 +- 0.201 (in-sample avg dev_std = 0.390)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.826
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.784
SUFF++ for r=0.8 class 0.0 = 0.772 +- 0.348 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.8 class 1.0 = 0.933 +- 0.348 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.8 all KL = 0.747 +- 0.348 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.8 all L1 = 0.856 +- 0.222 (in-sample avg dev_std = 0.375)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.64
SUFF++ for r=0.8 class 0.0 = 0.885 +- 0.445 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.8 class 1.0 = 0.719 +- 0.445 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.8 all KL = 0.619 +- 0.445 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.8 all L1 = 0.799 +- 0.259 (in-sample avg dev_std = 0.542)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.856
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.861
NEC for r=0.8 class 0.0 = 0.115 +- 0.235 (in-sample avg dev_std = 0.188)
NEC for r=0.8 class 1.0 = 0.042 +- 0.235 (in-sample avg dev_std = 0.188)
NEC for r=0.8 all KL = 0.101 +- 0.235 (in-sample avg dev_std = 0.188)
NEC for r=0.8 all L1 = 0.073 +- 0.177 (in-sample avg dev_std = 0.188)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.825
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.814
NEC for r=0.8 class 0.0 = 0.137 +- 0.249 (in-sample avg dev_std = 0.248)
NEC for r=0.8 class 1.0 = 0.044 +- 0.249 (in-sample avg dev_std = 0.248)
NEC for r=0.8 all KL = 0.118 +- 0.249 (in-sample avg dev_std = 0.248)
NEC for r=0.8 all L1 = 0.088 +- 0.186 (in-sample avg dev_std = 0.248)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.662
NEC for r=0.8 class 0.0 = 0.103 +- 0.417 (in-sample avg dev_std = 0.432)
NEC for r=0.8 class 1.0 = 0.209 +- 0.417 (in-sample avg dev_std = 0.432)
NEC for r=0.8 all KL = 0.307 +- 0.417 (in-sample avg dev_std = 0.432)
NEC for r=0.8 all L1 = 0.158 +- 0.246 (in-sample avg dev_std = 0.432)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 20:58:23 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:53 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:53 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:53 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:53 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 08:58:54 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 183...
[0m[1;37mINFO[0m: [1mCheckpoint 183: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8629
ID Validation Loss: 1.2999
ID Test ACCURACY: 0.8585
ID Test Loss: 1.1992
OOD Validation ACCURACY: 0.6298
OOD Validation Loss: 15.9629
OOD Test ACCURACY: 0.4839
OOD Test Loss: 201.9750

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 101...
[0m[1;37mINFO[0m: [1mCheckpoint 101: 
-----------------------------------
Train ACCURACY: 0.9483
Train Loss: 0.0774
ID Validation ACCURACY: 0.8534
ID Validation Loss: 0.7641
ID Test ACCURACY: 0.8512
ID Test Loss: 0.7085
OOD Validation ACCURACY: 0.8540
OOD Validation Loss: 1.8423
OOD Test ACCURACY: 0.7524
OOD Test Loss: 4.8235

[0m[1;37mINFO[0m: [1mChartInfo 0.8585 0.4839 0.8512 0.7524 0.8534 0.8540[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/18/2024 08:58:55 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.826
SUFF++ for r=0.8 class 0.0 = 0.795 +- 0.396 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.8 class 1.0 = 0.904 +- 0.396 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.8 all KL = 0.702 +- 0.396 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.8 all L1 = 0.859 +- 0.212 (in-sample avg dev_std = 0.440)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.604
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.584
SUFF++ for r=0.8 class 0.0 = 0.81 +- 0.448 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.8 class 1.0 = 0.704 +- 0.448 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.8 all KL = 0.57 +- 0.448 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.8 all L1 = 0.755 +- 0.287 (in-sample avg dev_std = 0.554)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.484
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.484
SUFF++ for r=0.8 class 0.0 = 1.0 +- 0.017 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 class 1.0 = 1.0 +- 0.017 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 all KL = 0.999 +- 0.017 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 all L1 = 1.0 +- 0.001 (in-sample avg dev_std = 0.001)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.842
NEC for r=0.8 class 0.0 = 0.109 +- 0.271 (in-sample avg dev_std = 0.216)
NEC for r=0.8 class 1.0 = 0.051 +- 0.271 (in-sample avg dev_std = 0.216)
NEC for r=0.8 all KL = 0.12 +- 0.271 (in-sample avg dev_std = 0.216)
NEC for r=0.8 all L1 = 0.075 +- 0.186 (in-sample avg dev_std = 0.216)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.598
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.649
NEC for r=0.8 class 0.0 = 0.151 +- 0.419 (in-sample avg dev_std = 0.424)
NEC for r=0.8 class 1.0 = 0.251 +- 0.419 (in-sample avg dev_std = 0.424)
NEC for r=0.8 all KL = 0.317 +- 0.419 (in-sample avg dev_std = 0.424)
NEC for r=0.8 all L1 = 0.203 +- 0.292 (in-sample avg dev_std = 0.424)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.484
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.483
NEC for r=0.8 class 0.0 = 0.002 +- 0.069 (in-sample avg dev_std = 0.043)
NEC for r=0.8 class 1.0 = 0.001 +- 0.069 (in-sample avg dev_std = 0.043)
NEC for r=0.8 all KL = 0.006 +- 0.069 (in-sample avg dev_std = 0.043)
NEC for r=0.8 all L1 = 0.001 +- 0.019 (in-sample avg dev_std = 0.043)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:00:18 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:44 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:44 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:00:45 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 199...
[0m[1;37mINFO[0m: [1mCheckpoint 199: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8619
ID Validation Loss: 0.8658
ID Test ACCURACY: 0.8593
ID Test Loss: 0.9042
OOD Validation ACCURACY: 0.6146
OOD Validation Loss: 17.0850
OOD Test ACCURACY: 0.5161
OOD Test Loss: 124.0430

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 0.9448
Train Loss: 0.0896
ID Validation ACCURACY: 0.8455
ID Validation Loss: 0.5447
ID Test ACCURACY: 0.8432
ID Test Loss: 0.5905
OOD Validation ACCURACY: 0.8541
OOD Validation Loss: 0.8881
OOD Test ACCURACY: 0.7646
OOD Test Loss: 1.8261

[0m[1;37mINFO[0m: [1mChartInfo 0.8593 0.5161 0.8432 0.7646 0.8455 0.8541[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/18/2024 09:00:46 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.823
SUFF++ for r=0.8 class 0.0 = 0.819 +- 0.378 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.8 class 1.0 = 0.903 +- 0.378 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.8 all KL = 0.714 +- 0.378 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.8 all L1 = 0.868 +- 0.204 (in-sample avg dev_std = 0.434)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.623
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.607
SUFF++ for r=0.8 class 0.0 = 0.765 +- 0.377 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 class 1.0 = 0.923 +- 0.377 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 all KL = 0.762 +- 0.377 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 all L1 = 0.847 +- 0.249 (in-sample avg dev_std = 0.433)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.516
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.516
SUFF++ for r=0.8 class 0.0 = 1.0 +- 0.026 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.8 class 1.0 = 0.999 +- 0.026 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.8 all KL = 0.999 +- 0.026 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.8 all L1 = 1.0 +- 0.010 (in-sample avg dev_std = 0.012)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.853
NEC for r=0.8 class 0.0 = 0.089 +- 0.260 (in-sample avg dev_std = 0.226)
NEC for r=0.8 class 1.0 = 0.059 +- 0.260 (in-sample avg dev_std = 0.226)
NEC for r=0.8 all KL = 0.105 +- 0.260 (in-sample avg dev_std = 0.226)
NEC for r=0.8 all L1 = 0.071 +- 0.191 (in-sample avg dev_std = 0.226)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.624
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.66
NEC for r=0.8 class 0.0 = 0.156 +- 0.302 (in-sample avg dev_std = 0.237)
NEC for r=0.8 class 1.0 = 0.059 +- 0.302 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all KL = 0.148 +- 0.302 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all L1 = 0.106 +- 0.225 (in-sample avg dev_std = 0.237)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.516
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.516
NEC for r=0.8 class 0.0 = 0.0 +- 0.002 (in-sample avg dev_std = 0.001)
NEC for r=0.8 class 1.0 = 0.0 +- 0.002 (in-sample avg dev_std = 0.001)
NEC for r=0.8 all KL = 0.0 +- 0.002 (in-sample avg dev_std = 0.001)
NEC for r=0.8 all L1 = 0.0 +- 0.000 (in-sample avg dev_std = 0.001)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.776], 'all_L1': [0.874]}), defaultdict(<class 'list'>, {'all_KL': [0.702], 'all_L1': [0.859]}), defaultdict(<class 'list'>, {'all_KL': [0.714], 'all_L1': [0.868]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.101], 'all_L1': [0.073]}), defaultdict(<class 'list'>, {'all_KL': [0.12], 'all_L1': [0.075]}), defaultdict(<class 'list'>, {'all_KL': [0.105], 'all_L1': [0.071]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.747], 'all_L1': [0.856]}), defaultdict(<class 'list'>, {'all_KL': [0.57], 'all_L1': [0.755]}), defaultdict(<class 'list'>, {'all_KL': [0.762], 'all_L1': [0.847]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.118], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.317], 'all_L1': [0.203]}), defaultdict(<class 'list'>, {'all_KL': [0.148], 'all_L1': [0.106]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.619], 'all_L1': [0.799]}), defaultdict(<class 'list'>, {'all_KL': [0.999], 'all_L1': [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.999], 'all_L1': [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.307], 'all_L1': [0.158]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.001]}), defaultdict(<class 'list'>, {'all_KL': [0.0], 'all_L1': [0.0]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.867 +- 0.006
suff++ class all_KL  =  0.731 +- 0.032
suff++_acc_int  =  0.824 +- 0.001
nec class all_L1  =  0.073 +- 0.002
nec class all_KL  =  0.109 +- 0.008
nec_acc_int  =  0.852 +- 0.008

Eval split val
suff++ class all_L1  =  0.819 +- 0.046
suff++ class all_KL  =  0.693 +- 0.087
suff++_acc_int  =  0.658 +- 0.089
nec class all_L1  =  0.132 +- 0.051
nec class all_KL  =  0.194 +- 0.088
nec_acc_int  =  0.707 +- 0.076

Eval split test
suff++ class all_L1  =  0.933 +- 0.095
suff++ class all_KL  =  0.872 +- 0.179
suff++_acc_int  =  0.547 +- 0.067
nec class all_L1  =  0.053 +- 0.074
nec class all_KL  =  0.104 +- 0.143
nec_acc_int  =  0.554 +- 0.078


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.470 +- 0.003
Faith. Armon (L1)= 		  =  0.135 +- 0.003
Faith. GMean (L1)= 	  =  0.252 +- 0.002
Faith. Aritm (KL)= 		  =  0.420 +- 0.013
Faith. Armon (KL)= 		  =  0.189 +- 0.011
Faith. GMean (KL)= 	  =  0.281 +- 0.007

Eval split val
Faith. Aritm (L1)= 		  =  0.476 +- 0.003
Faith. Armon (L1)= 		  =  0.223 +- 0.070
Faith. GMean (L1)= 	  =  0.322 +- 0.050
Faith. Aritm (KL)= 		  =  0.444 +- 0.009
Faith. Armon (KL)= 		  =  0.286 +- 0.087
Faith. GMean (KL)= 	  =  0.353 +- 0.054

Eval split test
Faith. Aritm (L1)= 		  =  0.493 +- 0.010
Faith. Armon (L1)= 		  =  0.089 +- 0.124
Faith. GMean (L1)= 	  =  0.129 +- 0.161
Faith. Aritm (KL)= 		  =  0.488 +- 0.018
Faith. Armon (KL)= 		  =  0.141 +- 0.191
Faith. GMean (KL)= 	  =  0.171 +- 0.190
Computed for split load_split = id



Completed in  0:05:56.145404  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:02:39 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:41 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:43 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:44 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:45 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:48 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:49 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:49 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:02:50 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 147...
[0m[1;37mINFO[0m: [1mCheckpoint 147: 
-----------------------------------
Train ACCURACY: 0.9888
Train Loss: 0.0383
ID Validation ACCURACY: 0.6354
ID Validation Loss: 2.4754
ID Test ACCURACY: 0.6047
ID Test Loss: 2.7614
OOD Validation ACCURACY: 0.6101
OOD Validation Loss: 2.5866
OOD Test ACCURACY: 0.5566
OOD Test Loss: 2.8049

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 25...
[0m[1;37mINFO[0m: [1mCheckpoint 25: 
-----------------------------------
Train ACCURACY: 0.8880
Train Loss: 0.2787
ID Validation ACCURACY: 0.5704
ID Validation Loss: 2.0731
ID Test ACCURACY: 0.5542
ID Test Loss: 2.2825
OOD Validation ACCURACY: 0.6151
OOD Validation Loss: 1.9070
OOD Test ACCURACY: 0.5676
OOD Test Loss: 2.3727

[0m[1;37mINFO[0m: [1mChartInfo 0.6047 0.5566 0.5542 0.5676 0.5704 0.6151[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.622
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.581
SUFF++ for r=0.6 class 0 = 0.718 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 class 1 = 0.862 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 class 2 = 0.724 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 all KL = 0.733 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 all L1 = 0.788 +- 0.218 (in-sample avg dev_std = 0.409)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.621
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.596
SUFF++ for r=0.6 class 0 = 0.76 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 class 1 = 0.886 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 class 2 = 0.805 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 all KL = 0.846 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 all L1 = 0.837 +- 0.196 (in-sample avg dev_std = 0.296)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.553
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.539
SUFF++ for r=0.6 class 0 = 0.788 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 class 1 = 0.868 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 class 2 = 0.813 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 all KL = 0.832 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 all L1 = 0.833 +- 0.194 (in-sample avg dev_std = 0.309)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.62
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.61
NEC for r=0.6 class 0 = 0.137 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 1 = 0.106 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 2 = 0.215 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 all KL = 0.123 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 all L1 = 0.144 +- 0.191 (in-sample avg dev_std = 0.229)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.624
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.61
NEC for r=0.6 class 0 = 0.191 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 class 1 = 0.108 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 class 2 = 0.18 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 all KL = 0.115 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 all L1 = 0.144 +- 0.185 (in-sample avg dev_std = 0.219)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.555
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.551
NEC for r=0.6 class 0 = 0.169 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 1 = 0.116 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 2 = 0.171 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all KL = 0.114 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all L1 = 0.143 +- 0.193 (in-sample avg dev_std = 0.208)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:04:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:32 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:35 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:35 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:37 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:40 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:04:41 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 120...
[0m[1;37mINFO[0m: [1mCheckpoint 120: 
-----------------------------------
Train ACCURACY: 0.9849
Train Loss: 0.0703
ID Validation ACCURACY: 0.6191
ID Validation Loss: 2.1533
ID Test ACCURACY: 0.5903
ID Test Loss: 2.1410
OOD Validation ACCURACY: 0.5025
OOD Validation Loss: 2.4391
OOD Test ACCURACY: 0.4468
OOD Test Loss: 3.6983

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 131...
[0m[1;37mINFO[0m: [1mCheckpoint 131: 
-----------------------------------
Train ACCURACY: 0.9815
Train Loss: 0.0601
ID Validation ACCURACY: 0.5758
ID Validation Loss: 2.4039
ID Test ACCURACY: 0.5830
ID Test Loss: 2.3059
OOD Validation ACCURACY: 0.5765
OOD Validation Loss: 2.1952
OOD Test ACCURACY: 0.5223
OOD Test Loss: 2.5406

[0m[1;37mINFO[0m: [1mChartInfo 0.5903 0.4468 0.5830 0.5223 0.5758 0.5765[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.541
SUFF++ for r=0.6 class 0 = 0.577 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 class 1 = 0.672 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 class 2 = 0.681 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 all KL = 0.54 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 all L1 = 0.651 +- 0.222 (in-sample avg dev_std = 0.546)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.488
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.457
SUFF++ for r=0.6 class 0 = 0.652 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 class 1 = 0.641 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 class 2 = 0.709 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 all KL = 0.608 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 all L1 = 0.658 +- 0.199 (in-sample avg dev_std = 0.479)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.445
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.406
SUFF++ for r=0.6 class 0 = 0.693 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 class 1 = 0.645 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 class 2 = 0.729 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 all KL = 0.69 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 all L1 = 0.678 +- 0.204 (in-sample avg dev_std = 0.410)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.606
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.566
NEC for r=0.6 class 0 = 0.356 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 class 1 = 0.243 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 class 2 = 0.217 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 all KL = 0.274 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 all L1 = 0.263 +- 0.251 (in-sample avg dev_std = 0.359)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.489
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.47
NEC for r=0.6 class 0 = 0.369 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 class 1 = 0.306 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 class 2 = 0.279 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 all KL = 0.299 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 all L1 = 0.316 +- 0.236 (in-sample avg dev_std = 0.366)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.451
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.43
NEC for r=0.6 class 0 = 0.317 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 class 1 = 0.315 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 class 2 = 0.257 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 all KL = 0.246 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 all L1 = 0.301 +- 0.211 (in-sample avg dev_std = 0.324)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:06:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:15 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:17 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:18 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:20 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:23 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:06:25 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ACCURACY: 0.9915
Train Loss: 0.0362
ID Validation ACCURACY: 0.6534
ID Validation Loss: 1.8080
ID Test ACCURACY: 0.5921
ID Test Loss: 2.0719
OOD Validation ACCURACY: 0.6168
OOD Validation Loss: 1.9392
OOD Test ACCURACY: 0.5395
OOD Test Loss: 2.4358

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 0.9749
Train Loss: 0.0628
ID Validation ACCURACY: 0.6173
ID Validation Loss: 1.7817
ID Test ACCURACY: 0.6029
ID Test Loss: 2.0648
OOD Validation ACCURACY: 0.6202
OOD Validation Loss: 1.9126
OOD Test ACCURACY: 0.5607
OOD Test Loss: 2.3389

[0m[1;37mINFO[0m: [1mChartInfo 0.5921 0.5395 0.6029 0.5607 0.6173 0.6202[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.657
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.602
SUFF++ for r=0.6 class 0 = 0.708 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 1 = 0.85 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 2 = 0.745 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all KL = 0.759 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all L1 = 0.786 +- 0.207 (in-sample avg dev_std = 0.379)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.62
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.607
SUFF++ for r=0.6 class 0 = 0.758 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 class 1 = 0.834 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 class 2 = 0.775 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 all KL = 0.818 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 all L1 = 0.802 +- 0.203 (in-sample avg dev_std = 0.330)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.553
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.535
SUFF++ for r=0.6 class 0 = 0.768 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 class 1 = 0.801 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 class 2 = 0.765 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 all KL = 0.795 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 all L1 = 0.784 +- 0.201 (in-sample avg dev_std = 0.355)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.65
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.619
NEC for r=0.6 class 0 = 0.32 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 class 1 = 0.136 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 class 2 = 0.146 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 all KL = 0.168 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 all L1 = 0.184 +- 0.221 (in-sample avg dev_std = 0.279)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.62
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.62
NEC for r=0.6 class 0 = 0.286 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 class 1 = 0.175 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 class 2 = 0.177 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 all KL = 0.185 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 all L1 = 0.204 +- 0.224 (in-sample avg dev_std = 0.266)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.55
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.561
NEC for r=0.6 class 0 = 0.232 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 class 1 = 0.192 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 class 2 = 0.196 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 all KL = 0.175 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 all L1 = 0.204 +- 0.226 (in-sample avg dev_std = 0.254)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.733], 'all_L1': [0.788]}), defaultdict(<class 'list'>, {'all_KL': [0.54], 'all_L1': [0.651]}), defaultdict(<class 'list'>, {'all_KL': [0.759], 'all_L1': [0.786]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.123], 'all_L1': [0.144]}), defaultdict(<class 'list'>, {'all_KL': [0.274], 'all_L1': [0.263]}), defaultdict(<class 'list'>, {'all_KL': [0.168], 'all_L1': [0.184]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.846], 'all_L1': [0.837]}), defaultdict(<class 'list'>, {'all_KL': [0.608], 'all_L1': [0.658]}), defaultdict(<class 'list'>, {'all_KL': [0.818], 'all_L1': [0.802]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.115], 'all_L1': [0.144]}), defaultdict(<class 'list'>, {'all_KL': [0.299], 'all_L1': [0.316]}), defaultdict(<class 'list'>, {'all_KL': [0.185], 'all_L1': [0.204]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.832], 'all_L1': [0.833]}), defaultdict(<class 'list'>, {'all_KL': [0.69], 'all_L1': [0.678]}), defaultdict(<class 'list'>, {'all_KL': [0.795], 'all_L1': [0.784]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.114], 'all_L1': [0.143]}), defaultdict(<class 'list'>, {'all_KL': [0.246], 'all_L1': [0.301]}), defaultdict(<class 'list'>, {'all_KL': [0.175], 'all_L1': [0.204]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.742 +- 0.064
suff++ class all_KL  =  0.677 +- 0.098
suff++_acc_int  =  0.575 +- 0.026
nec class all_L1  =  0.197 +- 0.049
nec class all_KL  =  0.188 +- 0.063
nec_acc_int  =  0.598 +- 0.023

Eval split val
suff++ class all_L1  =  0.766 +- 0.077
suff++ class all_KL  =  0.757 +- 0.106
suff++_acc_int  =  0.554 +- 0.068
nec class all_L1  =  0.221 +- 0.071
nec class all_KL  =  0.200 +- 0.076
nec_acc_int  =  0.567 +- 0.068

Eval split test
suff++ class all_L1  =  0.765 +- 0.065
suff++ class all_KL  =  0.772 +- 0.060
suff++_acc_int  =  0.493 +- 0.062
nec class all_L1  =  0.216 +- 0.065
nec class all_KL  =  0.178 +- 0.054
nec_acc_int  =  0.514 +- 0.060


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.469 +- 0.012
Faith. Armon (L1)= 		  =  0.305 +- 0.054
Faith. GMean (L1)= 	  =  0.377 +- 0.031
Faith. Aritm (KL)= 		  =  0.433 +- 0.023
Faith. Armon (KL)= 		  =  0.283 +- 0.063
Faith. GMean (KL)= 	  =  0.347 +- 0.035

Eval split val
Faith. Aritm (L1)= 		  =  0.493 +- 0.007
Faith. Armon (L1)= 		  =  0.333 +- 0.074
Faith. GMean (L1)= 	  =  0.403 +- 0.044
Faith. Aritm (KL)= 		  =  0.479 +- 0.020
Faith. Armon (KL)= 		  =  0.302 +- 0.081
Faith. GMean (KL)= 	  =  0.376 +- 0.048

Eval split test
Faith. Aritm (L1)= 		  =  0.490 +- 0.003
Faith. Armon (L1)= 		  =  0.328 +- 0.071
Faith. GMean (L1)= 	  =  0.399 +- 0.044
Faith. Aritm (KL)= 		  =  0.475 +- 0.007
Faith. Armon (KL)= 		  =  0.283 +- 0.066
Faith. GMean (KL)= 	  =  0.364 +- 0.043
Computed for split load_split = id



Completed in  0:05:25.584470  for CIGAvGIN GOODTwitter/length



DONE CIGA GOODTwitter/length

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:08:48 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/18/2024 09:08:50 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:08:53 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:08:57 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:00 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:09:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 150...
[0m[1;37mINFO[0m: [1mCheckpoint 150: 
-----------------------------------
Train ROC-AUC: 0.9355
Train Loss: 0.1115
ID Validation ROC-AUC: 0.8437
ID Validation Loss: 0.1686
ID Test ROC-AUC: 0.8421
ID Test Loss: 0.1567
OOD Validation ROC-AUC: 0.7498
OOD Validation Loss: 0.1557
OOD Test ROC-AUC: 0.7319
OOD Test Loss: 0.1092

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 73...
[0m[1;37mINFO[0m: [1mCheckpoint 73: 
-----------------------------------
Train ROC-AUC: 0.8303
Train Loss: 0.1860
ID Validation ROC-AUC: 0.7943
ID Validation Loss: 0.2115
ID Test ROC-AUC: 0.7798
ID Test Loss: 0.1994
OOD Validation ROC-AUC: 0.7827
OOD Validation Loss: 0.1548
OOD Test ROC-AUC: 0.7525
OOD Test Loss: 0.1116

[0m[1;37mINFO[0m: [1mChartInfo 0.8421 0.7319 0.7798 0.7525 0.7943 0.7827[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/18/2024 09:09:09 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/18/2024 09:09:18 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/18/2024 09:09:21 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.642
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.616
SUFF++ for r=0.8 class 0.0 = 0.943 +- 0.110 (in-sample avg dev_std = 0.169)
SUFF++ for r=0.8 class 1.0 = 0.903 +- 0.110 (in-sample avg dev_std = 0.169)
SUFF++ for r=0.8 all KL = 0.953 +- 0.110 (in-sample avg dev_std = 0.169)
SUFF++ for r=0.8 all L1 = 0.923 +- 0.095 (in-sample avg dev_std = 0.169)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.667
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.616
SUFF++ for r=0.8 class 0.0 = 0.953 +- 0.048 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.8 class 1.0 = 0.924 +- 0.048 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.8 all KL = 0.975 +- 0.048 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.8 all L1 = 0.938 +- 0.056 (in-sample avg dev_std = 0.099)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.703
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.672
SUFF++ for r=0.8 class 0.0 = 0.976 +- 0.009 (in-sample avg dev_std = 0.032)
SUFF++ for r=0.8 class 1.0 = 0.964 +- 0.009 (in-sample avg dev_std = 0.032)
SUFF++ for r=0.8 all KL = 0.993 +- 0.009 (in-sample avg dev_std = 0.032)
SUFF++ for r=0.8 all L1 = 0.97 +- 0.023 (in-sample avg dev_std = 0.032)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.637
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.605
NEC for r=0.8 class 0.0 = 0.024 +- 0.016 (in-sample avg dev_std = 0.022)
NEC for r=0.8 class 1.0 = 0.032 +- 0.016 (in-sample avg dev_std = 0.022)
NEC for r=0.8 all KL = 0.006 +- 0.016 (in-sample avg dev_std = 0.022)
NEC for r=0.8 all L1 = 0.028 +- 0.030 (in-sample avg dev_std = 0.022)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.662
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.617
NEC for r=0.8 class 0.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.016)
NEC for r=0.8 class 1.0 = 0.026 +- 0.004 (in-sample avg dev_std = 0.016)
NEC for r=0.8 all KL = 0.004 +- 0.004 (in-sample avg dev_std = 0.016)
NEC for r=0.8 all L1 = 0.025 +- 0.018 (in-sample avg dev_std = 0.016)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.701
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.68
NEC for r=0.8 class 0.0 = 0.014 +- 0.004 (in-sample avg dev_std = 0.014)
NEC for r=0.8 class 1.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.014)
NEC for r=0.8 all KL = 0.003 +- 0.004 (in-sample avg dev_std = 0.014)
NEC for r=0.8 all L1 = 0.019 +- 0.016 (in-sample avg dev_std = 0.014)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:09:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:46 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:50 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:54 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:09:57 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 188...
[0m[1;37mINFO[0m: [1mCheckpoint 188: 
-----------------------------------
Train ROC-AUC: 0.9379
Train Loss: 0.0794
ID Validation ROC-AUC: 0.8408
ID Validation Loss: 0.1328
ID Test ROC-AUC: 0.8294
ID Test Loss: 0.1170
OOD Validation ROC-AUC: 0.7585
OOD Validation Loss: 0.1303
OOD Test ROC-AUC: 0.6813
OOD Test Loss: 0.0992

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 84...
[0m[1;37mINFO[0m: [1mCheckpoint 84: 
-----------------------------------
Train ROC-AUC: 0.8280
Train Loss: 0.1624
ID Validation ROC-AUC: 0.7995
ID Validation Loss: 0.1831
ID Test ROC-AUC: 0.7772
ID Test Loss: 0.1748
OOD Validation ROC-AUC: 0.7790
OOD Validation Loss: 0.1618
OOD Test ROC-AUC: 0.7203
OOD Test Loss: 0.1230

[0m[1;37mINFO[0m: [1mChartInfo 0.8294 0.6813 0.7772 0.7203 0.7995 0.7790[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/18/2024 09:10:00 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/18/2024 09:10:04 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/18/2024 09:10:07 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.375
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.39
SUFF++ for r=0.8 class 0.0 = 0.983 +- 0.008 (in-sample avg dev_std = 0.006)
SUFF++ for r=0.8 class 1.0 = 0.989 +- 0.008 (in-sample avg dev_std = 0.006)
SUFF++ for r=0.8 all KL = 0.995 +- 0.008 (in-sample avg dev_std = 0.006)
SUFF++ for r=0.8 all L1 = 0.986 +- 0.019 (in-sample avg dev_std = 0.006)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.316
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.43
SUFF++ for r=0.8 class 0.0 = 0.985 +- 0.005 (in-sample avg dev_std = 0.005)
SUFF++ for r=0.8 class 1.0 = 0.992 +- 0.005 (in-sample avg dev_std = 0.005)
SUFF++ for r=0.8 all KL = 0.996 +- 0.005 (in-sample avg dev_std = 0.005)
SUFF++ for r=0.8 all L1 = 0.988 +- 0.015 (in-sample avg dev_std = 0.005)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.258
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.312
SUFF++ for r=0.8 class 0.0 = 0.983 +- 0.004 (in-sample avg dev_std = 0.007)
SUFF++ for r=0.8 class 1.0 = 0.992 +- 0.004 (in-sample avg dev_std = 0.007)
SUFF++ for r=0.8 all KL = 0.997 +- 0.004 (in-sample avg dev_std = 0.007)
SUFF++ for r=0.8 all L1 = 0.987 +- 0.015 (in-sample avg dev_std = 0.007)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.374
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.345
NEC for r=0.8 class 0.0 = 0.017 +- 0.007 (in-sample avg dev_std = 0.005)
NEC for r=0.8 class 1.0 = 0.01 +- 0.007 (in-sample avg dev_std = 0.005)
NEC for r=0.8 all KL = 0.004 +- 0.007 (in-sample avg dev_std = 0.005)
NEC for r=0.8 all L1 = 0.013 +- 0.020 (in-sample avg dev_std = 0.005)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.319
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.378
NEC for r=0.8 class 0.0 = 0.014 +- 0.005 (in-sample avg dev_std = 0.003)
NEC for r=0.8 class 1.0 = 0.007 +- 0.005 (in-sample avg dev_std = 0.003)
NEC for r=0.8 all KL = 0.003 +- 0.005 (in-sample avg dev_std = 0.003)
NEC for r=0.8 all L1 = 0.011 +- 0.015 (in-sample avg dev_std = 0.003)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.258
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.334
NEC for r=0.8 class 0.0 = 0.018 +- 0.007 (in-sample avg dev_std = 0.006)
NEC for r=0.8 class 1.0 = 0.006 +- 0.007 (in-sample avg dev_std = 0.006)
NEC for r=0.8 all KL = 0.003 +- 0.007 (in-sample avg dev_std = 0.006)
NEC for r=0.8 all L1 = 0.012 +- 0.019 (in-sample avg dev_std = 0.006)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:10:30 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:31 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:35 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:38 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:42 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.7462
Train Loss: 0.1487
ID Validation ROC-AUC: 0.7584
ID Validation Loss: 0.1618
ID Test ROC-AUC: 0.7168
ID Test Loss: 0.1380
OOD Validation ROC-AUC: 0.7424
OOD Validation Loss: 0.1213
OOD Test ROC-AUC: 0.6596
OOD Test Loss: 0.0979

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ROC-AUC: 0.7560
Train Loss: 0.1465
ID Validation ROC-AUC: 0.7444
ID Validation Loss: 0.1652
ID Test ROC-AUC: 0.7112
ID Test Loss: 0.1365
OOD Validation ROC-AUC: 0.7572
OOD Validation Loss: 0.1169
OOD Test ROC-AUC: 0.6751
OOD Test Loss: 0.0962

[0m[1;37mINFO[0m: [1mChartInfo 0.7168 0.6596 0.7112 0.6751 0.7444 0.7572[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/18/2024 09:10:45 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/18/2024 09:10:49 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/18/2024 09:10:52 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.559
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.538
SUFF++ for r=0.8 class 0.0 = 0.967 +- 0.023 (in-sample avg dev_std = 0.061)
SUFF++ for r=0.8 class 1.0 = 0.955 +- 0.023 (in-sample avg dev_std = 0.061)
SUFF++ for r=0.8 all KL = 0.988 +- 0.023 (in-sample avg dev_std = 0.061)
SUFF++ for r=0.8 all L1 = 0.961 +- 0.047 (in-sample avg dev_std = 0.061)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.621
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.538
SUFF++ for r=0.8 class 0.0 = 0.968 +- 0.012 (in-sample avg dev_std = 0.042)
SUFF++ for r=0.8 class 1.0 = 0.969 +- 0.012 (in-sample avg dev_std = 0.042)
SUFF++ for r=0.8 all KL = 0.991 +- 0.012 (in-sample avg dev_std = 0.042)
SUFF++ for r=0.8 all L1 = 0.969 +- 0.029 (in-sample avg dev_std = 0.042)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.442
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.439
SUFF++ for r=0.8 class 0.0 = 0.974 +- 0.004 (in-sample avg dev_std = 0.024)
SUFF++ for r=0.8 class 1.0 = 0.975 +- 0.004 (in-sample avg dev_std = 0.024)
SUFF++ for r=0.8 all KL = 0.996 +- 0.004 (in-sample avg dev_std = 0.024)
SUFF++ for r=0.8 all L1 = 0.974 +- 0.017 (in-sample avg dev_std = 0.024)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.561
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.532
NEC for r=0.8 class 0.0 = 0.026 +- 0.010 (in-sample avg dev_std = 0.018)
NEC for r=0.8 class 1.0 = 0.036 +- 0.010 (in-sample avg dev_std = 0.018)
NEC for r=0.8 all KL = 0.006 +- 0.010 (in-sample avg dev_std = 0.018)
NEC for r=0.8 all L1 = 0.031 +- 0.037 (in-sample avg dev_std = 0.018)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.616
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.505
NEC for r=0.8 class 0.0 = 0.031 +- 0.008 (in-sample avg dev_std = 0.009)
NEC for r=0.8 class 1.0 = 0.027 +- 0.008 (in-sample avg dev_std = 0.009)
NEC for r=0.8 all KL = 0.005 +- 0.008 (in-sample avg dev_std = 0.009)
NEC for r=0.8 all L1 = 0.029 +- 0.027 (in-sample avg dev_std = 0.009)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.444
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.439
NEC for r=0.8 class 0.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.009)
NEC for r=0.8 class 1.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.009)
NEC for r=0.8 all KL = 0.003 +- 0.004 (in-sample avg dev_std = 0.009)
NEC for r=0.8 all L1 = 0.024 +- 0.019 (in-sample avg dev_std = 0.009)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.953], 'all_L1': [0.923]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.986]}), defaultdict(<class 'list'>, {'all_KL': [0.988], 'all_L1': [0.961]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.028]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.013]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.031]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.975], 'all_L1': [0.938]}), defaultdict(<class 'list'>, {'all_KL': [0.996], 'all_L1': [0.988]}), defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.969]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.011]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.029]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.993], 'all_L1': [0.97]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.987]}), defaultdict(<class 'list'>, {'all_KL': [0.996], 'all_L1': [0.974]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.019]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.012]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.024]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.957 +- 0.026
suff++ class all_KL  =  0.979 +- 0.018
suff++_acc_int  =  0.515 +- 0.094
nec class all_L1  =  0.024 +- 0.008
nec class all_KL  =  0.005 +- 0.001
nec_acc_int  =  0.494 +- 0.110

Eval split val
suff++ class all_L1  =  0.965 +- 0.021
suff++ class all_KL  =  0.987 +- 0.009
suff++_acc_int  =  0.528 +- 0.076
nec class all_L1  =  0.022 +- 0.008
nec class all_KL  =  0.004 +- 0.001
nec_acc_int  =  0.500 +- 0.098

Eval split test
suff++ class all_L1  =  0.977 +- 0.007
suff++ class all_KL  =  0.995 +- 0.002
suff++_acc_int  =  0.475 +- 0.149
nec class all_L1  =  0.018 +- 0.005
nec class all_KL  =  0.003 +- 0.000
nec_acc_int  =  0.484 +- 0.145


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.490 +- 0.011
Faith. Armon (L1)= 		  =  0.047 +- 0.015
Faith. GMean (L1)= 	  =  0.149 +- 0.026
Faith. Aritm (KL)= 		  =  0.492 +- 0.009
Faith. Armon (KL)= 		  =  0.011 +- 0.002
Faith. GMean (KL)= 	  =  0.072 +- 0.006

Eval split val
Faith. Aritm (L1)= 		  =  0.493 +- 0.008
Faith. Armon (L1)= 		  =  0.042 +- 0.015
Faith. GMean (L1)= 	  =  0.142 +- 0.027
Faith. Aritm (KL)= 		  =  0.496 +- 0.004
Faith. Armon (KL)= 		  =  0.008 +- 0.002
Faith. GMean (KL)= 	  =  0.063 +- 0.006

Eval split test
Faith. Aritm (L1)= 		  =  0.498 +- 0.002
Faith. Armon (L1)= 		  =  0.036 +- 0.009
Faith. GMean (L1)= 	  =  0.132 +- 0.018
Faith. Aritm (KL)= 		  =  0.499 +- 0.001
Faith. Armon (KL)= 		  =  0.006 +- 0.000
Faith. GMean (KL)= 	  =  0.055 +- 0.000
Computed for split load_split = id



Completed in  0:02:36.362242  for CIGAvGIN GOODHIV/scaffold



DONE CIGA GOODHIV/scaffold

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:12:05 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:22 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:25 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:27 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:30 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:12:33 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 46...
[0m[1;37mINFO[0m: [1mCheckpoint 46: 
-----------------------------------
Train ACCURACY: 0.3282
Train Loss: 2.3518
ID Validation ACCURACY: 0.3286
ID Validation Loss: 2.3614
ID Test ACCURACY: 0.3219
ID Test Loss: 2.3849
OOD Validation ACCURACY: 0.2846
OOD Validation Loss: 3.8053
OOD Test ACCURACY: 0.1441
OOD Test Loss: 9.3433

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 44...
[0m[1;37mINFO[0m: [1mCheckpoint 44: 
-----------------------------------
Train ACCURACY: 0.3185
Train Loss: 2.4273
ID Validation ACCURACY: 0.3143
ID Validation Loss: 2.4540
ID Test ACCURACY: 0.3124
ID Test Loss: 2.4743
OOD Validation ACCURACY: 0.3104
OOD Validation Loss: 2.7678
OOD Test ACCURACY: 0.1749
OOD Test Loss: 4.9128

[0m[1;37mINFO[0m: [1mChartInfo 0.3219 0.1441 0.3124 0.1749 0.3143 0.3104[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.319
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.207
SUFF++ for r=0.6 class 0 = 0.324 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 1 = 0.926 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 2 = 0.344 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 3 = 0.354 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 4 = 0.352 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 5 = 0.371 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 6 = 0.361 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 7 = 0.349 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 8 = 0.377 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 9 = 0.389 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 all KL = 0.305 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 all L1 = 0.421 +- 0.210 (in-sample avg dev_std = 0.647)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.265
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.189
SUFF++ for r=0.6 class 0 = 0.314 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 1 = 0.969 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 2 = 0.36 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 3 = 0.367 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 4 = 0.449 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 5 = 0.39 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 6 = 0.467 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 7 = 0.421 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 8 = 0.417 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 9 = 0.525 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 all KL = 0.368 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 all L1 = 0.477 +- 0.255 (in-sample avg dev_std = 0.672)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.138
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.147
SUFF++ for r=0.6 class 0 = 0.423 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 1 = 0.949 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 2 = 0.488 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 3 = 0.544 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 4 = 0.733 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 5 = 0.651 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 6 = 0.644 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 7 = 0.738 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 8 = 0.678 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 9 = 0.777 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 all KL = 0.529 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 all L1 = 0.663 +- 0.282 (in-sample avg dev_std = 0.541)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.316
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.34
NEC for r=0.6 class 0 = 0.458 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 1 = 0.062 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 2 = 0.508 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 3 = 0.536 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 4 = 0.542 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 5 = 0.547 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 6 = 0.565 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 7 = 0.598 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 8 = 0.574 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 9 = 0.551 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 all KL = 0.455 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 all L1 = 0.488 +- 0.215 (in-sample avg dev_std = 0.207)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.264
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.285
NEC for r=0.6 class 0 = 0.477 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 1 = 0.046 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 2 = 0.539 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 3 = 0.575 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 4 = 0.554 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 5 = 0.513 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 6 = 0.569 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 7 = 0.604 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 8 = 0.622 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 9 = 0.539 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 all KL = 0.531 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 all L1 = 0.498 +- 0.255 (in-sample avg dev_std = 0.216)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.136
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.226
NEC for r=0.6 class 0 = 0.74 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 1 = 0.108 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 2 = 0.696 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 3 = 0.6 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 4 = 0.466 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 5 = 0.573 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 6 = 0.573 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 7 = 0.532 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 8 = 0.464 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 9 = 0.364 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 all KL = 0.656 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 all L1 = 0.508 +- 0.337 (in-sample avg dev_std = 0.206)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:20:23 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/18/2024 09:20:40 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:20:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:20:46 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:20:51 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:21:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ACCURACY: 0.2758
Train Loss: 2.1313
ID Validation ACCURACY: 0.2723
ID Validation Loss: 2.1460
ID Test ACCURACY: 0.2776
ID Test Loss: 2.1540
OOD Validation ACCURACY: 0.2164
OOD Validation Loss: 2.4175
OOD Test ACCURACY: 0.1419
OOD Test Loss: 3.0432

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.2489
Train Loss: 2.1703
ID Validation ACCURACY: 0.2416
ID Validation Loss: 2.1907
ID Test ACCURACY: 0.2473
ID Test Loss: 2.1887
OOD Validation ACCURACY: 0.2927
OOD Validation Loss: 2.0351
OOD Test ACCURACY: 0.1840
OOD Test Loss: 2.6631

[0m[1;37mINFO[0m: [1mChartInfo 0.2776 0.1419 0.2473 0.1840 0.2416 0.2927[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.27
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.18
SUFF++ for r=0.6 class 0 = 0.624 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 1 = 0.443 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 2 = 0.617 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 3 = 0.623 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 4 = 0.564 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 5 = 0.609 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 6 = 0.584 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 7 = 0.59 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 8 = 0.617 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 9 = 0.573 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 all KL = 0.672 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 all L1 = 0.583 +- 0.087 (in-sample avg dev_std = 0.297)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.214
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.159
SUFF++ for r=0.6 class 0 = 0.622 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 1 = 0.441 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 2 = 0.621 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 3 = 0.62 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 4 = 0.582 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 5 = 0.638 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 6 = 0.584 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 7 = 0.578 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 8 = 0.618 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 9 = 0.562 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 all KL = 0.705 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 all L1 = 0.583 +- 0.099 (in-sample avg dev_std = 0.272)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.14
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.131
SUFF++ for r=0.6 class 0 = 0.557 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 1 = 0.435 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 2 = 0.541 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 3 = 0.521 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 4 = 0.498 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 5 = 0.542 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 6 = 0.503 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 7 = 0.504 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 8 = 0.51 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 9 = 0.489 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 all KL = 0.653 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 all L1 = 0.509 +- 0.079 (in-sample avg dev_std = 0.324)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.271
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.26
NEC for r=0.6 class 0 = 0.426 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 1 = 0.536 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 2 = 0.43 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 3 = 0.41 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 4 = 0.445 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 5 = 0.425 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 6 = 0.459 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 7 = 0.451 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 8 = 0.429 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 9 = 0.48 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 all KL = 0.337 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 all L1 = 0.45 +- 0.127 (in-sample avg dev_std = 0.115)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.215
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.301
NEC for r=0.6 class 0 = 0.489 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 1 = 0.414 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 2 = 0.434 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 3 = 0.426 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 4 = 0.478 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 5 = 0.43 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 6 = 0.482 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 7 = 0.498 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 8 = 0.46 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 9 = 0.498 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 all KL = 0.367 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 all L1 = 0.461 +- 0.114 (in-sample avg dev_std = 0.125)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.14
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.246
NEC for r=0.6 class 0 = 0.54 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 1 = 0.368 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 2 = 0.595 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 3 = 0.563 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 4 = 0.612 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 5 = 0.546 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 6 = 0.601 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 7 = 0.626 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 8 = 0.579 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 9 = 0.586 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 all KL = 0.529 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 all L1 = 0.559 +- 0.132 (in-sample avg dev_std = 0.113)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:28:30 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/18/2024 09:28:47 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:28:49 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:28:52 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:28:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:29:07 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.3600
Train Loss: 1.8524
ID Validation ACCURACY: 0.3517
ID Validation Loss: 1.8640
ID Test ACCURACY: 0.3516
ID Test Loss: 1.8567
OOD Validation ACCURACY: 0.3029
OOD Validation Loss: 2.2550
OOD Test ACCURACY: 0.1203
OOD Test Loss: 3.8776

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.3600
Train Loss: 1.8524
ID Validation ACCURACY: 0.3517
ID Validation Loss: 1.8640
ID Test ACCURACY: 0.3516
ID Test Loss: 1.8567
OOD Validation ACCURACY: 0.3029
OOD Validation Loss: 2.2550
OOD Test ACCURACY: 0.1203
OOD Test Loss: 3.8776

[0m[1;37mINFO[0m: [1mChartInfo 0.3516 0.1203 0.3516 0.1203 0.3517 0.3029[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.343
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.222
SUFF++ for r=0.6 class 0 = 0.363 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 1 = 0.657 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 2 = 0.386 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 3 = 0.386 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 4 = 0.402 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 5 = 0.387 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 6 = 0.409 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 7 = 0.393 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 8 = 0.39 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 9 = 0.409 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all KL = 0.374 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all L1 = 0.421 +- 0.132 (in-sample avg dev_std = 0.528)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.298
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.22
SUFF++ for r=0.6 class 0 = 0.352 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 1 = 0.762 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 2 = 0.361 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 3 = 0.361 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 4 = 0.403 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 5 = 0.373 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 6 = 0.419 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 7 = 0.395 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 8 = 0.382 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 9 = 0.429 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 all KL = 0.357 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 all L1 = 0.429 +- 0.180 (in-sample avg dev_std = 0.583)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.109
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.172
SUFF++ for r=0.6 class 0 = 0.347 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 1 = 0.239 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 2 = 0.349 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 3 = 0.329 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 4 = 0.251 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 5 = 0.338 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 6 = 0.263 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 7 = 0.319 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 8 = 0.3 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 9 = 0.245 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 all KL = 0.196 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 all L1 = 0.298 +- 0.119 (in-sample avg dev_std = 0.436)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.341
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.301
NEC for r=0.6 class 0 = 0.355 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 1 = 0.297 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 2 = 0.516 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 3 = 0.544 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 4 = 0.506 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 5 = 0.507 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 6 = 0.533 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 7 = 0.531 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 8 = 0.576 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 9 = 0.501 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 all KL = 0.399 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 all L1 = 0.484 +- 0.171 (in-sample avg dev_std = 0.209)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.296
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.277
NEC for r=0.6 class 0 = 0.371 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 1 = 0.188 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 2 = 0.544 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 3 = 0.57 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 4 = 0.546 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 5 = 0.532 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 6 = 0.555 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 7 = 0.594 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 8 = 0.569 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 9 = 0.549 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 all KL = 0.446 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 all L1 = 0.498 +- 0.196 (in-sample avg dev_std = 0.227)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.109
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.147
NEC for r=0.6 class 0 = 0.624 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 1 = 0.381 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 2 = 0.431 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 3 = 0.498 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 4 = 0.287 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 5 = 0.53 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 6 = 0.361 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 7 = 0.51 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 8 = 0.457 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 9 = 0.369 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 all KL = 0.402 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 all L1 = 0.444 +- 0.270 (in-sample avg dev_std = 0.198)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.305], 'all_L1': [0.421]}), defaultdict(<class 'list'>, {'all_KL': [0.672], 'all_L1': [0.583]}), defaultdict(<class 'list'>, {'all_KL': [0.374], 'all_L1': [0.421]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.455], 'all_L1': [0.488]}), defaultdict(<class 'list'>, {'all_KL': [0.337], 'all_L1': [0.45]}), defaultdict(<class 'list'>, {'all_KL': [0.399], 'all_L1': [0.484]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.368], 'all_L1': [0.477]}), defaultdict(<class 'list'>, {'all_KL': [0.705], 'all_L1': [0.583]}), defaultdict(<class 'list'>, {'all_KL': [0.357], 'all_L1': [0.429]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.531], 'all_L1': [0.498]}), defaultdict(<class 'list'>, {'all_KL': [0.367], 'all_L1': [0.461]}), defaultdict(<class 'list'>, {'all_KL': [0.446], 'all_L1': [0.498]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.529], 'all_L1': [0.663]}), defaultdict(<class 'list'>, {'all_KL': [0.653], 'all_L1': [0.509]}), defaultdict(<class 'list'>, {'all_KL': [0.196], 'all_L1': [0.298]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.656], 'all_L1': [0.508]}), defaultdict(<class 'list'>, {'all_KL': [0.529], 'all_L1': [0.559]}), defaultdict(<class 'list'>, {'all_KL': [0.402], 'all_L1': [0.444]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.475 +- 0.076
suff++ class all_KL  =  0.450 +- 0.159
suff++_acc_int  =  0.203 +- 0.018
nec class all_L1  =  0.474 +- 0.017
nec class all_KL  =  0.397 +- 0.048
nec_acc_int  =  0.300 +- 0.032

Eval split val
suff++ class all_L1  =  0.496 +- 0.064
suff++ class all_KL  =  0.477 +- 0.162
suff++_acc_int  =  0.189 +- 0.025
nec class all_L1  =  0.486 +- 0.017
nec class all_KL  =  0.448 +- 0.067
nec_acc_int  =  0.288 +- 0.010

Eval split test
suff++ class all_L1  =  0.490 +- 0.150
suff++ class all_KL  =  0.459 +- 0.193
suff++_acc_int  =  0.150 +- 0.017
nec class all_L1  =  0.504 +- 0.047
nec class all_KL  =  0.529 +- 0.104
nec_acc_int  =  0.206 +- 0.043


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.475 +- 0.030
Faith. Armon (L1)= 		  =  0.470 +- 0.027
Faith. GMean (L1)= 	  =  0.472 +- 0.028
Faith. Aritm (KL)= 		  =  0.424 +- 0.057
Faith. Armon (KL)= 		  =  0.400 +- 0.036
Faith. GMean (KL)= 	  =  0.412 +- 0.046

Eval split val
Faith. Aritm (L1)= 		  =  0.491 +- 0.024
Faith. Armon (L1)= 		  =  0.488 +- 0.022
Faith. GMean (L1)= 	  =  0.489 +- 0.023
Faith. Aritm (KL)= 		  =  0.462 +- 0.056
Faith. Armon (KL)= 		  =  0.438 +- 0.035
Faith. GMean (KL)= 	  =  0.450 +- 0.045

Eval split test
Faith. Aritm (L1)= 		  =  0.497 +- 0.091
Faith. Armon (L1)= 		  =  0.488 +- 0.095
Faith. GMean (L1)= 	  =  0.493 +- 0.093
Faith. Aritm (KL)= 		  =  0.494 +- 0.138
Faith. Armon (KL)= 		  =  0.478 +- 0.152
Faith. GMean (KL)= 	  =  0.486 +- 0.145
Computed for split load_split = id



Completed in  0:24:43.401758  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:37:28 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/18/2024 09:37:34 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 09:38:08 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:38:21 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:38:35 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:38:55 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:39:12 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.8993
Train Loss: 0.3214
ID Validation ROC-AUC: 0.8837
ID Validation Loss: 0.3373
ID Test ROC-AUC: 0.8843
ID Test Loss: 0.3454
OOD Validation ROC-AUC: 0.6640
OOD Validation Loss: 0.3458
OOD Test ROC-AUC: 0.7067
OOD Test Loss: 0.5877

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ROC-AUC: 0.8741
Train Loss: 0.3395
ID Validation ROC-AUC: 0.8720
ID Validation Loss: 0.3474
ID Test ROC-AUC: 0.8700
ID Test Loss: 0.3479
OOD Validation ROC-AUC: 0.7037
OOD Validation Loss: 0.3123
OOD Test ROC-AUC: 0.7160
OOD Test Loss: 0.5638

[0m[1;37mINFO[0m: [1mChartInfo 0.8843 0.7067 0.8700 0.7160 0.8720 0.7037[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/18/2024 09:39:20 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/18/2024 09:39:34 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/18/2024 09:39:42 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.395
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.408
SUFF++ for r=0.6 class 0.0 = 0.918 +- 0.015 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 class 1.0 = 0.924 +- 0.015 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 all KL = 0.99 +- 0.015 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 all L1 = 0.923 +- 0.038 (in-sample avg dev_std = 0.080)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.331
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.366
SUFF++ for r=0.6 class 0.0 = 0.911 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 class 1.0 = 0.925 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 all KL = 0.991 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 all L1 = 0.924 +- 0.034 (in-sample avg dev_std = 0.077)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.465
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.468
SUFF++ for r=0.6 class 0.0 = 0.925 +- 0.019 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 class 1.0 = 0.923 +- 0.019 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all KL = 0.99 +- 0.019 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all L1 = 0.924 +- 0.038 (in-sample avg dev_std = 0.079)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.395
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.382
NEC for r=0.6 class 0.0 = 0.023 +- 0.003 (in-sample avg dev_std = 0.018)
NEC for r=0.6 class 1.0 = 0.024 +- 0.003 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all L1 = 0.024 +- 0.017 (in-sample avg dev_std = 0.018)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.331
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.325
NEC for r=0.6 class 0.0 = 0.026 +- 0.003 (in-sample avg dev_std = 0.018)
NEC for r=0.6 class 1.0 = 0.023 +- 0.003 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all L1 = 0.024 +- 0.018 (in-sample avg dev_std = 0.018)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.465
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.454
NEC for r=0.6 class 0.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.018)
NEC for r=0.6 class 1.0 = 0.025 +- 0.004 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all KL = 0.001 +- 0.004 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all L1 = 0.025 +- 0.019 (in-sample avg dev_std = 0.018)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:41:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/18/2024 09:41:12 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 09:41:47 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:41:59 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:13 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:33 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:42:49 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 18...
[0m[1;37mINFO[0m: [1mCheckpoint 18: 
-----------------------------------
Train ROC-AUC: 0.9106
Train Loss: 0.2882
ID Validation ROC-AUC: 0.8956
ID Validation Loss: 0.3124
ID Test ROC-AUC: 0.8957
ID Test Loss: 0.3130
OOD Validation ROC-AUC: 0.6611
OOD Validation Loss: 0.3429
OOD Test ROC-AUC: 0.7016
OOD Test Loss: 0.5746

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ROC-AUC: 0.8841
Train Loss: 0.3105
ID Validation ROC-AUC: 0.8731
ID Validation Loss: 0.3250
ID Test ROC-AUC: 0.8715
ID Test Loss: 0.3287
OOD Validation ROC-AUC: 0.6962
OOD Validation Loss: 0.3128
OOD Test ROC-AUC: 0.7163
OOD Test Loss: 0.5402

[0m[1;37mINFO[0m: [1mChartInfo 0.8957 0.7016 0.8715 0.7163 0.8731 0.6962[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/18/2024 09:42:50 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/18/2024 09:42:59 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/18/2024 09:43:08 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.348
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.379
SUFF++ for r=0.6 class 0.0 = 0.929 +- 0.021 (in-sample avg dev_std = 0.066)
SUFF++ for r=0.6 class 1.0 = 0.946 +- 0.021 (in-sample avg dev_std = 0.066)
SUFF++ for r=0.6 all KL = 0.993 +- 0.021 (in-sample avg dev_std = 0.066)
SUFF++ for r=0.6 all L1 = 0.944 +- 0.038 (in-sample avg dev_std = 0.066)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.377
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.394
SUFF++ for r=0.6 class 0.0 = 0.934 +- 0.012 (in-sample avg dev_std = 0.055)
SUFF++ for r=0.6 class 1.0 = 0.945 +- 0.012 (in-sample avg dev_std = 0.055)
SUFF++ for r=0.6 all KL = 0.995 +- 0.012 (in-sample avg dev_std = 0.055)
SUFF++ for r=0.6 all L1 = 0.944 +- 0.034 (in-sample avg dev_std = 0.055)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.435
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.446
SUFF++ for r=0.6 class 0.0 = 0.941 +- 0.023 (in-sample avg dev_std = 0.059)
SUFF++ for r=0.6 class 1.0 = 0.944 +- 0.023 (in-sample avg dev_std = 0.059)
SUFF++ for r=0.6 all KL = 0.994 +- 0.023 (in-sample avg dev_std = 0.059)
SUFF++ for r=0.6 all L1 = 0.943 +- 0.039 (in-sample avg dev_std = 0.059)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.348
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.343
NEC for r=0.6 class 0.0 = 0.024 +- 0.007 (in-sample avg dev_std = 0.016)
NEC for r=0.6 class 1.0 = 0.018 +- 0.007 (in-sample avg dev_std = 0.016)
NEC for r=0.6 all KL = 0.001 +- 0.007 (in-sample avg dev_std = 0.016)
NEC for r=0.6 all L1 = 0.019 +- 0.021 (in-sample avg dev_std = 0.016)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.377
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.354
NEC for r=0.6 class 0.0 = 0.027 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.02 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.021 +- 0.020 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.435
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.435
NEC for r=0.6 class 0.0 = 0.021 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.021 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.021 +- 0.018 (in-sample avg dev_std = 0.017)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu Apr 18 21:44:32 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/18/2024 09:44:38 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/18/2024 09:45:13 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/18/2024 09:45:25 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/18/2024 09:45:39 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/18/2024 09:45:58 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 69...
[0m[1;37mINFO[0m: [1mCheckpoint 69: 
-----------------------------------
Train ROC-AUC: 0.9695
Train Loss: 0.2138
ID Validation ROC-AUC: 0.9127
ID Validation Loss: 0.3345
ID Test ROC-AUC: 0.9168
ID Test Loss: 0.3404
OOD Validation ROC-AUC: 0.6338
OOD Validation Loss: 0.4146
OOD Test ROC-AUC: 0.6908
OOD Test Loss: 0.6400

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 5...
[0m[1;37mINFO[0m: [1mCheckpoint 5: 
-----------------------------------
Train ROC-AUC: 0.8656
Train Loss: 0.2766
ID Validation ROC-AUC: 0.8601
ID Validation Loss: 0.2825
ID Test ROC-AUC: 0.8664
ID Test Loss: 0.2815
OOD Validation ROC-AUC: 0.6911
OOD Validation Loss: 0.2740
OOD Test ROC-AUC: 0.7053
OOD Test Loss: 0.4338

[0m[1;37mINFO[0m: [1mChartInfo 0.9168 0.6908 0.8664 0.7053 0.8601 0.6911[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/18/2024 09:46:15 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/18/2024 09:46:25 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/18/2024 09:46:33 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.786
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.717
SUFF++ for r=0.6 class 0.0 = 0.987 +- 0.010 (in-sample avg dev_std = 0.010)
SUFF++ for r=0.6 class 1.0 = 0.995 +- 0.010 (in-sample avg dev_std = 0.010)
SUFF++ for r=0.6 all KL = 0.998 +- 0.010 (in-sample avg dev_std = 0.010)
SUFF++ for r=0.6 all L1 = 0.994 +- 0.021 (in-sample avg dev_std = 0.010)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.605
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.57
SUFF++ for r=0.6 class 0.0 = 0.989 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 class 1.0 = 0.993 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all KL = 0.997 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all L1 = 0.993 +- 0.023 (in-sample avg dev_std = 0.013)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.686
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.655
SUFF++ for r=0.6 class 0.0 = 0.982 +- 0.009 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 class 1.0 = 0.993 +- 0.009 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all KL = 0.997 +- 0.009 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all L1 = 0.991 +- 0.023 (in-sample avg dev_std = 0.013)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.786
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.778
NEC for r=0.6 class 0.0 = 0.012 +- 0.009 (in-sample avg dev_std = 0.009)
NEC for r=0.6 class 1.0 = 0.004 +- 0.009 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all KL = 0.001 +- 0.009 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all L1 = 0.005 +- 0.021 (in-sample avg dev_std = 0.009)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.604
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.622
NEC for r=0.6 class 0.0 = 0.008 +- 0.011 (in-sample avg dev_std = 0.007)
NEC for r=0.6 class 1.0 = 0.004 +- 0.011 (in-sample avg dev_std = 0.007)
NEC for r=0.6 all KL = 0.001 +- 0.011 (in-sample avg dev_std = 0.007)
NEC for r=0.6 all L1 = 0.005 +- 0.023 (in-sample avg dev_std = 0.007)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.686
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.686
NEC for r=0.6 class 0.0 = 0.017 +- 0.011 (in-sample avg dev_std = 0.009)
NEC for r=0.6 class 1.0 = 0.006 +- 0.011 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all KL = 0.002 +- 0.011 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all L1 = 0.008 +- 0.026 (in-sample avg dev_std = 0.009)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.923]}), defaultdict(<class 'list'>, {'all_KL': [0.993], 'all_L1': [0.944]}), defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [0.994]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.024]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.019]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.005]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.924]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.944]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.993]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.024]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.021]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.005]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.924]}), defaultdict(<class 'list'>, {'all_KL': [0.994], 'all_L1': [0.943]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.991]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.021]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.008]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.954 +- 0.030
suff++ class all_KL  =  0.994 +- 0.003
suff++_acc_int  =  0.501 +- 0.153
nec class all_L1  =  0.016 +- 0.008
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.501 +- 0.196

Eval split val
suff++ class all_L1  =  0.954 +- 0.029
suff++ class all_KL  =  0.994 +- 0.002
suff++_acc_int  =  0.443 +- 0.090
nec class all_L1  =  0.017 +- 0.008
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.433 +- 0.134

Eval split test
suff++ class all_L1  =  0.953 +- 0.028
suff++ class all_KL  =  0.994 +- 0.003
suff++_acc_int  =  0.523 +- 0.094
nec class all_L1  =  0.018 +- 0.007
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.525 +- 0.114


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.485 +- 0.011
Faith. Armon (L1)= 		  =  0.031 +- 0.016
Faith. GMean (L1)= 	  =  0.118 +- 0.034
Faith. Aritm (KL)= 		  =  0.497 +- 0.002
Faith. Armon (KL)= 		  =  0.002 +- 0.000
Faith. GMean (KL)= 	  =  0.032 +- 0.000

Eval split val
Faith. Aritm (L1)= 		  =  0.485 +- 0.010
Faith. Armon (L1)= 		  =  0.033 +- 0.016
Faith. GMean (L1)= 	  =  0.120 +- 0.035
Faith. Aritm (KL)= 		  =  0.498 +- 0.001
Faith. Armon (KL)= 		  =  0.002 +- 0.000
Faith. GMean (KL)= 	  =  0.032 +- 0.000

Eval split test
Faith. Aritm (L1)= 		  =  0.485 +- 0.010
Faith. Armon (L1)= 		  =  0.035 +- 0.014
Faith. GMean (L1)= 	  =  0.127 +- 0.027
Faith. Aritm (KL)= 		  =  0.498 +- 0.002
Faith. Armon (KL)= 		  =  0.003 +- 0.001
Faith. GMean (KL)= 	  =  0.036 +- 0.006
Computed for split load_split = id



Completed in  0:10:40.799682  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay
DONE all :)
