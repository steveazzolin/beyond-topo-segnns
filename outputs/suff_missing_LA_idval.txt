nohup: ignoring input
Time to compute metrics!
The PID of this script is: 351125
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:13:17 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:19 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 163...
[0m[1;37mINFO[0m: [1mCheckpoint 163: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0753
ID Validation ACCURACY: 0.8595
ID Validation Loss: 0.7797
ID Test ACCURACY: 0.8602
ID Test Loss: 0.8691
OOD Validation ACCURACY: 0.8551
OOD Validation Loss: 1.0470
OOD Test ACCURACY: 0.8144
OOD Test Loss: 1.2121

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 118...
[0m[1;37mINFO[0m: [1mCheckpoint 118: 
-----------------------------------
Train ACCURACY: 0.9463
Train Loss: 0.0867
ID Validation ACCURACY: 0.8510
ID Validation Loss: 0.5780
ID Test ACCURACY: 0.8459
ID Test Loss: 0.6869
OOD Validation ACCURACY: 0.8562
OOD Validation Loss: 0.6896
OOD Test ACCURACY: 0.8071
OOD Test Loss: 0.8209

[0m[1;37mINFO[0m: [1mChartInfo 0.8602 0.8144 0.8459 0.8071 0.8510 0.8562[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:13:20 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.86
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.831
SUFF++ for r=0.8 class 0.0 = 0.936 +- 0.244 (in-sample avg dev_std = 0.279)
SUFF++ for r=0.8 class 1.0 = 0.89 +- 0.244 (in-sample avg dev_std = 0.279)
SUFF++ for r=0.8 all KL = 0.847 +- 0.244 (in-sample avg dev_std = 0.279)
SUFF++ for r=0.8 all L1 = 0.909 +- 0.155 (in-sample avg dev_std = 0.279)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:13:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:32 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:33 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 181...
[0m[1;37mINFO[0m: [1mCheckpoint 181: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8589
ID Validation Loss: 1.0551
ID Test ACCURACY: 0.8568
ID Test Loss: 1.1414
OOD Validation ACCURACY: 0.8585
OOD Validation Loss: 1.4193
OOD Test ACCURACY: 0.8148
OOD Test Loss: 1.5478

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 181...
[0m[1;37mINFO[0m: [1mCheckpoint 181: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8589
ID Validation Loss: 1.0551
ID Test ACCURACY: 0.8568
ID Test Loss: 1.1414
OOD Validation ACCURACY: 0.8585
OOD Validation Loss: 1.4193
OOD Test ACCURACY: 0.8148
OOD Test Loss: 1.5478

[0m[1;37mINFO[0m: [1mChartInfo 0.8568 0.8148 0.8568 0.8148 0.8589 0.8585[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:13:33 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.86
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.831
SUFF++ for r=0.8 class 0.0 = 0.912 +- 0.278 (in-sample avg dev_std = 0.299)
SUFF++ for r=0.8 class 1.0 = 0.909 +- 0.278 (in-sample avg dev_std = 0.299)
SUFF++ for r=0.8 all KL = 0.846 +- 0.278 (in-sample avg dev_std = 0.299)
SUFF++ for r=0.8 all L1 = 0.91 +- 0.179 (in-sample avg dev_std = 0.299)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:13:41 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 140...
[0m[1;37mINFO[0m: [1mCheckpoint 140: 
-----------------------------------
Train ACCURACY: 0.9486
Train Loss: 0.0789
ID Validation ACCURACY: 0.8576
ID Validation Loss: 0.6405
ID Test ACCURACY: 0.8581
ID Test Loss: 0.6827
OOD Validation ACCURACY: 0.8483
OOD Validation Loss: 0.8788
OOD Test ACCURACY: 0.7888
OOD Test Loss: 1.2864

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 60...
[0m[1;37mINFO[0m: [1mCheckpoint 60: 
-----------------------------------
Train ACCURACY: 0.9444
Train Loss: 0.0955
ID Validation ACCURACY: 0.8453
ID Validation Loss: 0.4763
ID Test ACCURACY: 0.8466
ID Test Loss: 0.5186
OOD Validation ACCURACY: 0.8569
OOD Validation Loss: 0.5734
OOD Test ACCURACY: 0.7985
OOD Test Loss: 0.8428

[0m[1;37mINFO[0m: [1mChartInfo 0.8581 0.7888 0.8466 0.7985 0.8453 0.8569[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:13:43 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.87
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.85
SUFF++ for r=0.8 class 0.0 = 0.848 +- 0.237 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 class 1.0 = 0.942 +- 0.237 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all KL = 0.85 +- 0.237 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all L1 = 0.903 +- 0.148 (in-sample avg dev_std = 0.306)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:13:52 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 180...
[0m[1;37mINFO[0m: [1mCheckpoint 180: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8555
ID Validation Loss: 1.0889
ID Test ACCURACY: 0.8591
ID Test Loss: 1.1779
OOD Validation ACCURACY: 0.8291
OOD Validation Loss: 1.8486
OOD Test ACCURACY: 0.7282
OOD Test Loss: 3.5316

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 109...
[0m[1;37mINFO[0m: [1mCheckpoint 109: 
-----------------------------------
Train ACCURACY: 0.9418
Train Loss: 0.0951
ID Validation ACCURACY: 0.8398
ID Validation Loss: 0.7123
ID Test ACCURACY: 0.8421
ID Test Loss: 0.8161
OOD Validation ACCURACY: 0.8581
OOD Validation Loss: 0.8140
OOD Test ACCURACY: 0.8226
OOD Test Loss: 0.9192

[0m[1;37mINFO[0m: [1mChartInfo 0.8591 0.7282 0.8421 0.8226 0.8398 0.8581[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:13:54 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.829
SUFF++ for r=0.8 class 0.0 = 0.832 +- 0.314 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 class 1.0 = 0.963 +- 0.314 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 all KL = 0.809 +- 0.314 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 all L1 = 0.909 +- 0.165 (in-sample avg dev_std = 0.330)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:14:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 186...
[0m[1;37mINFO[0m: [1mCheckpoint 186: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8600
ID Validation Loss: 0.9380
ID Test ACCURACY: 0.8612
ID Test Loss: 1.0013
OOD Validation ACCURACY: 0.8654
OOD Validation Loss: 1.0363
OOD Test ACCURACY: 0.8221
OOD Test Loss: 1.1633

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 186...
[0m[1;37mINFO[0m: [1mCheckpoint 186: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8600
ID Validation Loss: 0.9380
ID Test ACCURACY: 0.8612
ID Test Loss: 1.0013
OOD Validation ACCURACY: 0.8654
OOD Validation Loss: 1.0363
OOD Test ACCURACY: 0.8221
OOD Test Loss: 1.1633

[0m[1;37mINFO[0m: [1mChartInfo 0.8612 0.8221 0.8612 0.8221 0.8600 0.8654[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:14:05 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.877
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.865
SUFF++ for r=0.8 class 0.0 = 0.896 +- 0.312 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 class 1.0 = 0.913 +- 0.312 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 all KL = 0.815 +- 0.312 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 all L1 = 0.906 +- 0.182 (in-sample avg dev_std = 0.333)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.847], 'all_L1': [0.909]}), defaultdict(<class 'list'>, {'all_KL': [0.846], 'all_L1': [0.91]}), defaultdict(<class 'list'>, {'all_KL': [0.85], 'all_L1': [0.903]}), defaultdict(<class 'list'>, {'all_KL': [0.809], 'all_L1': [0.909]}), defaultdict(<class 'list'>, {'all_KL': [0.815], 'all_L1': [0.906]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.907 +- 0.003
suff++ class all_KL  =  0.833 +- 0.018
suff++_acc_int  =  0.841 +- 0.014


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Computed for split load_split = id



Completed in  0:01:00.028996  for CIGAGIN GOODSST2/length



DONE CIGA GOODSST2/length all mitig
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:14:32 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:14:34 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 171...
[0m[1;37mINFO[0m: [1mCheckpoint 171: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.9108
ID Validation Loss: 0.6777
ID Test ACCURACY: 0.9070
ID Test Loss: 0.7678
OOD Validation ACCURACY: 0.8696
OOD Validation Loss: 0.8953
OOD Test ACCURACY: 0.8189
OOD Test Loss: 1.1731

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 172...
[0m[1;37mINFO[0m: [1mCheckpoint 172: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.9085
ID Validation Loss: 0.6426
ID Test ACCURACY: 0.9093
ID Test Loss: 0.7163
OOD Validation ACCURACY: 0.8700
OOD Validation Loss: 0.8695
OOD Test ACCURACY: 0.8171
OOD Test Loss: 1.1992

[0m[1;37mINFO[0m: [1mChartInfo 0.9070 0.8189 0.9093 0.8171 0.9085 0.8700[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:14:35 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.884
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.834
SUFF++ for r=0.6 class 0.0 = 0.865 +- 0.326 (in-sample avg dev_std = 0.424)
SUFF++ for r=0.6 class 1.0 = 0.83 +- 0.326 (in-sample avg dev_std = 0.424)
SUFF++ for r=0.6 all KL = 0.712 +- 0.326 (in-sample avg dev_std = 0.424)
SUFF++ for r=0.6 all L1 = 0.845 +- 0.201 (in-sample avg dev_std = 0.424)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.872
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.863
SUFF++ for r=0.9 class 0.0 = 0.937 +- 0.144 (in-sample avg dev_std = 0.114)
SUFF++ for r=0.9 class 1.0 = 0.965 +- 0.144 (in-sample avg dev_std = 0.114)
SUFF++ for r=0.9 all KL = 0.957 +- 0.144 (in-sample avg dev_std = 0.114)
SUFF++ for r=0.9 all L1 = 0.952 +- 0.135 (in-sample avg dev_std = 0.114)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:14:50 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:14:51 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 189...
[0m[1;37mINFO[0m: [1mCheckpoint 189: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.9106
ID Validation Loss: 0.6509
ID Test ACCURACY: 0.9106
ID Test Loss: 0.7601
OOD Validation ACCURACY: 0.8655
OOD Validation Loss: 0.9153
OOD Test ACCURACY: 0.8004
OOD Test Loss: 1.3538

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 196...
[0m[1;37mINFO[0m: [1mCheckpoint 196: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.9081
ID Validation Loss: 0.6904
ID Test ACCURACY: 0.9102
ID Test Loss: 0.8084
OOD Validation ACCURACY: 0.8689
OOD Validation Loss: 1.0019
OOD Test ACCURACY: 0.8170
OOD Test Loss: 1.3197

[0m[1;37mINFO[0m: [1mChartInfo 0.9106 0.8004 0.9102 0.8170 0.9081 0.8689[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:14:52 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.884
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.802
SUFF++ for r=0.6 class 0.0 = 0.773 +- 0.381 (in-sample avg dev_std = 0.578)
SUFF++ for r=0.6 class 1.0 = 0.778 +- 0.381 (in-sample avg dev_std = 0.578)
SUFF++ for r=0.6 all KL = 0.521 +- 0.381 (in-sample avg dev_std = 0.578)
SUFF++ for r=0.6 all L1 = 0.776 +- 0.202 (in-sample avg dev_std = 0.578)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.894
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.893
SUFF++ for r=0.9 class 0.0 = 0.947 +- 0.136 (in-sample avg dev_std = 0.107)
SUFF++ for r=0.9 class 1.0 = 0.974 +- 0.136 (in-sample avg dev_std = 0.107)
SUFF++ for r=0.9 all KL = 0.966 +- 0.136 (in-sample avg dev_std = 0.107)
SUFF++ for r=0.9 all L1 = 0.962 +- 0.114 (in-sample avg dev_std = 0.107)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:15:08 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:09 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:15:10 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 181...
[0m[1;37mINFO[0m: [1mCheckpoint 181: 
-----------------------------------
Train ACCURACY: 0.9999
Train Loss: 0.0003
ID Validation ACCURACY: 0.9083
ID Validation Loss: 0.6573
ID Test ACCURACY: 0.9010
ID Test Loss: 0.7687
OOD Validation ACCURACY: 0.8624
OOD Validation Loss: 0.9332
OOD Test ACCURACY: 0.7946
OOD Test Loss: 1.4098

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 144...
[0m[1;37mINFO[0m: [1mCheckpoint 144: 
-----------------------------------
Train ACCURACY: 0.9986
Train Loss: 0.0061
ID Validation ACCURACY: 0.9002
ID Validation Loss: 0.5441
ID Test ACCURACY: 0.9019
ID Test Loss: 0.6312
OOD Validation ACCURACY: 0.8690
OOD Validation Loss: 0.7354
OOD Test ACCURACY: 0.8188
OOD Test Loss: 0.9821

[0m[1;37mINFO[0m: [1mChartInfo 0.9010 0.7946 0.9019 0.8188 0.9002 0.8690[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:15:10 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.86
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.8
SUFF++ for r=0.6 class 0.0 = 0.728 +- 0.359 (in-sample avg dev_std = 0.429)
SUFF++ for r=0.6 class 1.0 = 0.934 +- 0.359 (in-sample avg dev_std = 0.429)
SUFF++ for r=0.6 all KL = 0.711 +- 0.359 (in-sample avg dev_std = 0.429)
SUFF++ for r=0.6 all L1 = 0.848 +- 0.209 (in-sample avg dev_std = 0.429)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.9
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.877
SUFF++ for r=0.9 class 0.0 = 0.913 +- 0.135 (in-sample avg dev_std = 0.142)
SUFF++ for r=0.9 class 1.0 = 0.966 +- 0.135 (in-sample avg dev_std = 0.142)
SUFF++ for r=0.9 all KL = 0.954 +- 0.135 (in-sample avg dev_std = 0.142)
SUFF++ for r=0.9 all L1 = 0.943 +- 0.146 (in-sample avg dev_std = 0.142)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:15:24 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 189...
[0m[1;37mINFO[0m: [1mCheckpoint 189: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.9123
ID Validation Loss: 0.7334
ID Test ACCURACY: 0.9047
ID Test Loss: 0.8153
OOD Validation ACCURACY: 0.8584
OOD Validation Loss: 1.0858
OOD Test ACCURACY: 0.8021
OOD Test Loss: 1.6001

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 192...
[0m[1;37mINFO[0m: [1mCheckpoint 192: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.9091
ID Validation Loss: 0.7365
ID Test ACCURACY: 0.9076
ID Test Loss: 0.8317
OOD Validation ACCURACY: 0.8656
OOD Validation Loss: 1.0826
OOD Test ACCURACY: 0.8208
OOD Test Loss: 1.3300

[0m[1;37mINFO[0m: [1mChartInfo 0.9047 0.8021 0.9076 0.8208 0.9091 0.8656[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:15:26 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.846
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.778
SUFF++ for r=0.6 class 0.0 = 0.703 +- 0.386 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 1.0 = 0.894 +- 0.386 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 all KL = 0.625 +- 0.386 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 all L1 = 0.814 +- 0.216 (in-sample avg dev_std = 0.490)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.878
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.878
SUFF++ for r=0.9 class 0.0 = 0.945 +- 0.129 (in-sample avg dev_std = 0.126)
SUFF++ for r=0.9 class 1.0 = 0.972 +- 0.129 (in-sample avg dev_std = 0.126)
SUFF++ for r=0.9 all KL = 0.957 +- 0.129 (in-sample avg dev_std = 0.126)
SUFF++ for r=0.9 all L1 = 0.96 +- 0.120 (in-sample avg dev_std = 0.126)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:15:41 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 179...
[0m[1;37mINFO[0m: [1mCheckpoint 179: 
-----------------------------------
Train ACCURACY: 0.9999
Train Loss: 0.0002
ID Validation ACCURACY: 0.9125
ID Validation Loss: 0.6781
ID Test ACCURACY: 0.9078
ID Test Loss: 0.7621
OOD Validation ACCURACY: 0.8649
OOD Validation Loss: 0.8836
OOD Test ACCURACY: 0.8196
OOD Test Loss: 1.0439

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 141...
[0m[1;37mINFO[0m: [1mCheckpoint 141: 
-----------------------------------
Train ACCURACY: 0.9983
Train Loss: 0.0070
ID Validation ACCURACY: 0.9044
ID Validation Loss: 0.4896
ID Test ACCURACY: 0.9053
ID Test Loss: 0.5898
OOD Validation ACCURACY: 0.8681
OOD Validation Loss: 0.7049
OOD Test ACCURACY: 0.8276
OOD Test Loss: 0.8462

[0m[1;37mINFO[0m: [1mChartInfo 0.9078 0.8196 0.9053 0.8276 0.9044 0.8681[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/21/2024 03:15:43 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.849
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.789
SUFF++ for r=0.6 class 0.0 = 0.748 +- 0.377 (in-sample avg dev_std = 0.483)
SUFF++ for r=0.6 class 1.0 = 0.857 +- 0.377 (in-sample avg dev_std = 0.483)
SUFF++ for r=0.6 all KL = 0.607 +- 0.377 (in-sample avg dev_std = 0.483)
SUFF++ for r=0.6 all L1 = 0.812 +- 0.201 (in-sample avg dev_std = 0.483)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.9
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.881
SUFF++ for r=0.9 class 0.0 = 0.934 +- 0.155 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.9 class 1.0 = 0.953 +- 0.155 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.9 all KL = 0.951 +- 0.155 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.9 all L1 = 0.945 +- 0.144 (in-sample avg dev_std = 0.128)


ratio=1.0



Zero intervened samples, skipping weight=1.0


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.712, 0.957, 1.0], 'all_L1': [0.845, 0.952, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.521, 0.966, 1.0], 'all_L1': [0.776, 0.962, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.711, 0.954, 1.0], 'all_L1': [0.848, 0.943, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.625, 0.957, 1.0], 'all_L1': [0.814, 0.96, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.607, 0.951, 1.0], 'all_L1': [0.812, 0.945, 1.0], 0.0: [1.0], 1.0: [1.0]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.819 +- 0.026, 0.952 +- 0.008, 1.000 +- 0.000
suff++ class all_KL  =  0.635 +- 0.072, 0.957 +- 0.005, 1.000 +- 0.000
suff++_acc_int  =  0.800 +- 0.019, 0.878 +- 0.010


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Computed for split load_split = id



Completed in  0:01:31.098187  for GSATGIN GOODSST2/length



DONE GSAT GOODSST2/length all mitig
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:16:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 03:16:20 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 03:17:07 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 03:17:24 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 03:17:41 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:07 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:33 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:33 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:33 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:33 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:33 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:18:34 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 60...
[0m[1;37mINFO[0m: [1mCheckpoint 60: 
-----------------------------------
Train ROC-AUC: 0.9644
Train Loss: 0.3162
ID Validation ROC-AUC: 0.9159
ID Validation Loss: 0.4976
ID Test ROC-AUC: 0.9149
ID Test Loss: 0.5133
OOD Validation ROC-AUC: 0.6062
OOD Validation Loss: 0.7427
OOD Test ROC-AUC: 0.6845
OOD Test Loss: 1.0938

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ROC-AUC: 0.9089
Train Loss: 0.3173
ID Validation ROC-AUC: 0.8910
ID Validation Loss: 0.3404
ID Test ROC-AUC: 0.8888
ID Test Loss: 0.3479
OOD Validation ROC-AUC: 0.6543
OOD Validation Loss: 0.3422
OOD Test ROC-AUC: 0.6990
OOD Test Loss: 0.5882

[0m[1;37mINFO[0m: [1mChartInfo 0.9149 0.6845 0.8888 0.6990 0.8910 0.6543[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 03:18:35 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


[1;31mERROR[0m: 09/21/2024 03:19:12 PM - utils.py - line 87 : [1mTraceback (most recent call last):
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/bin/goodtg", line 33, in <module>
    sys.exit(load_entry_point('graph-ood', 'console_scripts', 'goodtg')())
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 1020, in goodtg
    main()
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 815, in main
    evaluate_metric(args)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 610, in evaluate_metric
    score, acc_int, results = pipeline.compute_metric_ratio(
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 1117, in compute_metric_ratio
    int_dataset = CustomDataset("", eval_samples, belonging)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 122, in __init__
    self.data, self.slices = self.collate(data_list)
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py", line 139, in collate
    data, slices, _ = collate(
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/collate.py", line 78, in collate
    values = [store[attr] for store in stores]
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/collate.py", line 78, in <listcomp>
    values = [store[attr] for store in stores]
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/storage.py", line 111, in __getitem__
    return self._mapping[key]
KeyError: 'edge_attr'
[0mTime to compute metrics!
The PID of this script is: 356015
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:26:42 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:26:43 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 90...
[0m[1;37mINFO[0m: [1mCheckpoint 90: 
-----------------------------------
Train ACCURACY: 0.2997
Train Loss: 3.3387
ID Validation ACCURACY: 0.2909
ID Validation Loss: 3.4073
ID Test ACCURACY: 0.2920
ID Test Loss: 3.3954
OOD Validation ACCURACY: 0.2336
OOD Validation Loss: 5.2601
OOD Test ACCURACY: 0.1400
OOD Test Loss: 6.1619

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 90...
[0m[1;37mINFO[0m: [1mCheckpoint 90: 
-----------------------------------
Train ACCURACY: 0.2997
Train Loss: 3.3387
ID Validation ACCURACY: 0.2909
ID Validation Loss: 3.4073
ID Test ACCURACY: 0.2920
ID Test Loss: 3.3954
OOD Validation ACCURACY: 0.2336
OOD Validation Loss: 5.2601
OOD Test ACCURACY: 0.1400
OOD Test Loss: 6.1619

[0m[1;37mINFO[0m: [1mChartInfo 0.2920 0.1400 0.2920 0.1400 0.2909 0.2336[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.284
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.174
SUFF++ for r=0.6 class 0 = 0.314 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 1 = 0.339 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 2 = 0.222 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 3 = 0.239 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 4 = 0.29 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 5 = 0.25 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 6 = 0.377 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 7 = 0.295 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 8 = 0.244 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 9 = 0.286 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 all KL = 0.153 +- 0.198 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 all L1 = 0.286 +- 0.123 (in-sample avg dev_std = 0.575)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:29:12 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:12 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:29:13 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 70...
[0m[1;37mINFO[0m: [1mCheckpoint 70: 
-----------------------------------
Train ACCURACY: 0.6581
Train Loss: 0.9910
ID Validation ACCURACY: 0.6360
ID Validation Loss: 1.0899
ID Test ACCURACY: 0.6253
ID Test Loss: 1.1128
OOD Validation ACCURACY: 0.4889
OOD Validation Loss: 1.6348
OOD Test ACCURACY: 0.2900
OOD Test Loss: 2.5498

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 76...
[0m[1;37mINFO[0m: [1mCheckpoint 76: 
-----------------------------------
Train ACCURACY: 0.5949
Train Loss: 1.2307
ID Validation ACCURACY: 0.5809
ID Validation Loss: 1.3357
ID Test ACCURACY: 0.5690
ID Test Loss: 1.3701
OOD Validation ACCURACY: 0.5664
OOD Validation Loss: 1.3707
OOD Test ACCURACY: 0.3381
OOD Test Loss: 2.4209

[0m[1;37mINFO[0m: [1mChartInfo 0.6253 0.2900 0.5690 0.3381 0.5809 0.5664[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.64
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.263
SUFF++ for r=0.6 class 0 = 0.273 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 1 = 0.38 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 2 = 0.276 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 3 = 0.268 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 4 = 0.31 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 5 = 0.276 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 6 = 0.304 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 7 = 0.4 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 8 = 0.285 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 class 9 = 0.309 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 all KL = 0.166 +- 0.158 (in-sample avg dev_std = 0.573)
SUFF++ for r=0.6 all L1 = 0.31 +- 0.101 (in-sample avg dev_std = 0.573)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:32:27 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:28 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:32:29 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.6698
Train Loss: 0.9467
ID Validation ACCURACY: 0.6491
ID Validation Loss: 1.0314
ID Test ACCURACY: 0.6391
ID Test Loss: 1.0585
OOD Validation ACCURACY: 0.5524
OOD Validation Loss: 1.4077
OOD Test ACCURACY: 0.3830
OOD Test Loss: 2.2401

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 73...
[0m[1;37mINFO[0m: [1mCheckpoint 73: 
-----------------------------------
Train ACCURACY: 0.6334
Train Loss: 1.1120
ID Validation ACCURACY: 0.6076
ID Validation Loss: 1.2323
ID Test ACCURACY: 0.6007
ID Test Loss: 1.2595
OOD Validation ACCURACY: 0.5951
OOD Validation Loss: 1.2541
OOD Test ACCURACY: 0.3426
OOD Test Loss: 2.3810

[0m[1;37mINFO[0m: [1mChartInfo 0.6391 0.3830 0.6007 0.3426 0.6076 0.5951[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.642
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.261
SUFF++ for r=0.6 class 0 = 0.298 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 1 = 0.461 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 2 = 0.277 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 3 = 0.247 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 4 = 0.27 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 5 = 0.257 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 6 = 0.245 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 7 = 0.389 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 8 = 0.24 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 9 = 0.279 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all KL = 0.141 +- 0.165 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all L1 = 0.3 +- 0.130 (in-sample avg dev_std = 0.528)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:35:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:36:01 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.5377
Train Loss: 1.5075
ID Validation ACCURACY: 0.5361
ID Validation Loss: 1.5713
ID Test ACCURACY: 0.5206
ID Test Loss: 1.6479
OOD Validation ACCURACY: 0.4199
OOD Validation Loss: 2.4982
OOD Test ACCURACY: 0.3477
OOD Test Loss: 3.0782

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 52...
[0m[1;37mINFO[0m: [1mCheckpoint 52: 
-----------------------------------
Train ACCURACY: 0.4893
Train Loss: 1.5757
ID Validation ACCURACY: 0.4821
ID Validation Loss: 1.6006
ID Test ACCURACY: 0.4747
ID Test Loss: 1.6438
OOD Validation ACCURACY: 0.4277
OOD Validation Loss: 1.9595
OOD Test ACCURACY: 0.3373
OOD Test Loss: 2.5935

[0m[1;37mINFO[0m: [1mChartInfo 0.5206 0.3477 0.4747 0.3373 0.4821 0.4277[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.504
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.23
SUFF++ for r=0.6 class 0 = 0.459 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 1 = 0.24 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 2 = 0.266 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 3 = 0.355 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 4 = 0.249 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 5 = 0.313 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 6 = 0.273 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 7 = 0.25 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 8 = 0.306 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 9 = 0.257 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 all KL = 0.114 +- 0.147 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 all L1 = 0.296 +- 0.125 (in-sample avg dev_std = 0.549)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:39:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:36 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 03:39:37 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.5967
Train Loss: 1.3782
ID Validation ACCURACY: 0.5773
ID Validation Loss: 1.5029
ID Test ACCURACY: 0.5697
ID Test Loss: 1.5503
OOD Validation ACCURACY: 0.4626
OOD Validation Loss: 2.5270
OOD Test ACCURACY: 0.3526
OOD Test Loss: 8.1775

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.5321
Train Loss: 1.5191
ID Validation ACCURACY: 0.5236
ID Validation Loss: 1.5860
ID Test ACCURACY: 0.5176
ID Test Loss: 1.6001
OOD Validation ACCURACY: 0.4831
OOD Validation Loss: 1.8222
OOD Test ACCURACY: 0.3497
OOD Test Loss: 9.4069

[0m[1;37mINFO[0m: [1mChartInfo 0.5697 0.3526 0.5176 0.3497 0.5236 0.4831[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.574
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.282
SUFF++ for r=0.6 class 0 = 0.283 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 1 = 0.658 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 2 = 0.242 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 3 = 0.271 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 4 = 0.251 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 5 = 0.28 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 6 = 0.262 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 7 = 0.25 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 8 = 0.256 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 class 9 = 0.245 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 all KL = 0.059 +- 0.141 (in-sample avg dev_std = 0.674)
SUFF++ for r=0.6 all L1 = 0.305 +- 0.154 (in-sample avg dev_std = 0.674)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.153], 'all_L1': [0.286]}), defaultdict(<class 'list'>, {'all_KL': [0.166], 'all_L1': [0.31]}), defaultdict(<class 'list'>, {'all_KL': [0.141], 'all_L1': [0.3]}), defaultdict(<class 'list'>, {'all_KL': [0.114], 'all_L1': [0.296]}), defaultdict(<class 'list'>, {'all_KL': [0.059], 'all_L1': [0.305]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.299 +- 0.008
suff++ class all_KL  =  0.127 +- 0.038
suff++_acc_int  =  0.242 +- 0.038


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Computed for split load_split = id



Completed in  0:15:26.643541  for CIGAGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color all mitig
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:42:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:42:22 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 168...
[0m[1;37mINFO[0m: [1mCheckpoint 168: 
-----------------------------------
Train ACCURACY: 0.9710
Train Loss: 0.0888
ID Validation ACCURACY: 0.8913
ID Validation Loss: 0.3578
ID Test ACCURACY: 0.8891
ID Test Loss: 0.3689
OOD Validation ACCURACY: 0.7710
OOD Validation Loss: 0.8283
OOD Test ACCURACY: 0.2787
OOD Test Loss: 5.7327

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 175...
[0m[1;37mINFO[0m: [1mCheckpoint 175: 
-----------------------------------
Train ACCURACY: 0.9678
Train Loss: 0.0980
ID Validation ACCURACY: 0.8826
ID Validation Loss: 0.3955
ID Test ACCURACY: 0.8830
ID Test Loss: 0.3916
OOD Validation ACCURACY: 0.7823
OOD Validation Loss: 0.8443
OOD Test ACCURACY: 0.3099
OOD Test Loss: 7.0050

[0m[1;37mINFO[0m: [1mChartInfo 0.8891 0.2787 0.8830 0.3099 0.8826 0.7823[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.104
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.113
SUFF++ for r=0.3 class 0 = 0.547 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 1 = 0.495 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 2 = 0.584 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 3 = 0.615 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 4 = 0.581 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 5 = 0.688 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 6 = 0.588 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 7 = 0.504 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 8 = 0.51 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 class 9 = 0.616 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 all KL = 0.465 +- 0.310 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.3 all L1 = 0.569 +- 0.268 (in-sample avg dev_std = 0.387)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.203
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.137
SUFF++ for r=0.6 class 0 = 0.35 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 1 = 0.416 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 2 = 0.487 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 3 = 0.472 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 4 = 0.383 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 5 = 0.385 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 6 = 0.436 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 7 = 0.579 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 8 = 0.356 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 class 9 = 0.446 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 all KL = 0.329 +- 0.267 (in-sample avg dev_std = 0.484)
SUFF++ for r=0.6 all L1 = 0.434 +- 0.211 (in-sample avg dev_std = 0.484)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.762
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.369
SUFF++ for r=0.9 class 0 = 0.292 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 1 = 0.324 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 2 = 0.309 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 3 = 0.338 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 4 = 0.551 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 5 = 0.286 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 6 = 0.316 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 7 = 0.751 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 8 = 0.331 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 class 9 = 0.325 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 all KL = 0.185 +- 0.267 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.9 all L1 = 0.386 +- 0.216 (in-sample avg dev_std = 0.601)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 15:52:55 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:56 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:56 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 03:52:57 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 184...
[0m[1;37mINFO[0m: [1mCheckpoint 184: 
-----------------------------------
Train ACCURACY: 0.9697
Train Loss: 0.0910
ID Validation ACCURACY: 0.8901
ID Validation Loss: 0.3680
ID Test ACCURACY: 0.8897
ID Test Loss: 0.3824
OOD Validation ACCURACY: 0.7509
OOD Validation Loss: 1.0322
OOD Test ACCURACY: 0.2683
OOD Test Loss: 5.3981

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 168...
[0m[1;37mINFO[0m: [1mCheckpoint 168: 
-----------------------------------
Train ACCURACY: 0.9635
Train Loss: 0.1083
ID Validation ACCURACY: 0.8834
ID Validation Loss: 0.3827
ID Test ACCURACY: 0.8824
ID Test Loss: 0.3845
OOD Validation ACCURACY: 0.7814
OOD Validation Loss: 0.9042
OOD Test ACCURACY: 0.3949
OOD Test Loss: 2.9563

[0m[1;37mINFO[0m: [1mChartInfo 0.8897 0.2683 0.8824 0.3949 0.8834 0.7814[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.09
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.106
SUFF++ for r=0.3 class 0 = 0.476 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 1 = 0.817 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 2 = 0.443 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 3 = 0.453 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 4 = 0.424 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 5 = 0.52 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 6 = 0.457 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 7 = 0.467 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 8 = 0.444 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 class 9 = 0.545 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 all KL = 0.41 +- 0.311 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.3 all L1 = 0.508 +- 0.257 (in-sample avg dev_std = 0.529)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.185
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.143
SUFF++ for r=0.6 class 0 = 0.262 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 1 = 0.717 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 2 = 0.514 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 3 = 0.478 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 4 = 0.394 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 5 = 0.348 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 6 = 0.431 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 7 = 0.587 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 8 = 0.426 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 class 9 = 0.427 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 all KL = 0.349 +- 0.296 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.6 all L1 = 0.466 +- 0.254 (in-sample avg dev_std = 0.475)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.66
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.299
SUFF++ for r=0.9 class 0 = 0.252 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 1 = 0.309 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 2 = 0.327 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 3 = 0.28 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 4 = 0.447 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 5 = 0.292 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 6 = 0.276 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 7 = 0.758 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 8 = 0.314 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 class 9 = 0.389 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 all KL = 0.21 +- 0.278 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.9 all L1 = 0.368 +- 0.219 (in-sample avg dev_std = 0.545)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 16:01:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:35 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 04:01:36 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 158...
[0m[1;37mINFO[0m: [1mCheckpoint 158: 
-----------------------------------
Train ACCURACY: 0.9624
Train Loss: 0.1081
ID Validation ACCURACY: 0.8889
ID Validation Loss: 0.3704
ID Test ACCURACY: 0.8846
ID Test Loss: 0.3894
OOD Validation ACCURACY: 0.7016
OOD Validation Loss: 1.3742
OOD Test ACCURACY: 0.3063
OOD Test Loss: 4.9057

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 166...
[0m[1;37mINFO[0m: [1mCheckpoint 166: 
-----------------------------------
Train ACCURACY: 0.9646
Train Loss: 0.1029
ID Validation ACCURACY: 0.8820
ID Validation Loss: 0.3984
ID Test ACCURACY: 0.8826
ID Test Loss: 0.4026
OOD Validation ACCURACY: 0.7703
OOD Validation Loss: 0.9501
OOD Test ACCURACY: 0.2739
OOD Test Loss: 7.1712

[0m[1;37mINFO[0m: [1mChartInfo 0.8846 0.3063 0.8826 0.2739 0.8820 0.7703[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.174
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.137
SUFF++ for r=0.3 class 0 = 0.341 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 1 = 0.317 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 2 = 0.363 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 3 = 0.352 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 4 = 0.334 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 5 = 0.346 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 6 = 0.322 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 7 = 0.358 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 8 = 0.3 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 class 9 = 0.337 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 all KL = 0.184 +- 0.194 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.3 all L1 = 0.337 +- 0.127 (in-sample avg dev_std = 0.594)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.236
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.18
SUFF++ for r=0.6 class 0 = 0.28 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 1 = 0.329 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 2 = 0.337 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 3 = 0.317 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 4 = 0.304 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 5 = 0.282 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 6 = 0.293 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 7 = 0.324 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 8 = 0.28 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 9 = 0.308 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all KL = 0.158 +- 0.173 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all L1 = 0.307 +- 0.125 (in-sample avg dev_std = 0.554)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.756
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.395
SUFF++ for r=0.9 class 0 = 0.253 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 1 = 0.54 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 2 = 0.291 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 3 = 0.284 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 4 = 0.564 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 5 = 0.266 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 6 = 0.312 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 7 = 0.654 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 8 = 0.34 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 class 9 = 0.418 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 all KL = 0.195 +- 0.259 (in-sample avg dev_std = 0.591)
SUFF++ for r=0.9 all L1 = 0.397 +- 0.219 (in-sample avg dev_std = 0.591)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 16:11:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:51 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 04:11:52 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 196...
[0m[1;37mINFO[0m: [1mCheckpoint 196: 
-----------------------------------
Train ACCURACY: 0.9730
Train Loss: 0.0837
ID Validation ACCURACY: 0.8894
ID Validation Loss: 0.3769
ID Test ACCURACY: 0.8864
ID Test Loss: 0.3922
OOD Validation ACCURACY: 0.6770
OOD Validation Loss: 1.7135
OOD Test ACCURACY: 0.2230
OOD Test Loss: 8.2497

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 143...
[0m[1;37mINFO[0m: [1mCheckpoint 143: 
-----------------------------------
Train ACCURACY: 0.9545
Train Loss: 0.1347
ID Validation ACCURACY: 0.8867
ID Validation Loss: 0.3741
ID Test ACCURACY: 0.8890
ID Test Loss: 0.3766
OOD Validation ACCURACY: 0.7774
OOD Validation Loss: 0.7987
OOD Test ACCURACY: 0.3824
OOD Test Loss: 2.9937

[0m[1;37mINFO[0m: [1mChartInfo 0.8864 0.2230 0.8890 0.3824 0.8867 0.7774[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.112
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.12
SUFF++ for r=0.3 class 0 = 0.353 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 1 = 0.284 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 2 = 0.452 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 3 = 0.474 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 4 = 0.407 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 5 = 0.438 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 6 = 0.425 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 7 = 0.419 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 8 = 0.452 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 class 9 = 0.423 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 all KL = 0.212 +- 0.217 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.3 all L1 = 0.411 +- 0.162 (in-sample avg dev_std = 0.614)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.19
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.144
SUFF++ for r=0.6 class 0 = 0.226 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 1 = 0.416 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 2 = 0.336 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 3 = 0.387 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 4 = 0.307 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 5 = 0.345 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 6 = 0.317 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 7 = 0.376 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 8 = 0.337 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 class 9 = 0.347 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 all KL = 0.171 +- 0.197 (in-sample avg dev_std = 0.561)
SUFF++ for r=0.6 all L1 = 0.341 +- 0.152 (in-sample avg dev_std = 0.561)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.744
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.352
SUFF++ for r=0.9 class 0 = 0.25 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 1 = 0.328 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 2 = 0.334 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 3 = 0.276 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 4 = 0.486 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 5 = 0.256 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 6 = 0.322 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 7 = 0.62 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 8 = 0.311 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 class 9 = 0.492 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 all KL = 0.171 +- 0.231 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.9 all L1 = 0.369 +- 0.197 (in-sample avg dev_std = 0.580)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 16:22:01 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 04:22:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 194...
[0m[1;37mINFO[0m: [1mCheckpoint 194: 
-----------------------------------
Train ACCURACY: 0.9739
Train Loss: 0.0800
ID Validation ACCURACY: 0.8936
ID Validation Loss: 0.3724
ID Test ACCURACY: 0.8870
ID Test Loss: 0.3904
OOD Validation ACCURACY: 0.8023
OOD Validation Loss: 0.7265
OOD Test ACCURACY: 0.2739
OOD Test Loss: 4.8281

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 97...
[0m[1;37mINFO[0m: [1mCheckpoint 97: 
-----------------------------------
Train ACCURACY: 0.9201
Train Loss: 0.2337
ID Validation ACCURACY: 0.8701
ID Validation Loss: 0.4056
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4148
OOD Validation ACCURACY: 0.8056
OOD Validation Loss: 0.6300
OOD Test ACCURACY: 0.4093
OOD Test Loss: 2.5019

[0m[1;37mINFO[0m: [1mChartInfo 0.8870 0.2739 0.8727 0.4093 0.8701 0.8056[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.106
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.106
SUFF++ for r=0.3 class 0 = 0.277 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 1 = 0.299 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 2 = 0.292 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 3 = 0.306 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 4 = 0.294 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 5 = 0.291 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 6 = 0.292 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 7 = 0.305 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 8 = 0.291 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 class 9 = 0.294 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 all KL = 0.071 +- 0.104 (in-sample avg dev_std = 0.638)
SUFF++ for r=0.3 all L1 = 0.294 +- 0.094 (in-sample avg dev_std = 0.638)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.162
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.131
SUFF++ for r=0.6 class 0 = 0.26 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 1 = 0.317 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 2 = 0.34 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 3 = 0.349 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 4 = 0.394 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 5 = 0.329 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 6 = 0.294 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 7 = 0.341 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 8 = 0.318 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 class 9 = 0.371 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 all KL = 0.144 +- 0.164 (in-sample avg dev_std = 0.594)
SUFF++ for r=0.6 all L1 = 0.331 +- 0.133 (in-sample avg dev_std = 0.594)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.74
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.361
SUFF++ for r=0.9 class 0 = 0.254 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 1 = 0.276 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 2 = 0.351 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 3 = 0.337 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 4 = 0.688 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 5 = 0.282 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 6 = 0.316 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 7 = 0.661 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 8 = 0.318 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 class 9 = 0.365 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 all KL = 0.199 +- 0.279 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.9 all L1 = 0.386 +- 0.227 (in-sample avg dev_std = 0.567)


ratio=1.0



Zero intervened samples, skipping weight=1.0


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.465, 0.329, 0.185, 1.0], 'all_L1': [0.569, 0.434, 0.386, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.41, 0.349, 0.21, 1.0], 'all_L1': [0.508, 0.466, 0.368, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.184, 0.158, 0.195, 1.0], 'all_L1': [0.337, 0.307, 0.397, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.212, 0.171, 0.171, 1.0], 'all_L1': [0.411, 0.341, 0.369, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.071, 0.144, 0.199, 1.0], 'all_L1': [0.294, 0.331, 0.386, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.424 +- 0.103, 0.376 +- 0.062, 0.381 +- 0.011, 1.000 +- 0.000
suff++ class all_KL  =  0.268 +- 0.147, 0.230 +- 0.089, 0.192 +- 0.013, 1.000 +- 0.000
suff++_acc_int  =  0.116 +- 0.012, 0.147 +- 0.017, 0.355 +- 0.031


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Computed for split load_split = id



Completed in  0:49:24.368517  for GSATGIN GOODCMNIST/color



DONE GSAT GOODCMNIST/color all mitig
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 16:31:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 04:31:59 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 04:32:30 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 04:32:41 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 04:32:51 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:07 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 04:33:24 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 60...
[0m[1;37mINFO[0m: [1mCheckpoint 60: 
-----------------------------------
Train ROC-AUC: 0.9644
Train Loss: 0.3162
ID Validation ROC-AUC: 0.9159
ID Validation Loss: 0.4976
ID Test ROC-AUC: 0.9149
ID Test Loss: 0.5133
OOD Validation ROC-AUC: 0.6062
OOD Validation Loss: 0.7427
OOD Test ROC-AUC: 0.6845
OOD Test Loss: 1.0938

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ROC-AUC: 0.9089
Train Loss: 0.3173
ID Validation ROC-AUC: 0.8910
ID Validation Loss: 0.3404
ID Test ROC-AUC: 0.8888
ID Test Loss: 0.3479
OOD Validation ROC-AUC: 0.6543
OOD Validation Loss: 0.3422
OOD Test ROC-AUC: 0.6990
OOD Test Loss: 0.5882

[0m[1;37mINFO[0m: [1mChartInfo 0.9149 0.6845 0.8888 0.6990 0.8910 0.6543[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 04:33:25 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


[1;31mERROR[0m: 09/21/2024 04:33:57 PM - utils.py - line 87 : [1mTraceback (most recent call last):
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/bin/goodtg", line 33, in <module>
    sys.exit(load_entry_point('graph-ood', 'console_scripts', 'goodtg')())
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 1020, in goodtg
    main()
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 815, in main
    evaluate_metric(args)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 610, in evaluate_metric
    score, acc_int, results = pipeline.compute_metric_ratio(
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 1117, in compute_metric_ratio
    int_dataset = CustomDataset("", eval_samples, belonging)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 122, in __init__
    self.data, self.slices = self.collate(data_list)
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py", line 139, in collate
    data, slices, _ = collate(
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/collate.py", line 78, in collate
    values = [store[attr] for store in stores]
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/collate.py", line 78, in <listcomp>
    values = [store[attr] for store in stores]
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/storage.py", line 111, in __getitem__
    return self._mapping[key]
KeyError: 'edge_attr'
[0mTime to compute metrics!
The PID of this script is: 382704
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:13:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:13:56 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:14:32 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:14:43 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:14:54 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:10 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 60...
[0m[1;37mINFO[0m: [1mCheckpoint 60: 
-----------------------------------
Train ROC-AUC: 0.9644
Train Loss: 0.3162
ID Validation ROC-AUC: 0.9159
ID Validation Loss: 0.4976
ID Test ROC-AUC: 0.9149
ID Test Loss: 0.5133
OOD Validation ROC-AUC: 0.6062
OOD Validation Loss: 0.7427
OOD Test ROC-AUC: 0.6845
OOD Test Loss: 1.0938

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ROC-AUC: 0.9089
Train Loss: 0.3173
ID Validation ROC-AUC: 0.8910
ID Validation Loss: 0.3404
ID Test ROC-AUC: 0.8888
ID Test Loss: 0.3479
OOD Validation ROC-AUC: 0.6543
OOD Validation Loss: 0.3422
OOD Test ROC-AUC: 0.6990
OOD Test Loss: 0.5882

[0m[1;37mINFO[0m: [1mChartInfo 0.9149 0.6845 0.8888 0.6990 0.8910 0.6543[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:15:27 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.926
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.76
SUFF++ for r=0.6 class 0.0 = 0.779 +- 0.144 (in-sample avg dev_std = 0.092)
SUFF++ for r=0.6 class 1.0 = 0.975 +- 0.144 (in-sample avg dev_std = 0.092)
SUFF++ for r=0.6 all KL = 0.943 +- 0.144 (in-sample avg dev_std = 0.092)
SUFF++ for r=0.6 all L1 = 0.952 +- 0.130 (in-sample avg dev_std = 0.092)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:15:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:15:58 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:16:37 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:16:52 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:17:11 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:17:36 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 50...
[0m[1;37mINFO[0m: [1mCheckpoint 50: 
-----------------------------------
Train ROC-AUC: 0.9658
Train Loss: 0.1509
ID Validation ROC-AUC: 0.9099
ID Validation Loss: 0.2557
ID Test ROC-AUC: 0.9129
ID Test Loss: 0.2578
OOD Validation ROC-AUC: 0.6398
OOD Validation Loss: 0.4377
OOD Test ROC-AUC: 0.6781
OOD Test Loss: 0.5882

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 13...
[0m[1;37mINFO[0m: [1mCheckpoint 13: 
-----------------------------------
Train ROC-AUC: 0.9221
Train Loss: 0.2152
ID Validation ROC-AUC: 0.8909
ID Validation Loss: 0.2509
ID Test ROC-AUC: 0.8972
ID Test Loss: 0.2480
OOD Validation ROC-AUC: 0.6624
OOD Validation Loss: 0.3222
OOD Test ROC-AUC: 0.7061
OOD Test Loss: 0.4680

[0m[1;37mINFO[0m: [1mChartInfo 0.9129 0.6781 0.8972 0.7061 0.8909 0.6624[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:18:00 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.905
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.732
SUFF++ for r=0.6 class 0.0 = 0.593 +- 0.189 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 class 1.0 = 0.907 +- 0.189 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 all KL = 0.864 +- 0.189 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 all L1 = 0.87 +- 0.185 (in-sample avg dev_std = 0.226)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:18:43 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:18:43 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:19:25 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:19:41 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:19:57 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:18 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 20...
[0m[1;37mINFO[0m: [1mCheckpoint 20: 
-----------------------------------
Train ROC-AUC: 0.9373
Train Loss: 0.1953
ID Validation ROC-AUC: 0.9097
ID Validation Loss: 0.2289
ID Test ROC-AUC: 0.9081
ID Test Loss: 0.2333
OOD Validation ROC-AUC: 0.6243
OOD Validation Loss: 0.3584
OOD Test ROC-AUC: 0.6758
OOD Test Loss: 0.4758

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 9...
[0m[1;37mINFO[0m: [1mCheckpoint 9: 
-----------------------------------
Train ROC-AUC: 0.9088
Train Loss: 0.2311
ID Validation ROC-AUC: 0.8849
ID Validation Loss: 0.2505
ID Test ROC-AUC: 0.8879
ID Test Loss: 0.2510
OOD Validation ROC-AUC: 0.6619
OOD Validation Loss: 0.3260
OOD Test ROC-AUC: 0.6983
OOD Test Loss: 0.4346

[0m[1;37mINFO[0m: [1mChartInfo 0.9081 0.6758 0.8879 0.6983 0.8849 0.6619[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:20:41 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.909
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.772
SUFF++ for r=0.6 class 0.0 = 0.658 +- 0.129 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 1.0 = 0.843 +- 0.129 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 all KL = 0.865 +- 0.129 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 all L1 = 0.822 +- 0.137 (in-sample avg dev_std = 0.228)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:21:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:21:19 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:22:06 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:22:22 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:22:38 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:00 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:22 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:23 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 59...
[0m[1;37mINFO[0m: [1mCheckpoint 59: 
-----------------------------------
Train ROC-AUC: 0.9662
Train Loss: 0.1724
ID Validation ROC-AUC: 0.9138
ID Validation Loss: 0.3123
ID Test ROC-AUC: 0.9111
ID Test Loss: 0.3263
OOD Validation ROC-AUC: 0.5895
OOD Validation Loss: 0.6025
OOD Test ROC-AUC: 0.6681
OOD Test Loss: 0.7747

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.9269
Train Loss: 0.2093
ID Validation ROC-AUC: 0.8991
ID Validation Loss: 0.2421
ID Test ROC-AUC: 0.9021
ID Test Loss: 0.2424
OOD Validation ROC-AUC: 0.6642
OOD Validation Loss: 0.3339
OOD Test ROC-AUC: 0.6850
OOD Test Loss: 0.4826

[0m[1;37mINFO[0m: [1mChartInfo 0.9111 0.6681 0.9021 0.6850 0.8991 0.6642[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:23:23 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.919
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.793
SUFF++ for r=0.6 class 0.0 = 0.612 +- 0.229 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.6 class 1.0 = 0.917 +- 0.229 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.6 all KL = 0.844 +- 0.229 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.6 all L1 = 0.881 +- 0.184 (in-sample avg dev_std = 0.269)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:23:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:23:59 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:24:39 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:24:53 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:08 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:32 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ROC-AUC: 0.9661
Train Loss: 0.1473
ID Validation ROC-AUC: 0.9133
ID Validation Loss: 0.2363
ID Test ROC-AUC: 0.9109
ID Test Loss: 0.2418
OOD Validation ROC-AUC: 0.6302
OOD Validation Loss: 0.4522
OOD Test ROC-AUC: 0.6797
OOD Test Loss: 0.5703

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 9...
[0m[1;37mINFO[0m: [1mCheckpoint 9: 
-----------------------------------
Train ROC-AUC: 0.9108
Train Loss: 0.2361
ID Validation ROC-AUC: 0.8892
ID Validation Loss: 0.2523
ID Test ROC-AUC: 0.8864
ID Test Loss: 0.2555
OOD Validation ROC-AUC: 0.6513
OOD Validation Loss: 0.3289
OOD Test ROC-AUC: 0.7073
OOD Test Loss: 0.4218

[0m[1;37mINFO[0m: [1mChartInfo 0.9109 0.6797 0.8864 0.7073 0.8892 0.6513[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:25:50 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.915
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.712
SUFF++ for r=0.6 class 0.0 = 0.576 +- 0.251 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 class 1.0 = 0.788 +- 0.251 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 all KL = 0.695 +- 0.251 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 all L1 = 0.763 +- 0.179 (in-sample avg dev_std = 0.413)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.943], 'all_L1': [0.952]}), defaultdict(<class 'list'>, {'all_KL': [0.864], 'all_L1': [0.87]}), defaultdict(<class 'list'>, {'all_KL': [0.865], 'all_L1': [0.822]}), defaultdict(<class 'list'>, {'all_KL': [0.844], 'all_L1': [0.881]}), defaultdict(<class 'list'>, {'all_KL': [0.695], 'all_L1': [0.763]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.858 +- 0.063
suff++ class all_KL  =  0.842 +- 0.081
suff++_acc_int  =  0.754 +- 0.029


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Computed for split load_split = id



Completed in  0:12:26.374977  for CIGAGIN LBAPcore/assay



DONE CIGA LBAPcore/assay all mitig
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:26:34 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:26:34 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:06 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:16 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:27 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:43 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mUsing the fixed _explain_ functionality
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 05:27:59 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 197...
[0m[1;37mINFO[0m: [1mCheckpoint 197: 
-----------------------------------
Train ROC-AUC: 0.9575
Train Loss: 0.1735
ID Validation ROC-AUC: 0.9126
ID Validation Loss: 0.2431
ID Test ROC-AUC: 0.9180
ID Test Loss: 0.2370
OOD Validation ROC-AUC: 0.6443
OOD Validation Loss: 0.5116
OOD Test ROC-AUC: 0.6826
OOD Test Loss: 0.6087

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 54...
[0m[1;37mINFO[0m: [1mCheckpoint 54: 
-----------------------------------
Train ROC-AUC: 0.9308
Train Loss: 0.2188
ID Validation ROC-AUC: 0.9073
ID Validation Loss: 0.2593
ID Test ROC-AUC: 0.9108
ID Test Loss: 0.2562
OOD Validation ROC-AUC: 0.6898
OOD Validation Loss: 0.3322
OOD Test ROC-AUC: 0.7117
OOD Test Loss: 0.5314

[0m[1;37mINFO[0m: [1mChartInfo 0.9180 0.6826 0.9108 0.7117 0.9073 0.6898[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:28:00 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model ROC-AUC of binarized graphs for r=0.3 =  0.805
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.307 +- 0.005
Model ROC-AUC over intervened graphs for r=0.3 =  0.708
SUFF++ for r=0.3 class 0.0 = 0.616 +- 0.248 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.3 class 1.0 = 0.839 +- 0.248 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.3 all KL = 0.777 +- 0.248 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.3 all L1 = 0.813 +- 0.189 (in-sample avg dev_std = 0.375)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.873
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.782
SUFF++ for r=0.6 class 0.0 = 0.621 +- 0.207 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 class 1.0 = 0.843 +- 0.207 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 all KL = 0.814 +- 0.207 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 all L1 = 0.818 +- 0.177 (in-sample avg dev_std = 0.345)


ratio=0.9


Computing with masking

Model ROC-AUC of binarized graphs for r=0.9 =  0.898
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.907 +- 0.005
Model ROC-AUC over intervened graphs for r=0.9 =  0.821
SUFF++ for r=0.9 class 0.0 = 0.714 +- 0.186 (in-sample avg dev_std = 0.264)
SUFF++ for r=0.9 class 1.0 = 0.873 +- 0.186 (in-sample avg dev_std = 0.264)
SUFF++ for r=0.9 all KL = 0.882 +- 0.186 (in-sample avg dev_std = 0.264)
SUFF++ for r=0.9 all L1 = 0.855 +- 0.165 (in-sample avg dev_std = 0.264)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:29:08 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:29:08 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:29:39 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:29:49 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:00 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:16 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUsing the fixed _explain_ functionality
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 187...
[0m[1;37mINFO[0m: [1mCheckpoint 187: 
-----------------------------------
Train ROC-AUC: 0.9581
Train Loss: 0.1675
ID Validation ROC-AUC: 0.9155
ID Validation Loss: 0.2388
ID Test ROC-AUC: 0.9181
ID Test Loss: 0.2360
OOD Validation ROC-AUC: 0.6422
OOD Validation Loss: 0.5090
OOD Test ROC-AUC: 0.6891
OOD Test Loss: 0.6078

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 43...
[0m[1;37mINFO[0m: [1mCheckpoint 43: 
-----------------------------------
Train ROC-AUC: 0.9226
Train Loss: 0.2146
ID Validation ROC-AUC: 0.8997
ID Validation Loss: 0.2460
ID Test ROC-AUC: 0.9051
ID Test Loss: 0.2424
OOD Validation ROC-AUC: 0.6819
OOD Validation Loss: 0.3340
OOD Test ROC-AUC: 0.6983
OOD Test Loss: 0.4973

[0m[1;37mINFO[0m: [1mChartInfo 0.9181 0.6891 0.9051 0.6983 0.8997 0.6819[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:30:32 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model ROC-AUC of binarized graphs for r=0.3 =  0.804
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.307 +- 0.005
Model ROC-AUC over intervened graphs for r=0.3 =  0.714
SUFF++ for r=0.3 class 0.0 = 0.62 +- 0.259 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.3 class 1.0 = 0.852 +- 0.259 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.3 all KL = 0.772 +- 0.259 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.3 all L1 = 0.825 +- 0.189 (in-sample avg dev_std = 0.383)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.882
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.767
SUFF++ for r=0.6 class 0.0 = 0.56 +- 0.233 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 class 1.0 = 0.845 +- 0.233 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 all KL = 0.799 +- 0.233 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 all L1 = 0.812 +- 0.189 (in-sample avg dev_std = 0.366)


ratio=0.9


Computing with masking

Model ROC-AUC of binarized graphs for r=0.9 =  0.905
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.907 +- 0.005
Model ROC-AUC over intervened graphs for r=0.9 =  0.817
SUFF++ for r=0.9 class 0.0 = 0.686 +- 0.229 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.9 class 1.0 = 0.865 +- 0.229 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.9 all KL = 0.855 +- 0.229 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.9 all L1 = 0.844 +- 0.184 (in-sample avg dev_std = 0.308)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:31:40 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:31:40 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:32:12 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:32:26 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:32:43 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:05 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:33:30 PM : [1mUsing the fixed _explain_ functionality
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 05:33:31 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 172...
[0m[1;37mINFO[0m: [1mCheckpoint 172: 
-----------------------------------
Train ROC-AUC: 0.9565
Train Loss: 0.1673
ID Validation ROC-AUC: 0.9166
ID Validation Loss: 0.2335
ID Test ROC-AUC: 0.9184
ID Test Loss: 0.2312
OOD Validation ROC-AUC: 0.6443
OOD Validation Loss: 0.4868
OOD Test ROC-AUC: 0.6885
OOD Test Loss: 0.5753

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 13...
[0m[1;37mINFO[0m: [1mCheckpoint 13: 
-----------------------------------
Train ROC-AUC: 0.8916
Train Loss: 0.2426
ID Validation ROC-AUC: 0.8768
ID Validation Loss: 0.2559
ID Test ROC-AUC: 0.8832
ID Test Loss: 0.2531
OOD Validation ROC-AUC: 0.6829
OOD Validation Loss: 0.2941
OOD Test ROC-AUC: 0.6934
OOD Test Loss: 0.4478

[0m[1;37mINFO[0m: [1mChartInfo 0.9184 0.6885 0.8832 0.6934 0.8768 0.6829[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:33:31 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model ROC-AUC of binarized graphs for r=0.3 =  0.866
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.307 +- 0.005
Model ROC-AUC over intervened graphs for r=0.3 =  0.765
SUFF++ for r=0.3 class 0.0 = 0.595 +- 0.274 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.3 class 1.0 = 0.821 +- 0.274 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.3 all KL = 0.711 +- 0.274 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.3 all L1 = 0.795 +- 0.186 (in-sample avg dev_std = 0.432)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.908
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.82
SUFF++ for r=0.6 class 0.0 = 0.643 +- 0.262 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 class 1.0 = 0.819 +- 0.262 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 all KL = 0.738 +- 0.262 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 all L1 = 0.799 +- 0.179 (in-sample avg dev_std = 0.420)


ratio=0.9


Computing with masking

Model ROC-AUC of binarized graphs for r=0.9 =  0.903
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.907 +- 0.005
Model ROC-AUC over intervened graphs for r=0.9 =  0.83
SUFF++ for r=0.9 class 0.0 = 0.726 +- 0.248 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.9 class 1.0 = 0.842 +- 0.248 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.9 all KL = 0.826 +- 0.248 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.9 all L1 = 0.829 +- 0.181 (in-sample avg dev_std = 0.325)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:34:39 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:34:40 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:35:11 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:35:21 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:35:32 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:35:48 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUsing the fixed _explain_ functionality
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 185...
[0m[1;37mINFO[0m: [1mCheckpoint 185: 
-----------------------------------
Train ROC-AUC: 0.9582
Train Loss: 0.1732
ID Validation ROC-AUC: 0.9167
ID Validation Loss: 0.2422
ID Test ROC-AUC: 0.9192
ID Test Loss: 0.2373
OOD Validation ROC-AUC: 0.6602
OOD Validation Loss: 0.5268
OOD Test ROC-AUC: 0.6848
OOD Test Loss: 0.6351

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.8939
Train Loss: 0.2644
ID Validation ROC-AUC: 0.8796
ID Validation Loss: 0.2781
ID Test ROC-AUC: 0.8853
ID Test Loss: 0.2732
OOD Validation ROC-AUC: 0.6941
OOD Validation Loss: 0.3916
OOD Test ROC-AUC: 0.6881
OOD Test Loss: 0.4866

[0m[1;37mINFO[0m: [1mChartInfo 0.9192 0.6848 0.8853 0.6881 0.8796 0.6941[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:36:04 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model ROC-AUC of binarized graphs for r=0.3 =  0.814
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.307 +- 0.005
Model ROC-AUC over intervened graphs for r=0.3 =  0.712
SUFF++ for r=0.3 class 0.0 = 0.619 +- 0.279 (in-sample avg dev_std = 0.472)
SUFF++ for r=0.3 class 1.0 = 0.793 +- 0.279 (in-sample avg dev_std = 0.472)
SUFF++ for r=0.3 all KL = 0.686 +- 0.279 (in-sample avg dev_std = 0.472)
SUFF++ for r=0.3 all L1 = 0.773 +- 0.191 (in-sample avg dev_std = 0.472)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.907
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.791
SUFF++ for r=0.6 class 0.0 = 0.624 +- 0.259 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 class 1.0 = 0.801 +- 0.259 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 all KL = 0.733 +- 0.259 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 all L1 = 0.781 +- 0.187 (in-sample avg dev_std = 0.441)


ratio=0.9


Computing with masking

Model ROC-AUC of binarized graphs for r=0.9 =  0.92
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.907 +- 0.005
Model ROC-AUC over intervened graphs for r=0.9 =  0.836
SUFF++ for r=0.9 class 0.0 = 0.711 +- 0.233 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.9 class 1.0 = 0.845 +- 0.233 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.9 all KL = 0.838 +- 0.233 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.9 all L1 = 0.829 +- 0.182 (in-sample avg dev_std = 0.323)


ratio=1.0



Zero intervened samples, skipping weight=1.0
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 21 17:37:13 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/21/2024 05:37:13 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/21/2024 05:37:44 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/21/2024 05:37:54 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:05 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:21 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUsing the fixed _explain_ functionality
[0mUsing mitigation_expl_scores: default
[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 173...
[0m[1;37mINFO[0m: [1mCheckpoint 173: 
-----------------------------------
Train ROC-AUC: 0.9582
Train Loss: 0.1714
ID Validation ROC-AUC: 0.9187
ID Validation Loss: 0.2342
ID Test ROC-AUC: 0.9179
ID Test Loss: 0.2354
OOD Validation ROC-AUC: 0.6444
OOD Validation Loss: 0.5164
OOD Test ROC-AUC: 0.6886
OOD Test Loss: 0.5929

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 43...
[0m[1;37mINFO[0m: [1mCheckpoint 43: 
-----------------------------------
Train ROC-AUC: 0.9287
Train Loss: 0.2120
ID Validation ROC-AUC: 0.9078
ID Validation Loss: 0.2413
ID Test ROC-AUC: 0.9121
ID Test Loss: 0.2387
OOD Validation ROC-AUC: 0.6847
OOD Validation Loss: 0.3180
OOD Test ROC-AUC: 0.7131
OOD Test Loss: 0.4739

[0m[1;37mINFO[0m: [1mChartInfo 0.9179 0.6886 0.9121 0.7131 0.9078 0.6847[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/21/2024 05:38:37 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model ROC-AUC of binarized graphs for r=0.3 =  0.872
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.307 +- 0.005
Model ROC-AUC over intervened graphs for r=0.3 =  0.748
SUFF++ for r=0.3 class 0.0 = 0.607 +- 0.259 (in-sample avg dev_std = 0.458)
SUFF++ for r=0.3 class 1.0 = 0.78 +- 0.259 (in-sample avg dev_std = 0.458)
SUFF++ for r=0.3 all KL = 0.686 +- 0.259 (in-sample avg dev_std = 0.458)
SUFF++ for r=0.3 all L1 = 0.76 +- 0.179 (in-sample avg dev_std = 0.458)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.913
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.806
SUFF++ for r=0.6 class 0.0 = 0.651 +- 0.253 (in-sample avg dev_std = 0.429)
SUFF++ for r=0.6 class 1.0 = 0.799 +- 0.253 (in-sample avg dev_std = 0.429)
SUFF++ for r=0.6 all KL = 0.738 +- 0.253 (in-sample avg dev_std = 0.429)
SUFF++ for r=0.6 all L1 = 0.782 +- 0.176 (in-sample avg dev_std = 0.429)


ratio=0.9


Computing with masking

Model ROC-AUC of binarized graphs for r=0.9 =  0.915
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.907 +- 0.005
Model ROC-AUC over intervened graphs for r=0.9 =  0.836
SUFF++ for r=0.9 class 0.0 = 0.753 +- 0.235 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.9 class 1.0 = 0.833 +- 0.235 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.9 all KL = 0.831 +- 0.235 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.9 all L1 = 0.823 +- 0.176 (in-sample avg dev_std = 0.325)


ratio=1.0



Zero intervened samples, skipping weight=1.0


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.777, 0.814, 0.882, 1.0], 'all_L1': [0.813, 0.818, 0.855, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.772, 0.799, 0.855, 1.0], 'all_L1': [0.825, 0.812, 0.844, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.711, 0.738, 0.826, 1.0], 'all_L1': [0.795, 0.799, 0.829, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.686, 0.733, 0.838, 1.0], 'all_L1': [0.773, 0.781, 0.829, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.686, 0.738, 0.831, 1.0], 'all_L1': [0.76, 0.782, 0.823, 1.0], 0.0: [1.0], 1.0: [1.0]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.793 +- 0.024, 0.798 +- 0.015, 0.836 +- 0.012, 1.000 +- 0.000
suff++ class all_KL  =  0.726 +- 0.040, 0.764 +- 0.035, 0.846 +- 0.020, 1.000 +- 0.000
suff++_acc_int  =  0.729 +- 0.023, 0.793 +- 0.018, 0.828 +- 0.008


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Computed for split load_split = id



Completed in  0:13:35.663185  for GSATGIN LBAPcore/assay



DONE GSAT LBAPcore/assay all mitig
DONE all :)
