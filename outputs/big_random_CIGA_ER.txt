Time to compute metrics for random explanations!
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:15:39 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:15:39 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.9018
Train Loss: 0.4354
ID Validation ACCURACY: 0.9037
ID Validation Loss: 0.4370
ID Test ACCURACY: 0.8970
ID Test Loss: 0.4710
OOD Validation ACCURACY: 0.7420
OOD Validation Loss: 0.6904
OOD Test ACCURACY: 0.4850
OOD Test Loss: 11.6168

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 89...
[0m[1;37mINFO[0m: [1mCheckpoint 89: 
-----------------------------------
Train ACCURACY: 0.8768
Train Loss: 0.4361
ID Validation ACCURACY: 0.8860
ID Validation Loss: 0.4229
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4687
OOD Validation ACCURACY: 0.8807
OOD Validation Loss: 0.4940
OOD Test ACCURACY: 0.4280
OOD Test Loss: 6.8125

[0m[1;37mINFO[0m: [1mChartInfo 0.8970 0.4850 0.8727 0.4280 0.8860 0.8807[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.302
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.312
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.245
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.919
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3020175
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.511
SUFF++ for r=0.8 class 0 = 0.367 +- 0.305 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 class 1 = 0.652 +- 0.305 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 class 2 = 0.431 +- 0.305 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 all KL = 0.507 +- 0.305 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 all L1 = 0.484 +- 0.209 (in-sample avg dev_std = 0.446)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.491
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24475000000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.423
SUFF++ for r=0.8 class 0 = 0.602 +- 0.345 (in-sample avg dev_std = 0.491)
SUFF++ for r=0.8 class 1 = 0.738 +- 0.345 (in-sample avg dev_std = 0.491)
SUFF++ for r=0.8 class 2 = 0.568 +- 0.345 (in-sample avg dev_std = 0.491)
SUFF++ for r=0.8 all KL = 0.41 +- 0.345 (in-sample avg dev_std = 0.491)
SUFF++ for r=0.8 all L1 = 0.637 +- 0.263 (in-sample avg dev_std = 0.491)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.919
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3020175
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.507
NEC for r=0.8 class 0 = 0.633 +- 0.290 (in-sample avg dev_std = 0.510)
NEC for r=0.8 class 1 = 0.392 +- 0.290 (in-sample avg dev_std = 0.510)
NEC for r=0.8 class 2 = 0.626 +- 0.290 (in-sample avg dev_std = 0.510)
NEC for r=0.8 all KL = 0.538 +- 0.290 (in-sample avg dev_std = 0.510)
NEC for r=0.8 all L1 = 0.549 +- 0.178 (in-sample avg dev_std = 0.510)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.491
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24475000000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.426
NEC for r=0.8 class 0 = 0.38 +- 0.368 (in-sample avg dev_std = 0.548)
NEC for r=0.8 class 1 = 0.229 +- 0.368 (in-sample avg dev_std = 0.548)
NEC for r=0.8 class 2 = 0.45 +- 0.368 (in-sample avg dev_std = 0.548)
NEC for r=0.8 all KL = 0.552 +- 0.368 (in-sample avg dev_std = 0.548)
NEC for r=0.8 all L1 = 0.351 +- 0.269 (in-sample avg dev_std = 0.548)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:16:38 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:16:38 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 13...
[0m[1;37mINFO[0m: [1mCheckpoint 13: 
-----------------------------------
Train ACCURACY: 0.9209
Train Loss: 0.4610
ID Validation ACCURACY: 0.9260
ID Validation Loss: 0.4490
ID Test ACCURACY: 0.9150
ID Test Loss: 0.5098
OOD Validation ACCURACY: 0.8717
OOD Validation Loss: 0.5431
OOD Test ACCURACY: 0.3343
OOD Test Loss: 29.2798

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.8146
Train Loss: 0.6065
ID Validation ACCURACY: 0.8143
ID Validation Loss: 0.6034
ID Test ACCURACY: 0.8157
ID Test Loss: 0.6423
OOD Validation ACCURACY: 0.9270
OOD Validation Loss: 0.4652
OOD Test ACCURACY: 0.3343
OOD Test Loss: 40.5826

[0m[1;37mINFO[0m: [1mChartInfo 0.9150 0.3343 0.8157 0.3343 0.8143 0.9270[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.288
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.310
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.236
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.936
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.2884575
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.547
SUFF++ for r=0.8 class 0 = 0.38 +- 0.315 (in-sample avg dev_std = 0.585)
SUFF++ for r=0.8 class 1 = 0.595 +- 0.315 (in-sample avg dev_std = 0.585)
SUFF++ for r=0.8 class 2 = 0.531 +- 0.315 (in-sample avg dev_std = 0.585)
SUFF++ for r=0.8 all KL = 0.383 +- 0.315 (in-sample avg dev_std = 0.585)
SUFF++ for r=0.8 all L1 = 0.502 +- 0.239 (in-sample avg dev_std = 0.585)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.23612375
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.371
SUFF++ for r=0.8 class 0 = 0.857 +- 0.383 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 class 1 = 0.835 +- 0.383 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 class 2 = 0.929 +- 0.383 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 all KL = 0.763 +- 0.383 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 all L1 = 0.873 +- 0.230 (in-sample avg dev_std = 0.374)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.936
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.2884575
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.531
NEC for r=0.8 class 0 = 0.62 +- 0.256 (in-sample avg dev_std = 0.690)
NEC for r=0.8 class 1 = 0.466 +- 0.256 (in-sample avg dev_std = 0.690)
NEC for r=0.8 class 2 = 0.533 +- 0.256 (in-sample avg dev_std = 0.690)
NEC for r=0.8 all KL = 0.706 +- 0.256 (in-sample avg dev_std = 0.690)
NEC for r=0.8 all L1 = 0.54 +- 0.182 (in-sample avg dev_std = 0.690)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.23612375
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.375
NEC for r=0.8 class 0 = 0.185 +- 0.436 (in-sample avg dev_std = 0.487)
NEC for r=0.8 class 1 = 0.202 +- 0.436 (in-sample avg dev_std = 0.487)
NEC for r=0.8 class 2 = 0.1 +- 0.436 (in-sample avg dev_std = 0.487)
NEC for r=0.8 all KL = 0.328 +- 0.436 (in-sample avg dev_std = 0.487)
NEC for r=0.8 all L1 = 0.163 +- 0.238 (in-sample avg dev_std = 0.487)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:17:30 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:17:30 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 48...
[0m[1;37mINFO[0m: [1mCheckpoint 48: 
-----------------------------------
Train ACCURACY: 0.9083
Train Loss: 0.4178
ID Validation ACCURACY: 0.9150
ID Validation Loss: 0.3921
ID Test ACCURACY: 0.9070
ID Test Loss: 0.4430
OOD Validation ACCURACY: 0.7490
OOD Validation Loss: 0.6150
OOD Test ACCURACY: 0.4973
OOD Test Loss: 4.7022

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.8751
Train Loss: 0.4864
ID Validation ACCURACY: 0.8857
ID Validation Loss: 0.4608
ID Test ACCURACY: 0.8740
ID Test Loss: 0.5012
OOD Validation ACCURACY: 0.9083
OOD Validation Loss: 0.4673
OOD Test ACCURACY: 0.5683
OOD Test Loss: 3.0991

[0m[1;37mINFO[0m: [1mChartInfo 0.9070 0.4973 0.8740 0.5683 0.8857 0.9083[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.300
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.310
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.247
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.252


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.929
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29961
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.553
SUFF++ for r=0.8 class 0 = 0.355 +- 0.331 (in-sample avg dev_std = 0.515)
SUFF++ for r=0.8 class 1 = 0.708 +- 0.331 (in-sample avg dev_std = 0.515)
SUFF++ for r=0.8 class 2 = 0.468 +- 0.331 (in-sample avg dev_std = 0.515)
SUFF++ for r=0.8 all KL = 0.437 +- 0.331 (in-sample avg dev_std = 0.515)
SUFF++ for r=0.8 all L1 = 0.511 +- 0.261 (in-sample avg dev_std = 0.515)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.504
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24700624999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.496
SUFF++ for r=0.8 class 0 = 0.565 +- 0.313 (in-sample avg dev_std = 0.544)
SUFF++ for r=0.8 class 1 = 0.551 +- 0.313 (in-sample avg dev_std = 0.544)
SUFF++ for r=0.8 class 2 = 0.645 +- 0.313 (in-sample avg dev_std = 0.544)
SUFF++ for r=0.8 all KL = 0.376 +- 0.313 (in-sample avg dev_std = 0.544)
SUFF++ for r=0.8 all L1 = 0.587 +- 0.210 (in-sample avg dev_std = 0.544)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.929
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29961
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.538
NEC for r=0.8 class 0 = 0.651 +- 0.331 (in-sample avg dev_std = 0.618)
NEC for r=0.8 class 1 = 0.281 +- 0.331 (in-sample avg dev_std = 0.618)
NEC for r=0.8 class 2 = 0.608 +- 0.331 (in-sample avg dev_std = 0.618)
NEC for r=0.8 all KL = 0.601 +- 0.331 (in-sample avg dev_std = 0.618)
NEC for r=0.8 all L1 = 0.512 +- 0.248 (in-sample avg dev_std = 0.618)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.504
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24700624999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.517
NEC for r=0.8 class 0 = 0.45 +- 0.280 (in-sample avg dev_std = 0.614)
NEC for r=0.8 class 1 = 0.462 +- 0.280 (in-sample avg dev_std = 0.614)
NEC for r=0.8 class 2 = 0.36 +- 0.280 (in-sample avg dev_std = 0.614)
NEC for r=0.8 all KL = 0.683 +- 0.280 (in-sample avg dev_std = 0.614)
NEC for r=0.8 all L1 = 0.424 +- 0.184 (in-sample avg dev_std = 0.614)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:18:23 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:18:23 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 140...
[0m[1;37mINFO[0m: [1mCheckpoint 140: 
-----------------------------------
Train ACCURACY: 0.9114
Train Loss: 0.3792
ID Validation ACCURACY: 0.9143
ID Validation Loss: 0.3758
ID Test ACCURACY: 0.9077
ID Test Loss: 0.4035
OOD Validation ACCURACY: 0.8203
OOD Validation Loss: 0.5376
OOD Test ACCURACY: 0.5747
OOD Test Loss: 2.8057

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.8786
Train Loss: 0.4845
ID Validation ACCURACY: 0.8843
ID Validation Loss: 0.4701
ID Test ACCURACY: 0.8797
ID Test Loss: 0.5157
OOD Validation ACCURACY: 0.8787
OOD Validation Loss: 0.4998
OOD Test ACCURACY: 0.5490
OOD Test Loss: 1.6271

[0m[1;37mINFO[0m: [1mChartInfo 0.9077 0.5747 0.8797 0.5490 0.8843 0.8787[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.292
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.309
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.242
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.92
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29220375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.526
SUFF++ for r=0.8 class 0 = 0.39 +- 0.282 (in-sample avg dev_std = 0.526)
SUFF++ for r=0.8 class 1 = 0.594 +- 0.282 (in-sample avg dev_std = 0.526)
SUFF++ for r=0.8 class 2 = 0.412 +- 0.282 (in-sample avg dev_std = 0.526)
SUFF++ for r=0.8 all KL = 0.409 +- 0.282 (in-sample avg dev_std = 0.526)
SUFF++ for r=0.8 all L1 = 0.466 +- 0.198 (in-sample avg dev_std = 0.526)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.582
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24200125
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.455
SUFF++ for r=0.8 class 0 = 0.421 +- 0.274 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.8 class 1 = 0.482 +- 0.274 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.8 class 2 = 0.431 +- 0.274 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.8 all KL = 0.323 +- 0.274 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.8 all L1 = 0.445 +- 0.187 (in-sample avg dev_std = 0.568)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.92
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29220375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.53
NEC for r=0.8 class 0 = 0.63 +- 0.298 (in-sample avg dev_std = 0.595)
NEC for r=0.8 class 1 = 0.36 +- 0.298 (in-sample avg dev_std = 0.595)
NEC for r=0.8 class 2 = 0.615 +- 0.298 (in-sample avg dev_std = 0.595)
NEC for r=0.8 all KL = 0.597 +- 0.298 (in-sample avg dev_std = 0.595)
NEC for r=0.8 all L1 = 0.534 +- 0.196 (in-sample avg dev_std = 0.595)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.582
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24200125
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.455
NEC for r=0.8 class 0 = 0.547 +- 0.258 (in-sample avg dev_std = 0.599)
NEC for r=0.8 class 1 = 0.505 +- 0.258 (in-sample avg dev_std = 0.599)
NEC for r=0.8 class 2 = 0.579 +- 0.258 (in-sample avg dev_std = 0.599)
NEC for r=0.8 all KL = 0.65 +- 0.258 (in-sample avg dev_std = 0.599)
NEC for r=0.8 all L1 = 0.543 +- 0.175 (in-sample avg dev_std = 0.599)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:19:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:19:19 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 19...
[0m[1;37mINFO[0m: [1mCheckpoint 19: 
-----------------------------------
Train ACCURACY: 0.9057
Train Loss: 0.4458
ID Validation ACCURACY: 0.9100
ID Validation Loss: 0.4449
ID Test ACCURACY: 0.9023
ID Test Loss: 0.4743
OOD Validation ACCURACY: 0.6823
OOD Validation Loss: 0.7176
OOD Test ACCURACY: 0.3343
OOD Test Loss: 11.3225

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 14...
[0m[1;37mINFO[0m: [1mCheckpoint 14: 
-----------------------------------
Train ACCURACY: 0.8665
Train Loss: 0.5395
ID Validation ACCURACY: 0.8680
ID Validation Loss: 0.5306
ID Test ACCURACY: 0.8620
ID Test Loss: 0.5767
OOD Validation ACCURACY: 0.8170
OOD Validation Loss: 0.6338
OOD Test ACCURACY: 0.4653
OOD Test Loss: 8.5253

[0m[1;37mINFO[0m: [1mChartInfo 0.9023 0.3343 0.8620 0.4653 0.8680 0.8170[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.292
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.307
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.242
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.918
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29237125
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.485
SUFF++ for r=0.8 class 0 = 0.387 +- 0.253 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 class 1 = 0.503 +- 0.253 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 class 2 = 0.53 +- 0.253 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 all KL = 0.499 +- 0.253 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 all L1 = 0.474 +- 0.175 (in-sample avg dev_std = 0.448)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24218000000000003
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.384
SUFF++ for r=0.8 class 0 = 0.813 +- 0.301 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 class 1 = 0.766 +- 0.301 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 class 2 = 0.88 +- 0.301 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 all KL = 0.723 +- 0.301 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 all L1 = 0.819 +- 0.208 (in-sample avg dev_std = 0.323)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.918
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29237125
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.482
NEC for r=0.8 class 0 = 0.604 +- 0.234 (in-sample avg dev_std = 0.504)
NEC for r=0.8 class 1 = 0.514 +- 0.234 (in-sample avg dev_std = 0.504)
NEC for r=0.8 class 2 = 0.506 +- 0.234 (in-sample avg dev_std = 0.504)
NEC for r=0.8 all KL = 0.522 +- 0.234 (in-sample avg dev_std = 0.504)
NEC for r=0.8 all L1 = 0.541 +- 0.148 (in-sample avg dev_std = 0.504)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24218000000000003
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.397
NEC for r=0.8 class 0 = 0.227 +- 0.311 (in-sample avg dev_std = 0.413)
NEC for r=0.8 class 1 = 0.279 +- 0.311 (in-sample avg dev_std = 0.413)
NEC for r=0.8 class 2 = 0.156 +- 0.311 (in-sample avg dev_std = 0.413)
NEC for r=0.8 all KL = 0.367 +- 0.311 (in-sample avg dev_std = 0.413)
NEC for r=0.8 all L1 = 0.221 +- 0.210 (in-sample avg dev_std = 0.413)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.507], 'all_L1': [0.484]}), defaultdict(<class 'list'>, {'all_KL': [0.383], 'all_L1': [0.502]}), defaultdict(<class 'list'>, {'all_KL': [0.437], 'all_L1': [0.511]}), defaultdict(<class 'list'>, {'all_KL': [0.409], 'all_L1': [0.466]}), defaultdict(<class 'list'>, {'all_KL': [0.499], 'all_L1': [0.474]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.538], 'all_L1': [0.549]}), defaultdict(<class 'list'>, {'all_KL': [0.706], 'all_L1': [0.54]}), defaultdict(<class 'list'>, {'all_KL': [0.601], 'all_L1': [0.512]}), defaultdict(<class 'list'>, {'all_KL': [0.597], 'all_L1': [0.534]}), defaultdict(<class 'list'>, {'all_KL': [0.522], 'all_L1': [0.541]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.41], 'all_L1': [0.637]}), defaultdict(<class 'list'>, {'all_KL': [0.763], 'all_L1': [0.873]}), defaultdict(<class 'list'>, {'all_KL': [0.376], 'all_L1': [0.587]}), defaultdict(<class 'list'>, {'all_KL': [0.323], 'all_L1': [0.445]}), defaultdict(<class 'list'>, {'all_KL': [0.723], 'all_L1': [0.819]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.552], 'all_L1': [0.351]}), defaultdict(<class 'list'>, {'all_KL': [0.328], 'all_L1': [0.163]}), defaultdict(<class 'list'>, {'all_KL': [0.683], 'all_L1': [0.424]}), defaultdict(<class 'list'>, {'all_KL': [0.65], 'all_L1': [0.543]}), defaultdict(<class 'list'>, {'all_KL': [0.367], 'all_L1': [0.221]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.487 +- 0.017
suff++ class all_KL  =  0.447 +- 0.049
suff++_acc_int  =  0.524 +- 0.025
nec class all_L1  =  0.535 +- 0.013
nec class all_KL  =  0.593 +- 0.065
nec_acc_int  =  0.518 +- 0.021

Eval split test
suff++ class all_L1  =  0.672 +- 0.156
suff++ class all_KL  =  0.519 +- 0.185
suff++_acc_int  =  0.426 +- 0.046
nec class all_L1  =  0.340 +- 0.137
nec class all_KL  =  0.516 +- 0.145
nec_acc_int  =  0.434 +- 0.049


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.511 +- 0.007
Faith. Armon (L1)= 		  =  0.510 +- 0.008
Faith. GMean (L1)= 	  =  0.511 +- 0.008
Faith. Aritm (KL)= 		  =  0.520 +- 0.014
Faith. Armon (KL)= 		  =  0.504 +- 0.012
Faith. GMean (KL)= 	  =  0.512 +- 0.010

Eval split test
Faith. Aritm (L1)= 		  =  0.506 +- 0.011
Faith. Armon (L1)= 		  =  0.411 +- 0.086
Faith. GMean (L1)= 	  =  0.453 +- 0.046
Faith. Aritm (KL)= 		  =  0.518 +- 0.028
Faith. Armon (KL)= 		  =  0.467 +- 0.020
Faith. GMean (KL)= 	  =  0.491 +- 0.021
Computed for split load_split = id



Completed in  0:04:35.334564  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:20:27 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:20:28 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 174...
[0m[1;37mINFO[0m: [1mCheckpoint 174: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8570
ID Validation Loss: 1.0776
ID Test ACCURACY: 0.8538
ID Test Loss: 1.1832
OOD Validation ACCURACY: 0.8127
OOD Validation Loss: 1.9445
OOD Test ACCURACY: 0.7096
OOD Test Loss: 3.2829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.9462
Train Loss: 0.0859
ID Validation ACCURACY: 0.8468
ID Validation Loss: 0.5292
ID Test ACCURACY: 0.8487
ID Test Loss: 0.5857
OOD Validation ACCURACY: 0.8539
OOD Validation Loss: 0.6808
OOD Test ACCURACY: 0.8169
OOD Test Loss: 0.7504

[0m[1;37mINFO[0m: [1mChartInfo 0.8538 0.7096 0.8487 0.8169 0.8468 0.8539[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 02:20:30 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.846
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.836
SUFF++ for r=0.8 class 0.0 = 0.836 +- 0.341 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.8 class 1.0 = 0.939 +- 0.341 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.8 all KL = 0.774 +- 0.341 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.8 all L1 = 0.896 +- 0.175 (in-sample avg dev_std = 0.382)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.679
SUFF++ for r=0.8 class 0.0 = 0.767 +- 0.299 (in-sample avg dev_std = 0.346)
SUFF++ for r=0.8 class 1.0 = 0.97 +- 0.299 (in-sample avg dev_std = 0.346)
SUFF++ for r=0.8 all KL = 0.816 +- 0.299 (in-sample avg dev_std = 0.346)
SUFF++ for r=0.8 all L1 = 0.872 +- 0.217 (in-sample avg dev_std = 0.346)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.846
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.849
NEC for r=0.8 class 0.0 = 0.114 +- 0.252 (in-sample avg dev_std = 0.218)
NEC for r=0.8 class 1.0 = 0.043 +- 0.252 (in-sample avg dev_std = 0.218)
NEC for r=0.8 all KL = 0.098 +- 0.252 (in-sample avg dev_std = 0.218)
NEC for r=0.8 all L1 = 0.072 +- 0.184 (in-sample avg dev_std = 0.218)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.697
NEC for r=0.8 class 0.0 = 0.175 +- 0.238 (in-sample avg dev_std = 0.239)
NEC for r=0.8 class 1.0 = 0.022 +- 0.238 (in-sample avg dev_std = 0.239)
NEC for r=0.8 all KL = 0.111 +- 0.238 (in-sample avg dev_std = 0.239)
NEC for r=0.8 all L1 = 0.096 +- 0.200 (in-sample avg dev_std = 0.239)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:21:15 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:21:16 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 193...
[0m[1;37mINFO[0m: [1mCheckpoint 193: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8621
ID Validation Loss: 1.0883
ID Test ACCURACY: 0.8557
ID Test Loss: 1.1549
OOD Validation ACCURACY: 0.8230
OOD Validation Loss: 1.8365
OOD Test ACCURACY: 0.7082
OOD Test Loss: 3.4909

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 125...
[0m[1;37mINFO[0m: [1mCheckpoint 125: 
-----------------------------------
Train ACCURACY: 0.9435
Train Loss: 0.0905
ID Validation ACCURACY: 0.8459
ID Validation Loss: 0.6688
ID Test ACCURACY: 0.8398
ID Test Loss: 0.7578
OOD Validation ACCURACY: 0.8575
OOD Validation Loss: 0.7711
OOD Test ACCURACY: 0.8105
OOD Test Loss: 0.8099

[0m[1;37mINFO[0m: [1mChartInfo 0.8557 0.7082 0.8398 0.8105 0.8459 0.8575[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 02:21:17 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.842
SUFF++ for r=0.8 class 0.0 = 0.83 +- 0.338 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 class 1.0 = 0.948 +- 0.338 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 all KL = 0.783 +- 0.338 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.178 (in-sample avg dev_std = 0.368)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.679
SUFF++ for r=0.8 class 0.0 = 0.771 +- 0.294 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 class 1.0 = 0.973 +- 0.294 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 all KL = 0.822 +- 0.294 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 all L1 = 0.875 +- 0.215 (in-sample avg dev_std = 0.331)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.859
NEC for r=0.8 class 0.0 = 0.109 +- 0.208 (in-sample avg dev_std = 0.179)
NEC for r=0.8 class 1.0 = 0.023 +- 0.208 (in-sample avg dev_std = 0.179)
NEC for r=0.8 all KL = 0.079 +- 0.208 (in-sample avg dev_std = 0.179)
NEC for r=0.8 all L1 = 0.059 +- 0.164 (in-sample avg dev_std = 0.179)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.703
NEC for r=0.8 class 0.0 = 0.166 +- 0.228 (in-sample avg dev_std = 0.240)
NEC for r=0.8 class 1.0 = 0.025 +- 0.228 (in-sample avg dev_std = 0.240)
NEC for r=0.8 all KL = 0.106 +- 0.228 (in-sample avg dev_std = 0.240)
NEC for r=0.8 all L1 = 0.093 +- 0.195 (in-sample avg dev_std = 0.240)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:22:11 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:22:12 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0773
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.5674
ID Test ACCURACY: 0.8602
ID Test Loss: 0.6145
OOD Validation ACCURACY: 0.8564
OOD Validation Loss: 0.7958
OOD Test ACCURACY: 0.8034
OOD Test Loss: 0.9273

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0773
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.5674
ID Test ACCURACY: 0.8602
ID Test Loss: 0.6145
OOD Validation ACCURACY: 0.8564
OOD Validation Loss: 0.7958
OOD Test ACCURACY: 0.8034
OOD Test Loss: 0.9273

[0m[1;37mINFO[0m: [1mChartInfo 0.8602 0.8034 0.8602 0.8034 0.8581 0.8564[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 02:22:13 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.833
SUFF++ for r=0.8 class 0.0 = 0.878 +- 0.258 (in-sample avg dev_std = 0.343)
SUFF++ for r=0.8 class 1.0 = 0.893 +- 0.258 (in-sample avg dev_std = 0.343)
SUFF++ for r=0.8 all KL = 0.818 +- 0.258 (in-sample avg dev_std = 0.343)
SUFF++ for r=0.8 all L1 = 0.887 +- 0.172 (in-sample avg dev_std = 0.343)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.81
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.795
SUFF++ for r=0.8 class 0.0 = 0.845 +- 0.202 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 class 1.0 = 0.89 +- 0.202 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 all KL = 0.854 +- 0.202 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 all L1 = 0.868 +- 0.177 (in-sample avg dev_std = 0.292)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.852
NEC for r=0.8 class 0.0 = 0.072 +- 0.189 (in-sample avg dev_std = 0.166)
NEC for r=0.8 class 1.0 = 0.079 +- 0.189 (in-sample avg dev_std = 0.166)
NEC for r=0.8 all KL = 0.078 +- 0.189 (in-sample avg dev_std = 0.166)
NEC for r=0.8 all L1 = 0.076 +- 0.163 (in-sample avg dev_std = 0.166)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.81
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.808
NEC for r=0.8 class 0.0 = 0.097 +- 0.149 (in-sample avg dev_std = 0.182)
NEC for r=0.8 class 1.0 = 0.079 +- 0.149 (in-sample avg dev_std = 0.182)
NEC for r=0.8 all KL = 0.068 +- 0.149 (in-sample avg dev_std = 0.182)
NEC for r=0.8 all L1 = 0.088 +- 0.157 (in-sample avg dev_std = 0.182)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:22:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:00 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 192...
[0m[1;37mINFO[0m: [1mCheckpoint 192: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8600
ID Validation Loss: 1.1678
ID Test ACCURACY: 0.8613
ID Test Loss: 1.2094
OOD Validation ACCURACY: 0.8373
OOD Validation Loss: 1.8807
OOD Test ACCURACY: 0.7372
OOD Test Loss: 3.1834

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 145...
[0m[1;37mINFO[0m: [1mCheckpoint 145: 
-----------------------------------
Train ACCURACY: 0.9451
Train Loss: 0.0856
ID Validation ACCURACY: 0.8491
ID Validation Loss: 0.7959
ID Test ACCURACY: 0.8461
ID Test Loss: 0.9122
OOD Validation ACCURACY: 0.8587
OOD Validation Loss: 0.9326
OOD Test ACCURACY: 0.8134
OOD Test Loss: 1.1555

[0m[1;37mINFO[0m: [1mChartInfo 0.8613 0.7372 0.8461 0.8134 0.8491 0.8587[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 02:23:01 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.842
SUFF++ for r=0.8 class 0.0 = 0.83 +- 0.338 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 class 1.0 = 0.95 +- 0.338 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all KL = 0.783 +- 0.338 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all L1 = 0.9 +- 0.174 (in-sample avg dev_std = 0.371)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.703
SUFF++ for r=0.8 class 0.0 = 0.779 +- 0.307 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.8 class 1.0 = 0.977 +- 0.307 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.8 all KL = 0.805 +- 0.307 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.8 all L1 = 0.881 +- 0.199 (in-sample avg dev_std = 0.355)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.862
NEC for r=0.8 class 0.0 = 0.108 +- 0.230 (in-sample avg dev_std = 0.217)
NEC for r=0.8 class 1.0 = 0.033 +- 0.230 (in-sample avg dev_std = 0.217)
NEC for r=0.8 all KL = 0.086 +- 0.230 (in-sample avg dev_std = 0.217)
NEC for r=0.8 all L1 = 0.065 +- 0.178 (in-sample avg dev_std = 0.217)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.736
NEC for r=0.8 class 0.0 = 0.15 +- 0.233 (in-sample avg dev_std = 0.242)
NEC for r=0.8 class 1.0 = 0.022 +- 0.233 (in-sample avg dev_std = 0.242)
NEC for r=0.8 all KL = 0.105 +- 0.233 (in-sample avg dev_std = 0.242)
NEC for r=0.8 all L1 = 0.084 +- 0.179 (in-sample avg dev_std = 0.242)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:23:46 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 135...
[0m[1;37mINFO[0m: [1mCheckpoint 135: 
-----------------------------------
Train ACCURACY: 0.9480
Train Loss: 0.0788
ID Validation ACCURACY: 0.8570
ID Validation Loss: 0.7811
ID Test ACCURACY: 0.8510
ID Test Loss: 0.7955
OOD Validation ACCURACY: 0.8215
OOD Validation Loss: 1.3083
OOD Test ACCURACY: 0.7393
OOD Test Loss: 2.3604

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ACCURACY: 0.8942
Train Loss: 0.1948
ID Validation ACCURACY: 0.8127
ID Validation Loss: 0.4206
ID Test ACCURACY: 0.8148
ID Test Loss: 0.4362
OOD Validation ACCURACY: 0.8560
OOD Validation Loss: 0.4250
OOD Test ACCURACY: 0.8085
OOD Test Loss: 0.5168

[0m[1;37mINFO[0m: [1mChartInfo 0.8510 0.7393 0.8148 0.8085 0.8127 0.8560[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 02:23:47 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.877
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.843
SUFF++ for r=0.8 class 0.0 = 0.81 +- 0.287 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 class 1.0 = 0.948 +- 0.287 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 all KL = 0.819 +- 0.287 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 all L1 = 0.89 +- 0.175 (in-sample avg dev_std = 0.368)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.748
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.713
SUFF++ for r=0.8 class 0.0 = 0.776 +- 0.241 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 class 1.0 = 0.974 +- 0.241 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 all KL = 0.853 +- 0.241 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 all L1 = 0.878 +- 0.192 (in-sample avg dev_std = 0.310)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.877
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.868
NEC for r=0.8 class 0.0 = 0.1 +- 0.169 (in-sample avg dev_std = 0.164)
NEC for r=0.8 class 1.0 = 0.031 +- 0.169 (in-sample avg dev_std = 0.164)
NEC for r=0.8 all KL = 0.063 +- 0.169 (in-sample avg dev_std = 0.164)
NEC for r=0.8 all L1 = 0.06 +- 0.139 (in-sample avg dev_std = 0.164)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.748
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.735
NEC for r=0.8 class 0.0 = 0.163 +- 0.177 (in-sample avg dev_std = 0.202)
NEC for r=0.8 class 1.0 = 0.021 +- 0.177 (in-sample avg dev_std = 0.202)
NEC for r=0.8 all KL = 0.084 +- 0.177 (in-sample avg dev_std = 0.202)
NEC for r=0.8 all L1 = 0.09 +- 0.170 (in-sample avg dev_std = 0.202)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.774], 'all_L1': [0.896]}), defaultdict(<class 'list'>, {'all_KL': [0.783], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.818], 'all_L1': [0.887]}), defaultdict(<class 'list'>, {'all_KL': [0.783], 'all_L1': [0.9]}), defaultdict(<class 'list'>, {'all_KL': [0.819], 'all_L1': [0.89]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.072]}), defaultdict(<class 'list'>, {'all_KL': [0.079], 'all_L1': [0.059]}), defaultdict(<class 'list'>, {'all_KL': [0.078], 'all_L1': [0.076]}), defaultdict(<class 'list'>, {'all_KL': [0.086], 'all_L1': [0.065]}), defaultdict(<class 'list'>, {'all_KL': [0.063], 'all_L1': [0.06]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.816], 'all_L1': [0.872]}), defaultdict(<class 'list'>, {'all_KL': [0.822], 'all_L1': [0.875]}), defaultdict(<class 'list'>, {'all_KL': [0.854], 'all_L1': [0.868]}), defaultdict(<class 'list'>, {'all_KL': [0.805], 'all_L1': [0.881]}), defaultdict(<class 'list'>, {'all_KL': [0.853], 'all_L1': [0.878]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.111], 'all_L1': [0.096]}), defaultdict(<class 'list'>, {'all_KL': [0.106], 'all_L1': [0.093]}), defaultdict(<class 'list'>, {'all_KL': [0.068], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.105], 'all_L1': [0.084]}), defaultdict(<class 'list'>, {'all_KL': [0.084], 'all_L1': [0.09]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.894 +- 0.005
suff++ class all_KL  =  0.795 +- 0.019
suff++_acc_int  =  0.839 +- 0.004
nec class all_L1  =  0.066 +- 0.007
nec class all_KL  =  0.081 +- 0.011
nec_acc_int  =  0.858 +- 0.007

Eval split test
suff++ class all_L1  =  0.875 +- 0.005
suff++ class all_KL  =  0.830 +- 0.020
suff++_acc_int  =  0.714 +- 0.043
nec class all_L1  =  0.090 +- 0.004
nec class all_KL  =  0.095 +- 0.016
nec_acc_int  =  0.736 +- 0.039


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.480 +- 0.003
Faith. Armon (L1)= 		  =  0.124 +- 0.011
Faith. GMean (L1)= 	  =  0.243 +- 0.012
Faith. Aritm (KL)= 		  =  0.438 +- 0.006
Faith. Armon (KL)= 		  =  0.146 +- 0.019
Faith. GMean (KL)= 	  =  0.253 +- 0.016

Eval split test
Faith. Aritm (L1)= 		  =  0.482 +- 0.002
Faith. Armon (L1)= 		  =  0.164 +- 0.007
Faith. GMean (L1)= 	  =  0.281 +- 0.006
Faith. Aritm (KL)= 		  =  0.462 +- 0.004
Faith. Armon (KL)= 		  =  0.170 +- 0.026
Faith. GMean (KL)= 	  =  0.279 +- 0.022
Computed for split load_split = id



Completed in  0:04:08.268855  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:24:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 02:24:52 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 02:25:23 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 02:25:34 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 02:25:44 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:00 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:26:16 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 64...
[0m[1;37mINFO[0m: [1mCheckpoint 64: 
-----------------------------------
Train ROC-AUC: 0.9925
Train Loss: 0.0975
ID Validation ROC-AUC: 0.9178
ID Validation Loss: 0.3060
ID Test ROC-AUC: 0.9170
ID Test Loss: 0.3147
OOD Validation ROC-AUC: 0.6215
OOD Validation Loss: 0.5097
OOD Test ROC-AUC: 0.6702
OOD Test Loss: 0.7230

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ROC-AUC: 0.8878
Train Loss: 0.2467
ID Validation ROC-AUC: 0.8703
ID Validation Loss: 0.2604
ID Test ROC-AUC: 0.8722
ID Test Loss: 0.2607
OOD Validation ROC-AUC: 0.6813
OOD Validation Loss: 0.3373
OOD Test ROC-AUC: 0.6909
OOD Test Loss: 0.4560

[0m[1;37mINFO[0m: [1mChartInfo 0.9170 0.6702 0.8722 0.6909 0.8703 0.6813[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 02:26:17 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 02:26:27 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.789
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.721
SUFF++ for r=0.6 class 0.0 = 0.984 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 class 1.0 = 0.997 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all KL = 0.998 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all L1 = 0.996 +- 0.026 (in-sample avg dev_std = 0.013)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.646
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.611
SUFF++ for r=0.6 class 0.0 = 0.985 +- 0.012 (in-sample avg dev_std = 0.016)
SUFF++ for r=0.6 class 1.0 = 0.995 +- 0.012 (in-sample avg dev_std = 0.016)
SUFF++ for r=0.6 all KL = 0.997 +- 0.012 (in-sample avg dev_std = 0.016)
SUFF++ for r=0.6 all L1 = 0.994 +- 0.027 (in-sample avg dev_std = 0.016)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.789
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.774
NEC for r=0.6 class 0.0 = 0.013 +- 0.007 (in-sample avg dev_std = 0.009)
NEC for r=0.6 class 1.0 = 0.002 +- 0.007 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all KL = 0.001 +- 0.007 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all L1 = 0.003 +- 0.020 (in-sample avg dev_std = 0.009)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.646
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.639
NEC for r=0.6 class 0.0 = 0.011 +- 0.008 (in-sample avg dev_std = 0.013)
NEC for r=0.6 class 1.0 = 0.003 +- 0.008 (in-sample avg dev_std = 0.013)
NEC for r=0.6 all KL = 0.001 +- 0.008 (in-sample avg dev_std = 0.013)
NEC for r=0.6 all L1 = 0.005 +- 0.022 (in-sample avg dev_std = 0.013)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:27:36 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 02:27:37 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 02:28:11 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 02:28:21 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 02:28:31 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 02:28:47 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ROC-AUC: 0.9942
Train Loss: 0.0721
ID Validation ROC-AUC: 0.9179
ID Validation Loss: 0.2928
ID Test ROC-AUC: 0.9171
ID Test Loss: 0.2997
OOD Validation ROC-AUC: 0.6280
OOD Validation Loss: 0.5802
OOD Test ROC-AUC: 0.6756
OOD Test Loss: 0.7411

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 2...
[0m[1;37mINFO[0m: [1mCheckpoint 2: 
-----------------------------------
Train ROC-AUC: 0.8761
Train Loss: 0.2569
ID Validation ROC-AUC: 0.8739
ID Validation Loss: 0.2600
ID Test ROC-AUC: 0.8719
ID Test Loss: 0.2637
OOD Validation ROC-AUC: 0.6912
OOD Validation Loss: 0.2846
OOD Test ROC-AUC: 0.7078
OOD Test Loss: 0.4249

[0m[1;37mINFO[0m: [1mChartInfo 0.9171 0.6756 0.8719 0.7078 0.8739 0.6912[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 02:29:03 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 02:29:12 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.75
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.662
SUFF++ for r=0.6 class 0.0 = 0.927 +- 0.016 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 class 1.0 = 0.966 +- 0.016 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 all KL = 0.992 +- 0.016 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 all L1 = 0.962 +- 0.046 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.631
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.596
SUFF++ for r=0.6 class 0.0 = 0.924 +- 0.018 (in-sample avg dev_std = 0.047)
SUFF++ for r=0.6 class 1.0 = 0.952 +- 0.018 (in-sample avg dev_std = 0.047)
SUFF++ for r=0.6 all KL = 0.988 +- 0.018 (in-sample avg dev_std = 0.047)
SUFF++ for r=0.6 all L1 = 0.947 +- 0.056 (in-sample avg dev_std = 0.047)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.75
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.741
NEC for r=0.6 class 0.0 = 0.047 +- 0.008 (in-sample avg dev_std = 0.016)
NEC for r=0.6 class 1.0 = 0.021 +- 0.008 (in-sample avg dev_std = 0.016)
NEC for r=0.6 all KL = 0.003 +- 0.008 (in-sample avg dev_std = 0.016)
NEC for r=0.6 all L1 = 0.024 +- 0.032 (in-sample avg dev_std = 0.016)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.631
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.627
NEC for r=0.6 class 0.0 = 0.05 +- 0.008 (in-sample avg dev_std = 0.021)
NEC for r=0.6 class 1.0 = 0.032 +- 0.008 (in-sample avg dev_std = 0.021)
NEC for r=0.6 all KL = 0.004 +- 0.008 (in-sample avg dev_std = 0.021)
NEC for r=0.6 all L1 = 0.035 +- 0.040 (in-sample avg dev_std = 0.021)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:30:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 02:30:19 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 02:30:55 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:07 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:19 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:35 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ROC-AUC: 0.9750
Train Loss: 0.1417
ID Validation ROC-AUC: 0.9182
ID Validation Loss: 0.2388
ID Test ROC-AUC: 0.9183
ID Test Loss: 0.2437
OOD Validation ROC-AUC: 0.6336
OOD Validation Loss: 0.4159
OOD Test ROC-AUC: 0.6692
OOD Test Loss: 0.5811

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8791
Train Loss: 0.2903
ID Validation ROC-AUC: 0.8687
ID Validation Loss: 0.2995
ID Test ROC-AUC: 0.8711
ID Test Loss: 0.2998
OOD Validation ROC-AUC: 0.6926
OOD Validation Loss: 0.2862
OOD Test ROC-AUC: 0.7124
OOD Test Loss: 0.4811

[0m[1;37mINFO[0m: [1mChartInfo 0.9183 0.6692 0.8711 0.7124 0.8687 0.6926[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 02:31:52 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 02:32:05 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.801
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.723
SUFF++ for r=0.6 class 0.0 = 0.905 +- 0.019 (in-sample avg dev_std = 0.038)
SUFF++ for r=0.6 class 1.0 = 0.967 +- 0.019 (in-sample avg dev_std = 0.038)
SUFF++ for r=0.6 all KL = 0.988 +- 0.019 (in-sample avg dev_std = 0.038)
SUFF++ for r=0.6 all L1 = 0.959 +- 0.057 (in-sample avg dev_std = 0.038)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.675
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.635
SUFF++ for r=0.6 class 0.0 = 0.911 +- 0.023 (in-sample avg dev_std = 0.047)
SUFF++ for r=0.6 class 1.0 = 0.95 +- 0.023 (in-sample avg dev_std = 0.047)
SUFF++ for r=0.6 all KL = 0.984 +- 0.023 (in-sample avg dev_std = 0.047)
SUFF++ for r=0.6 all L1 = 0.944 +- 0.067 (in-sample avg dev_std = 0.047)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.801
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.779
NEC for r=0.6 class 0.0 = 0.074 +- 0.011 (in-sample avg dev_std = 0.022)
NEC for r=0.6 class 1.0 = 0.026 +- 0.011 (in-sample avg dev_std = 0.022)
NEC for r=0.6 all KL = 0.007 +- 0.011 (in-sample avg dev_std = 0.022)
NEC for r=0.6 all L1 = 0.031 +- 0.047 (in-sample avg dev_std = 0.022)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.675
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.664
NEC for r=0.6 class 0.0 = 0.073 +- 0.016 (in-sample avg dev_std = 0.028)
NEC for r=0.6 class 1.0 = 0.04 +- 0.016 (in-sample avg dev_std = 0.028)
NEC for r=0.6 all KL = 0.009 +- 0.016 (in-sample avg dev_std = 0.028)
NEC for r=0.6 all L1 = 0.045 +- 0.058 (in-sample avg dev_std = 0.028)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:33:24 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 02:33:24 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 02:33:59 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 02:34:13 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 02:34:27 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 02:34:49 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ROC-AUC: 0.9750
Train Loss: 0.1492
ID Validation ROC-AUC: 0.9191
ID Validation Loss: 0.2545
ID Test ROC-AUC: 0.9158
ID Test Loss: 0.2649
OOD Validation ROC-AUC: 0.6267
OOD Validation Loss: 0.4135
OOD Test ROC-AUC: 0.6863
OOD Test Loss: 0.5801

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 6...
[0m[1;37mINFO[0m: [1mCheckpoint 6: 
-----------------------------------
Train ROC-AUC: 0.9012
Train Loss: 0.2347
ID Validation ROC-AUC: 0.8845
ID Validation Loss: 0.2520
ID Test ROC-AUC: 0.8857
ID Test Loss: 0.2551
OOD Validation ROC-AUC: 0.6780
OOD Validation Loss: 0.3280
OOD Test ROC-AUC: 0.7060
OOD Test Loss: 0.4756

[0m[1;37mINFO[0m: [1mChartInfo 0.9158 0.6863 0.8857 0.7060 0.8845 0.6780[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 02:35:10 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 02:35:21 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.469
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.539
SUFF++ for r=0.6 class 0.0 = 0.825 +- 0.046 (in-sample avg dev_std = 0.163)
SUFF++ for r=0.6 class 1.0 = 0.845 +- 0.046 (in-sample avg dev_std = 0.163)
SUFF++ for r=0.6 all KL = 0.953 +- 0.046 (in-sample avg dev_std = 0.163)
SUFF++ for r=0.6 all L1 = 0.842 +- 0.060 (in-sample avg dev_std = 0.163)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.496
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.517
SUFF++ for r=0.6 class 0.0 = 0.827 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.6 class 1.0 = 0.837 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.6 all KL = 0.952 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.6 all L1 = 0.836 +- 0.057 (in-sample avg dev_std = 0.167)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.469
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.472
NEC for r=0.6 class 0.0 = 0.06 +- 0.006 (in-sample avg dev_std = 0.041)
NEC for r=0.6 class 1.0 = 0.06 +- 0.006 (in-sample avg dev_std = 0.041)
NEC for r=0.6 all KL = 0.006 +- 0.006 (in-sample avg dev_std = 0.041)
NEC for r=0.6 all L1 = 0.06 +- 0.026 (in-sample avg dev_std = 0.041)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.496
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.499
NEC for r=0.6 class 0.0 = 0.064 +- 0.006 (in-sample avg dev_std = 0.045)
NEC for r=0.6 class 1.0 = 0.062 +- 0.006 (in-sample avg dev_std = 0.045)
NEC for r=0.6 all KL = 0.006 +- 0.006 (in-sample avg dev_std = 0.045)
NEC for r=0.6 all L1 = 0.063 +- 0.028 (in-sample avg dev_std = 0.045)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:36:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 02:36:35 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 02:37:13 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 02:37:27 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 02:37:41 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 02:37:59 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 38...
[0m[1;37mINFO[0m: [1mCheckpoint 38: 
-----------------------------------
Train ROC-AUC: 0.9770
Train Loss: 0.1340
ID Validation ROC-AUC: 0.9225
ID Validation Loss: 0.2679
ID Test ROC-AUC: 0.9232
ID Test Loss: 0.2743
OOD Validation ROC-AUC: 0.6385
OOD Validation Loss: 0.5014
OOD Test ROC-AUC: 0.6848
OOD Test Loss: 0.6911

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 7...
[0m[1;37mINFO[0m: [1mCheckpoint 7: 
-----------------------------------
Train ROC-AUC: 0.8995
Train Loss: 0.2412
ID Validation ROC-AUC: 0.8839
ID Validation Loss: 0.2545
ID Test ROC-AUC: 0.8857
ID Test Loss: 0.2576
OOD Validation ROC-AUC: 0.6922
OOD Validation Loss: 0.2925
OOD Test ROC-AUC: 0.7003
OOD Test Loss: 0.4517

[0m[1;37mINFO[0m: [1mChartInfo 0.9232 0.6848 0.8857 0.7003 0.8839 0.6922[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 02:38:16 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 02:38:26 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.76
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.735
SUFF++ for r=0.6 class 0.0 = 0.872 +- 0.035 (in-sample avg dev_std = 0.076)
SUFF++ for r=0.6 class 1.0 = 0.95 +- 0.035 (in-sample avg dev_std = 0.076)
SUFF++ for r=0.6 all KL = 0.98 +- 0.035 (in-sample avg dev_std = 0.076)
SUFF++ for r=0.6 all L1 = 0.941 +- 0.076 (in-sample avg dev_std = 0.076)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.651
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.638
SUFF++ for r=0.6 class 0.0 = 0.88 +- 0.039 (in-sample avg dev_std = 0.094)
SUFF++ for r=0.6 class 1.0 = 0.933 +- 0.039 (in-sample avg dev_std = 0.094)
SUFF++ for r=0.6 all KL = 0.974 +- 0.039 (in-sample avg dev_std = 0.094)
SUFF++ for r=0.6 all L1 = 0.924 +- 0.089 (in-sample avg dev_std = 0.094)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.76
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.751
NEC for r=0.6 class 0.0 = 0.094 +- 0.018 (in-sample avg dev_std = 0.046)
NEC for r=0.6 class 1.0 = 0.034 +- 0.018 (in-sample avg dev_std = 0.046)
NEC for r=0.6 all KL = 0.009 +- 0.018 (in-sample avg dev_std = 0.046)
NEC for r=0.6 all L1 = 0.041 +- 0.056 (in-sample avg dev_std = 0.046)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.651
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.649
NEC for r=0.6 class 0.0 = 0.09 +- 0.027 (in-sample avg dev_std = 0.063)
NEC for r=0.6 class 1.0 = 0.048 +- 0.027 (in-sample avg dev_std = 0.063)
NEC for r=0.6 all KL = 0.013 +- 0.027 (in-sample avg dev_std = 0.063)
NEC for r=0.6 all L1 = 0.055 +- 0.074 (in-sample avg dev_std = 0.063)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [0.996]}), defaultdict(<class 'list'>, {'all_KL': [0.992], 'all_L1': [0.962]}), defaultdict(<class 'list'>, {'all_KL': [0.988], 'all_L1': [0.959]}), defaultdict(<class 'list'>, {'all_KL': [0.953], 'all_L1': [0.842]}), defaultdict(<class 'list'>, {'all_KL': [0.98], 'all_L1': [0.941]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.003]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.024]}), defaultdict(<class 'list'>, {'all_KL': [0.007], 'all_L1': [0.031]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.06]}), defaultdict(<class 'list'>, {'all_KL': [0.009], 'all_L1': [0.041]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.994]}), defaultdict(<class 'list'>, {'all_KL': [0.988], 'all_L1': [0.947]}), defaultdict(<class 'list'>, {'all_KL': [0.984], 'all_L1': [0.944]}), defaultdict(<class 'list'>, {'all_KL': [0.952], 'all_L1': [0.836]}), defaultdict(<class 'list'>, {'all_KL': [0.974], 'all_L1': [0.924]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.005]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.035]}), defaultdict(<class 'list'>, {'all_KL': [0.009], 'all_L1': [0.045]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.063]}), defaultdict(<class 'list'>, {'all_KL': [0.013], 'all_L1': [0.055]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.940 +- 0.052
suff++ class all_KL  =  0.982 +- 0.016
suff++_acc_int  =  0.676 +- 0.073
nec class all_L1  =  0.032 +- 0.019
nec class all_KL  =  0.005 +- 0.003
nec_acc_int  =  0.703 +- 0.116

Eval split test
suff++ class all_L1  =  0.929 +- 0.052
suff++ class all_KL  =  0.979 +- 0.015
suff++_acc_int  =  0.599 +- 0.044
nec class all_L1  =  0.041 +- 0.020
nec class all_KL  =  0.007 +- 0.004
nec_acc_int  =  0.616 +- 0.060


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.486 +- 0.018
Faith. Armon (L1)= 		  =  0.061 +- 0.035
Faith. GMean (L1)= 	  =  0.160 +- 0.058
Faith. Aritm (KL)= 		  =  0.494 +- 0.007
Faith. Armon (KL)= 		  =  0.010 +- 0.006
Faith. GMean (KL)= 	  =  0.068 +- 0.022

Eval split test
Faith. Aritm (L1)= 		  =  0.485 +- 0.018
Faith. Armon (L1)= 		  =  0.077 +- 0.037
Faith. GMean (L1)= 	  =  0.183 +- 0.059
Faith. Aritm (KL)= 		  =  0.493 +- 0.007
Faith. Armon (KL)= 		  =  0.013 +- 0.008
Faith. GMean (KL)= 	  =  0.075 +- 0.028
Computed for split load_split = id



Completed in  0:14:54.371943  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:40:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:40:01 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.6868
Train Loss: 1.1340
ID Validation ACCURACY: 0.6349
ID Validation Loss: 1.4523
ID Test ACCURACY: 0.6443
ID Test Loss: 1.4493
OOD Validation ACCURACY: 0.5267
OOD Validation Loss: 2.4705
OOD Test ACCURACY: 0.2393
OOD Test Loss: 50.3153

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 85...
[0m[1;37mINFO[0m: [1mCheckpoint 85: 
-----------------------------------
Train ACCURACY: 0.6818
Train Loss: 1.0209
ID Validation ACCURACY: 0.6344
ID Validation Loss: 1.2867
ID Test ACCURACY: 0.6417
ID Test Loss: 1.2611
OOD Validation ACCURACY: 0.5907
OOD Validation Loss: 1.6844
OOD Test ACCURACY: 0.2530
OOD Test Loss: 30.3575

[0m[1;37mINFO[0m: [1mChartInfo 0.6443 0.2393 0.6417 0.2530 0.6344 0.5907[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.696
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.352
SUFF++ for r=0.6 class 0 = 0.837 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 1 = 0.42 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 2 = 0.263 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 3 = 0.257 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 4 = 0.293 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 5 = 0.299 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 6 = 0.263 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 7 = 0.3 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 8 = 0.299 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 class 9 = 0.429 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 all KL = 0.117 +- 0.226 (in-sample avg dev_std = 0.730)
SUFF++ for r=0.6 all L1 = 0.366 +- 0.204 (in-sample avg dev_std = 0.730)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.211
SUFF++ for r=0.6 class 0 = 0.368 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 1 = 0.628 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 2 = 0.263 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 3 = 0.284 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 4 = 0.346 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 5 = 0.275 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 6 = 0.356 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 7 = 0.316 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 8 = 0.323 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 class 9 = 0.393 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 all KL = 0.022 +- 0.107 (in-sample avg dev_std = 0.792)
SUFF++ for r=0.6 all L1 = 0.358 +- 0.185 (in-sample avg dev_std = 0.792)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.696
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.562
NEC for r=0.6 class 0 = 0.079 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 1 = 0.045 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 2 = 0.512 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 3 = 0.565 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 4 = 0.498 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 5 = 0.477 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 6 = 0.541 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 7 = 0.398 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 8 = 0.542 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 class 9 = 0.415 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 all KL = 0.484 +- 0.333 (in-sample avg dev_std = 0.418)
NEC for r=0.6 all L1 = 0.402 +- 0.274 (in-sample avg dev_std = 0.418)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.278
NEC for r=0.6 class 0 = 0.305 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 1 = 0.012 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 2 = 0.585 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 3 = 0.496 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 4 = 0.415 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 5 = 0.532 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 6 = 0.363 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 7 = 0.455 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 8 = 0.42 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 9 = 0.194 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 all KL = 0.537 +- 0.442 (in-sample avg dev_std = 0.388)
NEC for r=0.6 all L1 = 0.373 +- 0.357 (in-sample avg dev_std = 0.388)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:45:54 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:55 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:55 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:55 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:45:56 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.3406
Train Loss: 2.8099
ID Validation ACCURACY: 0.3361
ID Validation Loss: 2.8348
ID Test ACCURACY: 0.3410
ID Test Loss: 2.8208
OOD Validation ACCURACY: 0.3401
OOD Validation Loss: 3.0656
OOD Test ACCURACY: 0.2519
OOD Test Loss: 4.4899

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.3406
Train Loss: 2.8099
ID Validation ACCURACY: 0.3361
ID Validation Loss: 2.8348
ID Test ACCURACY: 0.3410
ID Test Loss: 2.8208
OOD Validation ACCURACY: 0.3401
OOD Validation Loss: 3.0656
OOD Test ACCURACY: 0.2519
OOD Test Loss: 4.4899

[0m[1;37mINFO[0m: [1mChartInfo 0.3410 0.2519 0.3410 0.2519 0.3361 0.3401[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.334
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.184
SUFF++ for r=0.6 class 0 = 0.536 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 1 = 0.372 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 2 = 0.331 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 3 = 0.322 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 4 = 0.465 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 5 = 0.321 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 6 = 0.406 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 7 = 0.373 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 8 = 0.371 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 class 9 = 0.472 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 all KL = 0.173 +- 0.176 (in-sample avg dev_std = 0.643)
SUFF++ for r=0.6 all L1 = 0.396 +- 0.150 (in-sample avg dev_std = 0.643)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.182
SUFF++ for r=0.6 class 0 = 0.307 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 1 = 0.383 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 2 = 0.29 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 3 = 0.313 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 4 = 0.313 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 5 = 0.33 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 6 = 0.335 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 7 = 0.351 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 8 = 0.339 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 class 9 = 0.339 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 all KL = 0.228 +- 0.192 (in-sample avg dev_std = 0.621)
SUFF++ for r=0.6 all L1 = 0.33 +- 0.115 (in-sample avg dev_std = 0.621)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.334
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.282
NEC for r=0.6 class 0 = 0.16 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 1 = 0.333 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 2 = 0.49 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 3 = 0.505 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 4 = 0.285 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 5 = 0.481 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 6 = 0.421 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 7 = 0.456 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 8 = 0.438 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 9 = 0.238 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all KL = 0.313 +- 0.225 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all L1 = 0.382 +- 0.213 (in-sample avg dev_std = 0.261)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.256
NEC for r=0.6 class 0 = 0.494 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 1 = 0.181 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 2 = 0.498 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 3 = 0.491 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 4 = 0.506 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 5 = 0.498 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 6 = 0.527 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 7 = 0.517 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 8 = 0.48 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 class 9 = 0.52 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 all KL = 0.39 +- 0.211 (in-sample avg dev_std = 0.295)
NEC for r=0.6 all L1 = 0.468 +- 0.183 (in-sample avg dev_std = 0.295)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:51:44 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:51:45 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 90...
[0m[1;37mINFO[0m: [1mCheckpoint 90: 
-----------------------------------
Train ACCURACY: 0.7166
Train Loss: 0.8744
ID Validation ACCURACY: 0.6586
ID Validation Loss: 1.1864
ID Test ACCURACY: 0.6627
ID Test Loss: 1.1704
OOD Validation ACCURACY: 0.5016
OOD Validation Loss: 2.0563
OOD Test ACCURACY: 0.2053
OOD Test Loss: 7.1389

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 29...
[0m[1;37mINFO[0m: [1mCheckpoint 29: 
-----------------------------------
Train ACCURACY: 0.5742
Train Loss: 1.2504
ID Validation ACCURACY: 0.5544
ID Validation Loss: 1.3208
ID Test ACCURACY: 0.5559
ID Test Loss: 1.3027
OOD Validation ACCURACY: 0.5757
OOD Validation Loss: 1.2810
OOD Test ACCURACY: 0.2889
OOD Test Loss: 2.5977

[0m[1;37mINFO[0m: [1mChartInfo 0.6627 0.2053 0.5559 0.2889 0.5544 0.5757[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.694
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.276
SUFF++ for r=0.6 class 0 = 0.238 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 1 = 0.333 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 2 = 0.273 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 3 = 0.282 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 4 = 0.257 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 5 = 0.275 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 6 = 0.236 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 7 = 0.424 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 8 = 0.258 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 class 9 = 0.258 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 all KL = 0.084 +- 0.137 (in-sample avg dev_std = 0.650)
SUFF++ for r=0.6 all L1 = 0.286 +- 0.114 (in-sample avg dev_std = 0.650)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.188
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.173
SUFF++ for r=0.6 class 0 = 0.262 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 1 = 0.421 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 2 = 0.269 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 3 = 0.3 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 4 = 0.299 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 5 = 0.293 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 6 = 0.311 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 7 = 0.308 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 8 = 0.301 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 class 9 = 0.319 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 all KL = 0.097 +- 0.143 (in-sample avg dev_std = 0.695)
SUFF++ for r=0.6 all L1 = 0.309 +- 0.108 (in-sample avg dev_std = 0.695)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.694
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.435
NEC for r=0.6 class 0 = 0.602 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 1 = 0.155 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 2 = 0.585 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 3 = 0.627 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 4 = 0.635 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 5 = 0.628 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 6 = 0.651 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 7 = 0.552 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 8 = 0.6 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 9 = 0.613 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 all KL = 0.704 +- 0.270 (in-sample avg dev_std = 0.463)
NEC for r=0.6 all L1 = 0.558 +- 0.232 (in-sample avg dev_std = 0.463)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.188
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.214
NEC for r=0.6 class 0 = 0.577 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 1 = 0.173 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 2 = 0.589 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 3 = 0.561 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 4 = 0.492 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 5 = 0.56 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 6 = 0.434 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 7 = 0.483 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 8 = 0.48 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 class 9 = 0.382 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 all KL = 0.516 +- 0.311 (in-sample avg dev_std = 0.386)
NEC for r=0.6 all L1 = 0.47 +- 0.254 (in-sample avg dev_std = 0.386)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 14:57:41 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 02:57:42 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 94...
[0m[1;37mINFO[0m: [1mCheckpoint 94: 
-----------------------------------
Train ACCURACY: 0.4384
Train Loss: 2.1899
ID Validation ACCURACY: 0.4306
ID Validation Loss: 2.2851
ID Test ACCURACY: 0.4244
ID Test Loss: 2.2523
OOD Validation ACCURACY: 0.3311
OOD Validation Loss: 3.7102
OOD Test ACCURACY: 0.2449
OOD Test Loss: 7.9565

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.4047
Train Loss: 1.8306
ID Validation ACCURACY: 0.3959
ID Validation Loss: 1.8921
ID Test ACCURACY: 0.4023
ID Test Loss: 1.8483
OOD Validation ACCURACY: 0.3679
OOD Validation Loss: 2.1088
OOD Test ACCURACY: 0.2666
OOD Test Loss: 3.2335

[0m[1;37mINFO[0m: [1mChartInfo 0.4244 0.2449 0.4023 0.2666 0.3959 0.3679[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.261
SUFF++ for r=0.6 class 0 = 0.782 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 1 = 0.431 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 2 = 0.286 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 3 = 0.289 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 4 = 0.282 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 5 = 0.293 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 6 = 0.286 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 7 = 0.286 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 8 = 0.292 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 9 = 0.323 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 all KL = 0.148 +- 0.250 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 all L1 = 0.356 +- 0.200 (in-sample avg dev_std = 0.639)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.239
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.198
SUFF++ for r=0.6 class 0 = 0.358 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 1 = 0.633 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 2 = 0.283 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 3 = 0.274 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 4 = 0.286 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 5 = 0.306 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 6 = 0.297 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 7 = 0.295 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 8 = 0.279 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 class 9 = 0.332 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 all KL = 0.058 +- 0.143 (in-sample avg dev_std = 0.739)
SUFF++ for r=0.6 all L1 = 0.338 +- 0.171 (in-sample avg dev_std = 0.739)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.386
NEC for r=0.6 class 0 = 0.223 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 1 = 0.158 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 2 = 0.586 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 3 = 0.539 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 4 = 0.533 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 5 = 0.552 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 6 = 0.554 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 7 = 0.548 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 8 = 0.537 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 9 = 0.457 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 all KL = 0.472 +- 0.267 (in-sample avg dev_std = 0.353)
NEC for r=0.6 all L1 = 0.464 +- 0.237 (in-sample avg dev_std = 0.353)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.239
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.273
NEC for r=0.6 class 0 = 0.321 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 1 = 0.071 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 2 = 0.517 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 3 = 0.556 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 4 = 0.516 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 5 = 0.509 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 6 = 0.525 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 7 = 0.571 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 8 = 0.531 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 class 9 = 0.463 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 all KL = 0.553 +- 0.353 (in-sample avg dev_std = 0.449)
NEC for r=0.6 all L1 = 0.453 +- 0.289 (in-sample avg dev_std = 0.449)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 15:03:30 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 03:03:31 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 03:03:32 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 88...
[0m[1;37mINFO[0m: [1mCheckpoint 88: 
-----------------------------------
Train ACCURACY: 0.4462
Train Loss: 2.1957
ID Validation ACCURACY: 0.4340
ID Validation Loss: 2.2713
ID Test ACCURACY: 0.4387
ID Test Loss: 2.2521
OOD Validation ACCURACY: 0.3690
OOD Validation Loss: 3.1579
OOD Test ACCURACY: 0.2166
OOD Test Loss: 6.3998

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 75...
[0m[1;37mINFO[0m: [1mCheckpoint 75: 
-----------------------------------
Train ACCURACY: 0.4253
Train Loss: 2.0693
ID Validation ACCURACY: 0.4229
ID Validation Loss: 2.1638
ID Test ACCURACY: 0.4240
ID Test Loss: 2.1236
OOD Validation ACCURACY: 0.3857
OOD Validation Loss: 2.7142
OOD Test ACCURACY: 0.2043
OOD Test Loss: 5.5404

[0m[1;37mINFO[0m: [1mChartInfo 0.4387 0.2166 0.4240 0.2043 0.4229 0.3857[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.466
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.25
SUFF++ for r=0.6 class 0 = 0.303 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 1 = 0.598 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 2 = 0.231 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 3 = 0.264 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 4 = 0.234 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 5 = 0.242 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 6 = 0.237 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 7 = 0.269 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 8 = 0.242 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 class 9 = 0.276 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 all KL = 0.094 +- 0.202 (in-sample avg dev_std = 0.681)
SUFF++ for r=0.6 all L1 = 0.294 +- 0.148 (in-sample avg dev_std = 0.681)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.195
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.168
SUFF++ for r=0.6 class 0 = 0.429 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 1 = 0.391 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 2 = 0.384 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 3 = 0.344 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 4 = 0.412 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 5 = 0.369 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 6 = 0.384 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 7 = 0.359 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 8 = 0.37 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 class 9 = 0.377 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 all KL = 0.196 +- 0.196 (in-sample avg dev_std = 0.629)
SUFF++ for r=0.6 all L1 = 0.382 +- 0.149 (in-sample avg dev_std = 0.629)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.466
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.394
NEC for r=0.6 class 0 = 0.282 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 1 = 0.355 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 2 = 0.595 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 3 = 0.525 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 4 = 0.591 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 5 = 0.6 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 6 = 0.568 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 7 = 0.536 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 8 = 0.554 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 class 9 = 0.479 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 all KL = 0.541 +- 0.278 (in-sample avg dev_std = 0.380)
NEC for r=0.6 all L1 = 0.505 +- 0.220 (in-sample avg dev_std = 0.380)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.195
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.213
NEC for r=0.6 class 0 = 0.349 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 1 = 0.405 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 2 = 0.488 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 3 = 0.508 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 4 = 0.499 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 5 = 0.497 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 6 = 0.478 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 7 = 0.537 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 8 = 0.491 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 class 9 = 0.489 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 all KL = 0.525 +- 0.290 (in-sample avg dev_std = 0.446)
NEC for r=0.6 all L1 = 0.473 +- 0.218 (in-sample avg dev_std = 0.446)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.117], 'all_L1': [0.366]}), defaultdict(<class 'list'>, {'all_KL': [0.173], 'all_L1': [0.396]}), defaultdict(<class 'list'>, {'all_KL': [0.084], 'all_L1': [0.286]}), defaultdict(<class 'list'>, {'all_KL': [0.148], 'all_L1': [0.356]}), defaultdict(<class 'list'>, {'all_KL': [0.094], 'all_L1': [0.294]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.484], 'all_L1': [0.402]}), defaultdict(<class 'list'>, {'all_KL': [0.313], 'all_L1': [0.382]}), defaultdict(<class 'list'>, {'all_KL': [0.704], 'all_L1': [0.558]}), defaultdict(<class 'list'>, {'all_KL': [0.472], 'all_L1': [0.464]}), defaultdict(<class 'list'>, {'all_KL': [0.541], 'all_L1': [0.505]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.022], 'all_L1': [0.358]}), defaultdict(<class 'list'>, {'all_KL': [0.228], 'all_L1': [0.33]}), defaultdict(<class 'list'>, {'all_KL': [0.097], 'all_L1': [0.309]}), defaultdict(<class 'list'>, {'all_KL': [0.058], 'all_L1': [0.338]}), defaultdict(<class 'list'>, {'all_KL': [0.196], 'all_L1': [0.382]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.537], 'all_L1': [0.373]}), defaultdict(<class 'list'>, {'all_KL': [0.39], 'all_L1': [0.468]}), defaultdict(<class 'list'>, {'all_KL': [0.516], 'all_L1': [0.47]}), defaultdict(<class 'list'>, {'all_KL': [0.553], 'all_L1': [0.453]}), defaultdict(<class 'list'>, {'all_KL': [0.525], 'all_L1': [0.473]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.340 +- 0.043
suff++ class all_KL  =  0.123 +- 0.033
suff++_acc_int  =  0.264 +- 0.054
nec class all_L1  =  0.462 +- 0.065
nec class all_KL  =  0.503 +- 0.126
nec_acc_int  =  0.412 +- 0.091

Eval split test
suff++ class all_L1  =  0.343 +- 0.025
suff++ class all_KL  =  0.120 +- 0.079
suff++_acc_int  =  0.186 +- 0.016
nec class all_L1  =  0.447 +- 0.038
nec class all_KL  =  0.504 +- 0.058
nec_acc_int  =  0.247 +- 0.028


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.401 +- 0.014
Faith. Armon (L1)= 		  =  0.385 +- 0.011
Faith. GMean (L1)= 	  =  0.393 +- 0.009
Faith. Aritm (KL)= 		  =  0.313 +- 0.048
Faith. Armon (KL)= 		  =  0.189 +- 0.031
Faith. GMean (KL)= 	  =  0.241 +- 0.013

Eval split test
Faith. Aritm (L1)= 		  =  0.395 +- 0.020
Faith. Armon (L1)= 		  =  0.387 +- 0.020
Faith. GMean (L1)= 	  =  0.391 +- 0.020
Faith. Aritm (KL)= 		  =  0.312 +- 0.026
Faith. Armon (KL)= 		  =  0.177 +- 0.098
Faith. GMean (KL)= 	  =  0.226 +- 0.078
Computed for split load_split = id



Completed in  0:29:43.369896  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color
DONE all :)
