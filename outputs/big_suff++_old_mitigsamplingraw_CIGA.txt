nohup: ignoring input

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:26:08 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:08 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:21 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:23 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:25 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:27 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:26:29 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ACCURACY: 0.7836
Train Loss: 0.6268
ID Validation ACCURACY: 0.7810
ID Validation Loss: 0.6189
ID Test ACCURACY: 0.7937
ID Test Loss: 0.6184
OOD Validation ACCURACY: 0.9207
OOD Validation Loss: 0.3550
OOD Test ACCURACY: 0.8803
OOD Test Loss: 0.4248

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.7018
Train Loss: 0.6760
ID Validation ACCURACY: 0.7110
ID Validation Loss: 0.6712
ID Test ACCURACY: 0.7193
ID Test Loss: 0.6608
OOD Validation ACCURACY: 0.9307
OOD Validation Loss: 0.4142
OOD Test ACCURACY: 0.5897
OOD Test Loss: 0.7743

[0m[1;37mINFO[0m: [1mChartInfo 0.7937 0.8803 0.7193 0.5897 0.7110 0.9307[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.416
WIoU for r=0.6 = 0.385
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 1.000
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.717
WIoU for r=0.6 = 0.674


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.78
Model XAI F1 of binarized graphs for r=0.6 =  0.41569249999999996
Model XAI WIoU of binarized graphs for r=0.6 =  0.3853512499999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.58
SUFF++ for r=0.6 class 0 = 0.616 +- 0.208 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.6 class 1 = 0.747 +- 0.208 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.6 class 2 = 0.597 +- 0.208 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.6 all KL = 0.769 +- 0.208 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.6 all L1 = 0.654 +- 0.163 (in-sample avg dev_std = 0.328)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.928
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9999375
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.93
SUFF++ for r=0.6 class 0 = 0.9 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 class 1 = 0.916 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 class 2 = 0.893 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 all KL = 0.971 +- 0.046 (in-sample avg dev_std = 0.116)
SUFF++ for r=0.6 all L1 = 0.903 +- 0.073 (in-sample avg dev_std = 0.116)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.891
Model XAI F1 of binarized graphs for r=0.6 =  0.7171774999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.67429125
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.864
SUFF++ for r=0.6 class 0 = 0.795 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 class 1 = 0.94 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 class 2 = 0.86 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 all KL = 0.937 +- 0.088 (in-sample avg dev_std = 0.186)
SUFF++ for r=0.6 all L1 = 0.864 +- 0.108 (in-sample avg dev_std = 0.186)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.78
Model XAI F1 of binarized graphs for r=0.6 =  0.41569249999999996
Model XAI WIoU of binarized graphs for r=0.6 =  0.3853512499999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.433
NEC for r=0.6 class 0 = 0.528 +- 0.341 (in-sample avg dev_std = 0.288)
NEC for r=0.6 class 1 = 0.29 +- 0.341 (in-sample avg dev_std = 0.288)
NEC for r=0.6 class 2 = 0.583 +- 0.341 (in-sample avg dev_std = 0.288)
NEC for r=0.6 all KL = 0.379 +- 0.341 (in-sample avg dev_std = 0.288)
NEC for r=0.6 all L1 = 0.467 +- 0.246 (in-sample avg dev_std = 0.288)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.928
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9999375
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.574
NEC for r=0.6 class 0 = 0.541 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 class 1 = 0.111 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 class 2 = 0.62 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 all KL = 0.446 +- 0.372 (in-sample avg dev_std = 0.574)
NEC for r=0.6 all L1 = 0.425 +- 0.285 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.891
Model XAI F1 of binarized graphs for r=0.6 =  0.7171774999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.67429125
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.514
NEC for r=0.6 class 0 = 0.575 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 class 1 = 0.104 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 class 2 = 0.674 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 all KL = 0.461 +- 0.366 (in-sample avg dev_std = 0.491)
NEC for r=0.6 all L1 = 0.457 +- 0.295 (in-sample avg dev_std = 0.491)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:27:34 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:34 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:47 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:49 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:51 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:53 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:27:55 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 2...
[0m[1;37mINFO[0m: [1mCheckpoint 2: 
-----------------------------------
Train ACCURACY: 0.7278
Train Loss: 0.7861
ID Validation ACCURACY: 0.7147
ID Validation Loss: 0.7972
ID Test ACCURACY: 0.7297
ID Test Loss: 0.8069
OOD Validation ACCURACY: 0.3787
OOD Validation Loss: 1.8551
OOD Test ACCURACY: 0.3293
OOD Test Loss: 4.7809

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 53...
[0m[1;37mINFO[0m: [1mCheckpoint 53: 
-----------------------------------
Train ACCURACY: 0.5174
Train Loss: 1.0790
ID Validation ACCURACY: 0.4963
ID Validation Loss: 1.1107
ID Test ACCURACY: 0.5230
ID Test Loss: 1.0871
OOD Validation ACCURACY: 0.8353
OOD Validation Loss: 0.7218
OOD Test ACCURACY: 0.4657
OOD Test Loss: 1.6445

[0m[1;37mINFO[0m: [1mChartInfo 0.7297 0.3293 0.5230 0.4657 0.4963 0.8353[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.283
WIoU for r=0.6 = 0.182
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.089
WIoU for r=0.6 = 0.036
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.201
WIoU for r=0.6 = 0.103


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.701
Model XAI F1 of binarized graphs for r=0.6 =  0.28333624999999996
Model XAI WIoU of binarized graphs for r=0.6 =  0.18214249999999998
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.427
SUFF++ for r=0.6 class 0 = 0.342 +- 0.289 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 class 1 = 0.582 +- 0.289 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 class 2 = 0.363 +- 0.289 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 all KL = 0.399 +- 0.289 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.6 all L1 = 0.43 +- 0.210 (in-sample avg dev_std = 0.448)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.376
Model XAI F1 of binarized graphs for r=0.6 =  0.08861624999999998
Model XAI WIoU of binarized graphs for r=0.6 =  0.03595875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.332
SUFF++ for r=0.6 class 0 = 0.802 +- 0.127 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.6 class 1 = 0.856 +- 0.127 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.6 class 2 = 0.775 +- 0.127 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.6 all KL = 0.899 +- 0.127 (in-sample avg dev_std = 0.128)
SUFF++ for r=0.6 all L1 = 0.811 +- 0.170 (in-sample avg dev_std = 0.128)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.327
Model XAI F1 of binarized graphs for r=0.6 =  0.200915
Model XAI WIoU of binarized graphs for r=0.6 =  0.1025225
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.305
SUFF++ for r=0.6 class 0 = 0.922 +- 0.191 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.6 class 1 = 0.83 +- 0.191 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.6 class 2 = 0.919 +- 0.191 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.6 all KL = 0.86 +- 0.191 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.6 all L1 = 0.891 +- 0.138 (in-sample avg dev_std = 0.253)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.701
Model XAI F1 of binarized graphs for r=0.6 =  0.28333624999999996
Model XAI WIoU of binarized graphs for r=0.6 =  0.18214249999999998
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.522
NEC for r=0.6 class 0 = 0.468 +- 0.249 (in-sample avg dev_std = 0.436)
NEC for r=0.6 class 1 = 0.413 +- 0.249 (in-sample avg dev_std = 0.436)
NEC for r=0.6 class 2 = 0.467 +- 0.249 (in-sample avg dev_std = 0.436)
NEC for r=0.6 all KL = 0.408 +- 0.249 (in-sample avg dev_std = 0.436)
NEC for r=0.6 all L1 = 0.45 +- 0.175 (in-sample avg dev_std = 0.436)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.376
Model XAI F1 of binarized graphs for r=0.6 =  0.08861624999999998
Model XAI WIoU of binarized graphs for r=0.6 =  0.03595875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.391
NEC for r=0.6 class 0 = 0.184 +- 0.096 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 1 = 0.149 +- 0.096 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 2 = 0.167 +- 0.096 (in-sample avg dev_std = 0.113)
NEC for r=0.6 all KL = 0.072 +- 0.096 (in-sample avg dev_std = 0.113)
NEC for r=0.6 all L1 = 0.167 +- 0.143 (in-sample avg dev_std = 0.113)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.327
Model XAI F1 of binarized graphs for r=0.6 =  0.200915
Model XAI WIoU of binarized graphs for r=0.6 =  0.1025225
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.343
NEC for r=0.6 class 0 = 0.171 +- 0.254 (in-sample avg dev_std = 0.320)
NEC for r=0.6 class 1 = 0.187 +- 0.254 (in-sample avg dev_std = 0.320)
NEC for r=0.6 class 2 = 0.167 +- 0.254 (in-sample avg dev_std = 0.320)
NEC for r=0.6 all KL = 0.211 +- 0.254 (in-sample avg dev_std = 0.320)
NEC for r=0.6 all L1 = 0.175 +- 0.209 (in-sample avg dev_std = 0.320)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:28:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/12/2024 11:28:58 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:11 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:13 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:15 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:17 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:29:19 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 80...
[0m[1;37mINFO[0m: [1mCheckpoint 80: 
-----------------------------------
Train ACCURACY: 0.7508
Train Loss: 0.6072
ID Validation ACCURACY: 0.7547
ID Validation Loss: 0.6103
ID Test ACCURACY: 0.7617
ID Test Loss: 0.6033
OOD Validation ACCURACY: 0.9290
OOD Validation Loss: 0.3927
OOD Test ACCURACY: 0.7843
OOD Test Loss: 0.5272

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 24...
[0m[1;37mINFO[0m: [1mCheckpoint 24: 
-----------------------------------
Train ACCURACY: 0.7236
Train Loss: 0.6539
ID Validation ACCURACY: 0.7353
ID Validation Loss: 0.6573
ID Test ACCURACY: 0.7297
ID Test Loss: 0.6396
OOD Validation ACCURACY: 0.9310
OOD Validation Loss: 0.4148
OOD Test ACCURACY: 0.8723
OOD Test Loss: 0.4403

[0m[1;37mINFO[0m: [1mChartInfo 0.7617 0.7843 0.7297 0.8723 0.7353 0.9310[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.405
WIoU for r=0.6 = 0.375
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 0.998
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.711
WIoU for r=0.6 = 0.652


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.741
Model XAI F1 of binarized graphs for r=0.6 =  0.40483500000000006
Model XAI WIoU of binarized graphs for r=0.6 =  0.37461875000000006
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.547
SUFF++ for r=0.6 class 0 = 0.598 +- 0.225 (in-sample avg dev_std = 0.398)
SUFF++ for r=0.6 class 1 = 0.698 +- 0.225 (in-sample avg dev_std = 0.398)
SUFF++ for r=0.6 class 2 = 0.599 +- 0.225 (in-sample avg dev_std = 0.398)
SUFF++ for r=0.6 all KL = 0.734 +- 0.225 (in-sample avg dev_std = 0.398)
SUFF++ for r=0.6 all L1 = 0.632 +- 0.161 (in-sample avg dev_std = 0.398)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.93
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9978212500000001
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.927
SUFF++ for r=0.6 class 0 = 0.859 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 class 1 = 0.869 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 class 2 = 0.86 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 all KL = 0.951 +- 0.071 (in-sample avg dev_std = 0.166)
SUFF++ for r=0.6 all L1 = 0.863 +- 0.077 (in-sample avg dev_std = 0.166)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.794
Model XAI F1 of binarized graphs for r=0.6 =  0.7108775
Model XAI WIoU of binarized graphs for r=0.6 =  0.6517449999999999
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.805
SUFF++ for r=0.6 class 0 = 0.772 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 class 1 = 0.918 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 class 2 = 0.82 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 all KL = 0.914 +- 0.140 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.6 all L1 = 0.835 +- 0.118 (in-sample avg dev_std = 0.234)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.741
Model XAI F1 of binarized graphs for r=0.6 =  0.40483500000000006
Model XAI WIoU of binarized graphs for r=0.6 =  0.37461875000000006
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.431
NEC for r=0.6 class 0 = 0.544 +- 0.341 (in-sample avg dev_std = 0.302)
NEC for r=0.6 class 1 = 0.279 +- 0.341 (in-sample avg dev_std = 0.302)
NEC for r=0.6 class 2 = 0.583 +- 0.341 (in-sample avg dev_std = 0.302)
NEC for r=0.6 all KL = 0.379 +- 0.341 (in-sample avg dev_std = 0.302)
NEC for r=0.6 all L1 = 0.469 +- 0.254 (in-sample avg dev_std = 0.302)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.93
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9978212500000001
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.616
NEC for r=0.6 class 0 = 0.534 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 class 1 = 0.063 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 class 2 = 0.514 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 all KL = 0.376 +- 0.330 (in-sample avg dev_std = 0.591)
NEC for r=0.6 all L1 = 0.372 +- 0.277 (in-sample avg dev_std = 0.591)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.794
Model XAI F1 of binarized graphs for r=0.6 =  0.7108775
Model XAI WIoU of binarized graphs for r=0.6 =  0.6517449999999999
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.515
NEC for r=0.6 class 0 = 0.565 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 class 1 = 0.097 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 class 2 = 0.563 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 all KL = 0.37 +- 0.334 (in-sample avg dev_std = 0.460)
NEC for r=0.6 all L1 = 0.414 +- 0.275 (in-sample avg dev_std = 0.460)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.769], 'all_L1': [0.654]}), defaultdict(<class 'list'>, {'all_KL': [0.399], 'all_L1': [0.43]}), defaultdict(<class 'list'>, {'all_KL': [0.734], 'all_L1': [0.632]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.467]}), defaultdict(<class 'list'>, {'all_KL': [0.408], 'all_L1': [0.45]}), defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.469]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.971], 'all_L1': [0.903]}), defaultdict(<class 'list'>, {'all_KL': [0.899], 'all_L1': [0.811]}), defaultdict(<class 'list'>, {'all_KL': [0.951], 'all_L1': [0.863]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.446], 'all_L1': [0.425]}), defaultdict(<class 'list'>, {'all_KL': [0.072], 'all_L1': [0.167]}), defaultdict(<class 'list'>, {'all_KL': [0.376], 'all_L1': [0.372]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.937], 'all_L1': [0.864]}), defaultdict(<class 'list'>, {'all_KL': [0.86], 'all_L1': [0.891]}), defaultdict(<class 'list'>, {'all_KL': [0.914], 'all_L1': [0.835]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.461], 'all_L1': [0.457]}), defaultdict(<class 'list'>, {'all_KL': [0.211], 'all_L1': [0.175]}), defaultdict(<class 'list'>, {'all_KL': [0.37], 'all_L1': [0.414]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.572 +- 0.101
suff++ class all_KL  =  0.634 +- 0.167
suff++_acc_int  =  0.518 +- 0.066
nec class all_L1  =  0.462 +- 0.009
nec class all_KL  =  0.389 +- 0.014
nec_acc_int  =  0.462 +- 0.042

Eval split val
suff++ class all_L1  =  0.859 +- 0.038
suff++ class all_KL  =  0.940 +- 0.030
suff++_acc_int  =  0.730 +- 0.281
nec class all_L1  =  0.321 +- 0.111
nec class all_KL  =  0.298 +- 0.162
nec_acc_int  =  0.527 +- 0.098

Eval split test
suff++ class all_L1  =  0.863 +- 0.023
suff++ class all_KL  =  0.904 +- 0.032
suff++_acc_int  =  0.658 +- 0.251
nec class all_L1  =  0.349 +- 0.124
nec class all_KL  =  0.347 +- 0.103
nec_acc_int  =  0.458 +- 0.081


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.517 +- 0.055
Faith. Armon (L1)= 		  =  0.508 +- 0.048
Faith. GMean (L1)= 	  =  0.512 +- 0.051
Faith. Aritm (KL)= 		  =  0.511 +- 0.077
Faith. Armon (KL)= 		  =  0.470 +- 0.047
Faith. GMean (KL)= 	  =  0.490 +- 0.062

Eval split val
Faith. Aritm (L1)= 		  =  0.590 +- 0.074
Faith. Armon (L1)= 		  =  0.458 +- 0.130
Faith. GMean (L1)= 	  =  0.518 +- 0.108
Faith. Aritm (KL)= 		  =  0.619 +- 0.096
Faith. Armon (KL)= 		  =  0.428 +- 0.210
Faith. GMean (KL)= 	  =  0.503 +- 0.178

Eval split test
Faith. Aritm (L1)= 		  =  0.606 +- 0.054
Faith. Armon (L1)= 		  =  0.481 +- 0.135
Faith. GMean (L1)= 	  =  0.537 +- 0.102
Faith. Aritm (KL)= 		  =  0.625 +- 0.068
Faith. Armon (KL)= 		  =  0.495 +- 0.116
Faith. GMean (KL)= 	  =  0.555 +- 0.096
Computed for split load_split = id



Completed in  0:04:12.685953  for CIGAGIN GOODMotif/basis



DONE CIGA GOODMotif/basis

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:30:48 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:30:48 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ACCURACY: 0.6192
Train Loss: 0.8793
ID Validation ACCURACY: 0.6427
ID Validation Loss: 0.8548
ID Test ACCURACY: 0.6190
ID Test Loss: 0.8785
OOD Validation ACCURACY: 0.4187
OOD Validation Loss: 1.1603
OOD Test ACCURACY: 0.4730
OOD Test Loss: 1.9920

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 55...
[0m[1;37mINFO[0m: [1mCheckpoint 55: 
-----------------------------------
Train ACCURACY: 0.4834
Train Loss: 1.3233
ID Validation ACCURACY: 0.4867
ID Validation Loss: 1.3383
ID Test ACCURACY: 0.4863
ID Test Loss: 1.2866
OOD Validation ACCURACY: 0.6313
OOD Validation Loss: 0.8829
OOD Test ACCURACY: 0.4813
OOD Test Loss: 1.8714

[0m[1;37mINFO[0m: [1mChartInfo 0.6190 0.4730 0.4863 0.4813 0.4867 0.6313[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.306
WIoU for r=0.8 = 0.158
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.090
WIoU for r=0.8 = 0.051
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.407
WIoU for r=0.8 = 0.410


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.655
Model XAI F1 of binarized graphs for r=0.8 =  0.30566375
Model XAI WIoU of binarized graphs for r=0.8 =  0.1584375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.368
SUFF++ for r=0.8 class 0 = 0.678 +- 0.267 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 class 1 = 0.435 +- 0.267 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 class 2 = 0.602 +- 0.267 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 all KL = 0.705 +- 0.267 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 all L1 = 0.571 +- 0.191 (in-sample avg dev_std = 0.295)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.412
Model XAI F1 of binarized graphs for r=0.8 =  0.08994999999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.050555
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.339
SUFF++ for r=0.8 class 0 = 0.793 +- 0.144 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.8 class 1 = 0.712 +- 0.144 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.8 class 2 = 0.71 +- 0.144 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.8 all KL = 0.89 +- 0.144 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.8 all L1 = 0.738 +- 0.147 (in-sample avg dev_std = 0.155)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.494
Model XAI F1 of binarized graphs for r=0.8 =  0.4068825
Model XAI WIoU of binarized graphs for r=0.8 =  0.4104425
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.375
SUFF++ for r=0.8 class 0 = 0.688 +- 0.240 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 class 1 = 0.69 +- 0.240 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 class 2 = 0.609 +- 0.240 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 all KL = 0.73 +- 0.240 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.8 all L1 = 0.662 +- 0.186 (in-sample avg dev_std = 0.314)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.655
Model XAI F1 of binarized graphs for r=0.8 =  0.30566375
Model XAI WIoU of binarized graphs for r=0.8 =  0.1584375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.407
NEC for r=0.8 class 0 = 0.402 +- 0.214 (in-sample avg dev_std = 0.352)
NEC for r=0.8 class 1 = 0.428 +- 0.214 (in-sample avg dev_std = 0.352)
NEC for r=0.8 class 2 = 0.41 +- 0.214 (in-sample avg dev_std = 0.352)
NEC for r=0.8 all KL = 0.271 +- 0.214 (in-sample avg dev_std = 0.352)
NEC for r=0.8 all L1 = 0.413 +- 0.159 (in-sample avg dev_std = 0.352)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.412
Model XAI F1 of binarized graphs for r=0.8 =  0.08994999999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.050555
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.415
NEC for r=0.8 class 0 = 0.234 +- 0.097 (in-sample avg dev_std = 0.175)
NEC for r=0.8 class 1 = 0.249 +- 0.097 (in-sample avg dev_std = 0.175)
NEC for r=0.8 class 2 = 0.292 +- 0.097 (in-sample avg dev_std = 0.175)
NEC for r=0.8 all KL = 0.097 +- 0.097 (in-sample avg dev_std = 0.175)
NEC for r=0.8 all L1 = 0.258 +- 0.108 (in-sample avg dev_std = 0.175)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.494
Model XAI F1 of binarized graphs for r=0.8 =  0.4068825
Model XAI WIoU of binarized graphs for r=0.8 =  0.4104425
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.404
NEC for r=0.8 class 0 = 0.41 +- 0.233 (in-sample avg dev_std = 0.352)
NEC for r=0.8 class 1 = 0.403 +- 0.233 (in-sample avg dev_std = 0.352)
NEC for r=0.8 class 2 = 0.436 +- 0.233 (in-sample avg dev_std = 0.352)
NEC for r=0.8 all KL = 0.394 +- 0.233 (in-sample avg dev_std = 0.352)
NEC for r=0.8 all L1 = 0.416 +- 0.183 (in-sample avg dev_std = 0.352)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:32:01 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:32:02 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 133...
[0m[1;37mINFO[0m: [1mCheckpoint 133: 
-----------------------------------
Train ACCURACY: 0.9237
Train Loss: 0.3588
ID Validation ACCURACY: 0.9287
ID Validation Loss: 0.3522
ID Test ACCURACY: 0.9200
ID Test Loss: 0.3948
OOD Validation ACCURACY: 0.9140
OOD Validation Loss: 0.4234
OOD Test ACCURACY: 0.6333
OOD Test Loss: 4.8335

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.9002
Train Loss: 0.4180
ID Validation ACCURACY: 0.9057
ID Validation Loss: 0.4083
ID Test ACCURACY: 0.8980
ID Test Loss: 0.4580
OOD Validation ACCURACY: 0.9200
OOD Validation Loss: 0.3940
OOD Test ACCURACY: 0.5167
OOD Test Loss: 2.2389

[0m[1;37mINFO[0m: [1mChartInfo 0.9200 0.6333 0.8980 0.5167 0.9057 0.9200[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.455
WIoU for r=0.8 = 0.303
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.314
WIoU for r=0.8 = 0.136
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.381
WIoU for r=0.8 = 0.312


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.934
Model XAI F1 of binarized graphs for r=0.8 =  0.455265
Model XAI WIoU of binarized graphs for r=0.8 =  0.30283875
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.591
SUFF++ for r=0.8 class 0 = 0.42 +- 0.357 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 class 1 = 0.8 +- 0.357 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 class 2 = 0.481 +- 0.357 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 all KL = 0.557 +- 0.357 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.8 all L1 = 0.568 +- 0.279 (in-sample avg dev_std = 0.379)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.914
Model XAI F1 of binarized graphs for r=0.8 =  0.3140975
Model XAI WIoU of binarized graphs for r=0.8 =  0.1356175
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.712
SUFF++ for r=0.8 class 0 = 0.562 +- 0.231 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 class 1 = 0.707 +- 0.231 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 class 2 = 0.624 +- 0.231 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 all KL = 0.683 +- 0.231 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 all L1 = 0.63 +- 0.170 (in-sample avg dev_std = 0.396)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.649
Model XAI F1 of binarized graphs for r=0.8 =  0.38135749999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.31179124999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.482
SUFF++ for r=0.8 class 0 = 0.672 +- 0.340 (in-sample avg dev_std = 0.301)
SUFF++ for r=0.8 class 1 = 0.927 +- 0.340 (in-sample avg dev_std = 0.301)
SUFF++ for r=0.8 class 2 = 0.681 +- 0.340 (in-sample avg dev_std = 0.301)
SUFF++ for r=0.8 all KL = 0.718 +- 0.340 (in-sample avg dev_std = 0.301)
SUFF++ for r=0.8 all L1 = 0.763 +- 0.299 (in-sample avg dev_std = 0.301)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.934
Model XAI F1 of binarized graphs for r=0.8 =  0.455265
Model XAI WIoU of binarized graphs for r=0.8 =  0.30283875
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.53
NEC for r=0.8 class 0 = 0.611 +- 0.296 (in-sample avg dev_std = 0.585)
NEC for r=0.8 class 1 = 0.321 +- 0.296 (in-sample avg dev_std = 0.585)
NEC for r=0.8 class 2 = 0.646 +- 0.296 (in-sample avg dev_std = 0.585)
NEC for r=0.8 all KL = 0.576 +- 0.296 (in-sample avg dev_std = 0.585)
NEC for r=0.8 all L1 = 0.525 +- 0.215 (in-sample avg dev_std = 0.585)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.914
Model XAI F1 of binarized graphs for r=0.8 =  0.3140975
Model XAI WIoU of binarized graphs for r=0.8 =  0.1356175
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.521
NEC for r=0.8 class 0 = 0.591 +- 0.210 (in-sample avg dev_std = 0.567)
NEC for r=0.8 class 1 = 0.44 +- 0.210 (in-sample avg dev_std = 0.567)
NEC for r=0.8 class 2 = 0.564 +- 0.210 (in-sample avg dev_std = 0.567)
NEC for r=0.8 all KL = 0.544 +- 0.210 (in-sample avg dev_std = 0.567)
NEC for r=0.8 all L1 = 0.533 +- 0.137 (in-sample avg dev_std = 0.567)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.649
Model XAI F1 of binarized graphs for r=0.8 =  0.38135749999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.31179124999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.484
NEC for r=0.8 class 0 = 0.397 +- 0.345 (in-sample avg dev_std = 0.508)
NEC for r=0.8 class 1 = 0.207 +- 0.345 (in-sample avg dev_std = 0.508)
NEC for r=0.8 class 2 = 0.478 +- 0.345 (in-sample avg dev_std = 0.508)
NEC for r=0.8 all KL = 0.527 +- 0.345 (in-sample avg dev_std = 0.508)
NEC for r=0.8 all L1 = 0.359 +- 0.269 (in-sample avg dev_std = 0.508)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:33:13 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:33:14 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 79...
[0m[1;37mINFO[0m: [1mCheckpoint 79: 
-----------------------------------
Train ACCURACY: 0.8680
Train Loss: 0.4829
ID Validation ACCURACY: 0.8710
ID Validation Loss: 0.4693
ID Test ACCURACY: 0.8680
ID Test Loss: 0.4894
OOD Validation ACCURACY: 0.8327
OOD Validation Loss: 0.5758
OOD Test ACCURACY: 0.4990
OOD Test Loss: 3.3324

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 68...
[0m[1;37mINFO[0m: [1mCheckpoint 68: 
-----------------------------------
Train ACCURACY: 0.8033
Train Loss: 0.5322
ID Validation ACCURACY: 0.8120
ID Validation Loss: 0.5122
ID Test ACCURACY: 0.8153
ID Test Loss: 0.5128
OOD Validation ACCURACY: 0.9143
OOD Validation Loss: 0.5427
OOD Test ACCURACY: 0.5627
OOD Test Loss: 1.2968

[0m[1;37mINFO[0m: [1mChartInfo 0.8680 0.4990 0.8153 0.5627 0.8120 0.9143[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.413
WIoU for r=0.8 = 0.392
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.262
WIoU for r=0.8 = 0.367
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.323
WIoU for r=0.8 = 0.237


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  0.41340375
Model XAI WIoU of binarized graphs for r=0.8 =  0.39216375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.463
SUFF++ for r=0.8 class 0 = 0.256 +- 0.328 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 class 1 = 0.73 +- 0.328 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 class 2 = 0.45 +- 0.328 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 all KL = 0.482 +- 0.328 (in-sample avg dev_std = 0.368)
SUFF++ for r=0.8 all L1 = 0.479 +- 0.269 (in-sample avg dev_std = 0.368)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.839
Model XAI F1 of binarized graphs for r=0.8 =  0.26192375
Model XAI WIoU of binarized graphs for r=0.8 =  0.36654375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.465
SUFF++ for r=0.8 class 0 = 0.405 +- 0.199 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 class 1 = 0.612 +- 0.199 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 class 2 = 0.516 +- 0.199 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 all KL = 0.614 +- 0.199 (in-sample avg dev_std = 0.374)
SUFF++ for r=0.8 all L1 = 0.51 +- 0.156 (in-sample avg dev_std = 0.374)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.496
Model XAI F1 of binarized graphs for r=0.8 =  0.32278
Model XAI WIoU of binarized graphs for r=0.8 =  0.23695625
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.358
SUFF++ for r=0.8 class 0 = 0.614 +- 0.260 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 class 1 = 0.841 +- 0.260 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 class 2 = 0.683 +- 0.260 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 all KL = 0.739 +- 0.260 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 all L1 = 0.715 +- 0.261 (in-sample avg dev_std = 0.262)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  0.41340375
Model XAI WIoU of binarized graphs for r=0.8 =  0.39216375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.527
NEC for r=0.8 class 0 = 0.607 +- 0.257 (in-sample avg dev_std = 0.553)
NEC for r=0.8 class 1 = 0.336 +- 0.257 (in-sample avg dev_std = 0.553)
NEC for r=0.8 class 2 = 0.633 +- 0.257 (in-sample avg dev_std = 0.553)
NEC for r=0.8 all KL = 0.541 +- 0.257 (in-sample avg dev_std = 0.553)
NEC for r=0.8 all L1 = 0.524 +- 0.201 (in-sample avg dev_std = 0.553)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.839
Model XAI F1 of binarized graphs for r=0.8 =  0.26192375
Model XAI WIoU of binarized graphs for r=0.8 =  0.36654375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.511
NEC for r=0.8 class 0 = 0.555 +- 0.214 (in-sample avg dev_std = 0.442)
NEC for r=0.8 class 1 = 0.34 +- 0.214 (in-sample avg dev_std = 0.442)
NEC for r=0.8 class 2 = 0.563 +- 0.214 (in-sample avg dev_std = 0.442)
NEC for r=0.8 all KL = 0.395 +- 0.214 (in-sample avg dev_std = 0.442)
NEC for r=0.8 all L1 = 0.488 +- 0.165 (in-sample avg dev_std = 0.442)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.496
Model XAI F1 of binarized graphs for r=0.8 =  0.32278
Model XAI WIoU of binarized graphs for r=0.8 =  0.23695625
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.428
NEC for r=0.8 class 0 = 0.359 +- 0.222 (in-sample avg dev_std = 0.394)
NEC for r=0.8 class 1 = 0.25 +- 0.222 (in-sample avg dev_std = 0.394)
NEC for r=0.8 class 2 = 0.413 +- 0.222 (in-sample avg dev_std = 0.394)
NEC for r=0.8 all KL = 0.362 +- 0.222 (in-sample avg dev_std = 0.394)
NEC for r=0.8 all L1 = 0.34 +- 0.234 (in-sample avg dev_std = 0.394)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.705], 'all_L1': [0.571]}), defaultdict(<class 'list'>, {'all_KL': [0.557], 'all_L1': [0.568]}), defaultdict(<class 'list'>, {'all_KL': [0.482], 'all_L1': [0.479]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.271], 'all_L1': [0.413]}), defaultdict(<class 'list'>, {'all_KL': [0.576], 'all_L1': [0.525]}), defaultdict(<class 'list'>, {'all_KL': [0.541], 'all_L1': [0.524]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.89], 'all_L1': [0.738]}), defaultdict(<class 'list'>, {'all_KL': [0.683], 'all_L1': [0.63]}), defaultdict(<class 'list'>, {'all_KL': [0.614], 'all_L1': [0.51]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.097], 'all_L1': [0.258]}), defaultdict(<class 'list'>, {'all_KL': [0.544], 'all_L1': [0.533]}), defaultdict(<class 'list'>, {'all_KL': [0.395], 'all_L1': [0.488]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.73], 'all_L1': [0.662]}), defaultdict(<class 'list'>, {'all_KL': [0.718], 'all_L1': [0.763]}), defaultdict(<class 'list'>, {'all_KL': [0.739], 'all_L1': [0.715]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.394], 'all_L1': [0.416]}), defaultdict(<class 'list'>, {'all_KL': [0.527], 'all_L1': [0.359]}), defaultdict(<class 'list'>, {'all_KL': [0.362], 'all_L1': [0.34]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.539 +- 0.043
suff++ class all_KL  =  0.581 +- 0.093
suff++_acc_int  =  0.474 +- 0.092
nec class all_L1  =  0.487 +- 0.053
nec class all_KL  =  0.463 +- 0.136
nec_acc_int  =  0.488 +- 0.057

Eval split val
suff++ class all_L1  =  0.626 +- 0.093
suff++ class all_KL  =  0.729 +- 0.117
suff++_acc_int  =  0.505 +- 0.155
nec class all_L1  =  0.426 +- 0.120
nec class all_KL  =  0.345 +- 0.186
nec_acc_int  =  0.482 +- 0.048

Eval split test
suff++ class all_L1  =  0.713 +- 0.041
suff++ class all_KL  =  0.729 +- 0.009
suff++_acc_int  =  0.405 +- 0.055
nec class all_L1  =  0.372 +- 0.032
nec class all_KL  =  0.428 +- 0.071
nec_acc_int  =  0.439 +- 0.034


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.513 +- 0.024
Faith. Armon (L1)= 		  =  0.508 +- 0.028
Faith. GMean (L1)= 	  =  0.511 +- 0.026
Faith. Aritm (KL)= 		  =  0.522 +- 0.033
Faith. Armon (KL)= 		  =  0.489 +- 0.073
Faith. GMean (KL)= 	  =  0.505 +- 0.053

Eval split val
Faith. Aritm (L1)= 		  =  0.526 +- 0.039
Faith. Armon (L1)= 		  =  0.486 +- 0.080
Faith. GMean (L1)= 	  =  0.505 +- 0.059
Faith. Aritm (KL)= 		  =  0.537 +- 0.054
Faith. Armon (KL)= 		  =  0.420 +- 0.181
Faith. GMean (KL)= 	  =  0.465 +- 0.130

Eval split test
Faith. Aritm (L1)= 		  =  0.543 +- 0.014
Faith. Armon (L1)= 		  =  0.487 +- 0.020
Faith. GMean (L1)= 	  =  0.514 +- 0.015
Faith. Aritm (KL)= 		  =  0.578 +- 0.032
Faith. Armon (KL)= 		  =  0.535 +- 0.052
Faith. GMean (KL)= 	  =  0.556 +- 0.042
Computed for split load_split = id



Completed in  0:03:39.168524  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:34:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/12/2024 11:34:37 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:34:50 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:34:52 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:34:54 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:34:58 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:35:04 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.6862
Train Loss: 0.7386
ID Validation ACCURACY: 0.6850
ID Validation Loss: 0.7486
ID Test ACCURACY: 0.7003
ID Test Loss: 0.7175
OOD Validation ACCURACY: 0.4753
OOD Validation Loss: 1.1049
OOD Test ACCURACY: 0.3357
OOD Test Loss: 1.3917

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 65...
[0m[1;37mINFO[0m: [1mCheckpoint 65: 
-----------------------------------
Train ACCURACY: 0.6780
Train Loss: 0.8487
ID Validation ACCURACY: 0.6703
ID Validation Loss: 0.8743
ID Test ACCURACY: 0.6780
ID Test Loss: 0.8433
OOD Validation ACCURACY: 0.4927
OOD Validation Loss: 1.1806
OOD Test ACCURACY: 0.3360
OOD Test Loss: 1.4773

[0m[1;37mINFO[0m: [1mChartInfo 0.7003 0.3357 0.6780 0.3360 0.6703 0.4927[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.436
WIoU for r=0.8 = 0.375
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.238
WIoU for r=0.8 = 0.370
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.118
WIoU for r=0.8 = 0.408


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  0.43608
Model XAI WIoU of binarized graphs for r=0.8 =  0.37509125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.564
SUFF++ for r=0.8 class 0 = 0.589 +- 0.232 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 class 1 = 0.685 +- 0.232 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 class 2 = 0.614 +- 0.232 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 all KL = 0.743 +- 0.232 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 all L1 = 0.628 +- 0.171 (in-sample avg dev_std = 0.322)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.514
Model XAI F1 of binarized graphs for r=0.8 =  0.23802625
Model XAI WIoU of binarized graphs for r=0.8 =  0.36979
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.478
SUFF++ for r=0.8 class 0 = 0.627 +- 0.188 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 class 1 = 0.705 +- 0.188 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 class 2 = 0.655 +- 0.188 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 all KL = 0.814 +- 0.188 (in-sample avg dev_std = 0.242)
SUFF++ for r=0.8 all L1 = 0.662 +- 0.171 (in-sample avg dev_std = 0.242)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.338
Model XAI F1 of binarized graphs for r=0.8 =  0.11797125
Model XAI WIoU of binarized graphs for r=0.8 =  0.40825125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.343
SUFF++ for r=0.8 class 0 = 0.7 +- 0.116 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 class 1 = 0.741 +- 0.116 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 class 2 = 0.717 +- 0.116 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 all KL = 0.871 +- 0.116 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 all L1 = 0.719 +- 0.128 (in-sample avg dev_std = 0.239)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  0.43608
Model XAI WIoU of binarized graphs for r=0.8 =  0.37509125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.465
NEC for r=0.8 class 0 = 0.525 +- 0.286 (in-sample avg dev_std = 0.417)
NEC for r=0.8 class 1 = 0.375 +- 0.286 (in-sample avg dev_std = 0.417)
NEC for r=0.8 class 2 = 0.556 +- 0.286 (in-sample avg dev_std = 0.417)
NEC for r=0.8 all KL = 0.414 +- 0.286 (in-sample avg dev_std = 0.417)
NEC for r=0.8 all L1 = 0.487 +- 0.185 (in-sample avg dev_std = 0.417)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.514
Model XAI F1 of binarized graphs for r=0.8 =  0.23802625
Model XAI WIoU of binarized graphs for r=0.8 =  0.36979
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.415
NEC for r=0.8 class 0 = 0.388 +- 0.152 (in-sample avg dev_std = 0.269)
NEC for r=0.8 class 1 = 0.3 +- 0.152 (in-sample avg dev_std = 0.269)
NEC for r=0.8 class 2 = 0.395 +- 0.152 (in-sample avg dev_std = 0.269)
NEC for r=0.8 all KL = 0.191 +- 0.152 (in-sample avg dev_std = 0.269)
NEC for r=0.8 all L1 = 0.361 +- 0.138 (in-sample avg dev_std = 0.269)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.338
Model XAI F1 of binarized graphs for r=0.8 =  0.11797125
Model XAI WIoU of binarized graphs for r=0.8 =  0.40825125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.336
NEC for r=0.8 class 0 = 0.241 +- 0.093 (in-sample avg dev_std = 0.169)
NEC for r=0.8 class 1 = 0.231 +- 0.093 (in-sample avg dev_std = 0.169)
NEC for r=0.8 class 2 = 0.246 +- 0.093 (in-sample avg dev_std = 0.169)
NEC for r=0.8 all KL = 0.09 +- 0.093 (in-sample avg dev_std = 0.169)
NEC for r=0.8 all L1 = 0.239 +- 0.127 (in-sample avg dev_std = 0.169)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:37:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:03 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:17 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:19 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:21 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:25 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:37:32 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 82...
[0m[1;37mINFO[0m: [1mCheckpoint 82: 
-----------------------------------
Train ACCURACY: 0.7256
Train Loss: 0.7151
ID Validation ACCURACY: 0.7257
ID Validation Loss: 0.7191
ID Test ACCURACY: 0.7223
ID Test Loss: 0.7248
OOD Validation ACCURACY: 0.5067
OOD Validation Loss: 1.0661
OOD Test ACCURACY: 0.3497
OOD Test Loss: 8.1815

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 37...
[0m[1;37mINFO[0m: [1mCheckpoint 37: 
-----------------------------------
Train ACCURACY: 0.7199
Train Loss: 0.6887
ID Validation ACCURACY: 0.7147
ID Validation Loss: 0.6861
ID Test ACCURACY: 0.7243
ID Test Loss: 0.6787
OOD Validation ACCURACY: 0.5900
OOD Validation Loss: 0.9209
OOD Test ACCURACY: 0.3580
OOD Test Loss: 1.1206

[0m[1;37mINFO[0m: [1mChartInfo 0.7223 0.3497 0.7243 0.3580 0.7147 0.5900[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.422
WIoU for r=0.8 = 0.313
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.180
WIoU for r=0.8 = 0.194
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.065
WIoU for r=0.8 = 0.169


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.716
Model XAI F1 of binarized graphs for r=0.8 =  0.42215125
Model XAI WIoU of binarized graphs for r=0.8 =  0.3127175
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.505
SUFF++ for r=0.8 class 0 = 0.597 +- 0.205 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 class 1 = 0.62 +- 0.205 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 class 2 = 0.59 +- 0.205 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 all KL = 0.735 +- 0.205 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 all L1 = 0.602 +- 0.150 (in-sample avg dev_std = 0.331)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.553
Model XAI F1 of binarized graphs for r=0.8 =  0.17954874999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.19384999999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.42
SUFF++ for r=0.8 class 0 = 0.627 +- 0.145 (in-sample avg dev_std = 0.240)
SUFF++ for r=0.8 class 1 = 0.696 +- 0.145 (in-sample avg dev_std = 0.240)
SUFF++ for r=0.8 class 2 = 0.61 +- 0.145 (in-sample avg dev_std = 0.240)
SUFF++ for r=0.8 all KL = 0.812 +- 0.145 (in-sample avg dev_std = 0.240)
SUFF++ for r=0.8 all L1 = 0.645 +- 0.133 (in-sample avg dev_std = 0.240)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.361
Model XAI F1 of binarized graphs for r=0.8 =  0.06461875
Model XAI WIoU of binarized graphs for r=0.8 =  0.16936500000000002
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.337
SUFF++ for r=0.8 class 0 = 0.697 +- 0.255 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.8 class 1 = 0.724 +- 0.255 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.8 class 2 = 0.693 +- 0.255 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.8 all KL = 0.81 +- 0.255 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.8 all L1 = 0.705 +- 0.246 (in-sample avg dev_std = 0.148)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.716
Model XAI F1 of binarized graphs for r=0.8 =  0.42215125
Model XAI WIoU of binarized graphs for r=0.8 =  0.3127175
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.444
NEC for r=0.8 class 0 = 0.488 +- 0.254 (in-sample avg dev_std = 0.410)
NEC for r=0.8 class 1 = 0.417 +- 0.254 (in-sample avg dev_std = 0.410)
NEC for r=0.8 class 2 = 0.526 +- 0.254 (in-sample avg dev_std = 0.410)
NEC for r=0.8 all KL = 0.392 +- 0.254 (in-sample avg dev_std = 0.410)
NEC for r=0.8 all L1 = 0.478 +- 0.162 (in-sample avg dev_std = 0.410)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.553
Model XAI F1 of binarized graphs for r=0.8 =  0.17954874999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.19384999999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.433
NEC for r=0.8 class 0 = 0.342 +- 0.127 (in-sample avg dev_std = 0.239)
NEC for r=0.8 class 1 = 0.288 +- 0.127 (in-sample avg dev_std = 0.239)
NEC for r=0.8 class 2 = 0.379 +- 0.127 (in-sample avg dev_std = 0.239)
NEC for r=0.8 all KL = 0.167 +- 0.127 (in-sample avg dev_std = 0.239)
NEC for r=0.8 all L1 = 0.336 +- 0.132 (in-sample avg dev_std = 0.239)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.361
Model XAI F1 of binarized graphs for r=0.8 =  0.06461875
Model XAI WIoU of binarized graphs for r=0.8 =  0.16936500000000002
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.357
NEC for r=0.8 class 0 = 0.288 +- 0.182 (in-sample avg dev_std = 0.197)
NEC for r=0.8 class 1 = 0.27 +- 0.182 (in-sample avg dev_std = 0.197)
NEC for r=0.8 class 2 = 0.274 +- 0.182 (in-sample avg dev_std = 0.197)
NEC for r=0.8 all KL = 0.143 +- 0.182 (in-sample avg dev_std = 0.197)
NEC for r=0.8 all L1 = 0.278 +- 0.212 (in-sample avg dev_std = 0.197)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:39:30 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/12/2024 11:39:30 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:39:44 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:39:46 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:39:48 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:39:52 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:40:00 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.6818
Train Loss: 0.8315
ID Validation ACCURACY: 0.6730
ID Validation Loss: 0.8527
ID Test ACCURACY: 0.6913
ID Test Loss: 0.8316
OOD Validation ACCURACY: 0.3920
OOD Validation Loss: 1.4612
OOD Test ACCURACY: 0.3440
OOD Test Loss: 1.8131

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 58...
[0m[1;37mINFO[0m: [1mCheckpoint 58: 
-----------------------------------
Train ACCURACY: 0.6716
Train Loss: 0.8047
ID Validation ACCURACY: 0.6600
ID Validation Loss: 0.8160
ID Test ACCURACY: 0.6783
ID Test Loss: 0.7950
OOD Validation ACCURACY: 0.4573
OOD Validation Loss: 1.2737
OOD Test ACCURACY: 0.3547
OOD Test Loss: 1.4224

[0m[1;37mINFO[0m: [1mChartInfo 0.6913 0.3440 0.6783 0.3547 0.6600 0.4573[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.385
WIoU for r=0.8 = 0.273
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.180
WIoU for r=0.8 = 0.148
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.093
WIoU for r=0.8 = 0.112


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.675
Model XAI F1 of binarized graphs for r=0.8 =  0.384975
Model XAI WIoU of binarized graphs for r=0.8 =  0.27252875
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.51
SUFF++ for r=0.8 class 0 = 0.648 +- 0.218 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 class 1 = 0.593 +- 0.218 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 class 2 = 0.588 +- 0.218 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 all KL = 0.732 +- 0.218 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 all L1 = 0.61 +- 0.155 (in-sample avg dev_std = 0.326)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.43
Model XAI F1 of binarized graphs for r=0.8 =  0.1799725
Model XAI WIoU of binarized graphs for r=0.8 =  0.14762125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.411
SUFF++ for r=0.8 class 0 = 0.701 +- 0.128 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.8 class 1 = 0.768 +- 0.128 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.8 class 2 = 0.707 +- 0.128 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.8 all KL = 0.87 +- 0.128 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.8 all L1 = 0.726 +- 0.134 (in-sample avg dev_std = 0.250)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.345
Model XAI F1 of binarized graphs for r=0.8 =  0.09335750000000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.11152625000000001
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.365
SUFF++ for r=0.8 class 0 = 0.755 +- 0.196 (in-sample avg dev_std = 0.201)
SUFF++ for r=0.8 class 1 = 0.763 +- 0.196 (in-sample avg dev_std = 0.201)
SUFF++ for r=0.8 class 2 = 0.77 +- 0.196 (in-sample avg dev_std = 0.201)
SUFF++ for r=0.8 all KL = 0.867 +- 0.196 (in-sample avg dev_std = 0.201)
SUFF++ for r=0.8 all L1 = 0.762 +- 0.192 (in-sample avg dev_std = 0.201)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.675
Model XAI F1 of binarized graphs for r=0.8 =  0.384975
Model XAI WIoU of binarized graphs for r=0.8 =  0.27252875
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.476
NEC for r=0.8 class 0 = 0.514 +- 0.247 (in-sample avg dev_std = 0.374)
NEC for r=0.8 class 1 = 0.418 +- 0.247 (in-sample avg dev_std = 0.374)
NEC for r=0.8 class 2 = 0.48 +- 0.247 (in-sample avg dev_std = 0.374)
NEC for r=0.8 all KL = 0.379 +- 0.247 (in-sample avg dev_std = 0.374)
NEC for r=0.8 all L1 = 0.472 +- 0.162 (in-sample avg dev_std = 0.374)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.43
Model XAI F1 of binarized graphs for r=0.8 =  0.1799725
Model XAI WIoU of binarized graphs for r=0.8 =  0.14762125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.431
NEC for r=0.8 class 0 = 0.371 +- 0.250 (in-sample avg dev_std = 0.313)
NEC for r=0.8 class 1 = 0.367 +- 0.250 (in-sample avg dev_std = 0.313)
NEC for r=0.8 class 2 = 0.364 +- 0.250 (in-sample avg dev_std = 0.313)
NEC for r=0.8 all KL = 0.254 +- 0.250 (in-sample avg dev_std = 0.313)
NEC for r=0.8 all L1 = 0.368 +- 0.170 (in-sample avg dev_std = 0.313)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.345
Model XAI F1 of binarized graphs for r=0.8 =  0.09335750000000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.11152625000000001
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.345
NEC for r=0.8 class 0 = 0.268 +- 0.152 (in-sample avg dev_std = 0.177)
NEC for r=0.8 class 1 = 0.253 +- 0.152 (in-sample avg dev_std = 0.177)
NEC for r=0.8 class 2 = 0.266 +- 0.152 (in-sample avg dev_std = 0.177)
NEC for r=0.8 all KL = 0.134 +- 0.152 (in-sample avg dev_std = 0.177)
NEC for r=0.8 all L1 = 0.262 +- 0.149 (in-sample avg dev_std = 0.177)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.743], 'all_L1': [0.628]}), defaultdict(<class 'list'>, {'all_KL': [0.735], 'all_L1': [0.602]}), defaultdict(<class 'list'>, {'all_KL': [0.732], 'all_L1': [0.61]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.414], 'all_L1': [0.487]}), defaultdict(<class 'list'>, {'all_KL': [0.392], 'all_L1': [0.478]}), defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.472]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.814], 'all_L1': [0.662]}), defaultdict(<class 'list'>, {'all_KL': [0.812], 'all_L1': [0.645]}), defaultdict(<class 'list'>, {'all_KL': [0.87], 'all_L1': [0.726]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.191], 'all_L1': [0.361]}), defaultdict(<class 'list'>, {'all_KL': [0.167], 'all_L1': [0.336]}), defaultdict(<class 'list'>, {'all_KL': [0.254], 'all_L1': [0.368]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.871], 'all_L1': [0.719]}), defaultdict(<class 'list'>, {'all_KL': [0.81], 'all_L1': [0.705]}), defaultdict(<class 'list'>, {'all_KL': [0.867], 'all_L1': [0.762]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.09], 'all_L1': [0.239]}), defaultdict(<class 'list'>, {'all_KL': [0.143], 'all_L1': [0.278]}), defaultdict(<class 'list'>, {'all_KL': [0.134], 'all_L1': [0.262]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.613 +- 0.011
suff++ class all_KL  =  0.737 +- 0.005
suff++_acc_int  =  0.526 +- 0.027
nec class all_L1  =  0.479 +- 0.006
nec class all_KL  =  0.395 +- 0.014
nec_acc_int  =  0.462 +- 0.013

Eval split val
suff++ class all_L1  =  0.678 +- 0.035
suff++ class all_KL  =  0.832 +- 0.027
suff++_acc_int  =  0.436 +- 0.029
nec class all_L1  =  0.355 +- 0.014
nec class all_KL  =  0.204 +- 0.037
nec_acc_int  =  0.426 +- 0.008

Eval split test
suff++ class all_L1  =  0.729 +- 0.024
suff++ class all_KL  =  0.849 +- 0.028
suff++_acc_int  =  0.348 +- 0.012
nec class all_L1  =  0.260 +- 0.016
nec class all_KL  =  0.122 +- 0.023
nec_acc_int  =  0.346 +- 0.009


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.546 +- 0.008
Faith. Armon (L1)= 		  =  0.538 +- 0.008
Faith. GMean (L1)= 	  =  0.542 +- 0.008
Faith. Aritm (KL)= 		  =  0.566 +- 0.010
Faith. Armon (KL)= 		  =  0.514 +- 0.013
Faith. GMean (KL)= 	  =  0.539 +- 0.012

Eval split val
Faith. Aritm (L1)= 		  =  0.516 +- 0.023
Faith. Armon (L1)= 		  =  0.466 +- 0.019
Faith. GMean (L1)= 	  =  0.490 +- 0.021
Faith. Aritm (KL)= 		  =  0.518 +- 0.032
Faith. Armon (KL)= 		  =  0.327 +- 0.049
Faith. GMean (KL)= 	  =  0.411 +- 0.043

Eval split test
Faith. Aritm (L1)= 		  =  0.494 +- 0.014
Faith. Armon (L1)= 		  =  0.382 +- 0.017
Faith. GMean (L1)= 	  =  0.435 +- 0.014
Faith. Aritm (KL)= 		  =  0.486 +- 0.010
Faith. Armon (KL)= 		  =  0.213 +- 0.035
Faith. GMean (KL)= 	  =  0.320 +- 0.029
Computed for split load_split = id



Completed in  0:07:22.395243  for CIGAGIN GOODMotif/size



DONE CIGA GOODMotif/size

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:42:29 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:42:30 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 146...
[0m[1;37mINFO[0m: [1mCheckpoint 146: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0762
ID Validation ACCURACY: 0.8615
ID Validation Loss: 0.8340
ID Test ACCURACY: 0.8593
ID Test Loss: 0.7822
OOD Validation ACCURACY: 0.8238
OOD Validation Loss: 2.7555
OOD Test ACCURACY: 0.6532
OOD Test Loss: 15.7757

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 100...
[0m[1;37mINFO[0m: [1mCheckpoint 100: 
-----------------------------------
Train ACCURACY: 0.9482
Train Loss: 0.0806
ID Validation ACCURACY: 0.8523
ID Validation Loss: 0.9562
ID Test ACCURACY: 0.8555
ID Test Loss: 0.8378
OOD Validation ACCURACY: 0.8527
OOD Validation Loss: 3.1390
OOD Test ACCURACY: 0.8097
OOD Test Loss: 9.3985

[0m[1;37mINFO[0m: [1mChartInfo 0.8593 0.6532 0.8555 0.8097 0.8523 0.8527[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/12/2024 11:42:32 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.856
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.825
SUFF++ for r=0.8 class 0.0 = 0.805 +- 0.322 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 class 1.0 = 0.922 +- 0.322 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 all KL = 0.776 +- 0.322 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 all L1 = 0.874 +- 0.201 (in-sample avg dev_std = 0.390)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.826
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.784
SUFF++ for r=0.8 class 0.0 = 0.772 +- 0.348 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.8 class 1.0 = 0.933 +- 0.348 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.8 all KL = 0.747 +- 0.348 (in-sample avg dev_std = 0.375)
SUFF++ for r=0.8 all L1 = 0.856 +- 0.222 (in-sample avg dev_std = 0.375)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.64
SUFF++ for r=0.8 class 0.0 = 0.885 +- 0.445 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.8 class 1.0 = 0.719 +- 0.445 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.8 all KL = 0.619 +- 0.445 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.8 all L1 = 0.799 +- 0.259 (in-sample avg dev_std = 0.542)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.856
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.861
NEC for r=0.8 class 0.0 = 0.115 +- 0.235 (in-sample avg dev_std = 0.188)
NEC for r=0.8 class 1.0 = 0.042 +- 0.235 (in-sample avg dev_std = 0.188)
NEC for r=0.8 all KL = 0.101 +- 0.235 (in-sample avg dev_std = 0.188)
NEC for r=0.8 all L1 = 0.073 +- 0.177 (in-sample avg dev_std = 0.188)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.825
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.814
NEC for r=0.8 class 0.0 = 0.137 +- 0.249 (in-sample avg dev_std = 0.248)
NEC for r=0.8 class 1.0 = 0.044 +- 0.249 (in-sample avg dev_std = 0.248)
NEC for r=0.8 all KL = 0.118 +- 0.249 (in-sample avg dev_std = 0.248)
NEC for r=0.8 all L1 = 0.088 +- 0.186 (in-sample avg dev_std = 0.248)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.662
NEC for r=0.8 class 0.0 = 0.103 +- 0.417 (in-sample avg dev_std = 0.432)
NEC for r=0.8 class 1.0 = 0.209 +- 0.417 (in-sample avg dev_std = 0.432)
NEC for r=0.8 all KL = 0.307 +- 0.417 (in-sample avg dev_std = 0.432)
NEC for r=0.8 all L1 = 0.158 +- 0.246 (in-sample avg dev_std = 0.432)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:43:38 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:43:39 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:43:40 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 183...
[0m[1;37mINFO[0m: [1mCheckpoint 183: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8629
ID Validation Loss: 1.2999
ID Test ACCURACY: 0.8585
ID Test Loss: 1.1992
OOD Validation ACCURACY: 0.6298
OOD Validation Loss: 15.9629
OOD Test ACCURACY: 0.4839
OOD Test Loss: 201.9750

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 101...
[0m[1;37mINFO[0m: [1mCheckpoint 101: 
-----------------------------------
Train ACCURACY: 0.9483
Train Loss: 0.0774
ID Validation ACCURACY: 0.8534
ID Validation Loss: 0.7641
ID Test ACCURACY: 0.8512
ID Test Loss: 0.7085
OOD Validation ACCURACY: 0.8540
OOD Validation Loss: 1.8423
OOD Test ACCURACY: 0.7524
OOD Test Loss: 4.8235

[0m[1;37mINFO[0m: [1mChartInfo 0.8585 0.4839 0.8512 0.7524 0.8534 0.8540[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/12/2024 11:43:40 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.826
SUFF++ for r=0.8 class 0.0 = 0.795 +- 0.396 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.8 class 1.0 = 0.904 +- 0.396 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.8 all KL = 0.702 +- 0.396 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.8 all L1 = 0.859 +- 0.212 (in-sample avg dev_std = 0.440)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.604
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.584
SUFF++ for r=0.8 class 0.0 = 0.81 +- 0.448 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.8 class 1.0 = 0.704 +- 0.448 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.8 all KL = 0.57 +- 0.448 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.8 all L1 = 0.755 +- 0.287 (in-sample avg dev_std = 0.554)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.484
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.484
SUFF++ for r=0.8 class 0.0 = 1.0 +- 0.017 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 class 1.0 = 1.0 +- 0.017 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 all KL = 0.999 +- 0.017 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 all L1 = 1.0 +- 0.001 (in-sample avg dev_std = 0.001)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.841
NEC for r=0.8 class 0.0 = 0.11 +- 0.272 (in-sample avg dev_std = 0.216)
NEC for r=0.8 class 1.0 = 0.051 +- 0.272 (in-sample avg dev_std = 0.216)
NEC for r=0.8 all KL = 0.12 +- 0.272 (in-sample avg dev_std = 0.216)
NEC for r=0.8 all L1 = 0.075 +- 0.186 (in-sample avg dev_std = 0.216)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.598
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.649
NEC for r=0.8 class 0.0 = 0.151 +- 0.419 (in-sample avg dev_std = 0.424)
NEC for r=0.8 class 1.0 = 0.251 +- 0.419 (in-sample avg dev_std = 0.424)
NEC for r=0.8 all KL = 0.317 +- 0.419 (in-sample avg dev_std = 0.424)
NEC for r=0.8 all L1 = 0.203 +- 0.292 (in-sample avg dev_std = 0.424)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.484
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.483
NEC for r=0.8 class 0.0 = 0.002 +- 0.069 (in-sample avg dev_std = 0.043)
NEC for r=0.8 class 1.0 = 0.001 +- 0.069 (in-sample avg dev_std = 0.043)
NEC for r=0.8 all KL = 0.006 +- 0.069 (in-sample avg dev_std = 0.043)
NEC for r=0.8 all L1 = 0.001 +- 0.019 (in-sample avg dev_std = 0.043)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:44:43 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:44:44 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:44:45 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 199...
[0m[1;37mINFO[0m: [1mCheckpoint 199: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8619
ID Validation Loss: 0.8658
ID Test ACCURACY: 0.8593
ID Test Loss: 0.9042
OOD Validation ACCURACY: 0.6146
OOD Validation Loss: 17.0850
OOD Test ACCURACY: 0.5161
OOD Test Loss: 124.0430

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 0.9448
Train Loss: 0.0896
ID Validation ACCURACY: 0.8455
ID Validation Loss: 0.5447
ID Test ACCURACY: 0.8432
ID Test Loss: 0.5905
OOD Validation ACCURACY: 0.8541
OOD Validation Loss: 0.8881
OOD Test ACCURACY: 0.7646
OOD Test Loss: 1.8261

[0m[1;37mINFO[0m: [1mChartInfo 0.8593 0.5161 0.8432 0.7646 0.8455 0.8541[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/12/2024 11:44:45 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.823
SUFF++ for r=0.8 class 0.0 = 0.819 +- 0.378 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.8 class 1.0 = 0.903 +- 0.378 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.8 all KL = 0.714 +- 0.378 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.8 all L1 = 0.868 +- 0.204 (in-sample avg dev_std = 0.434)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.623
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.607
SUFF++ for r=0.8 class 0.0 = 0.765 +- 0.377 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 class 1.0 = 0.923 +- 0.377 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 all KL = 0.762 +- 0.377 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 all L1 = 0.847 +- 0.249 (in-sample avg dev_std = 0.433)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.516
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.516
SUFF++ for r=0.8 class 0.0 = 1.0 +- 0.026 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.8 class 1.0 = 0.999 +- 0.026 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.8 all KL = 0.999 +- 0.026 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.8 all L1 = 1.0 +- 0.010 (in-sample avg dev_std = 0.012)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.853
NEC for r=0.8 class 0.0 = 0.089 +- 0.260 (in-sample avg dev_std = 0.226)
NEC for r=0.8 class 1.0 = 0.059 +- 0.260 (in-sample avg dev_std = 0.226)
NEC for r=0.8 all KL = 0.105 +- 0.260 (in-sample avg dev_std = 0.226)
NEC for r=0.8 all L1 = 0.071 +- 0.191 (in-sample avg dev_std = 0.226)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.624
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.66
NEC for r=0.8 class 0.0 = 0.156 +- 0.302 (in-sample avg dev_std = 0.237)
NEC for r=0.8 class 1.0 = 0.059 +- 0.302 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all KL = 0.148 +- 0.302 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all L1 = 0.106 +- 0.225 (in-sample avg dev_std = 0.237)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.516
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.516
NEC for r=0.8 class 0.0 = 0.0 +- 0.002 (in-sample avg dev_std = 0.001)
NEC for r=0.8 class 1.0 = 0.0 +- 0.002 (in-sample avg dev_std = 0.001)
NEC for r=0.8 all KL = 0.0 +- 0.002 (in-sample avg dev_std = 0.001)
NEC for r=0.8 all L1 = 0.0 +- 0.000 (in-sample avg dev_std = 0.001)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.776], 'all_L1': [0.874]}), defaultdict(<class 'list'>, {'all_KL': [0.702], 'all_L1': [0.859]}), defaultdict(<class 'list'>, {'all_KL': [0.714], 'all_L1': [0.868]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.101], 'all_L1': [0.073]}), defaultdict(<class 'list'>, {'all_KL': [0.12], 'all_L1': [0.075]}), defaultdict(<class 'list'>, {'all_KL': [0.105], 'all_L1': [0.071]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.747], 'all_L1': [0.856]}), defaultdict(<class 'list'>, {'all_KL': [0.57], 'all_L1': [0.755]}), defaultdict(<class 'list'>, {'all_KL': [0.762], 'all_L1': [0.847]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.118], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.317], 'all_L1': [0.203]}), defaultdict(<class 'list'>, {'all_KL': [0.148], 'all_L1': [0.106]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.619], 'all_L1': [0.799]}), defaultdict(<class 'list'>, {'all_KL': [0.999], 'all_L1': [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.999], 'all_L1': [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.307], 'all_L1': [0.158]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.001]}), defaultdict(<class 'list'>, {'all_KL': [0.0], 'all_L1': [0.0]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.867 +- 0.006
suff++ class all_KL  =  0.731 +- 0.032
suff++_acc_int  =  0.824 +- 0.001
nec class all_L1  =  0.073 +- 0.002
nec class all_KL  =  0.109 +- 0.008
nec_acc_int  =  0.852 +- 0.008

Eval split val
suff++ class all_L1  =  0.819 +- 0.046
suff++ class all_KL  =  0.693 +- 0.087
suff++_acc_int  =  0.658 +- 0.089
nec class all_L1  =  0.132 +- 0.051
nec class all_KL  =  0.194 +- 0.088
nec_acc_int  =  0.707 +- 0.076

Eval split test
suff++ class all_L1  =  0.933 +- 0.095
suff++ class all_KL  =  0.872 +- 0.179
suff++_acc_int  =  0.547 +- 0.067
nec class all_L1  =  0.053 +- 0.074
nec class all_KL  =  0.104 +- 0.143
nec_acc_int  =  0.554 +- 0.078


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.470 +- 0.003
Faith. Armon (L1)= 		  =  0.135 +- 0.003
Faith. GMean (L1)= 	  =  0.252 +- 0.002
Faith. Aritm (KL)= 		  =  0.420 +- 0.013
Faith. Armon (KL)= 		  =  0.189 +- 0.011
Faith. GMean (KL)= 	  =  0.281 +- 0.007

Eval split val
Faith. Aritm (L1)= 		  =  0.476 +- 0.003
Faith. Armon (L1)= 		  =  0.223 +- 0.070
Faith. GMean (L1)= 	  =  0.322 +- 0.050
Faith. Aritm (KL)= 		  =  0.444 +- 0.009
Faith. Armon (KL)= 		  =  0.286 +- 0.087
Faith. GMean (KL)= 	  =  0.353 +- 0.054

Eval split test
Faith. Aritm (L1)= 		  =  0.493 +- 0.010
Faith. Armon (L1)= 		  =  0.089 +- 0.124
Faith. GMean (L1)= 	  =  0.129 +- 0.161
Faith. Aritm (KL)= 		  =  0.488 +- 0.018
Faith. Armon (KL)= 		  =  0.141 +- 0.191
Faith. GMean (KL)= 	  =  0.171 +- 0.190
Computed for split load_split = id



Completed in  0:03:23.251387  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:46:18 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:18 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:20 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:20 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:21 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:23 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:24 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:46:25 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 147...
[0m[1;37mINFO[0m: [1mCheckpoint 147: 
-----------------------------------
Train ACCURACY: 0.9888
Train Loss: 0.0383
ID Validation ACCURACY: 0.6354
ID Validation Loss: 2.4754
ID Test ACCURACY: 0.6047
ID Test Loss: 2.7614
OOD Validation ACCURACY: 0.6101
OOD Validation Loss: 2.5866
OOD Test ACCURACY: 0.5566
OOD Test Loss: 2.8049

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 25...
[0m[1;37mINFO[0m: [1mCheckpoint 25: 
-----------------------------------
Train ACCURACY: 0.8880
Train Loss: 0.2787
ID Validation ACCURACY: 0.5704
ID Validation Loss: 2.0731
ID Test ACCURACY: 0.5542
ID Test Loss: 2.2825
OOD Validation ACCURACY: 0.6151
OOD Validation Loss: 1.9070
OOD Test ACCURACY: 0.5676
OOD Test Loss: 2.3727

[0m[1;37mINFO[0m: [1mChartInfo 0.6047 0.5566 0.5542 0.5676 0.5704 0.6151[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.622
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.581
SUFF++ for r=0.6 class 0 = 0.718 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 class 1 = 0.862 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 class 2 = 0.724 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 all KL = 0.733 +- 0.301 (in-sample avg dev_std = 0.409)
SUFF++ for r=0.6 all L1 = 0.788 +- 0.218 (in-sample avg dev_std = 0.409)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.621
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.596
SUFF++ for r=0.6 class 0 = 0.76 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 class 1 = 0.886 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 class 2 = 0.805 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 all KL = 0.846 +- 0.221 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.6 all L1 = 0.837 +- 0.196 (in-sample avg dev_std = 0.296)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.553
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.539
SUFF++ for r=0.6 class 0 = 0.788 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 class 1 = 0.868 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 class 2 = 0.813 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 all KL = 0.832 +- 0.228 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.6 all L1 = 0.833 +- 0.194 (in-sample avg dev_std = 0.309)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.62
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.61
NEC for r=0.6 class 0 = 0.137 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 1 = 0.106 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 2 = 0.215 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 all KL = 0.123 +- 0.208 (in-sample avg dev_std = 0.229)
NEC for r=0.6 all L1 = 0.144 +- 0.191 (in-sample avg dev_std = 0.229)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.624
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.61
NEC for r=0.6 class 0 = 0.191 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 class 1 = 0.108 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 class 2 = 0.18 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 all KL = 0.115 +- 0.180 (in-sample avg dev_std = 0.219)
NEC for r=0.6 all L1 = 0.144 +- 0.185 (in-sample avg dev_std = 0.219)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.555
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.551
NEC for r=0.6 class 0 = 0.169 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 1 = 0.116 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 2 = 0.171 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all KL = 0.114 +- 0.188 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all L1 = 0.143 +- 0.193 (in-sample avg dev_std = 0.208)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:47:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/12/2024 11:47:59 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:01 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:01 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:02 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:03 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:48:05 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 120...
[0m[1;37mINFO[0m: [1mCheckpoint 120: 
-----------------------------------
Train ACCURACY: 0.9849
Train Loss: 0.0703
ID Validation ACCURACY: 0.6191
ID Validation Loss: 2.1533
ID Test ACCURACY: 0.5903
ID Test Loss: 2.1410
OOD Validation ACCURACY: 0.5025
OOD Validation Loss: 2.4391
OOD Test ACCURACY: 0.4468
OOD Test Loss: 3.6983

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 131...
[0m[1;37mINFO[0m: [1mCheckpoint 131: 
-----------------------------------
Train ACCURACY: 0.9815
Train Loss: 0.0601
ID Validation ACCURACY: 0.5758
ID Validation Loss: 2.4039
ID Test ACCURACY: 0.5830
ID Test Loss: 2.3059
OOD Validation ACCURACY: 0.5765
OOD Validation Loss: 2.1952
OOD Test ACCURACY: 0.5223
OOD Test Loss: 2.5406

[0m[1;37mINFO[0m: [1mChartInfo 0.5903 0.4468 0.5830 0.5223 0.5758 0.5765[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.541
SUFF++ for r=0.6 class 0 = 0.577 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 class 1 = 0.672 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 class 2 = 0.681 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 all KL = 0.54 +- 0.300 (in-sample avg dev_std = 0.546)
SUFF++ for r=0.6 all L1 = 0.651 +- 0.222 (in-sample avg dev_std = 0.546)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.488
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.457
SUFF++ for r=0.6 class 0 = 0.652 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 class 1 = 0.641 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 class 2 = 0.709 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 all KL = 0.608 +- 0.282 (in-sample avg dev_std = 0.479)
SUFF++ for r=0.6 all L1 = 0.658 +- 0.199 (in-sample avg dev_std = 0.479)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.445
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.406
SUFF++ for r=0.6 class 0 = 0.693 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 class 1 = 0.645 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 class 2 = 0.729 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 all KL = 0.69 +- 0.280 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.6 all L1 = 0.678 +- 0.204 (in-sample avg dev_std = 0.410)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.606
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.566
NEC for r=0.6 class 0 = 0.356 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 class 1 = 0.243 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 class 2 = 0.217 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 all KL = 0.274 +- 0.298 (in-sample avg dev_std = 0.359)
NEC for r=0.6 all L1 = 0.263 +- 0.251 (in-sample avg dev_std = 0.359)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.489
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.47
NEC for r=0.6 class 0 = 0.369 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 class 1 = 0.306 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 class 2 = 0.279 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 all KL = 0.299 +- 0.276 (in-sample avg dev_std = 0.366)
NEC for r=0.6 all L1 = 0.316 +- 0.236 (in-sample avg dev_std = 0.366)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.451
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.43
NEC for r=0.6 class 0 = 0.317 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 class 1 = 0.315 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 class 2 = 0.257 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 all KL = 0.246 +- 0.241 (in-sample avg dev_std = 0.324)
NEC for r=0.6 all L1 = 0.301 +- 0.211 (in-sample avg dev_std = 0.324)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:49:39 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:39 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:41 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:42 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:42 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:44 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:49:46 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ACCURACY: 0.9915
Train Loss: 0.0362
ID Validation ACCURACY: 0.6534
ID Validation Loss: 1.8080
ID Test ACCURACY: 0.5921
ID Test Loss: 2.0719
OOD Validation ACCURACY: 0.6168
OOD Validation Loss: 1.9392
OOD Test ACCURACY: 0.5395
OOD Test Loss: 2.4358

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 0.9749
Train Loss: 0.0628
ID Validation ACCURACY: 0.6173
ID Validation Loss: 1.7817
ID Test ACCURACY: 0.6029
ID Test Loss: 2.0648
OOD Validation ACCURACY: 0.6202
OOD Validation Loss: 1.9126
OOD Test ACCURACY: 0.5607
OOD Test Loss: 2.3389

[0m[1;37mINFO[0m: [1mChartInfo 0.5921 0.5395 0.6029 0.5607 0.6173 0.6202[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.657
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.602
SUFF++ for r=0.6 class 0 = 0.708 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 1 = 0.85 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 2 = 0.745 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all KL = 0.759 +- 0.272 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all L1 = 0.786 +- 0.207 (in-sample avg dev_std = 0.379)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.62
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.607
SUFF++ for r=0.6 class 0 = 0.758 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 class 1 = 0.834 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 class 2 = 0.775 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 all KL = 0.818 +- 0.232 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.6 all L1 = 0.802 +- 0.203 (in-sample avg dev_std = 0.330)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.553
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.535
SUFF++ for r=0.6 class 0 = 0.768 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 class 1 = 0.801 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 class 2 = 0.765 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 all KL = 0.795 +- 0.233 (in-sample avg dev_std = 0.355)
SUFF++ for r=0.6 all L1 = 0.784 +- 0.201 (in-sample avg dev_std = 0.355)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.65
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.619
NEC for r=0.6 class 0 = 0.32 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 class 1 = 0.136 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 class 2 = 0.146 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 all KL = 0.168 +- 0.242 (in-sample avg dev_std = 0.279)
NEC for r=0.6 all L1 = 0.184 +- 0.221 (in-sample avg dev_std = 0.279)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.62
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.62
NEC for r=0.6 class 0 = 0.286 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 class 1 = 0.175 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 class 2 = 0.177 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 all KL = 0.185 +- 0.246 (in-sample avg dev_std = 0.266)
NEC for r=0.6 all L1 = 0.204 +- 0.224 (in-sample avg dev_std = 0.266)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.55
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.561
NEC for r=0.6 class 0 = 0.232 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 class 1 = 0.192 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 class 2 = 0.196 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 all KL = 0.175 +- 0.241 (in-sample avg dev_std = 0.254)
NEC for r=0.6 all L1 = 0.204 +- 0.226 (in-sample avg dev_std = 0.254)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.733], 'all_L1': [0.788]}), defaultdict(<class 'list'>, {'all_KL': [0.54], 'all_L1': [0.651]}), defaultdict(<class 'list'>, {'all_KL': [0.759], 'all_L1': [0.786]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.123], 'all_L1': [0.144]}), defaultdict(<class 'list'>, {'all_KL': [0.274], 'all_L1': [0.263]}), defaultdict(<class 'list'>, {'all_KL': [0.168], 'all_L1': [0.184]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.846], 'all_L1': [0.837]}), defaultdict(<class 'list'>, {'all_KL': [0.608], 'all_L1': [0.658]}), defaultdict(<class 'list'>, {'all_KL': [0.818], 'all_L1': [0.802]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.115], 'all_L1': [0.144]}), defaultdict(<class 'list'>, {'all_KL': [0.299], 'all_L1': [0.316]}), defaultdict(<class 'list'>, {'all_KL': [0.185], 'all_L1': [0.204]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.832], 'all_L1': [0.833]}), defaultdict(<class 'list'>, {'all_KL': [0.69], 'all_L1': [0.678]}), defaultdict(<class 'list'>, {'all_KL': [0.795], 'all_L1': [0.784]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.114], 'all_L1': [0.143]}), defaultdict(<class 'list'>, {'all_KL': [0.246], 'all_L1': [0.301]}), defaultdict(<class 'list'>, {'all_KL': [0.175], 'all_L1': [0.204]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.742 +- 0.064
suff++ class all_KL  =  0.677 +- 0.098
suff++_acc_int  =  0.575 +- 0.026
nec class all_L1  =  0.197 +- 0.049
nec class all_KL  =  0.188 +- 0.063
nec_acc_int  =  0.598 +- 0.023

Eval split val
suff++ class all_L1  =  0.766 +- 0.077
suff++ class all_KL  =  0.757 +- 0.106
suff++_acc_int  =  0.554 +- 0.068
nec class all_L1  =  0.221 +- 0.071
nec class all_KL  =  0.200 +- 0.076
nec_acc_int  =  0.567 +- 0.068

Eval split test
suff++ class all_L1  =  0.765 +- 0.065
suff++ class all_KL  =  0.772 +- 0.060
suff++_acc_int  =  0.493 +- 0.062
nec class all_L1  =  0.216 +- 0.065
nec class all_KL  =  0.178 +- 0.054
nec_acc_int  =  0.514 +- 0.060


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.469 +- 0.012
Faith. Armon (L1)= 		  =  0.305 +- 0.054
Faith. GMean (L1)= 	  =  0.377 +- 0.031
Faith. Aritm (KL)= 		  =  0.433 +- 0.023
Faith. Armon (KL)= 		  =  0.283 +- 0.063
Faith. GMean (KL)= 	  =  0.347 +- 0.035

Eval split val
Faith. Aritm (L1)= 		  =  0.493 +- 0.007
Faith. Armon (L1)= 		  =  0.333 +- 0.074
Faith. GMean (L1)= 	  =  0.403 +- 0.044
Faith. Aritm (KL)= 		  =  0.479 +- 0.020
Faith. Armon (KL)= 		  =  0.302 +- 0.081
Faith. GMean (KL)= 	  =  0.376 +- 0.048

Eval split test
Faith. Aritm (L1)= 		  =  0.490 +- 0.003
Faith. Armon (L1)= 		  =  0.328 +- 0.071
Faith. GMean (L1)= 	  =  0.399 +- 0.044
Faith. Aritm (KL)= 		  =  0.475 +- 0.007
Faith. Armon (KL)= 		  =  0.283 +- 0.066
Faith. GMean (KL)= 	  =  0.364 +- 0.043
Computed for split load_split = id



Completed in  0:05:06.540492  for CIGAvGIN GOODTwitter/length



DONE CIGA GOODTwitter/length

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:51:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/12/2024 11:51:51 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:51:54 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:51:58 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:01 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:04 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 150...
[0m[1;37mINFO[0m: [1mCheckpoint 150: 
-----------------------------------
Train ROC-AUC: 0.9355
Train Loss: 0.1115
ID Validation ROC-AUC: 0.8437
ID Validation Loss: 0.1686
ID Test ROC-AUC: 0.8421
ID Test Loss: 0.1567
OOD Validation ROC-AUC: 0.7498
OOD Validation Loss: 0.1557
OOD Test ROC-AUC: 0.7319
OOD Test Loss: 0.1092

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 73...
[0m[1;37mINFO[0m: [1mCheckpoint 73: 
-----------------------------------
Train ROC-AUC: 0.8303
Train Loss: 0.1860
ID Validation ROC-AUC: 0.7943
ID Validation Loss: 0.2115
ID Test ROC-AUC: 0.7798
ID Test Loss: 0.1994
OOD Validation ROC-AUC: 0.7827
OOD Validation Loss: 0.1548
OOD Test ROC-AUC: 0.7525
OOD Test Loss: 0.1116

[0m[1;37mINFO[0m: [1mChartInfo 0.8421 0.7319 0.7798 0.7525 0.7943 0.7827[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/12/2024 11:52:05 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/12/2024 11:52:10 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/12/2024 11:52:13 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.638
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.619
SUFF++ for r=0.8 class 0.0 = 0.943 +- 0.112 (in-sample avg dev_std = 0.165)
SUFF++ for r=0.8 class 1.0 = 0.902 +- 0.112 (in-sample avg dev_std = 0.165)
SUFF++ for r=0.8 all KL = 0.952 +- 0.112 (in-sample avg dev_std = 0.165)
SUFF++ for r=0.8 all L1 = 0.922 +- 0.098 (in-sample avg dev_std = 0.165)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.66
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.612
SUFF++ for r=0.8 class 0.0 = 0.956 +- 0.048 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.8 class 1.0 = 0.925 +- 0.048 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.8 all KL = 0.976 +- 0.048 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.8 all L1 = 0.94 +- 0.055 (in-sample avg dev_std = 0.099)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.704
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.671
SUFF++ for r=0.8 class 0.0 = 0.977 +- 0.009 (in-sample avg dev_std = 0.034)
SUFF++ for r=0.8 class 1.0 = 0.964 +- 0.009 (in-sample avg dev_std = 0.034)
SUFF++ for r=0.8 all KL = 0.993 +- 0.009 (in-sample avg dev_std = 0.034)
SUFF++ for r=0.8 all L1 = 0.97 +- 0.022 (in-sample avg dev_std = 0.034)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.634
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.611
NEC for r=0.8 class 0.0 = 0.023 +- 0.013 (in-sample avg dev_std = 0.026)
NEC for r=0.8 class 1.0 = 0.031 +- 0.013 (in-sample avg dev_std = 0.026)
NEC for r=0.8 all KL = 0.006 +- 0.013 (in-sample avg dev_std = 0.026)
NEC for r=0.8 all L1 = 0.027 +- 0.026 (in-sample avg dev_std = 0.026)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.665
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.599
NEC for r=0.8 class 0.0 = 0.024 +- 0.005 (in-sample avg dev_std = 0.017)
NEC for r=0.8 class 1.0 = 0.027 +- 0.005 (in-sample avg dev_std = 0.017)
NEC for r=0.8 all KL = 0.004 +- 0.005 (in-sample avg dev_std = 0.017)
NEC for r=0.8 all L1 = 0.026 +- 0.017 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.702
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.667
NEC for r=0.8 class 0.0 = 0.015 +- 0.004 (in-sample avg dev_std = 0.014)
NEC for r=0.8 class 1.0 = 0.023 +- 0.004 (in-sample avg dev_std = 0.014)
NEC for r=0.8 all KL = 0.003 +- 0.004 (in-sample avg dev_std = 0.014)
NEC for r=0.8 all L1 = 0.019 +- 0.015 (in-sample avg dev_std = 0.014)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:52:36 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:37 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:40 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:43 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:46 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:52:49 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 188...
[0m[1;37mINFO[0m: [1mCheckpoint 188: 
-----------------------------------
Train ROC-AUC: 0.9379
Train Loss: 0.0794
ID Validation ROC-AUC: 0.8408
ID Validation Loss: 0.1328
ID Test ROC-AUC: 0.8294
ID Test Loss: 0.1170
OOD Validation ROC-AUC: 0.7585
OOD Validation Loss: 0.1303
OOD Test ROC-AUC: 0.6813
OOD Test Loss: 0.0992

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 84...
[0m[1;37mINFO[0m: [1mCheckpoint 84: 
-----------------------------------
Train ROC-AUC: 0.8280
Train Loss: 0.1624
ID Validation ROC-AUC: 0.7995
ID Validation Loss: 0.1831
ID Test ROC-AUC: 0.7772
ID Test Loss: 0.1748
OOD Validation ROC-AUC: 0.7790
OOD Validation Loss: 0.1618
OOD Test ROC-AUC: 0.7203
OOD Test Loss: 0.1230

[0m[1;37mINFO[0m: [1mChartInfo 0.8294 0.6813 0.7772 0.7203 0.7995 0.7790[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/12/2024 11:52:50 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/12/2024 11:52:54 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/12/2024 11:52:57 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.375
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.394
SUFF++ for r=0.8 class 0.0 = 0.983 +- 0.009 (in-sample avg dev_std = 0.006)
SUFF++ for r=0.8 class 1.0 = 0.989 +- 0.009 (in-sample avg dev_std = 0.006)
SUFF++ for r=0.8 all KL = 0.995 +- 0.009 (in-sample avg dev_std = 0.006)
SUFF++ for r=0.8 all L1 = 0.986 +- 0.019 (in-sample avg dev_std = 0.006)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.318
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.433
SUFF++ for r=0.8 class 0.0 = 0.985 +- 0.005 (in-sample avg dev_std = 0.005)
SUFF++ for r=0.8 class 1.0 = 0.992 +- 0.005 (in-sample avg dev_std = 0.005)
SUFF++ for r=0.8 all KL = 0.996 +- 0.005 (in-sample avg dev_std = 0.005)
SUFF++ for r=0.8 all L1 = 0.988 +- 0.015 (in-sample avg dev_std = 0.005)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.258
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.307
SUFF++ for r=0.8 class 0.0 = 0.983 +- 0.004 (in-sample avg dev_std = 0.007)
SUFF++ for r=0.8 class 1.0 = 0.992 +- 0.004 (in-sample avg dev_std = 0.007)
SUFF++ for r=0.8 all KL = 0.997 +- 0.004 (in-sample avg dev_std = 0.007)
SUFF++ for r=0.8 all L1 = 0.987 +- 0.015 (in-sample avg dev_std = 0.007)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.374
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.355
NEC for r=0.8 class 0.0 = 0.017 +- 0.007 (in-sample avg dev_std = 0.005)
NEC for r=0.8 class 1.0 = 0.01 +- 0.007 (in-sample avg dev_std = 0.005)
NEC for r=0.8 all KL = 0.004 +- 0.007 (in-sample avg dev_std = 0.005)
NEC for r=0.8 all L1 = 0.013 +- 0.020 (in-sample avg dev_std = 0.005)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.32
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.378
NEC for r=0.8 class 0.0 = 0.014 +- 0.005 (in-sample avg dev_std = 0.003)
NEC for r=0.8 class 1.0 = 0.007 +- 0.005 (in-sample avg dev_std = 0.003)
NEC for r=0.8 all KL = 0.003 +- 0.005 (in-sample avg dev_std = 0.003)
NEC for r=0.8 all L1 = 0.011 +- 0.015 (in-sample avg dev_std = 0.003)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.259
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.326
NEC for r=0.8 class 0.0 = 0.017 +- 0.007 (in-sample avg dev_std = 0.006)
NEC for r=0.8 class 1.0 = 0.006 +- 0.007 (in-sample avg dev_std = 0.006)
NEC for r=0.8 all KL = 0.003 +- 0.007 (in-sample avg dev_std = 0.006)
NEC for r=0.8 all L1 = 0.012 +- 0.019 (in-sample avg dev_std = 0.006)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:53:22 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:22 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:25 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:29 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:32 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.7462
Train Loss: 0.1487
ID Validation ROC-AUC: 0.7584
ID Validation Loss: 0.1618
ID Test ROC-AUC: 0.7168
ID Test Loss: 0.1380
OOD Validation ROC-AUC: 0.7424
OOD Validation Loss: 0.1213
OOD Test ROC-AUC: 0.6596
OOD Test Loss: 0.0979

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ROC-AUC: 0.7560
Train Loss: 0.1465
ID Validation ROC-AUC: 0.7444
ID Validation Loss: 0.1652
ID Test ROC-AUC: 0.7112
ID Test Loss: 0.1365
OOD Validation ROC-AUC: 0.7572
OOD Validation Loss: 0.1169
OOD Test ROC-AUC: 0.6751
OOD Test Loss: 0.0962

[0m[1;37mINFO[0m: [1mChartInfo 0.7168 0.6596 0.7112 0.6751 0.7444 0.7572[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/12/2024 11:53:35 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/12/2024 11:53:39 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/12/2024 11:53:43 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.557
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.534
SUFF++ for r=0.8 class 0.0 = 0.967 +- 0.020 (in-sample avg dev_std = 0.060)
SUFF++ for r=0.8 class 1.0 = 0.956 +- 0.020 (in-sample avg dev_std = 0.060)
SUFF++ for r=0.8 all KL = 0.989 +- 0.020 (in-sample avg dev_std = 0.060)
SUFF++ for r=0.8 all L1 = 0.961 +- 0.046 (in-sample avg dev_std = 0.060)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.619
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.539
SUFF++ for r=0.8 class 0.0 = 0.968 +- 0.012 (in-sample avg dev_std = 0.042)
SUFF++ for r=0.8 class 1.0 = 0.969 +- 0.012 (in-sample avg dev_std = 0.042)
SUFF++ for r=0.8 all KL = 0.991 +- 0.012 (in-sample avg dev_std = 0.042)
SUFF++ for r=0.8 all L1 = 0.969 +- 0.029 (in-sample avg dev_std = 0.042)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.444
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.437
SUFF++ for r=0.8 class 0.0 = 0.974 +- 0.004 (in-sample avg dev_std = 0.024)
SUFF++ for r=0.8 class 1.0 = 0.975 +- 0.004 (in-sample avg dev_std = 0.024)
SUFF++ for r=0.8 all KL = 0.996 +- 0.004 (in-sample avg dev_std = 0.024)
SUFF++ for r=0.8 all L1 = 0.975 +- 0.017 (in-sample avg dev_std = 0.024)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.56
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.529
NEC for r=0.8 class 0.0 = 0.026 +- 0.010 (in-sample avg dev_std = 0.020)
NEC for r=0.8 class 1.0 = 0.036 +- 0.010 (in-sample avg dev_std = 0.020)
NEC for r=0.8 all KL = 0.006 +- 0.010 (in-sample avg dev_std = 0.020)
NEC for r=0.8 all L1 = 0.031 +- 0.038 (in-sample avg dev_std = 0.020)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.618
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.506
NEC for r=0.8 class 0.0 = 0.031 +- 0.007 (in-sample avg dev_std = 0.010)
NEC for r=0.8 class 1.0 = 0.027 +- 0.007 (in-sample avg dev_std = 0.010)
NEC for r=0.8 all KL = 0.005 +- 0.007 (in-sample avg dev_std = 0.010)
NEC for r=0.8 all L1 = 0.029 +- 0.027 (in-sample avg dev_std = 0.010)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.444
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.438
NEC for r=0.8 class 0.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.009)
NEC for r=0.8 class 1.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.009)
NEC for r=0.8 all KL = 0.003 +- 0.004 (in-sample avg dev_std = 0.009)
NEC for r=0.8 all L1 = 0.024 +- 0.019 (in-sample avg dev_std = 0.009)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.952], 'all_L1': [0.922]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.986]}), defaultdict(<class 'list'>, {'all_KL': [0.989], 'all_L1': [0.961]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.027]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.013]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.031]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.976], 'all_L1': [0.94]}), defaultdict(<class 'list'>, {'all_KL': [0.996], 'all_L1': [0.988]}), defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.969]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.026]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.011]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.029]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.993], 'all_L1': [0.97]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.987]}), defaultdict(<class 'list'>, {'all_KL': [0.996], 'all_L1': [0.975]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.019]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.012]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.024]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.956 +- 0.026
suff++ class all_KL  =  0.979 +- 0.019
suff++_acc_int  =  0.516 +- 0.093
nec class all_L1  =  0.024 +- 0.008
nec class all_KL  =  0.005 +- 0.001
nec_acc_int  =  0.498 +- 0.107

Eval split val
suff++ class all_L1  =  0.966 +- 0.020
suff++ class all_KL  =  0.988 +- 0.008
suff++_acc_int  =  0.528 +- 0.073
nec class all_L1  =  0.022 +- 0.008
nec class all_KL  =  0.004 +- 0.001
nec_acc_int  =  0.494 +- 0.091

Eval split test
suff++ class all_L1  =  0.977 +- 0.007
suff++ class all_KL  =  0.995 +- 0.002
suff++_acc_int  =  0.472 +- 0.150
nec class all_L1  =  0.018 +- 0.005
nec class all_KL  =  0.003 +- 0.000
nec_acc_int  =  0.477 +- 0.142


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.490 +- 0.011
Faith. Armon (L1)= 		  =  0.046 +- 0.015
Faith. GMean (L1)= 	  =  0.148 +- 0.025
Faith. Aritm (KL)= 		  =  0.492 +- 0.009
Faith. Armon (KL)= 		  =  0.011 +- 0.002
Faith. GMean (KL)= 	  =  0.072 +- 0.006

Eval split val
Faith. Aritm (L1)= 		  =  0.494 +- 0.008
Faith. Armon (L1)= 		  =  0.043 +- 0.015
Faith. GMean (L1)= 	  =  0.143 +- 0.028
Faith. Aritm (KL)= 		  =  0.496 +- 0.004
Faith. Armon (KL)= 		  =  0.008 +- 0.002
Faith. GMean (KL)= 	  =  0.063 +- 0.006

Eval split test
Faith. Aritm (L1)= 		  =  0.498 +- 0.002
Faith. Armon (L1)= 		  =  0.036 +- 0.009
Faith. GMean (L1)= 	  =  0.133 +- 0.018
Faith. Aritm (KL)= 		  =  0.499 +- 0.001
Faith. Armon (KL)= 		  =  0.006 +- 0.000
Faith. GMean (KL)= 	  =  0.055 +- 0.000
Computed for split load_split = id



Completed in  0:02:18.938981  for CIGAvGIN GOODHIV/scaffold



DONE CIGA GOODHIV/scaffold

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:54:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/12/2024 11:54:37 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:55:14 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:55:26 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:55:37 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:55:53 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:56:10 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.8993
Train Loss: 0.3214
ID Validation ROC-AUC: 0.8837
ID Validation Loss: 0.3373
ID Test ROC-AUC: 0.8843
ID Test Loss: 0.3454
OOD Validation ROC-AUC: 0.6640
OOD Validation Loss: 0.3458
OOD Test ROC-AUC: 0.7067
OOD Test Loss: 0.5877

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ROC-AUC: 0.8741
Train Loss: 0.3395
ID Validation ROC-AUC: 0.8720
ID Validation Loss: 0.3474
ID Test ROC-AUC: 0.8700
ID Test Loss: 0.3479
OOD Validation ROC-AUC: 0.7037
OOD Validation Loss: 0.3123
OOD Test ROC-AUC: 0.7160
OOD Test Loss: 0.5638

[0m[1;37mINFO[0m: [1mChartInfo 0.8843 0.7067 0.8700 0.7160 0.8720 0.7037[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/12/2024 11:56:11 AM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/12/2024 11:56:21 AM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/12/2024 11:56:29 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.395
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.409
SUFF++ for r=0.6 class 0.0 = 0.918 +- 0.015 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 class 1.0 = 0.924 +- 0.015 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 all KL = 0.99 +- 0.015 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 all L1 = 0.923 +- 0.038 (in-sample avg dev_std = 0.080)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.33
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.369
SUFF++ for r=0.6 class 0.0 = 0.911 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 class 1.0 = 0.925 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 all KL = 0.991 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 all L1 = 0.923 +- 0.034 (in-sample avg dev_std = 0.077)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.465
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.467
SUFF++ for r=0.6 class 0.0 = 0.924 +- 0.019 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 class 1.0 = 0.923 +- 0.019 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all KL = 0.99 +- 0.019 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all L1 = 0.923 +- 0.038 (in-sample avg dev_std = 0.079)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.395
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.381
NEC for r=0.6 class 0.0 = 0.023 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 class 1.0 = 0.025 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all L1 = 0.024 +- 0.018 (in-sample avg dev_std = 0.019)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.331
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.329
NEC for r=0.6 class 0.0 = 0.026 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.023 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.023 +- 0.018 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.465
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.454
NEC for r=0.6 class 0.0 = 0.024 +- 0.004 (in-sample avg dev_std = 0.019)
NEC for r=0.6 class 1.0 = 0.025 +- 0.004 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all KL = 0.001 +- 0.004 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all L1 = 0.025 +- 0.018 (in-sample avg dev_std = 0.019)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 11:57:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/12/2024 11:57:56 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 11:58:28 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 11:58:39 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 11:58:51 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:09 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:26 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 18...
[0m[1;37mINFO[0m: [1mCheckpoint 18: 
-----------------------------------
Train ROC-AUC: 0.9106
Train Loss: 0.2882
ID Validation ROC-AUC: 0.8956
ID Validation Loss: 0.3124
ID Test ROC-AUC: 0.8957
ID Test Loss: 0.3130
OOD Validation ROC-AUC: 0.6611
OOD Validation Loss: 0.3429
OOD Test ROC-AUC: 0.7016
OOD Test Loss: 0.5746

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ROC-AUC: 0.8841
Train Loss: 0.3105
ID Validation ROC-AUC: 0.8731
ID Validation Loss: 0.3250
ID Test ROC-AUC: 0.8715
ID Test Loss: 0.3287
OOD Validation ROC-AUC: 0.6962
OOD Validation Loss: 0.3128
OOD Test ROC-AUC: 0.7163
OOD Test Loss: 0.5402

[0m[1;37mINFO[0m: [1mChartInfo 0.8957 0.7016 0.8715 0.7163 0.8731 0.6962[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/12/2024 11:59:27 AM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/12/2024 11:59:37 AM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/12/2024 11:59:48 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.349
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.38
SUFF++ for r=0.6 class 0.0 = 0.929 +- 0.022 (in-sample avg dev_std = 0.066)
SUFF++ for r=0.6 class 1.0 = 0.946 +- 0.022 (in-sample avg dev_std = 0.066)
SUFF++ for r=0.6 all KL = 0.993 +- 0.022 (in-sample avg dev_std = 0.066)
SUFF++ for r=0.6 all L1 = 0.944 +- 0.038 (in-sample avg dev_std = 0.066)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.376
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.394
SUFF++ for r=0.6 class 0.0 = 0.935 +- 0.012 (in-sample avg dev_std = 0.056)
SUFF++ for r=0.6 class 1.0 = 0.945 +- 0.012 (in-sample avg dev_std = 0.056)
SUFF++ for r=0.6 all KL = 0.995 +- 0.012 (in-sample avg dev_std = 0.056)
SUFF++ for r=0.6 all L1 = 0.944 +- 0.033 (in-sample avg dev_std = 0.056)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.447
SUFF++ for r=0.6 class 0.0 = 0.941 +- 0.023 (in-sample avg dev_std = 0.060)
SUFF++ for r=0.6 class 1.0 = 0.944 +- 0.023 (in-sample avg dev_std = 0.060)
SUFF++ for r=0.6 all KL = 0.994 +- 0.023 (in-sample avg dev_std = 0.060)
SUFF++ for r=0.6 all L1 = 0.943 +- 0.038 (in-sample avg dev_std = 0.060)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.348
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.346
NEC for r=0.6 class 0.0 = 0.025 +- 0.006 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.018 +- 0.006 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.001 +- 0.006 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.019 +- 0.020 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.377
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.355
NEC for r=0.6 class 0.0 = 0.027 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.02 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.021 +- 0.019 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.435
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.434
NEC for r=0.6 class 0.0 = 0.022 +- 0.003 (in-sample avg dev_std = 0.016)
NEC for r=0.6 class 1.0 = 0.021 +- 0.003 (in-sample avg dev_std = 0.016)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.016)
NEC for r=0.6 all L1 = 0.021 +- 0.018 (in-sample avg dev_std = 0.016)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 12:01:18 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/12/2024 12:01:19 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/12/2024 12:01:50 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:01 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:12 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:28 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/12/2024 12:02:44 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:02:45 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 69...
[0m[1;37mINFO[0m: [1mCheckpoint 69: 
-----------------------------------
Train ROC-AUC: 0.9695
Train Loss: 0.2138
ID Validation ROC-AUC: 0.9127
ID Validation Loss: 0.3345
ID Test ROC-AUC: 0.9168
ID Test Loss: 0.3404
OOD Validation ROC-AUC: 0.6338
OOD Validation Loss: 0.4146
OOD Test ROC-AUC: 0.6908
OOD Test Loss: 0.6400

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 5...
[0m[1;37mINFO[0m: [1mCheckpoint 5: 
-----------------------------------
Train ROC-AUC: 0.8656
Train Loss: 0.2766
ID Validation ROC-AUC: 0.8601
ID Validation Loss: 0.2825
ID Test ROC-AUC: 0.8664
ID Test Loss: 0.2815
OOD Validation ROC-AUC: 0.6911
OOD Validation Loss: 0.2740
OOD Test ROC-AUC: 0.7053
OOD Test Loss: 0.4338

[0m[1;37mINFO[0m: [1mChartInfo 0.9168 0.6908 0.8664 0.7053 0.8601 0.6911[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/12/2024 12:02:45 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/12/2024 12:02:54 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/12/2024 12:03:02 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.786
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.721
SUFF++ for r=0.6 class 0.0 = 0.987 +- 0.009 (in-sample avg dev_std = 0.009)
SUFF++ for r=0.6 class 1.0 = 0.995 +- 0.009 (in-sample avg dev_std = 0.009)
SUFF++ for r=0.6 all KL = 0.998 +- 0.009 (in-sample avg dev_std = 0.009)
SUFF++ for r=0.6 all L1 = 0.994 +- 0.021 (in-sample avg dev_std = 0.009)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.603
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.571
SUFF++ for r=0.6 class 0.0 = 0.989 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 class 1.0 = 0.993 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all KL = 0.997 +- 0.012 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all L1 = 0.993 +- 0.023 (in-sample avg dev_std = 0.013)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.686
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.657
SUFF++ for r=0.6 class 0.0 = 0.982 +- 0.009 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 class 1.0 = 0.993 +- 0.009 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all KL = 0.997 +- 0.009 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all L1 = 0.991 +- 0.023 (in-sample avg dev_std = 0.013)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.786
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.778
NEC for r=0.6 class 0.0 = 0.013 +- 0.009 (in-sample avg dev_std = 0.008)
NEC for r=0.6 class 1.0 = 0.003 +- 0.009 (in-sample avg dev_std = 0.008)
NEC for r=0.6 all KL = 0.001 +- 0.009 (in-sample avg dev_std = 0.008)
NEC for r=0.6 all L1 = 0.005 +- 0.020 (in-sample avg dev_std = 0.008)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.604
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.617
NEC for r=0.6 class 0.0 = 0.008 +- 0.012 (in-sample avg dev_std = 0.008)
NEC for r=0.6 class 1.0 = 0.004 +- 0.012 (in-sample avg dev_std = 0.008)
NEC for r=0.6 all KL = 0.002 +- 0.012 (in-sample avg dev_std = 0.008)
NEC for r=0.6 all L1 = 0.005 +- 0.023 (in-sample avg dev_std = 0.008)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.687
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.684
NEC for r=0.6 class 0.0 = 0.017 +- 0.011 (in-sample avg dev_std = 0.009)
NEC for r=0.6 class 1.0 = 0.006 +- 0.011 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all KL = 0.002 +- 0.011 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all L1 = 0.008 +- 0.026 (in-sample avg dev_std = 0.009)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.923]}), defaultdict(<class 'list'>, {'all_KL': [0.993], 'all_L1': [0.944]}), defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [0.994]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.024]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.019]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.005]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.923]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.944]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.993]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.023]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.021]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.005]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.923]}), defaultdict(<class 'list'>, {'all_KL': [0.994], 'all_L1': [0.943]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.991]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.021]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.008]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.954 +- 0.030
suff++ class all_KL  =  0.994 +- 0.003
suff++_acc_int  =  0.503 +- 0.155
nec class all_L1  =  0.016 +- 0.008
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.501 +- 0.196

Eval split val
suff++ class all_L1  =  0.953 +- 0.029
suff++ class all_KL  =  0.994 +- 0.002
suff++_acc_int  =  0.445 +- 0.090
nec class all_L1  =  0.016 +- 0.008
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.434 +- 0.130

Eval split test
suff++ class all_L1  =  0.952 +- 0.029
suff++ class all_KL  =  0.994 +- 0.003
suff++_acc_int  =  0.524 +- 0.094
nec class all_L1  =  0.018 +- 0.007
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.524 +- 0.113


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.485 +- 0.011
Faith. Armon (L1)= 		  =  0.031 +- 0.016
Faith. GMean (L1)= 	  =  0.118 +- 0.034
Faith. Aritm (KL)= 		  =  0.497 +- 0.002
Faith. Armon (KL)= 		  =  0.002 +- 0.000
Faith. GMean (KL)= 	  =  0.032 +- 0.000

Eval split val
Faith. Aritm (L1)= 		  =  0.485 +- 0.011
Faith. Armon (L1)= 		  =  0.032 +- 0.016
Faith. GMean (L1)= 	  =  0.119 +- 0.034
Faith. Aritm (KL)= 		  =  0.498 +- 0.001
Faith. Armon (KL)= 		  =  0.003 +- 0.001
Faith. GMean (KL)= 	  =  0.036 +- 0.006

Eval split test
Faith. Aritm (L1)= 		  =  0.485 +- 0.011
Faith. Armon (L1)= 		  =  0.035 +- 0.014
Faith. GMean (L1)= 	  =  0.127 +- 0.027
Faith. Aritm (KL)= 		  =  0.498 +- 0.002
Faith. Armon (KL)= 		  =  0.003 +- 0.001
Faith. GMean (KL)= 	  =  0.036 +- 0.006
Computed for split load_split = id



Completed in  0:10:03.271764  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 12:05:06 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:05:07 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 46...
[0m[1;37mINFO[0m: [1mCheckpoint 46: 
-----------------------------------
Train ACCURACY: 0.3282
Train Loss: 2.3518
ID Validation ACCURACY: 0.3286
ID Validation Loss: 2.3614
ID Test ACCURACY: 0.3219
ID Test Loss: 2.3849
OOD Validation ACCURACY: 0.2846
OOD Validation Loss: 3.8053
OOD Test ACCURACY: 0.1441
OOD Test Loss: 9.3433

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 44...
[0m[1;37mINFO[0m: [1mCheckpoint 44: 
-----------------------------------
Train ACCURACY: 0.3185
Train Loss: 2.4273
ID Validation ACCURACY: 0.3143
ID Validation Loss: 2.4540
ID Test ACCURACY: 0.3124
ID Test Loss: 2.4743
OOD Validation ACCURACY: 0.3104
OOD Validation Loss: 2.7678
OOD Test ACCURACY: 0.1749
OOD Test Loss: 4.9128

[0m[1;37mINFO[0m: [1mChartInfo 0.3219 0.1441 0.3124 0.1749 0.3143 0.3104[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.319
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.207
SUFF++ for r=0.6 class 0 = 0.324 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 1 = 0.926 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 2 = 0.344 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 3 = 0.354 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 4 = 0.352 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 5 = 0.371 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 6 = 0.361 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 7 = 0.349 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 8 = 0.377 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 class 9 = 0.389 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 all KL = 0.305 +- 0.241 (in-sample avg dev_std = 0.647)
SUFF++ for r=0.6 all L1 = 0.421 +- 0.210 (in-sample avg dev_std = 0.647)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.265
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.189
SUFF++ for r=0.6 class 0 = 0.314 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 1 = 0.969 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 2 = 0.36 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 3 = 0.367 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 4 = 0.449 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 5 = 0.39 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 6 = 0.467 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 7 = 0.421 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 8 = 0.417 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 class 9 = 0.525 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 all KL = 0.368 +- 0.309 (in-sample avg dev_std = 0.672)
SUFF++ for r=0.6 all L1 = 0.477 +- 0.255 (in-sample avg dev_std = 0.672)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.138
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.147
SUFF++ for r=0.6 class 0 = 0.423 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 1 = 0.949 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 2 = 0.488 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 3 = 0.544 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 4 = 0.733 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 5 = 0.651 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 6 = 0.644 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 7 = 0.738 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 8 = 0.678 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 class 9 = 0.777 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 all KL = 0.529 +- 0.362 (in-sample avg dev_std = 0.541)
SUFF++ for r=0.6 all L1 = 0.663 +- 0.282 (in-sample avg dev_std = 0.541)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.316
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.34
NEC for r=0.6 class 0 = 0.458 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 1 = 0.062 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 2 = 0.508 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 3 = 0.536 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 4 = 0.542 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 5 = 0.547 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 6 = 0.565 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 7 = 0.598 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 8 = 0.574 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 9 = 0.551 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 all KL = 0.455 +- 0.281 (in-sample avg dev_std = 0.207)
NEC for r=0.6 all L1 = 0.488 +- 0.215 (in-sample avg dev_std = 0.207)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.264
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.285
NEC for r=0.6 class 0 = 0.477 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 1 = 0.046 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 2 = 0.539 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 3 = 0.575 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 4 = 0.554 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 5 = 0.513 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 6 = 0.569 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 7 = 0.604 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 8 = 0.622 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 9 = 0.539 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 all KL = 0.531 +- 0.322 (in-sample avg dev_std = 0.216)
NEC for r=0.6 all L1 = 0.498 +- 0.255 (in-sample avg dev_std = 0.216)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.136
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.226
NEC for r=0.6 class 0 = 0.74 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 1 = 0.108 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 2 = 0.696 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 3 = 0.6 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 4 = 0.466 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 5 = 0.573 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 6 = 0.573 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 7 = 0.532 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 8 = 0.464 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 class 9 = 0.364 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 all KL = 0.656 +- 0.389 (in-sample avg dev_std = 0.206)
NEC for r=0.6 all L1 = 0.508 +- 0.337 (in-sample avg dev_std = 0.206)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 12:12:57 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:12:59 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ACCURACY: 0.2758
Train Loss: 2.1313
ID Validation ACCURACY: 0.2723
ID Validation Loss: 2.1460
ID Test ACCURACY: 0.2776
ID Test Loss: 2.1540
OOD Validation ACCURACY: 0.2164
OOD Validation Loss: 2.4175
OOD Test ACCURACY: 0.1419
OOD Test Loss: 3.0432

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.2489
Train Loss: 2.1703
ID Validation ACCURACY: 0.2416
ID Validation Loss: 2.1907
ID Test ACCURACY: 0.2473
ID Test Loss: 2.1887
OOD Validation ACCURACY: 0.2927
OOD Validation Loss: 2.0351
OOD Test ACCURACY: 0.1840
OOD Test Loss: 2.6631

[0m[1;37mINFO[0m: [1mChartInfo 0.2776 0.1419 0.2473 0.1840 0.2416 0.2927[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.27
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.18
SUFF++ for r=0.6 class 0 = 0.624 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 1 = 0.443 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 2 = 0.617 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 3 = 0.623 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 4 = 0.564 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 5 = 0.609 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 6 = 0.584 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 7 = 0.59 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 8 = 0.617 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 class 9 = 0.573 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 all KL = 0.672 +- 0.148 (in-sample avg dev_std = 0.297)
SUFF++ for r=0.6 all L1 = 0.583 +- 0.087 (in-sample avg dev_std = 0.297)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.214
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.159
SUFF++ for r=0.6 class 0 = 0.622 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 1 = 0.441 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 2 = 0.621 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 3 = 0.62 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 4 = 0.582 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 5 = 0.638 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 6 = 0.584 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 7 = 0.578 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 8 = 0.618 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 9 = 0.562 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 all KL = 0.705 +- 0.129 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 all L1 = 0.583 +- 0.099 (in-sample avg dev_std = 0.272)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.14
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.131
SUFF++ for r=0.6 class 0 = 0.557 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 1 = 0.435 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 2 = 0.541 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 3 = 0.521 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 4 = 0.498 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 5 = 0.542 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 6 = 0.503 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 7 = 0.504 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 8 = 0.51 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 class 9 = 0.489 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 all KL = 0.653 +- 0.108 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.6 all L1 = 0.509 +- 0.079 (in-sample avg dev_std = 0.324)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.271
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.26
NEC for r=0.6 class 0 = 0.426 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 1 = 0.536 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 2 = 0.43 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 3 = 0.41 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 4 = 0.445 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 5 = 0.425 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 6 = 0.459 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 7 = 0.451 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 8 = 0.429 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 class 9 = 0.48 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 all KL = 0.337 +- 0.201 (in-sample avg dev_std = 0.115)
NEC for r=0.6 all L1 = 0.45 +- 0.127 (in-sample avg dev_std = 0.115)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.215
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.301
NEC for r=0.6 class 0 = 0.489 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 1 = 0.414 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 2 = 0.434 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 3 = 0.426 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 4 = 0.478 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 5 = 0.43 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 6 = 0.482 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 7 = 0.498 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 8 = 0.46 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 class 9 = 0.498 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 all KL = 0.367 +- 0.191 (in-sample avg dev_std = 0.125)
NEC for r=0.6 all L1 = 0.461 +- 0.114 (in-sample avg dev_std = 0.125)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.14
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.246
NEC for r=0.6 class 0 = 0.54 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 1 = 0.368 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 2 = 0.595 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 3 = 0.563 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 4 = 0.612 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 5 = 0.546 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 6 = 0.601 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 7 = 0.626 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 8 = 0.579 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 class 9 = 0.586 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 all KL = 0.529 +- 0.210 (in-sample avg dev_std = 0.113)
NEC for r=0.6 all L1 = 0.559 +- 0.132 (in-sample avg dev_std = 0.113)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri Apr 12 12:21:04 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:05 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/12/2024 12:21:06 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.3600
Train Loss: 1.8524
ID Validation ACCURACY: 0.3517
ID Validation Loss: 1.8640
ID Test ACCURACY: 0.3516
ID Test Loss: 1.8567
OOD Validation ACCURACY: 0.3029
OOD Validation Loss: 2.2550
OOD Test ACCURACY: 0.1203
OOD Test Loss: 3.8776

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.3600
Train Loss: 1.8524
ID Validation ACCURACY: 0.3517
ID Validation Loss: 1.8640
ID Test ACCURACY: 0.3516
ID Test Loss: 1.8567
OOD Validation ACCURACY: 0.3029
OOD Validation Loss: 2.2550
OOD Test ACCURACY: 0.1203
OOD Test Loss: 3.8776

[0m[1;37mINFO[0m: [1mChartInfo 0.3516 0.1203 0.3516 0.1203 0.3517 0.3029[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.343
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.222
SUFF++ for r=0.6 class 0 = 0.363 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 1 = 0.657 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 2 = 0.386 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 3 = 0.386 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 4 = 0.402 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 5 = 0.387 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 6 = 0.409 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 7 = 0.393 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 8 = 0.39 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 9 = 0.409 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all KL = 0.374 +- 0.183 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all L1 = 0.421 +- 0.132 (in-sample avg dev_std = 0.528)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.298
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.22
SUFF++ for r=0.6 class 0 = 0.352 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 1 = 0.762 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 2 = 0.361 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 3 = 0.361 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 4 = 0.403 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 5 = 0.373 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 6 = 0.419 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 7 = 0.395 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 8 = 0.382 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 class 9 = 0.429 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 all KL = 0.357 +- 0.242 (in-sample avg dev_std = 0.583)
SUFF++ for r=0.6 all L1 = 0.429 +- 0.180 (in-sample avg dev_std = 0.583)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.109
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.172
SUFF++ for r=0.6 class 0 = 0.347 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 1 = 0.239 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 2 = 0.349 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 3 = 0.329 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 4 = 0.252 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 5 = 0.338 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 6 = 0.263 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 7 = 0.319 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 8 = 0.3 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 class 9 = 0.245 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 all KL = 0.196 +- 0.181 (in-sample avg dev_std = 0.436)
SUFF++ for r=0.6 all L1 = 0.298 +- 0.119 (in-sample avg dev_std = 0.436)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.341
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.301
NEC for r=0.6 class 0 = 0.355 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 1 = 0.297 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 2 = 0.516 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 3 = 0.544 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 4 = 0.506 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 5 = 0.507 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 6 = 0.533 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 7 = 0.531 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 8 = 0.576 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 class 9 = 0.501 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 all KL = 0.399 +- 0.229 (in-sample avg dev_std = 0.209)
NEC for r=0.6 all L1 = 0.484 +- 0.171 (in-sample avg dev_std = 0.209)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.296
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.277
NEC for r=0.6 class 0 = 0.371 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 1 = 0.189 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 2 = 0.544 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 3 = 0.57 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 4 = 0.546 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 5 = 0.532 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 6 = 0.555 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 7 = 0.594 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 8 = 0.569 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 class 9 = 0.549 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 all KL = 0.446 +- 0.250 (in-sample avg dev_std = 0.227)
NEC for r=0.6 all L1 = 0.498 +- 0.196 (in-sample avg dev_std = 0.227)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.109
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.147
NEC for r=0.6 class 0 = 0.624 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 1 = 0.381 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 2 = 0.431 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 3 = 0.498 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 4 = 0.287 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 5 = 0.53 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 6 = 0.361 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 7 = 0.51 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 8 = 0.457 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 class 9 = 0.369 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 all KL = 0.402 +- 0.321 (in-sample avg dev_std = 0.198)
NEC for r=0.6 all L1 = 0.444 +- 0.270 (in-sample avg dev_std = 0.198)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.305], 'all_L1': [0.421]}), defaultdict(<class 'list'>, {'all_KL': [0.672], 'all_L1': [0.583]}), defaultdict(<class 'list'>, {'all_KL': [0.374], 'all_L1': [0.421]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.455], 'all_L1': [0.488]}), defaultdict(<class 'list'>, {'all_KL': [0.337], 'all_L1': [0.45]}), defaultdict(<class 'list'>, {'all_KL': [0.399], 'all_L1': [0.484]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.368], 'all_L1': [0.477]}), defaultdict(<class 'list'>, {'all_KL': [0.705], 'all_L1': [0.583]}), defaultdict(<class 'list'>, {'all_KL': [0.357], 'all_L1': [0.429]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.531], 'all_L1': [0.498]}), defaultdict(<class 'list'>, {'all_KL': [0.367], 'all_L1': [0.461]}), defaultdict(<class 'list'>, {'all_KL': [0.446], 'all_L1': [0.498]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.529], 'all_L1': [0.663]}), defaultdict(<class 'list'>, {'all_KL': [0.653], 'all_L1': [0.509]}), defaultdict(<class 'list'>, {'all_KL': [0.196], 'all_L1': [0.298]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.656], 'all_L1': [0.508]}), defaultdict(<class 'list'>, {'all_KL': [0.529], 'all_L1': [0.559]}), defaultdict(<class 'list'>, {'all_KL': [0.402], 'all_L1': [0.444]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.475 +- 0.076
suff++ class all_KL  =  0.450 +- 0.159
suff++_acc_int  =  0.203 +- 0.018
nec class all_L1  =  0.474 +- 0.017
nec class all_KL  =  0.397 +- 0.048
nec_acc_int  =  0.300 +- 0.032

Eval split val
suff++ class all_L1  =  0.496 +- 0.064
suff++ class all_KL  =  0.477 +- 0.162
suff++_acc_int  =  0.189 +- 0.025
nec class all_L1  =  0.486 +- 0.017
nec class all_KL  =  0.448 +- 0.067
nec_acc_int  =  0.288 +- 0.010

Eval split test
suff++ class all_L1  =  0.490 +- 0.150
suff++ class all_KL  =  0.459 +- 0.193
suff++_acc_int  =  0.150 +- 0.017
nec class all_L1  =  0.504 +- 0.047
nec class all_KL  =  0.529 +- 0.104
nec_acc_int  =  0.206 +- 0.043


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.475 +- 0.030
Faith. Armon (L1)= 		  =  0.470 +- 0.027
Faith. GMean (L1)= 	  =  0.472 +- 0.028
Faith. Aritm (KL)= 		  =  0.424 +- 0.057
Faith. Armon (KL)= 		  =  0.400 +- 0.036
Faith. GMean (KL)= 	  =  0.412 +- 0.046

Eval split val
Faith. Aritm (L1)= 		  =  0.491 +- 0.024
Faith. Armon (L1)= 		  =  0.488 +- 0.022
Faith. GMean (L1)= 	  =  0.489 +- 0.023
Faith. Aritm (KL)= 		  =  0.462 +- 0.056
Faith. Armon (KL)= 		  =  0.438 +- 0.035
Faith. GMean (KL)= 	  =  0.450 +- 0.045

Eval split test
Faith. Aritm (L1)= 		  =  0.497 +- 0.091
Faith. Armon (L1)= 		  =  0.488 +- 0.095
Faith. GMean (L1)= 	  =  0.493 +- 0.093
Faith. Aritm (KL)= 		  =  0.494 +- 0.138
Faith. Armon (KL)= 		  =  0.478 +- 0.152
Faith. GMean (KL)= 	  =  0.486 +- 0.145
Computed for split load_split = id



Completed in  0:23:54.260392  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color
DONE all :)
