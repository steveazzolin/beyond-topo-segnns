nohup: ignoring input
Time to compute metrics!
The PID of this script is: 959981
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:53:18 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:18 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 133...
[0m[1;37mINFO[0m: [1mCheckpoint 133: 
-----------------------------------
Train ACCURACY: 0.4515
Train Loss: 2.0116
ID Validation ACCURACY: 0.4543
ID Validation Loss: 2.0095
ID Test ACCURACY: 0.4537
ID Test Loss: 1.9627
OOD Validation ACCURACY: 0.3667
OOD Validation Loss: 1.7075
OOD Test ACCURACY: 0.4907
OOD Test Loss: 20.5069

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.3873
Train Loss: 2.8169
ID Validation ACCURACY: 0.4017
ID Validation Loss: 2.8787
ID Test ACCURACY: 0.3987
ID Test Loss: 2.7126
OOD Validation ACCURACY: 0.4830
OOD Validation Loss: 1.4196
OOD Test ACCURACY: 0.3533
OOD Test Loss: 4.9358

[0m[1;37mINFO[0m: [1mChartInfo 0.4537 0.4907 0.3987 0.3533 0.4017 0.4830[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.311
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.323


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.441
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.31095375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.396
SUFF++ for r=0.8 class 0 = 0.559 +- 0.262 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 class 1 = 0.667 +- 0.262 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 class 2 = 0.624 +- 0.262 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 all KL = 0.669 +- 0.262 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 all L1 = 0.617 +- 0.189 (in-sample avg dev_std = 0.443)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.441
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.31095375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.358
NEC for r=0.8 class 0 = 0.346 +- 0.287 (in-sample avg dev_std = 0.427)
NEC for r=0.8 class 1 = 0.411 +- 0.287 (in-sample avg dev_std = 0.427)
NEC for r=0.8 class 2 = 0.297 +- 0.287 (in-sample avg dev_std = 0.427)
NEC for r=0.8 all KL = 0.309 +- 0.287 (in-sample avg dev_std = 0.427)
NEC for r=0.8 all L1 = 0.351 +- 0.201 (in-sample avg dev_std = 0.427)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:53:46 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:46 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 0.5550
Train Loss: 1.0486
ID Validation ACCURACY: 0.5667
ID Validation Loss: 1.0182
ID Test ACCURACY: 0.5497
ID Test Loss: 1.0586
OOD Validation ACCURACY: 0.2923
OOD Validation Loss: 1.8056
OOD Test ACCURACY: 0.3420
OOD Test Loss: 2.4359

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 156...
[0m[1;37mINFO[0m: [1mCheckpoint 156: 
-----------------------------------
Train ACCURACY: 0.3756
Train Loss: 1.3215
ID Validation ACCURACY: 0.3880
ID Validation Loss: 1.2841
ID Test ACCURACY: 0.3630
ID Test Loss: 1.3120
OOD Validation ACCURACY: 0.4910
OOD Validation Loss: 1.0719
OOD Test ACCURACY: 0.4573
OOD Test Loss: 1.3658

[0m[1;37mINFO[0m: [1mChartInfo 0.5497 0.3420 0.3630 0.4573 0.3880 0.4910[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.199
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.255


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.57
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.19867875000000002
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.444
SUFF++ for r=0.8 class 0 = 0.688 +- 0.182 (in-sample avg dev_std = 0.246)
SUFF++ for r=0.8 class 1 = 0.663 +- 0.182 (in-sample avg dev_std = 0.246)
SUFF++ for r=0.8 class 2 = 0.565 +- 0.182 (in-sample avg dev_std = 0.246)
SUFF++ for r=0.8 all KL = 0.787 +- 0.182 (in-sample avg dev_std = 0.246)
SUFF++ for r=0.8 all L1 = 0.639 +- 0.155 (in-sample avg dev_std = 0.246)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.57
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.19867875000000002
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.37
NEC for r=0.8 class 0 = 0.389 +- 0.186 (in-sample avg dev_std = 0.330)
NEC for r=0.8 class 1 = 0.357 +- 0.186 (in-sample avg dev_std = 0.330)
NEC for r=0.8 class 2 = 0.454 +- 0.186 (in-sample avg dev_std = 0.330)
NEC for r=0.8 all KL = 0.268 +- 0.186 (in-sample avg dev_std = 0.330)
NEC for r=0.8 all L1 = 0.4 +- 0.153 (in-sample avg dev_std = 0.330)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:54:11 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:11 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.3922
Train Loss: 2.5243
ID Validation ACCURACY: 0.4000
ID Validation Loss: 2.6128
ID Test ACCURACY: 0.4023
ID Test Loss: 2.4780
OOD Validation ACCURACY: 0.3473
OOD Validation Loss: 1.5777
OOD Test ACCURACY: 0.4613
OOD Test Loss: 12.1111

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 105...
[0m[1;37mINFO[0m: [1mCheckpoint 105: 
-----------------------------------
Train ACCURACY: 0.3601
Train Loss: 2.8396
ID Validation ACCURACY: 0.3580
ID Validation Loss: 2.9725
ID Test ACCURACY: 0.3643
ID Test Loss: 2.7594
OOD Validation ACCURACY: 0.4673
OOD Validation Loss: 1.2057
OOD Test ACCURACY: 0.3347
OOD Test Loss: 10.3926

[0m[1;37mINFO[0m: [1mChartInfo 0.4023 0.4613 0.3643 0.3347 0.3580 0.4673[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.300
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.319


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.379
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.30014375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.336
SUFF++ for r=0.8 class 0 = 0.69 +- 0.181 (in-sample avg dev_std = 0.291)
SUFF++ for r=0.8 class 1 = 0.739 +- 0.181 (in-sample avg dev_std = 0.291)
SUFF++ for r=0.8 class 2 = 0.733 +- 0.181 (in-sample avg dev_std = 0.291)
SUFF++ for r=0.8 all KL = 0.808 +- 0.181 (in-sample avg dev_std = 0.291)
SUFF++ for r=0.8 all L1 = 0.721 +- 0.189 (in-sample avg dev_std = 0.291)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.379
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.30014375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.331
NEC for r=0.8 class 0 = 0.269 +- 0.235 (in-sample avg dev_std = 0.327)
NEC for r=0.8 class 1 = 0.335 +- 0.235 (in-sample avg dev_std = 0.327)
NEC for r=0.8 class 2 = 0.213 +- 0.235 (in-sample avg dev_std = 0.327)
NEC for r=0.8 all KL = 0.202 +- 0.235 (in-sample avg dev_std = 0.327)
NEC for r=0.8 all L1 = 0.273 +- 0.194 (in-sample avg dev_std = 0.327)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:54:38 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:38 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 191...
[0m[1;37mINFO[0m: [1mCheckpoint 191: 
-----------------------------------
Train ACCURACY: 0.4899
Train Loss: 2.0486
ID Validation ACCURACY: 0.4887
ID Validation Loss: 2.1187
ID Test ACCURACY: 0.4963
ID Test Loss: 1.9416
OOD Validation ACCURACY: 0.3943
OOD Validation Loss: 1.1884
OOD Test ACCURACY: 0.4750
OOD Test Loss: 32.6168

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 43...
[0m[1;37mINFO[0m: [1mCheckpoint 43: 
-----------------------------------
Train ACCURACY: 0.3959
Train Loss: 2.6359
ID Validation ACCURACY: 0.4100
ID Validation Loss: 2.6788
ID Test ACCURACY: 0.4033
ID Test Loss: 2.6084
OOD Validation ACCURACY: 0.5240
OOD Validation Loss: 1.4202
OOD Test ACCURACY: 0.4643
OOD Test Loss: 2.3969

[0m[1;37mINFO[0m: [1mChartInfo 0.4963 0.4750 0.4033 0.4643 0.4100 0.5240[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.315
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.328


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.481
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3145825
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.363
SUFF++ for r=0.8 class 0 = 0.518 +- 0.225 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 class 1 = 0.543 +- 0.225 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 class 2 = 0.558 +- 0.225 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 all KL = 0.641 +- 0.225 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 all L1 = 0.54 +- 0.211 (in-sample avg dev_std = 0.324)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.481
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3145825
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.332
NEC for r=0.8 class 0 = 0.407 +- 0.250 (in-sample avg dev_std = 0.364)
NEC for r=0.8 class 1 = 0.518 +- 0.250 (in-sample avg dev_std = 0.364)
NEC for r=0.8 class 2 = 0.332 +- 0.250 (in-sample avg dev_std = 0.364)
NEC for r=0.8 all KL = 0.326 +- 0.250 (in-sample avg dev_std = 0.364)
NEC for r=0.8 all L1 = 0.42 +- 0.204 (in-sample avg dev_std = 0.364)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:55:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:04 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:04 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 42...
[0m[1;37mINFO[0m: [1mCheckpoint 42: 
-----------------------------------
Train ACCURACY: 0.5243
Train Loss: 1.6300
ID Validation ACCURACY: 0.5290
ID Validation Loss: 1.6217
ID Test ACCURACY: 0.5153
ID Test Loss: 1.6486
OOD Validation ACCURACY: 0.3360
OOD Validation Loss: 2.9312
OOD Test ACCURACY: 0.5983
OOD Test Loss: 1.1300

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.3722
Train Loss: 2.1560
ID Validation ACCURACY: 0.3763
ID Validation Loss: 2.0934
ID Test ACCURACY: 0.3590
ID Test Loss: 2.1927
OOD Validation ACCURACY: 0.4270
OOD Validation Loss: 1.7656
OOD Test ACCURACY: 0.5453
OOD Test Loss: 1.2780

[0m[1;37mINFO[0m: [1mChartInfo 0.5153 0.5983 0.3590 0.5453 0.3763 0.4270[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.188
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.234


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.541
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.18775124999999998
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.406
SUFF++ for r=0.8 class 0 = 0.684 +- 0.258 (in-sample avg dev_std = 0.350)
SUFF++ for r=0.8 class 1 = 0.631 +- 0.258 (in-sample avg dev_std = 0.350)
SUFF++ for r=0.8 class 2 = 0.539 +- 0.258 (in-sample avg dev_std = 0.350)
SUFF++ for r=0.8 all KL = 0.665 +- 0.258 (in-sample avg dev_std = 0.350)
SUFF++ for r=0.8 all L1 = 0.618 +- 0.219 (in-sample avg dev_std = 0.350)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.541
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.18775124999999998
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.407
NEC for r=0.8 class 0 = 0.287 +- 0.267 (in-sample avg dev_std = 0.402)
NEC for r=0.8 class 1 = 0.317 +- 0.267 (in-sample avg dev_std = 0.402)
NEC for r=0.8 class 2 = 0.478 +- 0.267 (in-sample avg dev_std = 0.402)
NEC for r=0.8 all KL = 0.31 +- 0.267 (in-sample avg dev_std = 0.402)
NEC for r=0.8 all L1 = 0.36 +- 0.216 (in-sample avg dev_std = 0.402)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.669], 'all_L1': [0.617]}), defaultdict(<class 'list'>, {'all_KL': [0.787], 'all_L1': [0.639]}), defaultdict(<class 'list'>, {'all_KL': [0.808], 'all_L1': [0.721]}), defaultdict(<class 'list'>, {'all_KL': [0.641], 'all_L1': [0.54]}), defaultdict(<class 'list'>, {'all_KL': [0.665], 'all_L1': [0.618]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.309], 'all_L1': [0.351]}), defaultdict(<class 'list'>, {'all_KL': [0.268], 'all_L1': [0.4]}), defaultdict(<class 'list'>, {'all_KL': [0.202], 'all_L1': [0.273]}), defaultdict(<class 'list'>, {'all_KL': [0.326], 'all_L1': [0.42]}), defaultdict(<class 'list'>, {'all_KL': [0.31], 'all_L1': [0.36]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.627 +- 0.058
suff++ class all_KL  =  0.714 +- 0.069
suff++_acc_int  =  0.389 +- 0.037
nec class all_L1  =  0.361 +- 0.051
nec class all_KL  =  0.283 +- 0.045
nec_acc_int  =  0.360 +- 0.028


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.494 +- 0.014
Faith. Armon (L1)= 		  =  0.453 +- 0.032
Faith. GMean (L1)= 	  =  0.473 +- 0.020
Faith. Aritm (KL)= 		  =  0.499 +- 0.016
Faith. Armon (KL)= 		  =  0.400 +- 0.040
Faith. GMean (KL)= 	  =  0.446 +- 0.021
Computed for split load_split = id



Completed in  0:02:13.498029  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:55:44 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:45 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 17...
[0m[1;37mINFO[0m: [1mCheckpoint 17: 
-----------------------------------
Train ACCURACY: 0.8550
Train Loss: 0.4111
ID Validation ACCURACY: 0.8098
ID Validation Loss: 0.4890
ID Test ACCURACY: 0.8080
ID Test Loss: 0.5001
OOD Validation ACCURACY: 0.7392
OOD Validation Loss: 0.5336
OOD Test ACCURACY: 0.5942
OOD Test Loss: 0.6023

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 17...
[0m[1;37mINFO[0m: [1mCheckpoint 17: 
-----------------------------------
Train ACCURACY: 0.8550
Train Loss: 0.4111
ID Validation ACCURACY: 0.8098
ID Validation Loss: 0.4890
ID Test ACCURACY: 0.8080
ID Test Loss: 0.5001
OOD Validation ACCURACY: 0.7392
OOD Validation Loss: 0.5336
OOD Test ACCURACY: 0.5942
OOD Test Loss: 0.6023

[0m[1;37mINFO[0m: [1mChartInfo 0.8080 0.5942 0.8080 0.5942 0.8098 0.7392[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 08:55:46 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.839
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.79
SUFF++ for r=0.8 class 0.0 = 0.913 +- 0.035 (in-sample avg dev_std = 0.103)
SUFF++ for r=0.8 class 1.0 = 0.933 +- 0.035 (in-sample avg dev_std = 0.103)
SUFF++ for r=0.8 all KL = 0.979 +- 0.035 (in-sample avg dev_std = 0.103)
SUFF++ for r=0.8 all L1 = 0.925 +- 0.072 (in-sample avg dev_std = 0.103)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.804
NEC for r=0.8 class 0.0 = 0.075 +- 0.065 (in-sample avg dev_std = 0.086)
NEC for r=0.8 class 1.0 = 0.095 +- 0.065 (in-sample avg dev_std = 0.086)
NEC for r=0.8 all KL = 0.037 +- 0.065 (in-sample avg dev_std = 0.086)
NEC for r=0.8 all L1 = 0.087 +- 0.085 (in-sample avg dev_std = 0.086)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:55:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0754
ID Validation ACCURACY: 0.8604
ID Validation Loss: 1.0229
ID Test ACCURACY: 0.8561
ID Test Loss: 1.1527
OOD Validation ACCURACY: 0.8477
OOD Validation Loss: 3.3872
OOD Test ACCURACY: 0.7903
OOD Test Loss: 4.9558

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 118...
[0m[1;37mINFO[0m: [1mCheckpoint 118: 
-----------------------------------
Train ACCURACY: 0.9484
Train Loss: 0.0768
ID Validation ACCURACY: 0.8532
ID Validation Loss: 1.2979
ID Test ACCURACY: 0.8521
ID Test Loss: 1.5484
OOD Validation ACCURACY: 0.8587
OOD Validation Loss: 3.8430
OOD Test ACCURACY: 0.8120
OOD Test Loss: 5.5150

[0m[1;37mINFO[0m: [1mChartInfo 0.8561 0.7903 0.8521 0.8120 0.8532 0.8587[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 08:55:57 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.884
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.835
SUFF++ for r=0.8 class 0.0 = 0.87 +- 0.325 (in-sample avg dev_std = 0.349)
SUFF++ for r=0.8 class 1.0 = 0.886 +- 0.325 (in-sample avg dev_std = 0.349)
SUFF++ for r=0.8 all KL = 0.783 +- 0.325 (in-sample avg dev_std = 0.349)
SUFF++ for r=0.8 all L1 = 0.879 +- 0.189 (in-sample avg dev_std = 0.349)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.857
NEC for r=0.8 class 0.0 = 0.083 +- 0.203 (in-sample avg dev_std = 0.172)
NEC for r=0.8 class 1.0 = 0.062 +- 0.203 (in-sample avg dev_std = 0.172)
NEC for r=0.8 all KL = 0.085 +- 0.203 (in-sample avg dev_std = 0.172)
NEC for r=0.8 all L1 = 0.07 +- 0.156 (in-sample avg dev_std = 0.172)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:56:06 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 194...
[0m[1;37mINFO[0m: [1mCheckpoint 194: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8623
ID Validation Loss: 2.4682
ID Test ACCURACY: 0.8612
ID Test Loss: 2.9010
OOD Validation ACCURACY: 0.8557
OOD Validation Loss: 5.3706
OOD Test ACCURACY: 0.7949
OOD Test Loss: 8.1971

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 74...
[0m[1;37mINFO[0m: [1mCheckpoint 74: 
-----------------------------------
Train ACCURACY: 0.9484
Train Loss: 0.0805
ID Validation ACCURACY: 0.8544
ID Validation Loss: 0.8464
ID Test ACCURACY: 0.8542
ID Test Loss: 1.0687
OOD Validation ACCURACY: 0.8595
OOD Validation Loss: 2.9836
OOD Test ACCURACY: 0.7982
OOD Test Loss: 3.4505

[0m[1;37mINFO[0m: [1mChartInfo 0.8612 0.7949 0.8542 0.7982 0.8544 0.8595[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 08:56:07 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.884
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.838
SUFF++ for r=0.8 class 0.0 = 0.872 +- 0.389 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 class 1.0 = 0.901 +- 0.389 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 all KL = 0.752 +- 0.389 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 all L1 = 0.889 +- 0.201 (in-sample avg dev_std = 0.416)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.884
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.868
NEC for r=0.8 class 0.0 = 0.064 +- 0.244 (in-sample avg dev_std = 0.205)
NEC for r=0.8 class 1.0 = 0.045 +- 0.244 (in-sample avg dev_std = 0.205)
NEC for r=0.8 all KL = 0.085 +- 0.244 (in-sample avg dev_std = 0.205)
NEC for r=0.8 all L1 = 0.053 +- 0.164 (in-sample avg dev_std = 0.205)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:56:16 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:17 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:18 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 170...
[0m[1;37mINFO[0m: [1mCheckpoint 170: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8623
ID Validation Loss: 1.5974
ID Test ACCURACY: 0.8544
ID Test Loss: 2.0365
OOD Validation ACCURACY: 0.8543
OOD Validation Loss: 3.4836
OOD Test ACCURACY: 0.7864
OOD Test Loss: 7.7740

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 132...
[0m[1;37mINFO[0m: [1mCheckpoint 132: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0755
ID Validation ACCURACY: 0.8536
ID Validation Loss: 1.0924
ID Test ACCURACY: 0.8563
ID Test Loss: 1.3289
OOD Validation ACCURACY: 0.8577
OOD Validation Loss: 2.8081
OOD Test ACCURACY: 0.7972
OOD Test Loss: 3.9612

[0m[1;37mINFO[0m: [1mChartInfo 0.8544 0.7864 0.8563 0.7972 0.8536 0.8577[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 08:56:18 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.845
SUFF++ for r=0.8 class 0.0 = 0.889 +- 0.357 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 1.0 = 0.904 +- 0.357 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all KL = 0.771 +- 0.357 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all L1 = 0.898 +- 0.185 (in-sample avg dev_std = 0.372)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.871
NEC for r=0.8 class 0.0 = 0.078 +- 0.254 (in-sample avg dev_std = 0.218)
NEC for r=0.8 class 1.0 = 0.067 +- 0.254 (in-sample avg dev_std = 0.218)
NEC for r=0.8 all KL = 0.098 +- 0.254 (in-sample avg dev_std = 0.218)
NEC for r=0.8 all L1 = 0.071 +- 0.188 (in-sample avg dev_std = 0.218)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:56:27 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 110...
[0m[1;37mINFO[0m: [1mCheckpoint 110: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0757
ID Validation ACCURACY: 0.8610
ID Validation Loss: 0.8028
ID Test ACCURACY: 0.8600
ID Test Loss: 1.0996
OOD Validation ACCURACY: 0.8599
OOD Validation Loss: 2.7628
OOD Test ACCURACY: 0.8133
OOD Test Loss: 3.9052

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 109...
[0m[1;37mINFO[0m: [1mCheckpoint 109: 
-----------------------------------
Train ACCURACY: 0.9481
Train Loss: 0.0779
ID Validation ACCURACY: 0.8563
ID Validation Loss: 0.7223
ID Test ACCURACY: 0.8585
ID Test Loss: 0.9870
OOD Validation ACCURACY: 0.8610
OOD Validation Loss: 2.2647
OOD Test ACCURACY: 0.8027
OOD Test Loss: 3.2630

[0m[1;37mINFO[0m: [1mChartInfo 0.8600 0.8133 0.8585 0.8027 0.8563 0.8610[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 08:56:27 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.888
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.867
SUFF++ for r=0.8 class 0.0 = 0.884 +- 0.311 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 class 1.0 = 0.901 +- 0.311 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 all KL = 0.806 +- 0.311 (in-sample avg dev_std = 0.331)
SUFF++ for r=0.8 all L1 = 0.894 +- 0.186 (in-sample avg dev_std = 0.331)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.888
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.881
NEC for r=0.8 class 0.0 = 0.072 +- 0.222 (in-sample avg dev_std = 0.163)
NEC for r=0.8 class 1.0 = 0.065 +- 0.222 (in-sample avg dev_std = 0.163)
NEC for r=0.8 all KL = 0.086 +- 0.222 (in-sample avg dev_std = 0.163)
NEC for r=0.8 all L1 = 0.068 +- 0.168 (in-sample avg dev_std = 0.163)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.979], 'all_L1': [0.925]}), defaultdict(<class 'list'>, {'all_KL': [0.783], 'all_L1': [0.879]}), defaultdict(<class 'list'>, {'all_KL': [0.752], 'all_L1': [0.889]}), defaultdict(<class 'list'>, {'all_KL': [0.771], 'all_L1': [0.898]}), defaultdict(<class 'list'>, {'all_KL': [0.806], 'all_L1': [0.894]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.037], 'all_L1': [0.087]}), defaultdict(<class 'list'>, {'all_KL': [0.085], 'all_L1': [0.07]}), defaultdict(<class 'list'>, {'all_KL': [0.085], 'all_L1': [0.053]}), defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.071]}), defaultdict(<class 'list'>, {'all_KL': [0.086], 'all_L1': [0.068]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.897 +- 0.015
suff++ class all_KL  =  0.818 +- 0.082
suff++_acc_int  =  0.835 +- 0.025
nec class all_L1  =  0.070 +- 0.011
nec class all_KL  =  0.078 +- 0.021
nec_acc_int  =  0.856 +- 0.027


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.483 +- 0.012
Faith. Armon (L1)= 		  =  0.129 +- 0.019
Faith. GMean (L1)= 	  =  0.250 +- 0.021
Faith. Aritm (KL)= 		  =  0.448 +- 0.031
Faith. Armon (KL)= 		  =  0.141 +- 0.036
Faith. GMean (KL)= 	  =  0.248 +- 0.030
Computed for split load_split = id



Completed in  0:00:54.772689  for CIGAGIN GOODSST2/length



DONE CIGA GOODSST2/length ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:56:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:52 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 98...
[0m[1;37mINFO[0m: [1mCheckpoint 98: 
-----------------------------------
Train ACCURACY: 0.2955
Train Loss: 2.1309
ID Validation ACCURACY: 0.2990
ID Validation Loss: 2.1134
ID Test ACCURACY: 0.2890
ID Test Loss: 2.1875
OOD Validation ACCURACY: 0.2706
OOD Validation Loss: 2.4837
OOD Test ACCURACY: 0.2086
OOD Test Loss: 4.6387

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.2797
Train Loss: 2.1018
ID Validation ACCURACY: 0.2841
ID Validation Loss: 2.0829
ID Test ACCURACY: 0.2744
ID Test Loss: 2.1502
OOD Validation ACCURACY: 0.2713
OOD Validation Loss: 2.2479
OOD Test ACCURACY: 0.1836
OOD Test Loss: 3.7387

[0m[1;37mINFO[0m: [1mChartInfo 0.2890 0.2086 0.2744 0.1836 0.2841 0.2713[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.304
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.212
SUFF++ for r=0.6 class 0 = 0.332 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 1 = 0.459 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 2 = 0.464 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 3 = 0.477 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 4 = 0.477 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 5 = 0.45 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 6 = 0.469 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 7 = 0.463 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 8 = 0.455 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 9 = 0.46 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 all KL = 0.466 +- 0.150 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 all L1 = 0.451 +- 0.083 (in-sample avg dev_std = 0.531)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.303
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.329
NEC for r=0.6 class 0 = 0.378 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 1 = 0.47 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 2 = 0.474 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 3 = 0.504 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 4 = 0.479 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 5 = 0.508 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 6 = 0.505 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 7 = 0.516 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 8 = 0.511 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 class 9 = 0.475 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 all KL = 0.475 +- 0.242 (in-sample avg dev_std = 0.191)
NEC for r=0.6 all L1 = 0.482 +- 0.134 (in-sample avg dev_std = 0.191)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:59:49 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 08:59:50 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.4261
Train Loss: 1.5957
ID Validation ACCURACY: 0.4141
ID Validation Loss: 1.6418
ID Test ACCURACY: 0.4197
ID Test Loss: 1.6257
OOD Validation ACCURACY: 0.3443
OOD Validation Loss: 2.1405
OOD Test ACCURACY: 0.2526
OOD Test Loss: 2.5709

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 98...
[0m[1;37mINFO[0m: [1mCheckpoint 98: 
-----------------------------------
Train ACCURACY: 0.3952
Train Loss: 1.7198
ID Validation ACCURACY: 0.3889
ID Validation Loss: 1.7556
ID Test ACCURACY: 0.3903
ID Test Loss: 1.7504
OOD Validation ACCURACY: 0.3587
OOD Validation Loss: 2.0597
OOD Test ACCURACY: 0.2260
OOD Test Loss: 2.7268

[0m[1;37mINFO[0m: [1mChartInfo 0.4197 0.2526 0.3903 0.2260 0.3889 0.3587[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.411
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.265
SUFF++ for r=0.6 class 0 = 0.469 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 1 = 0.429 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 2 = 0.559 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 3 = 0.538 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 4 = 0.506 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 5 = 0.525 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 6 = 0.505 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 7 = 0.516 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 8 = 0.502 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 class 9 = 0.463 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 all KL = 0.543 +- 0.243 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.6 all L1 = 0.501 +- 0.107 (in-sample avg dev_std = 0.451)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.416
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.348
NEC for r=0.6 class 0 = 0.294 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 1 = 0.266 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 2 = 0.531 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 3 = 0.502 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 4 = 0.446 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 5 = 0.523 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 6 = 0.501 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 7 = 0.519 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 8 = 0.496 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 9 = 0.44 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 all KL = 0.368 +- 0.229 (in-sample avg dev_std = 0.213)
NEC for r=0.6 all L1 = 0.45 +- 0.184 (in-sample avg dev_std = 0.213)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:02:52 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:52 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 09:02:53 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 96...
[0m[1;37mINFO[0m: [1mCheckpoint 96: 
-----------------------------------
Train ACCURACY: 0.3742
Train Loss: 1.7882
ID Validation ACCURACY: 0.3836
ID Validation Loss: 1.7873
ID Test ACCURACY: 0.3733
ID Test Loss: 1.8005
OOD Validation ACCURACY: 0.3237
OOD Validation Loss: 2.1818
OOD Test ACCURACY: 0.1913
OOD Test Loss: 3.2680

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.3845
Train Loss: 1.7577
ID Validation ACCURACY: 0.3803
ID Validation Loss: 1.7604
ID Test ACCURACY: 0.3764
ID Test Loss: 1.7715
OOD Validation ACCURACY: 0.3377
OOD Validation Loss: 2.0441
OOD Test ACCURACY: 0.1530
OOD Test Loss: 4.8804

[0m[1;37mINFO[0m: [1mChartInfo 0.3733 0.1913 0.3764 0.1530 0.3803 0.3377[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.35
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.269
SUFF++ for r=0.6 class 0 = 0.387 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 1 = 0.649 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 2 = 0.51 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 3 = 0.502 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 4 = 0.513 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 5 = 0.511 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 6 = 0.523 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 7 = 0.515 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 8 = 0.487 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 9 = 0.524 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 all KL = 0.543 +- 0.202 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 all L1 = 0.514 +- 0.115 (in-sample avg dev_std = 0.473)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.35
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.332
NEC for r=0.6 class 0 = 0.299 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 1 = 0.198 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 2 = 0.539 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 3 = 0.53 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 4 = 0.441 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 5 = 0.558 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 6 = 0.492 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 7 = 0.507 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 8 = 0.542 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 class 9 = 0.455 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 all KL = 0.37 +- 0.233 (in-sample avg dev_std = 0.229)
NEC for r=0.6 all L1 = 0.452 +- 0.192 (in-sample avg dev_std = 0.229)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:05:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:05:59 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:00 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 88...
[0m[1;37mINFO[0m: [1mCheckpoint 88: 
-----------------------------------
Train ACCURACY: 0.4318
Train Loss: 1.6191
ID Validation ACCURACY: 0.4249
ID Validation Loss: 1.6400
ID Test ACCURACY: 0.4277
ID Test Loss: 1.6411
OOD Validation ACCURACY: 0.3363
OOD Validation Loss: 2.1037
OOD Test ACCURACY: 0.2310
OOD Test Loss: 2.6250

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.4277
Train Loss: 1.6056
ID Validation ACCURACY: 0.4169
ID Validation Loss: 1.6171
ID Test ACCURACY: 0.4223
ID Test Loss: 1.6322
OOD Validation ACCURACY: 0.3510
OOD Validation Loss: 1.9996
OOD Test ACCURACY: 0.2556
OOD Test Loss: 2.4187

[0m[1;37mINFO[0m: [1mChartInfo 0.4277 0.2310 0.4223 0.2556 0.4169 0.3510[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.424
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.276
SUFF++ for r=0.6 class 0 = 0.419 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 1 = 0.626 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 2 = 0.535 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 3 = 0.534 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 4 = 0.492 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 5 = 0.507 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 6 = 0.496 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 7 = 0.486 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 8 = 0.494 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 class 9 = 0.475 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 all KL = 0.569 +- 0.213 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.6 all L1 = 0.508 +- 0.120 (in-sample avg dev_std = 0.416)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.426
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.358
NEC for r=0.6 class 0 = 0.29 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 1 = 0.171 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 2 = 0.513 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 3 = 0.476 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 4 = 0.413 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 5 = 0.52 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 6 = 0.48 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 7 = 0.506 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 8 = 0.49 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 9 = 0.432 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all KL = 0.321 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all L1 = 0.425 +- 0.185 (in-sample avg dev_std = 0.208)
model_dirname= repr_CIGAGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:08:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:57 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.4111
Train Loss: 1.6693
ID Validation ACCURACY: 0.4090
ID Validation Loss: 1.6845
ID Test ACCURACY: 0.4119
ID Test Loss: 1.6805
OOD Validation ACCURACY: 0.3519
OOD Validation Loss: 2.0506
OOD Test ACCURACY: 0.2664
OOD Test Loss: 2.5518

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.4111
Train Loss: 1.6693
ID Validation ACCURACY: 0.4090
ID Validation Loss: 1.6845
ID Test ACCURACY: 0.4119
ID Test Loss: 1.6805
OOD Validation ACCURACY: 0.3519
OOD Validation Loss: 2.0506
OOD Test ACCURACY: 0.2664
OOD Test Loss: 2.5518

[0m[1;37mINFO[0m: [1mChartInfo 0.4119 0.2664 0.4119 0.2664 0.4090 0.3519[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.409
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.281
SUFF++ for r=0.6 class 0 = 0.465 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 1 = 0.505 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 2 = 0.574 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 3 = 0.558 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 4 = 0.522 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 5 = 0.549 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 6 = 0.519 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 7 = 0.536 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 8 = 0.525 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 class 9 = 0.508 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 all KL = 0.582 +- 0.236 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.6 all L1 = 0.526 +- 0.114 (in-sample avg dev_std = 0.443)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.405
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.351
NEC for r=0.6 class 0 = 0.305 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 1 = 0.168 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 2 = 0.468 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 3 = 0.469 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 4 = 0.407 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 5 = 0.464 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 6 = 0.45 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 7 = 0.485 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 8 = 0.468 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 9 = 0.412 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all KL = 0.301 +- 0.214 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all L1 = 0.407 +- 0.180 (in-sample avg dev_std = 0.208)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.466], 'all_L1': [0.451]}), defaultdict(<class 'list'>, {'all_KL': [0.543], 'all_L1': [0.501]}), defaultdict(<class 'list'>, {'all_KL': [0.543], 'all_L1': [0.514]}), defaultdict(<class 'list'>, {'all_KL': [0.569], 'all_L1': [0.508]}), defaultdict(<class 'list'>, {'all_KL': [0.582], 'all_L1': [0.526]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.475], 'all_L1': [0.482]}), defaultdict(<class 'list'>, {'all_KL': [0.368], 'all_L1': [0.45]}), defaultdict(<class 'list'>, {'all_KL': [0.37], 'all_L1': [0.452]}), defaultdict(<class 'list'>, {'all_KL': [0.321], 'all_L1': [0.425]}), defaultdict(<class 'list'>, {'all_KL': [0.301], 'all_L1': [0.407]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.500 +- 0.026
suff++ class all_KL  =  0.541 +- 0.040
suff++_acc_int  =  0.261 +- 0.025
nec class all_L1  =  0.443 +- 0.026
nec class all_KL  =  0.367 +- 0.060
nec_acc_int  =  0.344 +- 0.011


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.472 +- 0.007
Faith. Armon (L1)= 		  =  0.469 +- 0.008
Faith. GMean (L1)= 	  =  0.470 +- 0.007
Faith. Aritm (KL)= 		  =  0.454 +- 0.010
Faith. Armon (KL)= 		  =  0.431 +- 0.026
Faith. GMean (KL)= 	  =  0.442 +- 0.018
Computed for split load_split = id



Completed in  0:15:21.322721  for CIGAGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color ALL
DONE all :)
