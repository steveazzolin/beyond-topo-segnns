nohup: ignoring input
Time to compute metrics!
The PID of this script is: 959805
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:53:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:53:03 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 103...
[0m[1;37mINFO[0m: [1mCheckpoint 103: 
-----------------------------------
Train ACCURACY: 0.8598
Train Loss: 0.5235
ID Validation ACCURACY: 0.8683
ID Validation Loss: 0.5030
ID Test ACCURACY: 0.8607
ID Test Loss: 0.5282
OOD Validation ACCURACY: 0.8537
OOD Validation Loss: 0.6475
OOD Test ACCURACY: 0.5690
OOD Test Loss: 1.2550

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 163...
[0m[1;37mINFO[0m: [1mCheckpoint 163: 
-----------------------------------
Train ACCURACY: 0.8449
Train Loss: 0.5037
ID Validation ACCURACY: 0.8490
ID Validation Loss: 0.4950
ID Test ACCURACY: 0.8447
ID Test Loss: 0.5158
OOD Validation ACCURACY: 0.8813
OOD Validation Loss: 0.5724
OOD Test ACCURACY: 0.8553
OOD Test Loss: 0.5144

[0m[1;37mINFO[0m: [1mChartInfo 0.8607 0.5690 0.8447 0.8553 0.8490 0.8813[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.389
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.540
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.567
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.567


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.586
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.38900375000000004
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.408
SUFF++ for r=0.3 class 0 = 0.574 +- 0.280 (in-sample avg dev_std = 0.543)
SUFF++ for r=0.3 class 1 = 0.534 +- 0.280 (in-sample avg dev_std = 0.543)
SUFF++ for r=0.3 class 2 = 0.499 +- 0.280 (in-sample avg dev_std = 0.543)
SUFF++ for r=0.3 all KL = 0.557 +- 0.280 (in-sample avg dev_std = 0.543)
SUFF++ for r=0.3 all L1 = 0.536 +- 0.177 (in-sample avg dev_std = 0.543)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.757
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.54038625
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.585
SUFF++ for r=0.6 class 0 = 0.502 +- 0.242 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 class 1 = 0.629 +- 0.242 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 class 2 = 0.626 +- 0.242 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 all KL = 0.642 +- 0.242 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 all L1 = 0.586 +- 0.157 (in-sample avg dev_std = 0.462)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.884
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.56702375
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.817
SUFF++ for r=0.9 class 0 = 0.722 +- 0.112 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.9 class 1 = 0.867 +- 0.112 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.9 class 2 = 0.877 +- 0.112 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.9 all KL = 0.91 +- 0.112 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.9 all L1 = 0.822 +- 0.145 (in-sample avg dev_std = 0.232)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.586
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.38900375000000004
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.347
NEC for r=0.3 class 0 = 0.362 +- 0.333 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 1 = 0.508 +- 0.333 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 2 = 0.527 +- 0.333 (in-sample avg dev_std = 0.400)
NEC for r=0.3 all KL = 0.423 +- 0.333 (in-sample avg dev_std = 0.400)
NEC for r=0.3 all L1 = 0.465 +- 0.244 (in-sample avg dev_std = 0.400)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.757
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.54038625
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.415
NEC for r=0.6 class 0 = 0.511 +- 0.291 (in-sample avg dev_std = 0.519)
NEC for r=0.6 class 1 = 0.53 +- 0.291 (in-sample avg dev_std = 0.519)
NEC for r=0.6 class 2 = 0.546 +- 0.291 (in-sample avg dev_std = 0.519)
NEC for r=0.6 all KL = 0.502 +- 0.291 (in-sample avg dev_std = 0.519)
NEC for r=0.6 all L1 = 0.529 +- 0.177 (in-sample avg dev_std = 0.519)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.884
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.56702375
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.49
NEC for r=0.9 class 0 = 0.472 +- 0.299 (in-sample avg dev_std = 0.545)
NEC for r=0.9 class 1 = 0.498 +- 0.299 (in-sample avg dev_std = 0.545)
NEC for r=0.9 class 2 = 0.576 +- 0.299 (in-sample avg dev_std = 0.545)
NEC for r=0.9 all KL = 0.499 +- 0.299 (in-sample avg dev_std = 0.545)
NEC for r=0.9 all L1 = 0.515 +- 0.179 (in-sample avg dev_std = 0.545)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.882
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.56702375
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.501
NEC for r=1.0 class 0 = 0.466 +- 0.298 (in-sample avg dev_std = 0.555)
NEC for r=1.0 class 1 = 0.491 +- 0.298 (in-sample avg dev_std = 0.555)
NEC for r=1.0 class 2 = 0.552 +- 0.298 (in-sample avg dev_std = 0.555)
NEC for r=1.0 all KL = 0.482 +- 0.298 (in-sample avg dev_std = 0.555)
NEC for r=1.0 all L1 = 0.503 +- 0.173 (in-sample avg dev_std = 0.555)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:54:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:54:21 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 106...
[0m[1;37mINFO[0m: [1mCheckpoint 106: 
-----------------------------------
Train ACCURACY: 0.9087
Train Loss: 0.4218
ID Validation ACCURACY: 0.9113
ID Validation Loss: 0.4062
ID Test ACCURACY: 0.9043
ID Test Loss: 0.4406
OOD Validation ACCURACY: 0.9257
OOD Validation Loss: 0.4153
OOD Test ACCURACY: 0.9043
OOD Test Loss: 0.4070

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 107...
[0m[1;37mINFO[0m: [1mCheckpoint 107: 
-----------------------------------
Train ACCURACY: 0.8882
Train Loss: 0.4551
ID Validation ACCURACY: 0.8927
ID Validation Loss: 0.4382
ID Test ACCURACY: 0.8887
ID Test Loss: 0.4541
OOD Validation ACCURACY: 0.9283
OOD Validation Loss: 0.4551
OOD Test ACCURACY: 0.9143
OOD Test Loss: 0.3807

[0m[1;37mINFO[0m: [1mChartInfo 0.9043 0.9043 0.8887 0.9143 0.8927 0.9283[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.341
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.520
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.615
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.620


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.482
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.34070875
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.41
SUFF++ for r=0.3 class 0 = 0.634 +- 0.286 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.3 class 1 = 0.618 +- 0.286 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.3 class 2 = 0.555 +- 0.286 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.3 all KL = 0.643 +- 0.286 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.3 all L1 = 0.603 +- 0.184 (in-sample avg dev_std = 0.469)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.68
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.52035
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.558
SUFF++ for r=0.6 class 0 = 0.601 +- 0.263 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.6 class 1 = 0.598 +- 0.263 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.6 class 2 = 0.585 +- 0.263 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.6 all KL = 0.648 +- 0.263 (in-sample avg dev_std = 0.469)
SUFF++ for r=0.6 all L1 = 0.595 +- 0.167 (in-sample avg dev_std = 0.469)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.87
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.61521125
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.84
SUFF++ for r=0.9 class 0 = 0.833 +- 0.169 (in-sample avg dev_std = 0.248)
SUFF++ for r=0.9 class 1 = 0.842 +- 0.169 (in-sample avg dev_std = 0.248)
SUFF++ for r=0.9 class 2 = 0.849 +- 0.169 (in-sample avg dev_std = 0.248)
SUFF++ for r=0.9 all KL = 0.884 +- 0.169 (in-sample avg dev_std = 0.248)
SUFF++ for r=0.9 all L1 = 0.841 +- 0.160 (in-sample avg dev_std = 0.248)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.482
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.34070875
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.332
NEC for r=0.3 class 0 = 0.451 +- 0.316 (in-sample avg dev_std = 0.388)
NEC for r=0.3 class 1 = 0.439 +- 0.316 (in-sample avg dev_std = 0.388)
NEC for r=0.3 class 2 = 0.493 +- 0.316 (in-sample avg dev_std = 0.388)
NEC for r=0.3 all KL = 0.408 +- 0.316 (in-sample avg dev_std = 0.388)
NEC for r=0.3 all L1 = 0.461 +- 0.216 (in-sample avg dev_std = 0.388)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.68
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.52035
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.425
NEC for r=0.6 class 0 = 0.521 +- 0.254 (in-sample avg dev_std = 0.507)
NEC for r=0.6 class 1 = 0.532 +- 0.254 (in-sample avg dev_std = 0.507)
NEC for r=0.6 class 2 = 0.566 +- 0.254 (in-sample avg dev_std = 0.507)
NEC for r=0.6 all KL = 0.504 +- 0.254 (in-sample avg dev_std = 0.507)
NEC for r=0.6 all L1 = 0.539 +- 0.143 (in-sample avg dev_std = 0.507)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.87
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.61521125
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.509
NEC for r=0.9 class 0 = 0.528 +- 0.269 (in-sample avg dev_std = 0.569)
NEC for r=0.9 class 1 = 0.529 +- 0.269 (in-sample avg dev_std = 0.569)
NEC for r=0.9 class 2 = 0.553 +- 0.269 (in-sample avg dev_std = 0.569)
NEC for r=0.9 all KL = 0.572 +- 0.269 (in-sample avg dev_std = 0.569)
NEC for r=0.9 all L1 = 0.537 +- 0.153 (in-sample avg dev_std = 0.569)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.923
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.61983875
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.557
NEC for r=1.0 class 0 = 0.496 +- 0.281 (in-sample avg dev_std = 0.574)
NEC for r=1.0 class 1 = 0.507 +- 0.281 (in-sample avg dev_std = 0.574)
NEC for r=1.0 class 2 = 0.53 +- 0.281 (in-sample avg dev_std = 0.574)
NEC for r=1.0 all KL = 0.551 +- 0.281 (in-sample avg dev_std = 0.574)
NEC for r=1.0 all L1 = 0.511 +- 0.155 (in-sample avg dev_std = 0.574)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:55:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:37 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:55:38 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 111...
[0m[1;37mINFO[0m: [1mCheckpoint 111: 
-----------------------------------
Train ACCURACY: 0.8349
Train Loss: 0.5629
ID Validation ACCURACY: 0.8427
ID Validation Loss: 0.5385
ID Test ACCURACY: 0.8290
ID Test Loss: 0.5847
OOD Validation ACCURACY: 0.7613
OOD Validation Loss: 0.7380
OOD Test ACCURACY: 0.7983
OOD Test Loss: 0.5704

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 147...
[0m[1;37mINFO[0m: [1mCheckpoint 147: 
-----------------------------------
Train ACCURACY: 0.8374
Train Loss: 0.5193
ID Validation ACCURACY: 0.8337
ID Validation Loss: 0.5058
ID Test ACCURACY: 0.8260
ID Test Loss: 0.5460
OOD Validation ACCURACY: 0.8300
OOD Validation Loss: 0.6757
OOD Test ACCURACY: 0.8513
OOD Test Loss: 0.4627

[0m[1;37mINFO[0m: [1mChartInfo 0.8290 0.7983 0.8260 0.8513 0.8337 0.8300[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.644
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.757
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.768
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.768


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.616
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.64364875
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.474
SUFF++ for r=0.3 class 0 = 0.47 +- 0.307 (in-sample avg dev_std = 0.555)
SUFF++ for r=0.3 class 1 = 0.718 +- 0.307 (in-sample avg dev_std = 0.555)
SUFF++ for r=0.3 class 2 = 0.538 +- 0.307 (in-sample avg dev_std = 0.555)
SUFF++ for r=0.3 all KL = 0.506 +- 0.307 (in-sample avg dev_std = 0.555)
SUFF++ for r=0.3 all L1 = 0.576 +- 0.211 (in-sample avg dev_std = 0.555)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.8
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.75747625
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.684
SUFF++ for r=0.6 class 0 = 0.518 +- 0.247 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 class 1 = 0.79 +- 0.247 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 class 2 = 0.704 +- 0.247 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 all KL = 0.71 +- 0.247 (in-sample avg dev_std = 0.420)
SUFF++ for r=0.6 all L1 = 0.671 +- 0.192 (in-sample avg dev_std = 0.420)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.841
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.7680162500000001
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.771
SUFF++ for r=0.9 class 0 = 0.609 +- 0.179 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.9 class 1 = 0.858 +- 0.179 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.9 class 2 = 0.858 +- 0.179 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.9 all KL = 0.858 +- 0.179 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.9 all L1 = 0.775 +- 0.197 (in-sample avg dev_std = 0.249)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.616
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.64364875
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.282
NEC for r=0.3 class 0 = 0.521 +- 0.323 (in-sample avg dev_std = 0.473)
NEC for r=0.3 class 1 = 0.567 +- 0.323 (in-sample avg dev_std = 0.473)
NEC for r=0.3 class 2 = 0.598 +- 0.323 (in-sample avg dev_std = 0.473)
NEC for r=0.3 all KL = 0.631 +- 0.323 (in-sample avg dev_std = 0.473)
NEC for r=0.3 all L1 = 0.562 +- 0.206 (in-sample avg dev_std = 0.473)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.8
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.75747625
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.435
NEC for r=0.6 class 0 = 0.519 +- 0.314 (in-sample avg dev_std = 0.505)
NEC for r=0.6 class 1 = 0.482 +- 0.314 (in-sample avg dev_std = 0.505)
NEC for r=0.6 class 2 = 0.552 +- 0.314 (in-sample avg dev_std = 0.505)
NEC for r=0.6 all KL = 0.505 +- 0.314 (in-sample avg dev_std = 0.505)
NEC for r=0.6 all L1 = 0.517 +- 0.178 (in-sample avg dev_std = 0.505)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.841
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.7680162500000001
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.534
NEC for r=0.9 class 0 = 0.437 +- 0.292 (in-sample avg dev_std = 0.497)
NEC for r=0.9 class 1 = 0.419 +- 0.292 (in-sample avg dev_std = 0.497)
NEC for r=0.9 class 2 = 0.504 +- 0.292 (in-sample avg dev_std = 0.497)
NEC for r=0.9 all KL = 0.416 +- 0.292 (in-sample avg dev_std = 0.497)
NEC for r=0.9 all L1 = 0.453 +- 0.171 (in-sample avg dev_std = 0.497)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.853
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.7680162500000001
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.549
NEC for r=1.0 class 0 = 0.408 +- 0.290 (in-sample avg dev_std = 0.472)
NEC for r=1.0 class 1 = 0.435 +- 0.290 (in-sample avg dev_std = 0.472)
NEC for r=1.0 class 2 = 0.482 +- 0.290 (in-sample avg dev_std = 0.472)
NEC for r=1.0 all KL = 0.388 +- 0.290 (in-sample avg dev_std = 0.472)
NEC for r=1.0 all L1 = 0.441 +- 0.173 (in-sample avg dev_std = 0.472)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:56:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:56:56 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 142...
[0m[1;37mINFO[0m: [1mCheckpoint 142: 
-----------------------------------
Train ACCURACY: 0.8396
Train Loss: 0.5054
ID Validation ACCURACY: 0.8567
ID Validation Loss: 0.4677
ID Test ACCURACY: 0.8283
ID Test Loss: 0.5320
OOD Validation ACCURACY: 0.7540
OOD Validation Loss: 0.7147
OOD Test ACCURACY: 0.8567
OOD Test Loss: 0.4681

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 142...
[0m[1;37mINFO[0m: [1mCheckpoint 142: 
-----------------------------------
Train ACCURACY: 0.8396
Train Loss: 0.5054
ID Validation ACCURACY: 0.8567
ID Validation Loss: 0.4677
ID Test ACCURACY: 0.8283
ID Test Loss: 0.5320
OOD Validation ACCURACY: 0.7540
OOD Validation Loss: 0.7147
OOD Test ACCURACY: 0.8567
OOD Test Loss: 0.4681

[0m[1;37mINFO[0m: [1mChartInfo 0.8283 0.8567 0.8283 0.8567 0.8567 0.7540[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.368
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.517
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.579
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.578


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.505
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.36833875
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.444
SUFF++ for r=0.3 class 0 = 0.54 +- 0.259 (in-sample avg dev_std = 0.589)
SUFF++ for r=0.3 class 1 = 0.652 +- 0.259 (in-sample avg dev_std = 0.589)
SUFF++ for r=0.3 class 2 = 0.531 +- 0.259 (in-sample avg dev_std = 0.589)
SUFF++ for r=0.3 all KL = 0.498 +- 0.259 (in-sample avg dev_std = 0.589)
SUFF++ for r=0.3 all L1 = 0.574 +- 0.200 (in-sample avg dev_std = 0.589)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.619
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.5168225000000001
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.538
SUFF++ for r=0.6 class 0 = 0.585 +- 0.291 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.6 class 1 = 0.698 +- 0.291 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.6 class 2 = 0.583 +- 0.291 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.6 all KL = 0.604 +- 0.291 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.6 all L1 = 0.622 +- 0.208 (in-sample avg dev_std = 0.514)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.837
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.5785849999999999
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.781
SUFF++ for r=0.9 class 0 = 0.773 +- 0.168 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.9 class 1 = 0.859 +- 0.168 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.9 class 2 = 0.814 +- 0.168 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.9 all KL = 0.87 +- 0.168 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.9 all L1 = 0.816 +- 0.165 (in-sample avg dev_std = 0.283)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.505
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.36833875
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.345
NEC for r=0.3 class 0 = 0.508 +- 0.310 (in-sample avg dev_std = 0.481)
NEC for r=0.3 class 1 = 0.487 +- 0.310 (in-sample avg dev_std = 0.481)
NEC for r=0.3 class 2 = 0.461 +- 0.310 (in-sample avg dev_std = 0.481)
NEC for r=0.3 all KL = 0.538 +- 0.310 (in-sample avg dev_std = 0.481)
NEC for r=0.3 all L1 = 0.486 +- 0.245 (in-sample avg dev_std = 0.481)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.619
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.5168225000000001
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.445
NEC for r=0.6 class 0 = 0.527 +- 0.291 (in-sample avg dev_std = 0.569)
NEC for r=0.6 class 1 = 0.461 +- 0.291 (in-sample avg dev_std = 0.569)
NEC for r=0.6 class 2 = 0.538 +- 0.291 (in-sample avg dev_std = 0.569)
NEC for r=0.6 all KL = 0.56 +- 0.291 (in-sample avg dev_std = 0.569)
NEC for r=0.6 all L1 = 0.508 +- 0.197 (in-sample avg dev_std = 0.569)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.837
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.5785849999999999
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.53
NEC for r=0.9 class 0 = 0.518 +- 0.306 (in-sample avg dev_std = 0.562)
NEC for r=0.9 class 1 = 0.373 +- 0.306 (in-sample avg dev_std = 0.562)
NEC for r=0.9 class 2 = 0.518 +- 0.306 (in-sample avg dev_std = 0.562)
NEC for r=0.9 all KL = 0.489 +- 0.306 (in-sample avg dev_std = 0.562)
NEC for r=0.9 all L1 = 0.469 +- 0.202 (in-sample avg dev_std = 0.562)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.864
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.5782025000000001
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.543
NEC for r=1.0 class 0 = 0.508 +- 0.311 (in-sample avg dev_std = 0.557)
NEC for r=1.0 class 1 = 0.376 +- 0.311 (in-sample avg dev_std = 0.557)
NEC for r=1.0 class 2 = 0.496 +- 0.311 (in-sample avg dev_std = 0.557)
NEC for r=1.0 all KL = 0.476 +- 0.311 (in-sample avg dev_std = 0.557)
NEC for r=1.0 all L1 = 0.459 +- 0.201 (in-sample avg dev_std = 0.557)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 08:58:30 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 08:58:30 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 183...
[0m[1;37mINFO[0m: [1mCheckpoint 183: 
-----------------------------------
Train ACCURACY: 0.8734
Train Loss: 0.4599
ID Validation ACCURACY: 0.8810
ID Validation Loss: 0.4475
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4786
OOD Validation ACCURACY: 0.8663
OOD Validation Loss: 0.5284
OOD Test ACCURACY: 0.8430
OOD Test Loss: 0.5743

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 187...
[0m[1;37mINFO[0m: [1mCheckpoint 187: 
-----------------------------------
Train ACCURACY: 0.8522
Train Loss: 0.4709
ID Validation ACCURACY: 0.8573
ID Validation Loss: 0.4632
ID Test ACCURACY: 0.8450
ID Test Loss: 0.4949
OOD Validation ACCURACY: 0.9020
OOD Validation Loss: 0.5173
OOD Test ACCURACY: 0.8550
OOD Test Loss: 0.5385

[0m[1;37mINFO[0m: [1mChartInfo 0.8727 0.8430 0.8450 0.8550 0.8573 0.9020[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.376
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.474
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.593
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.621


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.493
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.3756975
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.416
SUFF++ for r=0.3 class 0 = 0.6 +- 0.295 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.3 class 1 = 0.597 +- 0.295 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.3 class 2 = 0.547 +- 0.295 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.3 all KL = 0.552 +- 0.295 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.3 all L1 = 0.581 +- 0.166 (in-sample avg dev_std = 0.562)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.47438625
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.48
SUFF++ for r=0.6 class 0 = 0.644 +- 0.275 (in-sample avg dev_std = 0.500)
SUFF++ for r=0.6 class 1 = 0.586 +- 0.275 (in-sample avg dev_std = 0.500)
SUFF++ for r=0.6 class 2 = 0.638 +- 0.275 (in-sample avg dev_std = 0.500)
SUFF++ for r=0.6 all KL = 0.637 +- 0.275 (in-sample avg dev_std = 0.500)
SUFF++ for r=0.6 all L1 = 0.623 +- 0.184 (in-sample avg dev_std = 0.500)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.73
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.5929337499999999
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.677
SUFF++ for r=0.9 class 0 = 0.724 +- 0.193 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.9 class 1 = 0.737 +- 0.193 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.9 class 2 = 0.765 +- 0.193 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.9 all KL = 0.806 +- 0.193 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.9 all L1 = 0.742 +- 0.161 (in-sample avg dev_std = 0.379)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.493
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.3756975
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.334
NEC for r=0.3 class 0 = 0.446 +- 0.312 (in-sample avg dev_std = 0.461)
NEC for r=0.3 class 1 = 0.461 +- 0.312 (in-sample avg dev_std = 0.461)
NEC for r=0.3 class 2 = 0.525 +- 0.312 (in-sample avg dev_std = 0.461)
NEC for r=0.3 all KL = 0.463 +- 0.312 (in-sample avg dev_std = 0.461)
NEC for r=0.3 all L1 = 0.477 +- 0.190 (in-sample avg dev_std = 0.461)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.47438625
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.418
NEC for r=0.6 class 0 = 0.475 +- 0.281 (in-sample avg dev_std = 0.523)
NEC for r=0.6 class 1 = 0.464 +- 0.281 (in-sample avg dev_std = 0.523)
NEC for r=0.6 class 2 = 0.548 +- 0.281 (in-sample avg dev_std = 0.523)
NEC for r=0.6 all KL = 0.478 +- 0.281 (in-sample avg dev_std = 0.523)
NEC for r=0.6 all L1 = 0.495 +- 0.176 (in-sample avg dev_std = 0.523)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.73
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.5929337499999999
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.473
NEC for r=0.9 class 0 = 0.485 +- 0.253 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 1 = 0.484 +- 0.253 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 2 = 0.507 +- 0.253 (in-sample avg dev_std = 0.499)
NEC for r=0.9 all KL = 0.456 +- 0.253 (in-sample avg dev_std = 0.499)
NEC for r=0.9 all L1 = 0.492 +- 0.162 (in-sample avg dev_std = 0.499)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.899
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.62089375
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.589
NEC for r=1.0 class 0 = 0.477 +- 0.270 (in-sample avg dev_std = 0.545)
NEC for r=1.0 class 1 = 0.392 +- 0.270 (in-sample avg dev_std = 0.545)
NEC for r=1.0 class 2 = 0.522 +- 0.270 (in-sample avg dev_std = 0.545)
NEC for r=1.0 all KL = 0.478 +- 0.270 (in-sample avg dev_std = 0.545)
NEC for r=1.0 all L1 = 0.463 +- 0.176 (in-sample avg dev_std = 0.545)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.557, 0.642, 0.91, 1.0], 'all_L1': [0.536, 0.586, 0.822, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.643, 0.648, 0.884, 1.0], 'all_L1': [0.603, 0.595, 0.841, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.506, 0.71, 0.858, 1.0], 'all_L1': [0.576, 0.671, 0.775, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.498, 0.604, 0.87, 1.0], 'all_L1': [0.574, 0.622, 0.816, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.552, 0.637, 0.806, 1.0], 'all_L1': [0.581, 0.623, 0.742, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.423, 0.502, 0.499, 0.482], 'all_L1': [0.465, 0.529, 0.515, 0.503]}), defaultdict(<class 'list'>, {'all_KL': [0.408, 0.504, 0.572, 0.551], 'all_L1': [0.461, 0.539, 0.537, 0.511]}), defaultdict(<class 'list'>, {'all_KL': [0.631, 0.505, 0.416, 0.388], 'all_L1': [0.562, 0.517, 0.453, 0.441]}), defaultdict(<class 'list'>, {'all_KL': [0.538, 0.56, 0.489, 0.476], 'all_L1': [0.486, 0.508, 0.469, 0.459]}), defaultdict(<class 'list'>, {'all_KL': [0.463, 0.478, 0.456, 0.478], 'all_L1': [0.477, 0.495, 0.492, 0.463]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.574 +- 0.022, 0.619 +- 0.030, 0.799 +- 0.036, 1.000 +- 0.000
suff++ class all_KL  =  0.551 +- 0.052, 0.648 +- 0.034, 0.866 +- 0.034, 1.000 +- 0.000
suff++_acc_int  =  0.430 +- 0.025, 0.569 +- 0.067, 0.777 +- 0.056
nec class all_L1  =  0.490 +- 0.037, 0.518 +- 0.015, 0.493 +- 0.030, 0.475 +- 0.027
nec class all_KL  =  0.493 +- 0.083, 0.510 +- 0.027, 0.486 +- 0.052, 0.475 +- 0.052
nec_acc_int  =  0.328 +- 0.024, 0.428 +- 0.011, 0.507 +- 0.023, 0.548 +- 0.028


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.532 +- 0.022, 0.568 +- 0.013, 0.646 +- 0.029, 0.738 +- 0.013
Faith. Armon (L1)= 		  =  0.528 +- 0.023, 0.563 +- 0.011, 0.610 +- 0.030, 0.644 +- 0.025
Faith. GMean (L1)= 	  =  0.530 +- 0.022, 0.566 +- 0.012, 0.628 +- 0.030, 0.689 +- 0.020
Faith. Aritm (KL)= 		  =  0.522 +- 0.026, 0.579 +- 0.016, 0.676 +- 0.038, 0.738 +- 0.026
Faith. Armon (KL)= 		  =  0.513 +- 0.027, 0.570 +- 0.015, 0.622 +- 0.047, 0.642 +- 0.048
Faith. GMean (KL)= 	  =  0.517 +- 0.026, 0.574 +- 0.016, 0.648 +- 0.042, 0.688 +- 0.038
Computed for split load_split = id



Completed in  0:06:49.523696  for LECIGIN GOODMotif2/basis



DONE LECI GOODMotif2/basis ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:00:08 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:00:08 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 18...
[0m[1;37mINFO[0m: [1mCheckpoint 18: 
-----------------------------------
Train ACCURACY: 0.6871
Train Loss: 0.8504
ID Validation ACCURACY: 0.6993
ID Validation Loss: 0.8351
ID Test ACCURACY: 0.6983
ID Test Loss: 0.8449
OOD Validation ACCURACY: 0.5330
OOD Validation Loss: 1.1042
OOD Test ACCURACY: 0.5190
OOD Test Loss: 1.0268

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.5497
Train Loss: 1.1856
ID Validation ACCURACY: 0.5620
ID Validation Loss: 1.1989
ID Test ACCURACY: 0.5707
ID Test Loss: 1.1955
OOD Validation ACCURACY: 0.6903
OOD Validation Loss: 0.7487
OOD Test ACCURACY: 0.6773
OOD Test Loss: 0.7183

[0m[1;37mINFO[0m: [1mChartInfo 0.6983 0.5190 0.5707 0.6773 0.5620 0.6903[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.052
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.136
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.305
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.352


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.344
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.05201625
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.336
SUFF++ for r=0.3 class 0 = 0.709 +- 0.140 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.3 class 1 = 0.713 +- 0.140 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.3 class 2 = 0.707 +- 0.140 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.3 all KL = 0.855 +- 0.140 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.3 all L1 = 0.71 +- 0.112 (in-sample avg dev_std = 0.254)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.416
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.13597875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.356
SUFF++ for r=0.6 class 0 = 0.739 +- 0.124 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 class 1 = 0.746 +- 0.124 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 class 2 = 0.741 +- 0.124 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 all KL = 0.891 +- 0.124 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 all L1 = 0.742 +- 0.130 (in-sample avg dev_std = 0.215)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.445
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.30544625
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.446
SUFF++ for r=0.9 class 0 = 0.809 +- 0.090 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.9 class 1 = 0.838 +- 0.090 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.9 class 2 = 0.777 +- 0.090 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.9 all KL = 0.93 +- 0.090 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.9 all L1 = 0.808 +- 0.111 (in-sample avg dev_std = 0.172)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.344
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.05201625
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.373
NEC for r=0.3 class 0 = 0.34 +- 0.137 (in-sample avg dev_std = 0.237)
NEC for r=0.3 class 1 = 0.319 +- 0.137 (in-sample avg dev_std = 0.237)
NEC for r=0.3 class 2 = 0.346 +- 0.137 (in-sample avg dev_std = 0.237)
NEC for r=0.3 all KL = 0.173 +- 0.137 (in-sample avg dev_std = 0.237)
NEC for r=0.3 all L1 = 0.335 +- 0.115 (in-sample avg dev_std = 0.237)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.416
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.13597875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.331
NEC for r=0.6 class 0 = 0.344 +- 0.141 (in-sample avg dev_std = 0.203)
NEC for r=0.6 class 1 = 0.288 +- 0.141 (in-sample avg dev_std = 0.203)
NEC for r=0.6 class 2 = 0.336 +- 0.141 (in-sample avg dev_std = 0.203)
NEC for r=0.6 all KL = 0.147 +- 0.141 (in-sample avg dev_std = 0.203)
NEC for r=0.6 all L1 = 0.323 +- 0.140 (in-sample avg dev_std = 0.203)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.445
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.30544625
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.392
NEC for r=0.9 class 0 = 0.323 +- 0.119 (in-sample avg dev_std = 0.209)
NEC for r=0.9 class 1 = 0.272 +- 0.119 (in-sample avg dev_std = 0.209)
NEC for r=0.9 class 2 = 0.345 +- 0.119 (in-sample avg dev_std = 0.209)
NEC for r=0.9 all KL = 0.144 +- 0.119 (in-sample avg dev_std = 0.209)
NEC for r=0.9 all L1 = 0.313 +- 0.120 (in-sample avg dev_std = 0.209)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.72
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.3521075
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.48
NEC for r=1.0 class 0 = 0.316 +- 0.136 (in-sample avg dev_std = 0.208)
NEC for r=1.0 class 1 = 0.243 +- 0.136 (in-sample avg dev_std = 0.208)
NEC for r=1.0 class 2 = 0.355 +- 0.136 (in-sample avg dev_std = 0.208)
NEC for r=1.0 all KL = 0.147 +- 0.136 (in-sample avg dev_std = 0.208)
NEC for r=1.0 all L1 = 0.304 +- 0.134 (in-sample avg dev_std = 0.208)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:01:44 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:01:44 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 70...
[0m[1;37mINFO[0m: [1mCheckpoint 70: 
-----------------------------------
Train ACCURACY: 0.6884
Train Loss: 0.7575
ID Validation ACCURACY: 0.7133
ID Validation Loss: 0.7174
ID Test ACCURACY: 0.6943
ID Test Loss: 0.7583
OOD Validation ACCURACY: 0.5157
OOD Validation Loss: 1.1036
OOD Test ACCURACY: 0.7193
OOD Test Loss: 0.7663

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 75...
[0m[1;37mINFO[0m: [1mCheckpoint 75: 
-----------------------------------
Train ACCURACY: 0.5467
Train Loss: 0.9902
ID Validation ACCURACY: 0.5517
ID Validation Loss: 0.9905
ID Test ACCURACY: 0.5537
ID Test Loss: 0.9969
OOD Validation ACCURACY: 0.7237
OOD Validation Loss: 0.7625
OOD Test ACCURACY: 0.6420
OOD Test Loss: 0.7160

[0m[1;37mINFO[0m: [1mChartInfo 0.6943 0.7193 0.5537 0.6420 0.5517 0.7237[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.038
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.297
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.370
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.371


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.4
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.03828
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.344
SUFF++ for r=0.3 class 0 = 0.465 +- 0.215 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.3 class 1 = 0.488 +- 0.215 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.3 class 2 = 0.547 +- 0.215 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.3 all KL = 0.473 +- 0.215 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.3 all L1 = 0.5 +- 0.142 (in-sample avg dev_std = 0.507)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.488
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.29662875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.412
SUFF++ for r=0.6 class 0 = 0.442 +- 0.205 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.6 class 1 = 0.497 +- 0.205 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.6 class 2 = 0.48 +- 0.205 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.6 all KL = 0.499 +- 0.205 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.6 all L1 = 0.473 +- 0.123 (in-sample avg dev_std = 0.495)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.702
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.37027125
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.648
SUFF++ for r=0.9 class 0 = 0.539 +- 0.194 (in-sample avg dev_std = 0.417)
SUFF++ for r=0.9 class 1 = 0.689 +- 0.194 (in-sample avg dev_std = 0.417)
SUFF++ for r=0.9 class 2 = 0.624 +- 0.194 (in-sample avg dev_std = 0.417)
SUFF++ for r=0.9 all KL = 0.679 +- 0.194 (in-sample avg dev_std = 0.417)
SUFF++ for r=0.9 all L1 = 0.617 +- 0.174 (in-sample avg dev_std = 0.417)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.4
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.03828
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.348
NEC for r=0.3 class 0 = 0.582 +- 0.251 (in-sample avg dev_std = 0.460)
NEC for r=0.3 class 1 = 0.539 +- 0.251 (in-sample avg dev_std = 0.460)
NEC for r=0.3 class 2 = 0.504 +- 0.251 (in-sample avg dev_std = 0.460)
NEC for r=0.3 all KL = 0.574 +- 0.251 (in-sample avg dev_std = 0.460)
NEC for r=0.3 all L1 = 0.542 +- 0.175 (in-sample avg dev_std = 0.460)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.488
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.29662875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.356
NEC for r=0.6 class 0 = 0.567 +- 0.234 (in-sample avg dev_std = 0.490)
NEC for r=0.6 class 1 = 0.55 +- 0.234 (in-sample avg dev_std = 0.490)
NEC for r=0.6 class 2 = 0.559 +- 0.234 (in-sample avg dev_std = 0.490)
NEC for r=0.6 all KL = 0.542 +- 0.234 (in-sample avg dev_std = 0.490)
NEC for r=0.6 all L1 = 0.559 +- 0.127 (in-sample avg dev_std = 0.490)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.702
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.37027125
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.471
NEC for r=0.9 class 0 = 0.544 +- 0.228 (in-sample avg dev_std = 0.478)
NEC for r=0.9 class 1 = 0.568 +- 0.228 (in-sample avg dev_std = 0.478)
NEC for r=0.9 class 2 = 0.485 +- 0.228 (in-sample avg dev_std = 0.478)
NEC for r=0.9 all KL = 0.515 +- 0.228 (in-sample avg dev_std = 0.478)
NEC for r=0.9 all L1 = 0.532 +- 0.135 (in-sample avg dev_std = 0.478)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.724
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.37111625000000004
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.506
NEC for r=1.0 class 0 = 0.511 +- 0.224 (in-sample avg dev_std = 0.458)
NEC for r=1.0 class 1 = 0.551 +- 0.224 (in-sample avg dev_std = 0.458)
NEC for r=1.0 class 2 = 0.457 +- 0.224 (in-sample avg dev_std = 0.458)
NEC for r=1.0 all KL = 0.472 +- 0.224 (in-sample avg dev_std = 0.458)
NEC for r=1.0 all L1 = 0.507 +- 0.136 (in-sample avg dev_std = 0.458)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:03:06 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:03:06 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 26...
[0m[1;37mINFO[0m: [1mCheckpoint 26: 
-----------------------------------
Train ACCURACY: 0.6692
Train Loss: 0.8985
ID Validation ACCURACY: 0.6720
ID Validation Loss: 0.8802
ID Test ACCURACY: 0.6757
ID Test Loss: 0.8848
OOD Validation ACCURACY: 0.3387
OOD Validation Loss: 1.1799
OOD Test ACCURACY: 0.5467
OOD Test Loss: 0.8602

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 100...
[0m[1;37mINFO[0m: [1mCheckpoint 100: 
-----------------------------------
Train ACCURACY: 0.5163
Train Loss: 1.2847
ID Validation ACCURACY: 0.5163
ID Validation Loss: 1.3363
ID Test ACCURACY: 0.5163
ID Test Loss: 1.3361
OOD Validation ACCURACY: 0.6937
OOD Validation Loss: 0.7726
OOD Test ACCURACY: 0.6027
OOD Test Loss: 0.9304

[0m[1;37mINFO[0m: [1mChartInfo 0.6757 0.5467 0.5163 0.6027 0.5163 0.6937[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.000
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.051
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.257
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.313


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.352
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.0
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.341
SUFF++ for r=0.3 class 0 = 0.722 +- 0.148 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.3 class 1 = 0.732 +- 0.148 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.3 class 2 = 0.725 +- 0.148 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.3 all KL = 0.87 +- 0.148 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.3 all L1 = 0.726 +- 0.141 (in-sample avg dev_std = 0.241)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.24
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.05111875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.355
SUFF++ for r=0.6 class 0 = 0.776 +- 0.064 (in-sample avg dev_std = 0.200)
SUFF++ for r=0.6 class 1 = 0.773 +- 0.064 (in-sample avg dev_std = 0.200)
SUFF++ for r=0.6 class 2 = 0.779 +- 0.064 (in-sample avg dev_std = 0.200)
SUFF++ for r=0.6 all KL = 0.923 +- 0.064 (in-sample avg dev_std = 0.200)
SUFF++ for r=0.6 all L1 = 0.776 +- 0.096 (in-sample avg dev_std = 0.200)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.379
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.2572775
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.399
SUFF++ for r=0.9 class 0 = 0.784 +- 0.092 (in-sample avg dev_std = 0.182)
SUFF++ for r=0.9 class 1 = 0.87 +- 0.092 (in-sample avg dev_std = 0.182)
SUFF++ for r=0.9 class 2 = 0.814 +- 0.092 (in-sample avg dev_std = 0.182)
SUFF++ for r=0.9 all KL = 0.935 +- 0.092 (in-sample avg dev_std = 0.182)
SUFF++ for r=0.9 all L1 = 0.823 +- 0.121 (in-sample avg dev_std = 0.182)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.352
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.0
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.36
NEC for r=0.3 class 0 = 0.302 +- 0.127 (in-sample avg dev_std = 0.233)
NEC for r=0.3 class 1 = 0.263 +- 0.127 (in-sample avg dev_std = 0.233)
NEC for r=0.3 class 2 = 0.309 +- 0.127 (in-sample avg dev_std = 0.233)
NEC for r=0.3 all KL = 0.136 +- 0.127 (in-sample avg dev_std = 0.233)
NEC for r=0.3 all L1 = 0.291 +- 0.137 (in-sample avg dev_std = 0.233)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.24
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.05111875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.356
NEC for r=0.6 class 0 = 0.239 +- 0.102 (in-sample avg dev_std = 0.190)
NEC for r=0.6 class 1 = 0.296 +- 0.102 (in-sample avg dev_std = 0.190)
NEC for r=0.6 class 2 = 0.266 +- 0.102 (in-sample avg dev_std = 0.190)
NEC for r=0.6 all KL = 0.107 +- 0.102 (in-sample avg dev_std = 0.190)
NEC for r=0.6 all L1 = 0.267 +- 0.135 (in-sample avg dev_std = 0.190)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.379
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.2572775
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.363
NEC for r=0.9 class 0 = 0.272 +- 0.127 (in-sample avg dev_std = 0.181)
NEC for r=0.9 class 1 = 0.209 +- 0.127 (in-sample avg dev_std = 0.181)
NEC for r=0.9 class 2 = 0.251 +- 0.127 (in-sample avg dev_std = 0.181)
NEC for r=0.9 all KL = 0.1 +- 0.127 (in-sample avg dev_std = 0.181)
NEC for r=0.9 all L1 = 0.244 +- 0.147 (in-sample avg dev_std = 0.181)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.696
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.31263874999999997
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.487
NEC for r=1.0 class 0 = 0.24 +- 0.144 (in-sample avg dev_std = 0.202)
NEC for r=1.0 class 1 = 0.205 +- 0.144 (in-sample avg dev_std = 0.202)
NEC for r=1.0 class 2 = 0.279 +- 0.144 (in-sample avg dev_std = 0.202)
NEC for r=1.0 all KL = 0.108 +- 0.144 (in-sample avg dev_std = 0.202)
NEC for r=1.0 all L1 = 0.241 +- 0.144 (in-sample avg dev_std = 0.202)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:04:32 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:04:32 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.7740
Train Loss: 0.6713
ID Validation ACCURACY: 0.7883
ID Validation Loss: 0.6566
ID Test ACCURACY: 0.7730
ID Test Loss: 0.7012
OOD Validation ACCURACY: 0.4643
OOD Validation Loss: 1.2859
OOD Test ACCURACY: 0.7057
OOD Test Loss: 0.6370

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 68...
[0m[1;37mINFO[0m: [1mCheckpoint 68: 
-----------------------------------
Train ACCURACY: 0.6974
Train Loss: 0.7951
ID Validation ACCURACY: 0.7197
ID Validation Loss: 0.7468
ID Test ACCURACY: 0.7103
ID Test Loss: 0.8074
OOD Validation ACCURACY: 0.6740
OOD Validation Loss: 0.8290
OOD Test ACCURACY: 0.6273
OOD Test Loss: 0.9364

[0m[1;37mINFO[0m: [1mChartInfo 0.7730 0.7057 0.7103 0.6273 0.7197 0.6740[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.035
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.277
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.386
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.392


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.287
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.03517625
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.329
SUFF++ for r=0.3 class 0 = 0.53 +- 0.231 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.3 class 1 = 0.537 +- 0.231 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.3 class 2 = 0.545 +- 0.231 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.3 all KL = 0.569 +- 0.231 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.3 all L1 = 0.537 +- 0.132 (in-sample avg dev_std = 0.477)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.482
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.27737875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.392
SUFF++ for r=0.6 class 0 = 0.471 +- 0.253 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 1 = 0.517 +- 0.253 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 2 = 0.61 +- 0.253 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 all KL = 0.565 +- 0.253 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 all L1 = 0.532 +- 0.159 (in-sample avg dev_std = 0.449)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.7
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.38569875
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.682
SUFF++ for r=0.9 class 0 = 0.628 +- 0.202 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.9 class 1 = 0.713 +- 0.202 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.9 class 2 = 0.855 +- 0.202 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.9 all KL = 0.798 +- 0.202 (in-sample avg dev_std = 0.314)
SUFF++ for r=0.9 all L1 = 0.731 +- 0.199 (in-sample avg dev_std = 0.314)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.287
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.03517625
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.392
NEC for r=0.3 class 0 = 0.547 +- 0.265 (in-sample avg dev_std = 0.447)
NEC for r=0.3 class 1 = 0.523 +- 0.265 (in-sample avg dev_std = 0.447)
NEC for r=0.3 class 2 = 0.522 +- 0.265 (in-sample avg dev_std = 0.447)
NEC for r=0.3 all KL = 0.507 +- 0.265 (in-sample avg dev_std = 0.447)
NEC for r=0.3 all L1 = 0.53 +- 0.163 (in-sample avg dev_std = 0.447)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.482
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.27737875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.35
NEC for r=0.6 class 0 = 0.555 +- 0.263 (in-sample avg dev_std = 0.433)
NEC for r=0.6 class 1 = 0.56 +- 0.263 (in-sample avg dev_std = 0.433)
NEC for r=0.6 class 2 = 0.506 +- 0.263 (in-sample avg dev_std = 0.433)
NEC for r=0.6 all KL = 0.523 +- 0.263 (in-sample avg dev_std = 0.433)
NEC for r=0.6 all L1 = 0.54 +- 0.134 (in-sample avg dev_std = 0.433)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.7
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.38569875
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.499
NEC for r=0.9 class 0 = 0.512 +- 0.266 (in-sample avg dev_std = 0.470)
NEC for r=0.9 class 1 = 0.559 +- 0.266 (in-sample avg dev_std = 0.470)
NEC for r=0.9 class 2 = 0.413 +- 0.266 (in-sample avg dev_std = 0.470)
NEC for r=0.9 all KL = 0.506 +- 0.266 (in-sample avg dev_std = 0.470)
NEC for r=0.9 all L1 = 0.495 +- 0.150 (in-sample avg dev_std = 0.470)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.806
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.39224875
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.573
NEC for r=1.0 class 0 = 0.489 +- 0.250 (in-sample avg dev_std = 0.432)
NEC for r=1.0 class 1 = 0.494 +- 0.250 (in-sample avg dev_std = 0.432)
NEC for r=1.0 class 2 = 0.337 +- 0.250 (in-sample avg dev_std = 0.432)
NEC for r=1.0 all KL = 0.441 +- 0.250 (in-sample avg dev_std = 0.432)
NEC for r=1.0 all L1 = 0.441 +- 0.158 (in-sample avg dev_std = 0.432)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:06:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:06:03 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 24...
[0m[1;37mINFO[0m: [1mCheckpoint 24: 
-----------------------------------
Train ACCURACY: 0.6885
Train Loss: 0.8644
ID Validation ACCURACY: 0.6977
ID Validation Loss: 0.8397
ID Test ACCURACY: 0.6903
ID Test Loss: 0.8483
OOD Validation ACCURACY: 0.5057
OOD Validation Loss: 1.1208
OOD Test ACCURACY: 0.6890
OOD Test Loss: 0.8000

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.4692
Train Loss: 1.2687
ID Validation ACCURACY: 0.4743
ID Validation Loss: 1.2830
ID Test ACCURACY: 0.4663
ID Test Loss: 1.2951
OOD Validation ACCURACY: 0.6260
OOD Validation Loss: 0.8764
OOD Test ACCURACY: 0.3913
OOD Test Loss: 1.4173

[0m[1;37mINFO[0m: [1mChartInfo 0.6903 0.6890 0.4663 0.3913 0.4743 0.6260[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = 0.000
WIoU for r=0.3 = 0.000
F1 for r=0.6 = 0.000
WIoU for r=0.6 = 0.043
F1 for r=0.9 = 0.000
WIoU for r=0.9 = 0.229
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.311


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.366
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.0
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.324
SUFF++ for r=0.3 class 0 = 0.694 +- 0.192 (in-sample avg dev_std = 0.304)
SUFF++ for r=0.3 class 1 = 0.694 +- 0.192 (in-sample avg dev_std = 0.304)
SUFF++ for r=0.3 class 2 = 0.703 +- 0.192 (in-sample avg dev_std = 0.304)
SUFF++ for r=0.3 all KL = 0.825 +- 0.192 (in-sample avg dev_std = 0.304)
SUFF++ for r=0.3 all L1 = 0.697 +- 0.174 (in-sample avg dev_std = 0.304)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.334
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.04292875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.345
SUFF++ for r=0.6 class 0 = 0.815 +- 0.084 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 class 1 = 0.763 +- 0.084 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 class 2 = 0.793 +- 0.084 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 all KL = 0.926 +- 0.084 (in-sample avg dev_std = 0.226)
SUFF++ for r=0.6 all L1 = 0.79 +- 0.105 (in-sample avg dev_std = 0.226)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.365
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.2291875
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.377
SUFF++ for r=0.9 class 0 = 0.861 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.9 class 1 = 0.884 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.9 class 2 = 0.843 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.9 all KL = 0.965 +- 0.043 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.9 all L1 = 0.863 +- 0.072 (in-sample avg dev_std = 0.167)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.366
Model XAI F1 of binarized graphs for r=0.3 =  0.0
Model XAI WIoU of binarized graphs for r=0.3 =  0.0
len(reference) = 800
Effective ratio: 0.312 +- 0.010
Model Accuracy over intervened graphs for r=0.3 =  0.378
NEC for r=0.3 class 0 = 0.349 +- 0.179 (in-sample avg dev_std = 0.269)
NEC for r=0.3 class 1 = 0.311 +- 0.179 (in-sample avg dev_std = 0.269)
NEC for r=0.3 class 2 = 0.354 +- 0.179 (in-sample avg dev_std = 0.269)
NEC for r=0.3 all KL = 0.189 +- 0.179 (in-sample avg dev_std = 0.269)
NEC for r=0.3 all L1 = 0.338 +- 0.167 (in-sample avg dev_std = 0.269)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.334
Model XAI F1 of binarized graphs for r=0.6 =  0.0
Model XAI WIoU of binarized graphs for r=0.6 =  0.04292875
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.334
NEC for r=0.6 class 0 = 0.266 +- 0.116 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 1 = 0.273 +- 0.116 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 2 = 0.263 +- 0.116 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all KL = 0.111 +- 0.116 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all L1 = 0.267 +- 0.146 (in-sample avg dev_std = 0.211)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.365
Model XAI F1 of binarized graphs for r=0.9 =  0.0
Model XAI WIoU of binarized graphs for r=0.9 =  0.2291875
len(reference) = 800
Effective ratio: 0.914 +- 0.011
Model Accuracy over intervened graphs for r=0.9 =  0.344
NEC for r=0.9 class 0 = 0.203 +- 0.102 (in-sample avg dev_std = 0.159)
NEC for r=0.9 class 1 = 0.221 +- 0.102 (in-sample avg dev_std = 0.159)
NEC for r=0.9 class 2 = 0.242 +- 0.102 (in-sample avg dev_std = 0.159)
NEC for r=0.9 all KL = 0.075 +- 0.102 (in-sample avg dev_std = 0.159)
NEC for r=0.9 all L1 = 0.222 +- 0.125 (in-sample avg dev_std = 0.159)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.716
Model XAI F1 of binarized graphs for r=1.0 =  0.0
Model XAI WIoU of binarized graphs for r=1.0 =  0.31083750000000004
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.467
NEC for r=1.0 class 0 = 0.278 +- 0.180 (in-sample avg dev_std = 0.216)
NEC for r=1.0 class 1 = 0.219 +- 0.180 (in-sample avg dev_std = 0.216)
NEC for r=1.0 class 2 = 0.324 +- 0.180 (in-sample avg dev_std = 0.216)
NEC for r=1.0 all KL = 0.138 +- 0.180 (in-sample avg dev_std = 0.216)
NEC for r=1.0 all L1 = 0.273 +- 0.159 (in-sample avg dev_std = 0.216)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.855, 0.891, 0.93, 1.0], 'all_L1': [0.71, 0.742, 0.808, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.473, 0.499, 0.679, 1.0], 'all_L1': [0.5, 0.473, 0.617, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.87, 0.923, 0.935, 1.0], 'all_L1': [0.726, 0.776, 0.823, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.569, 0.565, 0.798, 1.0], 'all_L1': [0.537, 0.532, 0.731, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.825, 0.926, 0.965, 1.0], 'all_L1': [0.697, 0.79, 0.863, 1.0], 0: [1.0], 1: [1.0], 2: [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.173, 0.147, 0.144, 0.147], 'all_L1': [0.335, 0.323, 0.313, 0.304]}), defaultdict(<class 'list'>, {'all_KL': [0.574, 0.542, 0.515, 0.472], 'all_L1': [0.542, 0.559, 0.532, 0.507]}), defaultdict(<class 'list'>, {'all_KL': [0.136, 0.107, 0.1, 0.108], 'all_L1': [0.291, 0.267, 0.244, 0.241]}), defaultdict(<class 'list'>, {'all_KL': [0.507, 0.523, 0.506, 0.441], 'all_L1': [0.53, 0.54, 0.495, 0.441]}), defaultdict(<class 'list'>, {'all_KL': [0.189, 0.111, 0.075, 0.138], 'all_L1': [0.338, 0.267, 0.222, 0.273]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.634 +- 0.095, 0.663 +- 0.133, 0.768 +- 0.087, 1.000 +- 0.000
suff++ class all_KL  =  0.718 +- 0.165, 0.761 +- 0.188, 0.861 +- 0.108, 1.000 +- 0.000
suff++_acc_int  =  0.335 +- 0.008, 0.372 +- 0.026, 0.510 +- 0.129
nec class all_L1  =  0.407 +- 0.107, 0.391 +- 0.131, 0.361 +- 0.128, 0.353 +- 0.103
nec class all_KL  =  0.316 +- 0.185, 0.286 +- 0.202, 0.268 +- 0.199, 0.261 +- 0.160
nec_acc_int  =  0.370 +- 0.015, 0.345 +- 0.011, 0.414 +- 0.061, 0.502 +- 0.037


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.521 +- 0.008, 0.527 +- 0.007, 0.565 +- 0.028, 0.677 +- 0.051
Faith. Armon (L1)= 		  =  0.476 +- 0.044, 0.459 +- 0.057, 0.468 +- 0.097, 0.514 +- 0.110
Faith. GMean (L1)= 	  =  0.497 +- 0.026, 0.491 +- 0.031, 0.513 +- 0.065, 0.588 +- 0.085
Faith. Aritm (KL)= 		  =  0.517 +- 0.013, 0.523 +- 0.010, 0.565 +- 0.052, 0.631 +- 0.080
Faith. Armon (KL)= 		  =  0.377 +- 0.125, 0.341 +- 0.157, 0.355 +- 0.206, 0.389 +- 0.195
Faith. GMean (KL)= 	  =  0.436 +- 0.078, 0.412 +- 0.099, 0.434 +- 0.151, 0.487 +- 0.155
Computed for split load_split = id



Completed in  0:07:29.901276  for GSATGIN GOODMotif2/basis



DONE GSAT GOODMotif2/basis ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:07:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0.01
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:07:52 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 178...
[0m[1;37mINFO[0m: [1mCheckpoint 178: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8721
ID Validation Loss: 0.5525
ID Test ACCURACY: 0.8738
ID Test Loss: 0.6177
OOD Validation ACCURACY: 0.8730
OOD Validation Loss: 0.7070
OOD Test ACCURACY: 0.8114
OOD Test Loss: 1.1250

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 182...
[0m[1;37mINFO[0m: [1mCheckpoint 182: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8698
ID Validation Loss: 0.5684
ID Test ACCURACY: 0.8749
ID Test Loss: 0.6247
OOD Validation ACCURACY: 0.8748
OOD Validation Loss: 0.7016
OOD Test ACCURACY: 0.8157
OOD Test Loss: 1.0466

[0m[1;37mINFO[0m: [1mChartInfo 0.8738 0.8114 0.8749 0.8157 0.8698 0.8748[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:07:53 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.86
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.855
SUFF++ for r=0.6 class 0.0 = 0.922 +- 0.146 (in-sample avg dev_std = 0.177)
SUFF++ for r=0.6 class 1.0 = 0.962 +- 0.146 (in-sample avg dev_std = 0.177)
SUFF++ for r=0.6 all KL = 0.947 +- 0.146 (in-sample avg dev_std = 0.177)
SUFF++ for r=0.6 all L1 = 0.945 +- 0.115 (in-sample avg dev_std = 0.177)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.872
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.877
SUFF++ for r=0.9 class 0.0 = 0.948 +- 0.127 (in-sample avg dev_std = 0.138)
SUFF++ for r=0.9 class 1.0 = 0.947 +- 0.127 (in-sample avg dev_std = 0.138)
SUFF++ for r=0.9 all KL = 0.96 +- 0.127 (in-sample avg dev_std = 0.138)
SUFF++ for r=0.9 all L1 = 0.947 +- 0.133 (in-sample avg dev_std = 0.138)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.86
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.856
NEC for r=0.6 class 0.0 = 0.09 +- 0.163 (in-sample avg dev_std = 0.132)
NEC for r=0.6 class 1.0 = 0.05 +- 0.163 (in-sample avg dev_std = 0.132)
NEC for r=0.6 all KL = 0.058 +- 0.163 (in-sample avg dev_std = 0.132)
NEC for r=0.6 all L1 = 0.067 +- 0.146 (in-sample avg dev_std = 0.132)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.877
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.875
NEC for r=0.9 class 0.0 = 0.051 +- 0.118 (in-sample avg dev_std = 0.105)
NEC for r=0.9 class 1.0 = 0.038 +- 0.118 (in-sample avg dev_std = 0.105)
NEC for r=0.9 all KL = 0.034 +- 0.118 (in-sample avg dev_std = 0.105)
NEC for r=0.9 all L1 = 0.044 +- 0.124 (in-sample avg dev_std = 0.105)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.874
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.868
NEC for r=1.0 class 0.0 = 0.041 +- 0.100 (in-sample avg dev_std = 0.081)
NEC for r=1.0 class 1.0 = 0.034 +- 0.100 (in-sample avg dev_std = 0.081)
NEC for r=1.0 all KL = 0.029 +- 0.100 (in-sample avg dev_std = 0.081)
NEC for r=1.0 all L1 = 0.037 +- 0.109 (in-sample avg dev_std = 0.081)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:08:13 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0.01
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 105...
[0m[1;37mINFO[0m: [1mCheckpoint 105: 
-----------------------------------
Train ACCURACY: 0.9485
Train Loss: 0.0766
ID Validation ACCURACY: 0.8749
ID Validation Loss: 0.5049
ID Test ACCURACY: 0.8668
ID Test Loss: 0.5615
OOD Validation ACCURACY: 0.8653
OOD Validation Loss: 0.6072
OOD Test ACCURACY: 0.7872
OOD Test Loss: 0.9617

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 145...
[0m[1;37mINFO[0m: [1mCheckpoint 145: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0757
ID Validation ACCURACY: 0.8723
ID Validation Loss: 0.5268
ID Test ACCURACY: 0.8719
ID Test Loss: 0.6128
OOD Validation ACCURACY: 0.8778
OOD Validation Loss: 0.6058
OOD Test ACCURACY: 0.8173
OOD Test Loss: 0.8048

[0m[1;37mINFO[0m: [1mChartInfo 0.8668 0.7872 0.8719 0.8173 0.8723 0.8778[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:08:14 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.881
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.87
SUFF++ for r=0.6 class 0.0 = 0.928 +- 0.136 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.6 class 1.0 = 0.949 +- 0.136 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.6 all KL = 0.946 +- 0.136 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.6 all L1 = 0.94 +- 0.107 (in-sample avg dev_std = 0.168)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.883
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.872
SUFF++ for r=0.9 class 0.0 = 0.936 +- 0.111 (in-sample avg dev_std = 0.089)
SUFF++ for r=0.9 class 1.0 = 0.952 +- 0.111 (in-sample avg dev_std = 0.089)
SUFF++ for r=0.9 all KL = 0.964 +- 0.111 (in-sample avg dev_std = 0.089)
SUFF++ for r=0.9 all L1 = 0.945 +- 0.135 (in-sample avg dev_std = 0.089)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.881
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.871
NEC for r=0.6 class 0.0 = 0.091 +- 0.159 (in-sample avg dev_std = 0.108)
NEC for r=0.6 class 1.0 = 0.061 +- 0.159 (in-sample avg dev_std = 0.108)
NEC for r=0.6 all KL = 0.063 +- 0.159 (in-sample avg dev_std = 0.108)
NEC for r=0.6 all L1 = 0.074 +- 0.132 (in-sample avg dev_std = 0.108)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.884
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.873
NEC for r=0.9 class 0.0 = 0.066 +- 0.115 (in-sample avg dev_std = 0.086)
NEC for r=0.9 class 1.0 = 0.039 +- 0.115 (in-sample avg dev_std = 0.086)
NEC for r=0.9 all KL = 0.034 +- 0.115 (in-sample avg dev_std = 0.086)
NEC for r=0.9 all L1 = 0.05 +- 0.130 (in-sample avg dev_std = 0.086)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.891
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.874
NEC for r=1.0 class 0.0 = 0.059 +- 0.106 (in-sample avg dev_std = 0.073)
NEC for r=1.0 class 1.0 = 0.033 +- 0.106 (in-sample avg dev_std = 0.073)
NEC for r=1.0 all KL = 0.028 +- 0.106 (in-sample avg dev_std = 0.073)
NEC for r=1.0 all L1 = 0.044 +- 0.122 (in-sample avg dev_std = 0.073)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:08:32 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0.01
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:33 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 155...
[0m[1;37mINFO[0m: [1mCheckpoint 155: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8755
ID Validation Loss: 0.5193
ID Test ACCURACY: 0.8710
ID Test Loss: 0.6143
OOD Validation ACCURACY: 0.8741
OOD Validation Loss: 0.6579
OOD Test ACCURACY: 0.7991
OOD Test Loss: 1.0166

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 144...
[0m[1;37mINFO[0m: [1mCheckpoint 144: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0757
ID Validation ACCURACY: 0.8713
ID Validation Loss: 0.4845
ID Test ACCURACY: 0.8713
ID Test Loss: 0.5534
OOD Validation ACCURACY: 0.8785
OOD Validation Loss: 0.6028
OOD Test ACCURACY: 0.8218
OOD Test Loss: 0.7450

[0m[1;37mINFO[0m: [1mChartInfo 0.8710 0.7991 0.8713 0.8218 0.8713 0.8785[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:08:34 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.888
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.877
SUFF++ for r=0.6 class 0.0 = 0.94 +- 0.149 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.6 class 1.0 = 0.965 +- 0.149 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.6 all KL = 0.947 +- 0.149 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.6 all L1 = 0.954 +- 0.090 (in-sample avg dev_std = 0.172)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.894
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.902
SUFF++ for r=0.9 class 0.0 = 0.935 +- 0.132 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 class 1.0 = 0.977 +- 0.132 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 all KL = 0.966 +- 0.132 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 all L1 = 0.958 +- 0.130 (in-sample avg dev_std = 0.113)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.888
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.891
NEC for r=0.6 class 0.0 = 0.066 +- 0.147 (in-sample avg dev_std = 0.093)
NEC for r=0.6 class 1.0 = 0.046 +- 0.147 (in-sample avg dev_std = 0.093)
NEC for r=0.6 all KL = 0.052 +- 0.147 (in-sample avg dev_std = 0.093)
NEC for r=0.6 all L1 = 0.055 +- 0.110 (in-sample avg dev_std = 0.093)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.891
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.891
NEC for r=0.9 class 0.0 = 0.045 +- 0.105 (in-sample avg dev_std = 0.092)
NEC for r=0.9 class 1.0 = 0.025 +- 0.105 (in-sample avg dev_std = 0.092)
NEC for r=0.9 all KL = 0.025 +- 0.105 (in-sample avg dev_std = 0.092)
NEC for r=0.9 all L1 = 0.033 +- 0.110 (in-sample avg dev_std = 0.092)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.898
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.887
NEC for r=1.0 class 0.0 = 0.039 +- 0.101 (in-sample avg dev_std = 0.090)
NEC for r=1.0 class 1.0 = 0.025 +- 0.101 (in-sample avg dev_std = 0.090)
NEC for r=1.0 all KL = 0.023 +- 0.101 (in-sample avg dev_std = 0.090)
NEC for r=1.0 all L1 = 0.031 +- 0.102 (in-sample avg dev_std = 0.090)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:08:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0.01
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:08:53 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 114...
[0m[1;37mINFO[0m: [1mCheckpoint 114: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0765
ID Validation ACCURACY: 0.8764
ID Validation Loss: 0.4270
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4723
OOD Validation ACCURACY: 0.8786
OOD Validation Loss: 0.5089
OOD Test ACCURACY: 0.8355
OOD Test Loss: 0.5859

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 114...
[0m[1;37mINFO[0m: [1mCheckpoint 114: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0765
ID Validation ACCURACY: 0.8764
ID Validation Loss: 0.4270
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4723
OOD Validation ACCURACY: 0.8786
OOD Validation Loss: 0.5089
OOD Test ACCURACY: 0.8355
OOD Test Loss: 0.5859

[0m[1;37mINFO[0m: [1mChartInfo 0.8727 0.8355 0.8727 0.8355 0.8764 0.8786[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:08:53 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.86
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.858
SUFF++ for r=0.6 class 0.0 = 0.926 +- 0.091 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.6 class 1.0 = 0.956 +- 0.091 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.6 all KL = 0.963 +- 0.091 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.6 all L1 = 0.943 +- 0.098 (in-sample avg dev_std = 0.156)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.906
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.885
SUFF++ for r=0.9 class 0.0 = 0.946 +- 0.074 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.9 class 1.0 = 0.96 +- 0.074 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.9 all KL = 0.976 +- 0.074 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.9 all L1 = 0.954 +- 0.111 (in-sample avg dev_std = 0.079)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.86
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.861
NEC for r=0.6 class 0.0 = 0.092 +- 0.114 (in-sample avg dev_std = 0.076)
NEC for r=0.6 class 1.0 = 0.046 +- 0.114 (in-sample avg dev_std = 0.076)
NEC for r=0.6 all KL = 0.043 +- 0.114 (in-sample avg dev_std = 0.076)
NEC for r=0.6 all L1 = 0.065 +- 0.115 (in-sample avg dev_std = 0.076)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.895
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.881
NEC for r=0.9 class 0.0 = 0.045 +- 0.095 (in-sample avg dev_std = 0.082)
NEC for r=0.9 class 1.0 = 0.038 +- 0.095 (in-sample avg dev_std = 0.082)
NEC for r=0.9 all KL = 0.026 +- 0.095 (in-sample avg dev_std = 0.082)
NEC for r=0.9 all L1 = 0.041 +- 0.107 (in-sample avg dev_std = 0.082)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.895
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.885
NEC for r=1.0 class 0.0 = 0.047 +- 0.105 (in-sample avg dev_std = 0.077)
NEC for r=1.0 class 1.0 = 0.031 +- 0.105 (in-sample avg dev_std = 0.077)
NEC for r=1.0 all KL = 0.026 +- 0.105 (in-sample avg dev_std = 0.077)
NEC for r=1.0 all L1 = 0.038 +- 0.109 (in-sample avg dev_std = 0.077)
model_dirname= repr_LECIGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:09:13 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 0.01
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 106...
[0m[1;37mINFO[0m: [1mCheckpoint 106: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0764
ID Validation ACCURACY: 0.8762
ID Validation Loss: 0.4746
ID Test ACCURACY: 0.8687
ID Test Loss: 0.5282
OOD Validation ACCURACY: 0.8716
OOD Validation Loss: 0.5888
OOD Test ACCURACY: 0.8079
OOD Test Loss: 0.8214

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 128...
[0m[1;37mINFO[0m: [1mCheckpoint 128: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0760
ID Validation ACCURACY: 0.8732
ID Validation Loss: 0.4817
ID Test ACCURACY: 0.8685
ID Test Loss: 0.5451
OOD Validation ACCURACY: 0.8787
OOD Validation Loss: 0.5826
OOD Test ACCURACY: 0.8277
OOD Test Loss: 0.6909

[0m[1;37mINFO[0m: [1mChartInfo 0.8687 0.8079 0.8685 0.8277 0.8732 0.8787[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:09:14 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.853
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.847
SUFF++ for r=0.6 class 0.0 = 0.925 +- 0.104 (in-sample avg dev_std = 0.151)
SUFF++ for r=0.6 class 1.0 = 0.952 +- 0.104 (in-sample avg dev_std = 0.151)
SUFF++ for r=0.6 all KL = 0.957 +- 0.104 (in-sample avg dev_std = 0.151)
SUFF++ for r=0.6 all L1 = 0.941 +- 0.108 (in-sample avg dev_std = 0.151)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.856
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.843
SUFF++ for r=0.9 class 0.0 = 0.922 +- 0.132 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 class 1.0 = 0.959 +- 0.132 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 all KL = 0.958 +- 0.132 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 all L1 = 0.943 +- 0.138 (in-sample avg dev_std = 0.113)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.853
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.854
NEC for r=0.6 class 0.0 = 0.093 +- 0.128 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 1.0 = 0.055 +- 0.128 (in-sample avg dev_std = 0.118)
NEC for r=0.6 all KL = 0.052 +- 0.128 (in-sample avg dev_std = 0.118)
NEC for r=0.6 all L1 = 0.071 +- 0.129 (in-sample avg dev_std = 0.118)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.86
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.855
NEC for r=0.9 class 0.0 = 0.056 +- 0.116 (in-sample avg dev_std = 0.098)
NEC for r=0.9 class 1.0 = 0.04 +- 0.116 (in-sample avg dev_std = 0.098)
NEC for r=0.9 all KL = 0.033 +- 0.116 (in-sample avg dev_std = 0.098)
NEC for r=0.9 all L1 = 0.047 +- 0.124 (in-sample avg dev_std = 0.098)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.863
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.852
NEC for r=1.0 class 0.0 = 0.05 +- 0.123 (in-sample avg dev_std = 0.088)
NEC for r=1.0 class 1.0 = 0.038 +- 0.123 (in-sample avg dev_std = 0.088)
NEC for r=1.0 all KL = 0.034 +- 0.123 (in-sample avg dev_std = 0.088)
NEC for r=1.0 all L1 = 0.043 +- 0.120 (in-sample avg dev_std = 0.088)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.947, 0.96, 1.0], 'all_L1': [0.945, 0.947, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.946, 0.964, 1.0], 'all_L1': [0.94, 0.945, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.947, 0.966, 1.0], 'all_L1': [0.954, 0.958, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.963, 0.976, 1.0], 'all_L1': [0.943, 0.954, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.957, 0.958, 1.0], 'all_L1': [0.941, 0.943, 1.0], 0.0: [1.0], 1.0: [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.058, 0.034, 0.029], 'all_L1': [0.067, 0.044, 0.037]}), defaultdict(<class 'list'>, {'all_KL': [0.063, 0.034, 0.028], 'all_L1': [0.074, 0.05, 0.044]}), defaultdict(<class 'list'>, {'all_KL': [0.052, 0.025, 0.023], 'all_L1': [0.055, 0.033, 0.031]}), defaultdict(<class 'list'>, {'all_KL': [0.043, 0.026, 0.026], 'all_L1': [0.065, 0.041, 0.038]}), defaultdict(<class 'list'>, {'all_KL': [0.052, 0.033, 0.034], 'all_L1': [0.071, 0.047, 0.043]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.945 +- 0.005, 0.949 +- 0.006, 1.000 +- 0.000
suff++ class all_KL  =  0.952 +- 0.007, 0.965 +- 0.006, 1.000 +- 0.000
suff++_acc_int  =  0.862 +- 0.011, 0.876 +- 0.019
nec class all_L1  =  0.066 +- 0.006, 0.043 +- 0.006, 0.039 +- 0.005
nec class all_KL  =  0.054 +- 0.007, 0.030 +- 0.004, 0.028 +- 0.004
nec_acc_int  =  0.866 +- 0.014, 0.875 +- 0.012, 0.873 +- 0.013


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.505 +- 0.001, 0.496 +- 0.001, 0.519 +- 0.002
Faith. Armon (L1)= 		  =  0.124 +- 0.011, 0.082 +- 0.011, 0.074 +- 0.009
Faith. GMean (L1)= 	  =  0.250 +- 0.012, 0.202 +- 0.014, 0.196 +- 0.012
Faith. Aritm (KL)= 		  =  0.503 +- 0.002, 0.498 +- 0.002, 0.514 +- 0.002
Faith. Armon (KL)= 		  =  0.101 +- 0.012, 0.059 +- 0.008, 0.054 +- 0.007
Faith. GMean (KL)= 	  =  0.225 +- 0.014, 0.171 +- 0.011, 0.167 +- 0.011
Computed for split load_split = id



Completed in  0:01:42.391166  for LECIGIN GOODSST2/length



DONE LECI GOODSST2/length ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:09:46 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:09:47 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 197...
[0m[1;37mINFO[0m: [1mCheckpoint 197: 
-----------------------------------
Train ACCURACY: 0.8946
Train Loss: 0.2294
ID Validation ACCURACY: 0.8451
ID Validation Loss: 0.4596
ID Test ACCURACY: 0.8359
ID Test Loss: 0.4928
OOD Validation ACCURACY: 0.8442
OOD Validation Loss: 0.7055
OOD Test ACCURACY: 0.7909
OOD Test Loss: 1.2081

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 150...
[0m[1;37mINFO[0m: [1mCheckpoint 150: 
-----------------------------------
Train ACCURACY: 0.8613
Train Loss: 0.3029
ID Validation ACCURACY: 0.8268
ID Validation Loss: 0.4327
ID Test ACCURACY: 0.8231
ID Test Loss: 0.4451
OOD Validation ACCURACY: 0.8502
OOD Validation Loss: 0.5097
OOD Test ACCURACY: 0.8121
OOD Test Loss: 0.6943

[0m[1;37mINFO[0m: [1mChartInfo 0.8359 0.7909 0.8231 0.8121 0.8268 0.8502[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:09:48 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.849
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.844
SUFF++ for r=0.6 class 0.0 = 0.948 +- 0.100 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.6 class 1.0 = 0.957 +- 0.100 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.6 all KL = 0.965 +- 0.100 (in-sample avg dev_std = 0.148)
SUFF++ for r=0.6 all L1 = 0.953 +- 0.101 (in-sample avg dev_std = 0.148)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.861
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.84
SUFF++ for r=0.9 class 0.0 = 0.91 +- 0.135 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.9 class 1.0 = 0.948 +- 0.135 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.9 all KL = 0.952 +- 0.135 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.9 all L1 = 0.931 +- 0.142 (in-sample avg dev_std = 0.132)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.849
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.86
NEC for r=0.6 class 0.0 = 0.082 +- 0.120 (in-sample avg dev_std = 0.110)
NEC for r=0.6 class 1.0 = 0.052 +- 0.120 (in-sample avg dev_std = 0.110)
NEC for r=0.6 all KL = 0.045 +- 0.120 (in-sample avg dev_std = 0.110)
NEC for r=0.6 all L1 = 0.065 +- 0.129 (in-sample avg dev_std = 0.110)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.87
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.845
NEC for r=0.9 class 0.0 = 0.092 +- 0.132 (in-sample avg dev_std = 0.139)
NEC for r=0.9 class 1.0 = 0.042 +- 0.132 (in-sample avg dev_std = 0.139)
NEC for r=0.9 all KL = 0.045 +- 0.132 (in-sample avg dev_std = 0.139)
NEC for r=0.9 all L1 = 0.063 +- 0.139 (in-sample avg dev_std = 0.139)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.87
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.844
NEC for r=1.0 class 0.0 = 0.109 +- 0.148 (in-sample avg dev_std = 0.136)
NEC for r=1.0 class 1.0 = 0.041 +- 0.148 (in-sample avg dev_std = 0.136)
NEC for r=1.0 all KL = 0.053 +- 0.148 (in-sample avg dev_std = 0.136)
NEC for r=1.0 all L1 = 0.07 +- 0.159 (in-sample avg dev_std = 0.136)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:10:09 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 191...
[0m[1;37mINFO[0m: [1mCheckpoint 191: 
-----------------------------------
Train ACCURACY: 0.9069
Train Loss: 0.1959
ID Validation ACCURACY: 0.8449
ID Validation Loss: 0.5034
ID Test ACCURACY: 0.8380
ID Test Loss: 0.5283
OOD Validation ACCURACY: 0.8458
OOD Validation Loss: 0.8693
OOD Test ACCURACY: 0.8026
OOD Test Loss: 1.3095

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 35...
[0m[1;37mINFO[0m: [1mCheckpoint 35: 
-----------------------------------
Train ACCURACY: 0.8494
Train Loss: 0.3303
ID Validation ACCURACY: 0.8246
ID Validation Loss: 0.3965
ID Test ACCURACY: 0.8274
ID Test Loss: 0.4033
OOD Validation ACCURACY: 0.8494
OOD Validation Loss: 0.3884
OOD Test ACCURACY: 0.8215
OOD Test Loss: 0.4617

[0m[1;37mINFO[0m: [1mChartInfo 0.8380 0.8026 0.8274 0.8215 0.8246 0.8494[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:10:10 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.842
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.837
SUFF++ for r=0.6 class 0.0 = 0.94 +- 0.143 (in-sample avg dev_std = 0.188)
SUFF++ for r=0.6 class 1.0 = 0.951 +- 0.143 (in-sample avg dev_std = 0.188)
SUFF++ for r=0.6 all KL = 0.943 +- 0.143 (in-sample avg dev_std = 0.188)
SUFF++ for r=0.6 all L1 = 0.946 +- 0.112 (in-sample avg dev_std = 0.188)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.844
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.873
SUFF++ for r=0.9 class 0.0 = 0.934 +- 0.110 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 class 1.0 = 0.95 +- 0.110 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 all KL = 0.961 +- 0.110 (in-sample avg dev_std = 0.113)
SUFF++ for r=0.9 all L1 = 0.943 +- 0.139 (in-sample avg dev_std = 0.113)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.842
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.852
NEC for r=0.6 class 0.0 = 0.073 +- 0.146 (in-sample avg dev_std = 0.157)
NEC for r=0.6 class 1.0 = 0.051 +- 0.146 (in-sample avg dev_std = 0.157)
NEC for r=0.6 all KL = 0.055 +- 0.146 (in-sample avg dev_std = 0.157)
NEC for r=0.6 all L1 = 0.06 +- 0.134 (in-sample avg dev_std = 0.157)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.853
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.871
NEC for r=0.9 class 0.0 = 0.072 +- 0.119 (in-sample avg dev_std = 0.124)
NEC for r=0.9 class 1.0 = 0.047 +- 0.119 (in-sample avg dev_std = 0.124)
NEC for r=0.9 all KL = 0.039 +- 0.119 (in-sample avg dev_std = 0.124)
NEC for r=0.9 all L1 = 0.058 +- 0.140 (in-sample avg dev_std = 0.124)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.863
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.875
NEC for r=1.0 class 0.0 = 0.083 +- 0.130 (in-sample avg dev_std = 0.128)
NEC for r=1.0 class 1.0 = 0.047 +- 0.130 (in-sample avg dev_std = 0.128)
NEC for r=1.0 all KL = 0.044 +- 0.130 (in-sample avg dev_std = 0.128)
NEC for r=1.0 all L1 = 0.062 +- 0.150 (in-sample avg dev_std = 0.128)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:10:34 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 192...
[0m[1;37mINFO[0m: [1mCheckpoint 192: 
-----------------------------------
Train ACCURACY: 0.9232
Train Loss: 0.1496
ID Validation ACCURACY: 0.8449
ID Validation Loss: 0.5913
ID Test ACCURACY: 0.8447
ID Test Loss: 0.5983
OOD Validation ACCURACY: 0.8434
OOD Validation Loss: 0.9268
OOD Test ACCURACY: 0.7956
OOD Test Loss: 1.1860

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 58...
[0m[1;37mINFO[0m: [1mCheckpoint 58: 
-----------------------------------
Train ACCURACY: 0.8865
Train Loss: 0.2502
ID Validation ACCURACY: 0.8387
ID Validation Loss: 0.4508
ID Test ACCURACY: 0.8359
ID Test Loss: 0.4682
OOD Validation ACCURACY: 0.8523
OOD Validation Loss: 0.5568
OOD Test ACCURACY: 0.8184
OOD Test Loss: 0.7482

[0m[1;37mINFO[0m: [1mChartInfo 0.8447 0.7956 0.8359 0.8184 0.8387 0.8523[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:10:35 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.856
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.856
SUFF++ for r=0.6 class 0.0 = 0.947 +- 0.145 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.6 class 1.0 = 0.953 +- 0.145 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.6 all KL = 0.951 +- 0.145 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.6 all L1 = 0.951 +- 0.121 (in-sample avg dev_std = 0.194)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.878
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.873
SUFF++ for r=0.9 class 0.0 = 0.931 +- 0.144 (in-sample avg dev_std = 0.171)
SUFF++ for r=0.9 class 1.0 = 0.951 +- 0.144 (in-sample avg dev_std = 0.171)
SUFF++ for r=0.9 all KL = 0.949 +- 0.144 (in-sample avg dev_std = 0.171)
SUFF++ for r=0.9 all L1 = 0.942 +- 0.132 (in-sample avg dev_std = 0.171)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.856
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.866
NEC for r=0.6 class 0.0 = 0.087 +- 0.189 (in-sample avg dev_std = 0.202)
NEC for r=0.6 class 1.0 = 0.063 +- 0.189 (in-sample avg dev_std = 0.202)
NEC for r=0.6 all KL = 0.072 +- 0.189 (in-sample avg dev_std = 0.202)
NEC for r=0.6 all L1 = 0.073 +- 0.169 (in-sample avg dev_std = 0.202)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.874
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.867
NEC for r=0.9 class 0.0 = 0.075 +- 0.135 (in-sample avg dev_std = 0.145)
NEC for r=0.9 class 1.0 = 0.04 +- 0.135 (in-sample avg dev_std = 0.145)
NEC for r=0.9 all KL = 0.046 +- 0.135 (in-sample avg dev_std = 0.145)
NEC for r=0.9 all L1 = 0.055 +- 0.139 (in-sample avg dev_std = 0.145)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.867
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.866
NEC for r=1.0 class 0.0 = 0.082 +- 0.139 (in-sample avg dev_std = 0.145)
NEC for r=1.0 class 1.0 = 0.037 +- 0.139 (in-sample avg dev_std = 0.145)
NEC for r=1.0 all KL = 0.046 +- 0.139 (in-sample avg dev_std = 0.145)
NEC for r=1.0 all L1 = 0.056 +- 0.145 (in-sample avg dev_std = 0.145)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:10:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:10:59 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:11:00 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 194...
[0m[1;37mINFO[0m: [1mCheckpoint 194: 
-----------------------------------
Train ACCURACY: 0.8962
Train Loss: 0.2153
ID Validation ACCURACY: 0.8427
ID Validation Loss: 0.4803
ID Test ACCURACY: 0.8374
ID Test Loss: 0.4933
OOD Validation ACCURACY: 0.8439
OOD Validation Loss: 0.7998
OOD Test ACCURACY: 0.7991
OOD Test Loss: 1.0971

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 82...
[0m[1;37mINFO[0m: [1mCheckpoint 82: 
-----------------------------------
Train ACCURACY: 0.8763
Train Loss: 0.2680
ID Validation ACCURACY: 0.8308
ID Validation Loss: 0.4440
ID Test ACCURACY: 0.8295
ID Test Loss: 0.4615
OOD Validation ACCURACY: 0.8541
OOD Validation Loss: 0.6125
OOD Test ACCURACY: 0.8222
OOD Test Loss: 0.7056

[0m[1;37mINFO[0m: [1mChartInfo 0.8374 0.7991 0.8295 0.8222 0.8308 0.8541[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:11:00 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.849
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.849
SUFF++ for r=0.6 class 0.0 = 0.919 +- 0.151 (in-sample avg dev_std = 0.192)
SUFF++ for r=0.6 class 1.0 = 0.954 +- 0.151 (in-sample avg dev_std = 0.192)
SUFF++ for r=0.6 all KL = 0.943 +- 0.151 (in-sample avg dev_std = 0.192)
SUFF++ for r=0.6 all L1 = 0.94 +- 0.119 (in-sample avg dev_std = 0.192)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.867
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.863
SUFF++ for r=0.9 class 0.0 = 0.925 +- 0.127 (in-sample avg dev_std = 0.139)
SUFF++ for r=0.9 class 1.0 = 0.935 +- 0.127 (in-sample avg dev_std = 0.139)
SUFF++ for r=0.9 all KL = 0.955 +- 0.127 (in-sample avg dev_std = 0.139)
SUFF++ for r=0.9 all L1 = 0.93 +- 0.146 (in-sample avg dev_std = 0.139)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.849
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.842
NEC for r=0.6 class 0.0 = 0.092 +- 0.154 (in-sample avg dev_std = 0.153)
NEC for r=0.6 class 1.0 = 0.055 +- 0.154 (in-sample avg dev_std = 0.153)
NEC for r=0.6 all KL = 0.056 +- 0.154 (in-sample avg dev_std = 0.153)
NEC for r=0.6 all L1 = 0.07 +- 0.147 (in-sample avg dev_std = 0.153)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.867
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.859
NEC for r=0.9 class 0.0 = 0.071 +- 0.114 (in-sample avg dev_std = 0.128)
NEC for r=0.9 class 1.0 = 0.061 +- 0.114 (in-sample avg dev_std = 0.128)
NEC for r=0.9 all KL = 0.04 +- 0.114 (in-sample avg dev_std = 0.128)
NEC for r=0.9 all L1 = 0.065 +- 0.141 (in-sample avg dev_std = 0.128)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.867
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.861
NEC for r=1.0 class 0.0 = 0.07 +- 0.109 (in-sample avg dev_std = 0.117)
NEC for r=1.0 class 1.0 = 0.05 +- 0.109 (in-sample avg dev_std = 0.117)
NEC for r=1.0 all KL = 0.037 +- 0.109 (in-sample avg dev_std = 0.117)
NEC for r=1.0 all L1 = 0.059 +- 0.136 (in-sample avg dev_std = 0.117)
model_dirname= repr_GSATGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:11:20 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 191...
[0m[1;37mINFO[0m: [1mCheckpoint 191: 
-----------------------------------
Train ACCURACY: 0.9018
Train Loss: 0.2011
ID Validation ACCURACY: 0.8468
ID Validation Loss: 0.5144
ID Test ACCURACY: 0.8415
ID Test Loss: 0.5419
OOD Validation ACCURACY: 0.8424
OOD Validation Loss: 0.9462
OOD Test ACCURACY: 0.7918
OOD Test Loss: 1.4711

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.8617
Train Loss: 0.3062
ID Validation ACCURACY: 0.8221
ID Validation Loss: 0.4222
ID Test ACCURACY: 0.8200
ID Test Loss: 0.4485
OOD Validation ACCURACY: 0.8539
OOD Validation Loss: 0.4468
OOD Test ACCURACY: 0.8237
OOD Test Loss: 0.5712

[0m[1;37mINFO[0m: [1mChartInfo 0.8415 0.7918 0.8200 0.8237 0.8221 0.8539[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/23/2024 09:11:22 AM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.832
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.84
SUFF++ for r=0.6 class 0.0 = 0.921 +- 0.171 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 class 1.0 = 0.931 +- 0.171 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 all KL = 0.926 +- 0.171 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 all L1 = 0.927 +- 0.144 (in-sample avg dev_std = 0.206)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.844
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 180
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.837
SUFF++ for r=0.9 class 0.0 = 0.916 +- 0.138 (in-sample avg dev_std = 0.169)
SUFF++ for r=0.9 class 1.0 = 0.92 +- 0.138 (in-sample avg dev_std = 0.169)
SUFF++ for r=0.9 all KL = 0.938 +- 0.138 (in-sample avg dev_std = 0.169)
SUFF++ for r=0.9 all L1 = 0.918 +- 0.162 (in-sample avg dev_std = 0.169)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.832
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 285
Effective ratio: 0.647 +- 0.269
Model Accuracy over intervened graphs for r=0.6 =  0.829
NEC for r=0.6 class 0.0 = 0.092 +- 0.172 (in-sample avg dev_std = 0.178)
NEC for r=0.6 class 1.0 = 0.08 +- 0.172 (in-sample avg dev_std = 0.178)
NEC for r=0.6 all KL = 0.077 +- 0.172 (in-sample avg dev_std = 0.178)
NEC for r=0.6 all L1 = 0.085 +- 0.160 (in-sample avg dev_std = 0.178)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.839
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 285
Effective ratio: 0.862 +- 0.317
Model Accuracy over intervened graphs for r=0.9 =  0.836
NEC for r=0.9 class 0.0 = 0.095 +- 0.143 (in-sample avg dev_std = 0.163)
NEC for r=0.9 class 1.0 = 0.064 +- 0.143 (in-sample avg dev_std = 0.163)
NEC for r=0.9 all KL = 0.059 +- 0.143 (in-sample avg dev_std = 0.163)
NEC for r=0.9 all L1 = 0.077 +- 0.162 (in-sample avg dev_std = 0.163)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.849
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 285
Effective ratio: 0.882 +- 0.322
Model Accuracy over intervened graphs for r=1.0 =  0.843
NEC for r=1.0 class 0.0 = 0.087 +- 0.136 (in-sample avg dev_std = 0.136)
NEC for r=1.0 class 1.0 = 0.051 +- 0.136 (in-sample avg dev_std = 0.136)
NEC for r=1.0 all KL = 0.049 +- 0.136 (in-sample avg dev_std = 0.136)
NEC for r=1.0 all L1 = 0.066 +- 0.151 (in-sample avg dev_std = 0.136)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.965, 0.952, 1.0], 'all_L1': [0.953, 0.931, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.943, 0.961, 1.0], 'all_L1': [0.946, 0.943, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.951, 0.949, 1.0], 'all_L1': [0.951, 0.942, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.943, 0.955, 1.0], 'all_L1': [0.94, 0.93, 1.0], 0.0: [1.0], 1.0: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.926, 0.938, 1.0], 'all_L1': [0.927, 0.918, 1.0], 0.0: [1.0], 1.0: [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.045, 0.045, 0.053], 'all_L1': [0.065, 0.063, 0.07]}), defaultdict(<class 'list'>, {'all_KL': [0.055, 0.039, 0.044], 'all_L1': [0.06, 0.058, 0.062]}), defaultdict(<class 'list'>, {'all_KL': [0.072, 0.046, 0.046], 'all_L1': [0.073, 0.055, 0.056]}), defaultdict(<class 'list'>, {'all_KL': [0.056, 0.04, 0.037], 'all_L1': [0.07, 0.065, 0.059]}), defaultdict(<class 'list'>, {'all_KL': [0.077, 0.059, 0.049], 'all_L1': [0.085, 0.077, 0.066]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.943 +- 0.009, 0.933 +- 0.009, 1.000 +- 0.000
suff++ class all_KL  =  0.946 +- 0.013, 0.951 +- 0.008, 1.000 +- 0.000
suff++_acc_int  =  0.845 +- 0.007, 0.857 +- 0.016
nec class all_L1  =  0.071 +- 0.008, 0.064 +- 0.008, 0.063 +- 0.005
nec class all_KL  =  0.061 +- 0.012, 0.046 +- 0.007, 0.046 +- 0.005
nec_acc_int  =  0.850 +- 0.013, 0.856 +- 0.013, 0.858 +- 0.012


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.507 +- 0.003, 0.498 +- 0.001, 0.531 +- 0.002
Faith. Armon (L1)= 		  =  0.131 +- 0.014, 0.119 +- 0.013, 0.118 +- 0.009
Faith. GMean (L1)= 	  =  0.258 +- 0.014, 0.243 +- 0.013, 0.250 +- 0.010
Faith. Aritm (KL)= 		  =  0.503 +- 0.005, 0.498 +- 0.001, 0.523 +- 0.003
Faith. Armon (KL)= 		  =  0.114 +- 0.021, 0.087 +- 0.013, 0.088 +- 0.010
Faith. GMean (KL)= 	  =  0.239 +- 0.022, 0.208 +- 0.015, 0.214 +- 0.013
Computed for split load_split = id



Completed in  0:01:59.248820  for GSATGIN GOODSST2/length



DONE GSAT GOODSST2/length ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:12:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 1
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:12:01 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 160...
[0m[1;37mINFO[0m: [1mCheckpoint 160: 
-----------------------------------
Train ACCURACY: 0.9989
Train Loss: 0.0081
ID Validation ACCURACY: 0.8799
ID Validation Loss: 0.4755
ID Test ACCURACY: 0.8819
ID Test Loss: 0.4854
OOD Validation ACCURACY: 0.8617
OOD Validation Loss: 0.5669
OOD Test ACCURACY: 0.7476
OOD Test Loss: 1.1971

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 81...
[0m[1;37mINFO[0m: [1mCheckpoint 81: 
-----------------------------------
Train ACCURACY: 0.9611
Train Loss: 0.1229
ID Validation ACCURACY: 0.8749
ID Validation Loss: 0.4002
ID Test ACCURACY: 0.8791
ID Test Loss: 0.3989
OOD Validation ACCURACY: 0.8756
OOD Validation Loss: 0.4110
OOD Test ACCURACY: 0.7587
OOD Test Loss: 0.8224

[0m[1;37mINFO[0m: [1mChartInfo 0.8819 0.7476 0.8791 0.7587 0.8749 0.8756[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.097
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.105
SUFF++ for r=0.3 class 0 = 0.783 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 1 = 0.71 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 2 = 0.729 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 3 = 0.783 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 4 = 0.767 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 5 = 0.782 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 6 = 0.754 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 7 = 0.794 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 8 = 0.87 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 class 9 = 0.772 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 all KL = 0.855 +- 0.176 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.3 all L1 = 0.773 +- 0.222 (in-sample avg dev_std = 0.174)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.162
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.139
SUFF++ for r=0.6 class 0 = 0.445 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 1 = 0.485 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 2 = 0.486 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 3 = 0.501 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 4 = 0.514 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 5 = 0.514 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 6 = 0.491 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 7 = 0.469 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 8 = 0.535 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 class 9 = 0.445 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 all KL = 0.476 +- 0.320 (in-sample avg dev_std = 0.197)
SUFF++ for r=0.6 all L1 = 0.488 +- 0.243 (in-sample avg dev_std = 0.197)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.775
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.717
SUFF++ for r=0.9 class 0 = 0.933 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 1 = 0.904 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 2 = 0.748 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 3 = 0.707 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 4 = 0.789 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 5 = 0.692 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 6 = 0.724 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 7 = 0.764 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 8 = 0.822 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 class 9 = 0.719 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 all KL = 0.776 +- 0.289 (in-sample avg dev_std = 0.315)
SUFF++ for r=0.9 all L1 = 0.783 +- 0.238 (in-sample avg dev_std = 0.315)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.097
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.107
NEC for r=0.3 class 0 = 0.225 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 1 = 0.291 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 2 = 0.272 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 3 = 0.225 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 4 = 0.25 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 5 = 0.223 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 6 = 0.261 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 7 = 0.217 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 8 = 0.142 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 class 9 = 0.235 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 all KL = 0.155 +- 0.192 (in-sample avg dev_std = 0.151)
NEC for r=0.3 all L1 = 0.235 +- 0.227 (in-sample avg dev_std = 0.151)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.162
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.139
NEC for r=0.6 class 0 = 0.573 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 1 = 0.525 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 2 = 0.536 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 3 = 0.523 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 4 = 0.499 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 5 = 0.491 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 6 = 0.514 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 7 = 0.547 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 8 = 0.477 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 9 = 0.567 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 all KL = 0.578 +- 0.277 (in-sample avg dev_std = 0.405)
NEC for r=0.6 all L1 = 0.526 +- 0.202 (in-sample avg dev_std = 0.405)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.775
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.54
NEC for r=0.9 class 0 = 0.218 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 1 = 0.268 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 2 = 0.559 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 3 = 0.561 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 4 = 0.459 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 5 = 0.647 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 6 = 0.556 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 7 = 0.463 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 8 = 0.339 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 class 9 = 0.589 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 all KL = 0.663 +- 0.342 (in-sample avg dev_std = 0.504)
NEC for r=0.9 all L1 = 0.461 +- 0.281 (in-sample avg dev_std = 0.504)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.942
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.796
NEC for r=1.0 class 0 = 0.089 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 1 = 0.054 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 2 = 0.319 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 3 = 0.391 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 4 = 0.241 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 5 = 0.421 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 6 = 0.399 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 7 = 0.268 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 8 = 0.204 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 class 9 = 0.408 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 all KL = 0.476 +- 0.387 (in-sample avg dev_std = 0.451)
NEC for r=1.0 all L1 = 0.275 +- 0.275 (in-sample avg dev_std = 0.451)
model_dirname= repr_LECIGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:18:40 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 1
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:18:41 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 178...
[0m[1;37mINFO[0m: [1mCheckpoint 178: 
-----------------------------------
Train ACCURACY: 0.9985
Train Loss: 0.0098
ID Validation ACCURACY: 0.8837
ID Validation Loss: 0.5031
ID Test ACCURACY: 0.8831
ID Test Loss: 0.4930
OOD Validation ACCURACY: 0.8623
OOD Validation Loss: 0.5645
OOD Test ACCURACY: 0.6190
OOD Test Loss: 2.3743

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.9724
Train Loss: 0.0901
ID Validation ACCURACY: 0.8793
ID Validation Loss: 0.4008
ID Test ACCURACY: 0.8846
ID Test Loss: 0.3936
OOD Validation ACCURACY: 0.8716
OOD Validation Loss: 0.4173
OOD Test ACCURACY: 0.7290
OOD Test Loss: 0.9761

[0m[1;37mINFO[0m: [1mChartInfo 0.8831 0.6190 0.8846 0.7290 0.8793 0.8716[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.124
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.116
SUFF++ for r=0.3 class 0 = 0.652 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 1 = 0.639 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 2 = 0.646 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 3 = 0.653 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 4 = 0.627 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 5 = 0.636 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 6 = 0.646 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 7 = 0.642 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 8 = 0.694 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 class 9 = 0.655 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 all KL = 0.775 +- 0.197 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.3 all L1 = 0.649 +- 0.184 (in-sample avg dev_std = 0.229)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.145
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.137
SUFF++ for r=0.6 class 0 = 0.417 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 1 = 0.404 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 2 = 0.439 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 3 = 0.493 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 4 = 0.453 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 5 = 0.477 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 6 = 0.465 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 7 = 0.407 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 8 = 0.507 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 class 9 = 0.424 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 all KL = 0.404 +- 0.323 (in-sample avg dev_std = 0.228)
SUFF++ for r=0.6 all L1 = 0.447 +- 0.241 (in-sample avg dev_std = 0.228)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.8
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.719
SUFF++ for r=0.9 class 0 = 0.927 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 1 = 0.85 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 2 = 0.754 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 3 = 0.692 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 4 = 0.742 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 5 = 0.676 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 6 = 0.712 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 7 = 0.766 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 8 = 0.789 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 class 9 = 0.769 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 all KL = 0.753 +- 0.295 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.9 all L1 = 0.77 +- 0.234 (in-sample avg dev_std = 0.334)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.124
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.114
NEC for r=0.3 class 0 = 0.394 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 1 = 0.439 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 2 = 0.424 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 3 = 0.439 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 4 = 0.459 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 5 = 0.428 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 6 = 0.466 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 7 = 0.434 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 8 = 0.371 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 class 9 = 0.414 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 all KL = 0.338 +- 0.254 (in-sample avg dev_std = 0.301)
NEC for r=0.3 all L1 = 0.427 +- 0.195 (in-sample avg dev_std = 0.301)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.145
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.124
NEC for r=0.6 class 0 = 0.607 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 1 = 0.582 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 2 = 0.589 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 3 = 0.551 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 4 = 0.548 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 5 = 0.601 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 6 = 0.527 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 7 = 0.567 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 8 = 0.569 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 class 9 = 0.595 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 all KL = 0.674 +- 0.245 (in-sample avg dev_std = 0.480)
NEC for r=0.6 all L1 = 0.573 +- 0.180 (in-sample avg dev_std = 0.480)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.8
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.533
NEC for r=0.9 class 0 = 0.312 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 1 = 0.459 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 2 = 0.496 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 3 = 0.588 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 4 = 0.504 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 5 = 0.602 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 6 = 0.567 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 7 = 0.466 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 8 = 0.433 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 class 9 = 0.549 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 all KL = 0.718 +- 0.298 (in-sample avg dev_std = 0.536)
NEC for r=0.9 all L1 = 0.496 +- 0.260 (in-sample avg dev_std = 0.536)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.956
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.806
NEC for r=1.0 class 0 = 0.099 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 1 = 0.102 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 2 = 0.298 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 3 = 0.366 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 4 = 0.228 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 5 = 0.429 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 6 = 0.382 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 7 = 0.248 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 8 = 0.22 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 class 9 = 0.358 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 all KL = 0.468 +- 0.380 (in-sample avg dev_std = 0.461)
NEC for r=1.0 all L1 = 0.268 +- 0.266 (in-sample avg dev_std = 0.461)
model_dirname= repr_LECIGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:25:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:22 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:22 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:22 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 1
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:25:23 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 92...
[0m[1;37mINFO[0m: [1mCheckpoint 92: 
-----------------------------------
Train ACCURACY: 0.9771
Train Loss: 0.0788
ID Validation ACCURACY: 0.8796
ID Validation Loss: 0.4118
ID Test ACCURACY: 0.8799
ID Test Loss: 0.4099
OOD Validation ACCURACY: 0.8694
OOD Validation Loss: 0.4406
OOD Test ACCURACY: 0.7794
OOD Test Loss: 0.8262

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 151...
[0m[1;37mINFO[0m: [1mCheckpoint 151: 
-----------------------------------
Train ACCURACY: 0.9966
Train Loss: 0.0192
ID Validation ACCURACY: 0.8714
ID Validation Loss: 0.4738
ID Test ACCURACY: 0.8740
ID Test Loss: 0.4719
OOD Validation ACCURACY: 0.8739
OOD Validation Loss: 0.4821
OOD Test ACCURACY: 0.8027
OOD Test Loss: 0.8199

[0m[1;37mINFO[0m: [1mChartInfo 0.8799 0.7794 0.8740 0.8027 0.8714 0.8739[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.111
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.111
SUFF++ for r=0.3 class 0 = 0.829 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 1 = 0.787 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 2 = 0.786 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 3 = 0.831 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 4 = 0.805 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 5 = 0.806 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 6 = 0.781 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 7 = 0.832 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 8 = 0.878 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 class 9 = 0.792 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 all KL = 0.895 +- 0.144 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.3 all L1 = 0.813 +- 0.189 (in-sample avg dev_std = 0.147)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.18
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.14
SUFF++ for r=0.6 class 0 = 0.443 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 1 = 0.468 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 2 = 0.487 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 3 = 0.553 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 4 = 0.513 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 5 = 0.524 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 6 = 0.539 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 7 = 0.489 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 8 = 0.583 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 class 9 = 0.521 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 all KL = 0.594 +- 0.250 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.6 all L1 = 0.511 +- 0.182 (in-sample avg dev_std = 0.149)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.775
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.729
SUFF++ for r=0.9 class 0 = 0.944 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 1 = 0.928 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 2 = 0.751 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 3 = 0.725 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 4 = 0.782 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 5 = 0.709 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 6 = 0.788 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 7 = 0.78 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 8 = 0.804 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 class 9 = 0.751 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 all KL = 0.837 +- 0.213 (in-sample avg dev_std = 0.244)
SUFF++ for r=0.9 all L1 = 0.799 +- 0.199 (in-sample avg dev_std = 0.244)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.111
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.111
NEC for r=0.3 class 0 = 0.19 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 1 = 0.206 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 2 = 0.219 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 3 = 0.172 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 4 = 0.191 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 5 = 0.199 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 6 = 0.229 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 7 = 0.182 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 8 = 0.127 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 class 9 = 0.205 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 all KL = 0.107 +- 0.159 (in-sample avg dev_std = 0.111)
NEC for r=0.3 all L1 = 0.192 +- 0.200 (in-sample avg dev_std = 0.111)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.18
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.148
NEC for r=0.6 class 0 = 0.564 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 1 = 0.533 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 2 = 0.504 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 3 = 0.51 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 4 = 0.506 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 5 = 0.483 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 6 = 0.487 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 7 = 0.534 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 8 = 0.449 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 class 9 = 0.528 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 all KL = 0.459 +- 0.206 (in-sample avg dev_std = 0.326)
NEC for r=0.6 all L1 = 0.511 +- 0.142 (in-sample avg dev_std = 0.326)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.775
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.587
NEC for r=0.9 class 0 = 0.253 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 1 = 0.21 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 2 = 0.527 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 3 = 0.515 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 4 = 0.467 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 5 = 0.595 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 6 = 0.471 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 7 = 0.445 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 8 = 0.402 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 class 9 = 0.526 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 all KL = 0.56 +- 0.296 (in-sample avg dev_std = 0.432)
NEC for r=0.9 all L1 = 0.436 +- 0.243 (in-sample avg dev_std = 0.432)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.939
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.806
NEC for r=1.0 class 0 = 0.117 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 1 = 0.039 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 2 = 0.325 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 3 = 0.339 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 4 = 0.28 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 5 = 0.417 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 6 = 0.325 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 7 = 0.295 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 8 = 0.277 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 class 9 = 0.378 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 all KL = 0.388 +- 0.318 (in-sample avg dev_std = 0.393)
NEC for r=1.0 all L1 = 0.274 +- 0.245 (in-sample avg dev_std = 0.393)
model_dirname= repr_LECIGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:32:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 1
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:32:20 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 169...
[0m[1;37mINFO[0m: [1mCheckpoint 169: 
-----------------------------------
Train ACCURACY: 0.9992
Train Loss: 0.0075
ID Validation ACCURACY: 0.8839
ID Validation Loss: 0.4781
ID Test ACCURACY: 0.8827
ID Test Loss: 0.5041
OOD Validation ACCURACY: 0.8734
OOD Validation Loss: 0.5221
OOD Test ACCURACY: 0.7557
OOD Test Loss: 1.1829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 110...
[0m[1;37mINFO[0m: [1mCheckpoint 110: 
-----------------------------------
Train ACCURACY: 0.9862
Train Loss: 0.0531
ID Validation ACCURACY: 0.8741
ID Validation Loss: 0.4244
ID Test ACCURACY: 0.8776
ID Test Loss: 0.4244
OOD Validation ACCURACY: 0.8810
OOD Validation Loss: 0.4194
OOD Test ACCURACY: 0.7921
OOD Test Loss: 0.8078

[0m[1;37mINFO[0m: [1mChartInfo 0.8827 0.7557 0.8776 0.7921 0.8741 0.8810[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.11
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.11
SUFF++ for r=0.3 class 0 = 0.687 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 1 = 0.683 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 2 = 0.674 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 3 = 0.67 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 4 = 0.676 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 5 = 0.692 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 6 = 0.649 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 7 = 0.653 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 8 = 0.773 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 class 9 = 0.674 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 all KL = 0.806 +- 0.166 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.3 all L1 = 0.682 +- 0.205 (in-sample avg dev_std = 0.227)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.201
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.176
SUFF++ for r=0.6 class 0 = 0.521 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 1 = 0.439 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 2 = 0.525 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 3 = 0.499 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 4 = 0.52 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 5 = 0.469 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 6 = 0.478 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 7 = 0.465 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 8 = 0.516 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 class 9 = 0.452 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 all KL = 0.508 +- 0.286 (in-sample avg dev_std = 0.170)
SUFF++ for r=0.6 all L1 = 0.488 +- 0.207 (in-sample avg dev_std = 0.170)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.777
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.728
SUFF++ for r=0.9 class 0 = 0.906 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 1 = 0.949 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 2 = 0.785 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 3 = 0.664 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 4 = 0.769 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 5 = 0.715 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 6 = 0.73 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 7 = 0.786 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 8 = 0.865 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 class 9 = 0.745 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 all KL = 0.789 +- 0.283 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.9 all L1 = 0.794 +- 0.232 (in-sample avg dev_std = 0.303)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.11
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.112
NEC for r=0.3 class 0 = 0.304 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 1 = 0.3 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 2 = 0.304 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 3 = 0.336 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 4 = 0.311 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 5 = 0.301 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 6 = 0.357 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 7 = 0.34 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 8 = 0.239 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 class 9 = 0.322 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 all KL = 0.191 +- 0.171 (in-sample avg dev_std = 0.179)
NEC for r=0.3 all L1 = 0.312 +- 0.198 (in-sample avg dev_std = 0.179)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.201
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.194
NEC for r=0.6 class 0 = 0.441 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 1 = 0.519 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 2 = 0.492 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 3 = 0.508 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 4 = 0.501 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 5 = 0.506 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 6 = 0.506 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 7 = 0.53 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 8 = 0.486 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 9 = 0.528 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 all KL = 0.504 +- 0.228 (in-sample avg dev_std = 0.353)
NEC for r=0.6 all L1 = 0.502 +- 0.161 (in-sample avg dev_std = 0.353)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.777
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.568
NEC for r=0.9 class 0 = 0.292 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 1 = 0.149 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 2 = 0.456 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 3 = 0.617 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 4 = 0.513 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 5 = 0.582 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 6 = 0.606 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 7 = 0.464 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 8 = 0.32 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 class 9 = 0.503 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 all KL = 0.635 +- 0.337 (in-sample avg dev_std = 0.486)
NEC for r=0.9 all L1 = 0.445 +- 0.274 (in-sample avg dev_std = 0.486)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.944
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.803
NEC for r=1.0 class 0 = 0.115 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 1 = 0.02 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 2 = 0.255 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 3 = 0.455 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 4 = 0.277 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 5 = 0.418 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 6 = 0.359 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 7 = 0.235 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 8 = 0.179 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 class 9 = 0.381 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 all KL = 0.463 +- 0.378 (in-sample avg dev_std = 0.455)
NEC for r=1.0 all L1 = 0.264 +- 0.263 (in-sample avg dev_std = 0.455)
model_dirname= repr_LECIGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:39:06 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:06 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:06 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0mUsing feature sampling := feat
self.EF = 1
mitigation_readout =  None
[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:39:07 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 107...
[0m[1;37mINFO[0m: [1mCheckpoint 107: 
-----------------------------------
Train ACCURACY: 0.9837
Train Loss: 0.0597
ID Validation ACCURACY: 0.8830
ID Validation Loss: 0.4132
ID Test ACCURACY: 0.8796
ID Test Loss: 0.4055
OOD Validation ACCURACY: 0.8721
OOD Validation Loss: 0.4230
OOD Test ACCURACY: 0.7870
OOD Test Loss: 0.8251

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 102...
[0m[1;37mINFO[0m: [1mCheckpoint 102: 
-----------------------------------
Train ACCURACY: 0.9769
Train Loss: 0.0782
ID Validation ACCURACY: 0.8811
ID Validation Loss: 0.4010
ID Test ACCURACY: 0.8799
ID Test Loss: 0.4049
OOD Validation ACCURACY: 0.8813
OOD Validation Loss: 0.4007
OOD Test ACCURACY: 0.7771
OOD Test Loss: 0.8287

[0m[1;37mINFO[0m: [1mChartInfo 0.8796 0.7870 0.8799 0.7771 0.8811 0.8813[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.117
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.114
SUFF++ for r=0.3 class 0 = 0.666 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 1 = 0.708 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 2 = 0.638 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 3 = 0.688 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 4 = 0.645 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 5 = 0.681 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 6 = 0.678 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 7 = 0.663 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 8 = 0.729 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 class 9 = 0.685 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 all KL = 0.836 +- 0.121 (in-sample avg dev_std = 0.223)
SUFF++ for r=0.3 all L1 = 0.678 +- 0.140 (in-sample avg dev_std = 0.223)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.174
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.151
SUFF++ for r=0.6 class 0 = 0.522 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 1 = 0.491 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 2 = 0.487 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 3 = 0.514 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 4 = 0.525 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 5 = 0.508 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 6 = 0.498 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 7 = 0.487 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 8 = 0.559 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 class 9 = 0.519 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 all KL = 0.595 +- 0.245 (in-sample avg dev_std = 0.155)
SUFF++ for r=0.6 all L1 = 0.51 +- 0.173 (in-sample avg dev_std = 0.155)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.786
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.708
SUFF++ for r=0.9 class 0 = 0.891 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 1 = 0.896 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 2 = 0.783 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 3 = 0.675 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 4 = 0.749 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 5 = 0.647 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 6 = 0.779 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 7 = 0.744 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 8 = 0.871 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 class 9 = 0.746 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 all KL = 0.811 +- 0.239 (in-sample avg dev_std = 0.287)
SUFF++ for r=0.9 all L1 = 0.781 +- 0.214 (in-sample avg dev_std = 0.287)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.117
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.115
NEC for r=0.3 class 0 = 0.311 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 1 = 0.254 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 2 = 0.328 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 3 = 0.277 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 4 = 0.302 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 5 = 0.299 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 6 = 0.329 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 7 = 0.307 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 8 = 0.237 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 class 9 = 0.3 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 all KL = 0.145 +- 0.145 (in-sample avg dev_std = 0.164)
NEC for r=0.3 all L1 = 0.294 +- 0.167 (in-sample avg dev_std = 0.164)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.174
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.158
NEC for r=0.6 class 0 = 0.503 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 1 = 0.482 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 2 = 0.496 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 3 = 0.494 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 4 = 0.473 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 5 = 0.482 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 6 = 0.484 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 7 = 0.514 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 8 = 0.484 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 class 9 = 0.491 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 all KL = 0.419 +- 0.200 (in-sample avg dev_std = 0.293)
NEC for r=0.6 all L1 = 0.491 +- 0.129 (in-sample avg dev_std = 0.293)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.786
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.53
NEC for r=0.9 class 0 = 0.42 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 1 = 0.245 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 2 = 0.434 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 3 = 0.557 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 4 = 0.546 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 5 = 0.656 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 6 = 0.541 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 7 = 0.547 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 8 = 0.333 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 class 9 = 0.518 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 all KL = 0.619 +- 0.296 (in-sample avg dev_std = 0.450)
NEC for r=0.9 all L1 = 0.475 +- 0.250 (in-sample avg dev_std = 0.450)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.941
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.796
NEC for r=1.0 class 0 = 0.225 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 1 = 0.061 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 2 = 0.224 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 3 = 0.448 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 4 = 0.294 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 5 = 0.459 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 6 = 0.311 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 7 = 0.332 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 8 = 0.2 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 class 9 = 0.321 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 all KL = 0.428 +- 0.326 (in-sample avg dev_std = 0.413)
NEC for r=1.0 all L1 = 0.283 +- 0.252 (in-sample avg dev_std = 0.413)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.855, 0.476, 0.776, 1.0], 'all_L1': [0.773, 0.488, 0.783, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.775, 0.404, 0.753, 1.0], 'all_L1': [0.649, 0.447, 0.77, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.895, 0.594, 0.837, 1.0], 'all_L1': [0.813, 0.511, 0.799, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.806, 0.508, 0.789, 1.0], 'all_L1': [0.682, 0.488, 0.794, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.836, 0.595, 0.811, 1.0], 'all_L1': [0.678, 0.51, 0.781, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.155, 0.578, 0.663, 0.476], 'all_L1': [0.235, 0.526, 0.461, 0.275]}), defaultdict(<class 'list'>, {'all_KL': [0.338, 0.674, 0.718, 0.468], 'all_L1': [0.427, 0.573, 0.496, 0.268]}), defaultdict(<class 'list'>, {'all_KL': [0.107, 0.459, 0.56, 0.388], 'all_L1': [0.192, 0.511, 0.436, 0.274]}), defaultdict(<class 'list'>, {'all_KL': [0.191, 0.504, 0.635, 0.463], 'all_L1': [0.312, 0.502, 0.445, 0.264]}), defaultdict(<class 'list'>, {'all_KL': [0.145, 0.419, 0.619, 0.428], 'all_L1': [0.294, 0.491, 0.475, 0.283]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.719 +- 0.063, 0.489 +- 0.023, 0.785 +- 0.010, 1.000 +- 0.000
suff++ class all_KL  =  0.833 +- 0.041, 0.515 +- 0.073, 0.793 +- 0.029, 1.000 +- 0.000
suff++_acc_int  =  0.111 +- 0.004, 0.149 +- 0.014, 0.720 +- 0.008
nec class all_L1  =  0.292 +- 0.080, 0.521 +- 0.029, 0.463 +- 0.021, 0.273 +- 0.006
nec class all_KL  =  0.187 +- 0.080, 0.527 +- 0.091, 0.639 +- 0.052, 0.445 +- 0.033
nec_acc_int  =  0.112 +- 0.003, 0.153 +- 0.024, 0.552 +- 0.022, 0.802 +- 0.005


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.506 +- 0.017, 0.505 +- 0.006, 0.624 +- 0.006, 0.636 +- 0.003
Faith. Armon (L1)= 		  =  0.405 +- 0.069, 0.503 +- 0.005, 0.582 +- 0.014, 0.429 +- 0.008
Faith. GMean (L1)= 	  =  0.451 +- 0.044, 0.504 +- 0.006, 0.603 +- 0.010, 0.522 +- 0.006
Faith. Aritm (KL)= 		  =  0.510 +- 0.024, 0.521 +- 0.013, 0.716 +- 0.012, 0.722 +- 0.016
Faith. Armon (KL)= 		  =  0.296 +- 0.095, 0.509 +- 0.011, 0.705 +- 0.021, 0.615 +- 0.032
Faith. GMean (KL)= 	  =  0.385 +- 0.069, 0.515 +- 0.010, 0.711 +- 0.016, 0.666 +- 0.025
Computed for split load_split = id



Completed in  0:34:05.077596  for LECIGIN GOODCMNIST/color



DONE LECI GOODCMNIST/color ALL
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:46:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:46:15 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 87...
[0m[1;37mINFO[0m: [1mCheckpoint 87: 
-----------------------------------
Train ACCURACY: 0.3520
Train Loss: 4.1012
ID Validation ACCURACY: 0.3500
ID Validation Loss: 4.1593
ID Test ACCURACY: 0.3524
ID Test Loss: 4.1588
OOD Validation ACCURACY: 0.3167
OOD Validation Loss: 6.5550
OOD Test ACCURACY: 0.1637
OOD Test Loss: 33.3198

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 87...
[0m[1;37mINFO[0m: [1mCheckpoint 87: 
-----------------------------------
Train ACCURACY: 0.3520
Train Loss: 4.1012
ID Validation ACCURACY: 0.3500
ID Validation Loss: 4.1593
ID Test ACCURACY: 0.3524
ID Test Loss: 4.1588
OOD Validation ACCURACY: 0.3167
OOD Validation Loss: 6.5550
OOD Test ACCURACY: 0.1637
OOD Test Loss: 33.3198

[0m[1;37mINFO[0m: [1mChartInfo 0.3524 0.1637 0.3524 0.1637 0.3500 0.3167[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.126
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.11
SUFF++ for r=0.3 class 0 = 0.381 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 1 = 0.45 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 2 = 0.37 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 3 = 0.347 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 4 = 0.376 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 5 = 0.368 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 6 = 0.322 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 7 = 0.401 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 8 = 0.357 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 class 9 = 0.353 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 all KL = 0.273 +- 0.194 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.3 all L1 = 0.374 +- 0.127 (in-sample avg dev_std = 0.579)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.29
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.224
SUFF++ for r=0.6 class 0 = 0.264 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 1 = 0.424 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 2 = 0.308 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 3 = 0.326 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 4 = 0.372 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 5 = 0.359 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 6 = 0.335 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 7 = 0.476 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 8 = 0.318 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 class 9 = 0.367 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 all KL = 0.174 +- 0.193 (in-sample avg dev_std = 0.598)
SUFF++ for r=0.6 all L1 = 0.356 +- 0.162 (in-sample avg dev_std = 0.598)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.345
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.261
SUFF++ for r=0.9 class 0 = 0.259 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 1 = 0.503 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 2 = 0.315 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 3 = 0.298 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 4 = 0.418 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 5 = 0.35 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 6 = 0.399 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 7 = 0.536 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 8 = 0.316 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 class 9 = 0.367 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 all KL = 0.164 +- 0.220 (in-sample avg dev_std = 0.612)
SUFF++ for r=0.9 all L1 = 0.379 +- 0.203 (in-sample avg dev_std = 0.612)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.126
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.135
NEC for r=0.3 class 0 = 0.511 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 1 = 0.419 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 2 = 0.513 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 3 = 0.584 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 4 = 0.556 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 5 = 0.545 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 6 = 0.592 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 7 = 0.528 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 8 = 0.483 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 class 9 = 0.596 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 all KL = 0.562 +- 0.294 (in-sample avg dev_std = 0.453)
NEC for r=0.3 all L1 = 0.531 +- 0.213 (in-sample avg dev_std = 0.453)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.29
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.263
NEC for r=0.6 class 0 = 0.624 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 1 = 0.266 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 2 = 0.615 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 3 = 0.609 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 4 = 0.587 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 5 = 0.598 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 6 = 0.645 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 7 = 0.5 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 8 = 0.528 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 9 = 0.649 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 all KL = 0.703 +- 0.316 (in-sample avg dev_std = 0.503)
NEC for r=0.6 all L1 = 0.557 +- 0.260 (in-sample avg dev_std = 0.503)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.345
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.367
NEC for r=0.9 class 0 = 0.656 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 1 = 0.037 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 2 = 0.628 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 3 = 0.632 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 4 = 0.483 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 5 = 0.624 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 6 = 0.616 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 7 = 0.4 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 8 = 0.593 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 class 9 = 0.606 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 all KL = 0.682 +- 0.363 (in-sample avg dev_std = 0.499)
NEC for r=0.9 all L1 = 0.519 +- 0.295 (in-sample avg dev_std = 0.499)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.351
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.384
NEC for r=1.0 class 0 = 0.652 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 1 = 0.055 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 2 = 0.609 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 3 = 0.658 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 4 = 0.467 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 5 = 0.626 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 6 = 0.626 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 7 = 0.405 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 8 = 0.615 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 class 9 = 0.612 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 all KL = 0.662 +- 0.353 (in-sample avg dev_std = 0.475)
NEC for r=1.0 all L1 = 0.524 +- 0.288 (in-sample avg dev_std = 0.475)
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 09:53:22 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:23 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:23 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:23 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:23 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 09:53:24 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 65...
[0m[1;37mINFO[0m: [1mCheckpoint 65: 
-----------------------------------
Train ACCURACY: 0.4144
Train Loss: 2.4823
ID Validation ACCURACY: 0.4157
ID Validation Loss: 2.5162
ID Test ACCURACY: 0.4114
ID Test Loss: 2.4803
OOD Validation ACCURACY: 0.3104
OOD Validation Loss: 6.7585
OOD Test ACCURACY: 0.1446
OOD Test Loss: 41.0285

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 67...
[0m[1;37mINFO[0m: [1mCheckpoint 67: 
-----------------------------------
Train ACCURACY: 0.2914
Train Loss: 3.5159
ID Validation ACCURACY: 0.2877
ID Validation Loss: 3.5219
ID Test ACCURACY: 0.2904
ID Test Loss: 3.4864
OOD Validation ACCURACY: 0.3116
OOD Validation Loss: 9.4187
OOD Test ACCURACY: 0.1699
OOD Test Loss: 50.5791

[0m[1;37mINFO[0m: [1mChartInfo 0.4114 0.1446 0.2904 0.1699 0.2877 0.3116[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.126
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.1
SUFF++ for r=0.3 class 0 = 0.474 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 1 = 0.457 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 2 = 0.465 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 3 = 0.461 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 4 = 0.391 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 5 = 0.491 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 6 = 0.462 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 7 = 0.447 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 8 = 0.517 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 class 9 = 0.428 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 all KL = 0.376 +- 0.223 (in-sample avg dev_std = 0.512)
SUFF++ for r=0.3 all L1 = 0.459 +- 0.178 (in-sample avg dev_std = 0.512)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.326
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.239
SUFF++ for r=0.6 class 0 = 0.233 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 1 = 0.446 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 2 = 0.274 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 3 = 0.273 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 4 = 0.311 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 5 = 0.303 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 6 = 0.348 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 7 = 0.469 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 8 = 0.279 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 class 9 = 0.381 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 all KL = 0.199 +- 0.212 (in-sample avg dev_std = 0.506)
SUFF++ for r=0.6 all L1 = 0.334 +- 0.165 (in-sample avg dev_std = 0.506)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.421
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.296
SUFF++ for r=0.9 class 0 = 0.273 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 1 = 0.374 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 2 = 0.297 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 3 = 0.288 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 4 = 0.334 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 5 = 0.34 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 6 = 0.379 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 7 = 0.583 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 8 = 0.268 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 class 9 = 0.405 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 all KL = 0.232 +- 0.266 (in-sample avg dev_std = 0.551)
SUFF++ for r=0.9 all L1 = 0.356 +- 0.198 (in-sample avg dev_std = 0.551)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.126
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.1
NEC for r=0.3 class 0 = 0.468 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 1 = 0.44 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 2 = 0.44 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 3 = 0.469 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 4 = 0.541 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 5 = 0.409 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 6 = 0.456 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 7 = 0.484 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 8 = 0.369 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 class 9 = 0.537 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 all KL = 0.465 +- 0.282 (in-sample avg dev_std = 0.380)
NEC for r=0.3 all L1 = 0.462 +- 0.238 (in-sample avg dev_std = 0.380)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.326
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.259
NEC for r=0.6 class 0 = 0.702 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 1 = 0.374 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 2 = 0.655 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 3 = 0.681 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 4 = 0.699 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 5 = 0.705 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 6 = 0.692 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 7 = 0.588 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 8 = 0.644 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 class 9 = 0.671 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 all KL = 0.769 +- 0.263 (in-sample avg dev_std = 0.476)
NEC for r=0.6 all L1 = 0.636 +- 0.211 (in-sample avg dev_std = 0.476)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.421
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.424
NEC for r=0.9 class 0 = 0.661 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 1 = 0.187 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 2 = 0.599 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 3 = 0.666 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 4 = 0.612 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 5 = 0.678 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 6 = 0.66 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 7 = 0.457 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 8 = 0.679 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 class 9 = 0.642 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 all KL = 0.702 +- 0.299 (in-sample avg dev_std = 0.453)
NEC for r=0.9 all L1 = 0.576 +- 0.245 (in-sample avg dev_std = 0.453)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.417
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.443
NEC for r=1.0 class 0 = 0.649 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 1 = 0.187 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 2 = 0.577 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 3 = 0.666 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 4 = 0.587 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 5 = 0.662 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 6 = 0.639 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 7 = 0.455 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 8 = 0.661 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 class 9 = 0.624 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 all KL = 0.668 +- 0.292 (in-sample avg dev_std = 0.445)
NEC for r=1.0 all L1 = 0.563 +- 0.238 (in-sample avg dev_std = 0.445)
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 10:00:36 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:37 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:37 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:37 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 10:00:38 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.3567
Train Loss: 3.2947
ID Validation ACCURACY: 0.3594
ID Validation Loss: 3.2787
ID Test ACCURACY: 0.3510
ID Test Loss: 3.2852
OOD Validation ACCURACY: 0.3019
OOD Validation Loss: 3.8731
OOD Test ACCURACY: 0.1820
OOD Test Loss: 4.9479

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 39...
[0m[1;37mINFO[0m: [1mCheckpoint 39: 
-----------------------------------
Train ACCURACY: 0.3426
Train Loss: 2.6924
ID Validation ACCURACY: 0.3353
ID Validation Loss: 2.7255
ID Test ACCURACY: 0.3387
ID Test Loss: 2.7041
OOD Validation ACCURACY: 0.3136
OOD Validation Loss: 2.9546
OOD Test ACCURACY: 0.1916
OOD Test Loss: 4.1809

[0m[1;37mINFO[0m: [1mChartInfo 0.3510 0.1820 0.3387 0.1916 0.3353 0.3136[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.114
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.104
SUFF++ for r=0.3 class 0 = 0.351 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 1 = 0.334 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 2 = 0.352 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 3 = 0.306 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 4 = 0.313 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 5 = 0.343 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 6 = 0.33 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 7 = 0.329 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 8 = 0.337 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 class 9 = 0.291 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 all KL = 0.222 +- 0.173 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.3 all L1 = 0.329 +- 0.116 (in-sample avg dev_std = 0.570)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.27
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.191
SUFF++ for r=0.6 class 0 = 0.267 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 1 = 0.487 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 2 = 0.291 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 3 = 0.34 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 4 = 0.354 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 5 = 0.342 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 6 = 0.367 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 7 = 0.471 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 8 = 0.262 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 9 = 0.341 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 all KL = 0.166 +- 0.206 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 all L1 = 0.355 +- 0.189 (in-sample avg dev_std = 0.531)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.347
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.282
SUFF++ for r=0.9 class 0 = 0.314 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 1 = 0.573 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 2 = 0.297 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 3 = 0.32 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 4 = 0.392 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 5 = 0.328 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 6 = 0.361 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 7 = 0.499 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 8 = 0.256 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 class 9 = 0.328 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 all KL = 0.18 +- 0.246 (in-sample avg dev_std = 0.529)
SUFF++ for r=0.9 all L1 = 0.371 +- 0.212 (in-sample avg dev_std = 0.529)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.114
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.118
NEC for r=0.3 class 0 = 0.566 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 1 = 0.449 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 2 = 0.53 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 3 = 0.596 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 4 = 0.602 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 5 = 0.572 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 6 = 0.556 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 7 = 0.536 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 8 = 0.51 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 class 9 = 0.604 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 all KL = 0.58 +- 0.266 (in-sample avg dev_std = 0.418)
NEC for r=0.3 all L1 = 0.55 +- 0.202 (in-sample avg dev_std = 0.418)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.27
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.245
NEC for r=0.6 class 0 = 0.642 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 1 = 0.379 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 2 = 0.672 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 3 = 0.642 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 4 = 0.647 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 5 = 0.686 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 6 = 0.675 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 7 = 0.506 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 8 = 0.699 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 9 = 0.686 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 all KL = 0.762 +- 0.260 (in-sample avg dev_std = 0.497)
NEC for r=0.6 all L1 = 0.618 +- 0.210 (in-sample avg dev_std = 0.497)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.347
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.385
NEC for r=0.9 class 0 = 0.672 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 1 = 0.163 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 2 = 0.623 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 3 = 0.679 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 4 = 0.613 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 5 = 0.689 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 6 = 0.678 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 7 = 0.486 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 8 = 0.71 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 class 9 = 0.699 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 all KL = 0.725 +- 0.304 (in-sample avg dev_std = 0.455)
NEC for r=0.9 all L1 = 0.592 +- 0.245 (in-sample avg dev_std = 0.455)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.359
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.407
NEC for r=1.0 class 0 = 0.675 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 1 = 0.15 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 2 = 0.594 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 3 = 0.674 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 4 = 0.598 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 5 = 0.683 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 6 = 0.679 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 7 = 0.485 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 8 = 0.684 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 class 9 = 0.68 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 all KL = 0.695 +- 0.302 (in-sample avg dev_std = 0.433)
NEC for r=1.0 all L1 = 0.582 +- 0.240 (in-sample avg dev_std = 0.433)
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 10:07:47 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:48 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:48 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:48 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:48 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 10:07:49 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 110...
[0m[1;37mINFO[0m: [1mCheckpoint 110: 
-----------------------------------
Train ACCURACY: 0.3909
Train Loss: 2.8134
ID Validation ACCURACY: 0.4021
ID Validation Loss: 2.7741
ID Test ACCURACY: 0.3860
ID Test Loss: 2.8356
OOD Validation ACCURACY: 0.3126
OOD Validation Loss: 4.1696
OOD Test ACCURACY: 0.2140
OOD Test Loss: 12.2054

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 62...
[0m[1;37mINFO[0m: [1mCheckpoint 62: 
-----------------------------------
Train ACCURACY: 0.3350
Train Loss: 4.2162
ID Validation ACCURACY: 0.3359
ID Validation Loss: 4.3002
ID Test ACCURACY: 0.3386
ID Test Loss: 4.2674
OOD Validation ACCURACY: 0.3293
OOD Validation Loss: 8.2922
OOD Test ACCURACY: 0.1864
OOD Test Loss: 53.4631

[0m[1;37mINFO[0m: [1mChartInfo 0.3860 0.2140 0.3386 0.1864 0.3359 0.3293[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.106
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.096
SUFF++ for r=0.3 class 0 = 0.374 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 1 = 0.377 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 2 = 0.365 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 3 = 0.343 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 4 = 0.342 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 5 = 0.383 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 6 = 0.337 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 7 = 0.352 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 8 = 0.347 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 class 9 = 0.297 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 all KL = 0.317 +- 0.201 (in-sample avg dev_std = 0.492)
SUFF++ for r=0.3 all L1 = 0.352 +- 0.119 (in-sample avg dev_std = 0.492)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.285
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.202
SUFF++ for r=0.6 class 0 = 0.222 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 1 = 0.357 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 2 = 0.277 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 3 = 0.263 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 4 = 0.304 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 5 = 0.292 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 6 = 0.318 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 7 = 0.393 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 8 = 0.252 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 class 9 = 0.34 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 all KL = 0.112 +- 0.155 (in-sample avg dev_std = 0.570)
SUFF++ for r=0.6 all L1 = 0.303 +- 0.140 (in-sample avg dev_std = 0.570)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.379
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.274
SUFF++ for r=0.9 class 0 = 0.282 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 1 = 0.358 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 2 = 0.299 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 3 = 0.301 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 4 = 0.332 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 5 = 0.349 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 6 = 0.386 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 7 = 0.451 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 8 = 0.258 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 class 9 = 0.397 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 all KL = 0.171 +- 0.205 (in-sample avg dev_std = 0.588)
SUFF++ for r=0.9 all L1 = 0.342 +- 0.172 (in-sample avg dev_std = 0.588)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.106
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.102
NEC for r=0.3 class 0 = 0.575 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 1 = 0.461 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 2 = 0.551 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 3 = 0.584 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 4 = 0.571 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 5 = 0.535 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 6 = 0.547 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 7 = 0.57 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 8 = 0.569 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 class 9 = 0.612 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 all KL = 0.542 +- 0.259 (in-sample avg dev_std = 0.400)
NEC for r=0.3 all L1 = 0.557 +- 0.176 (in-sample avg dev_std = 0.400)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.285
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.274
NEC for r=0.6 class 0 = 0.733 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 1 = 0.351 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 2 = 0.625 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 3 = 0.609 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 4 = 0.646 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 5 = 0.662 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 6 = 0.642 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 7 = 0.488 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 8 = 0.599 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 class 9 = 0.641 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 all KL = 0.729 +- 0.278 (in-sample avg dev_std = 0.485)
NEC for r=0.6 all L1 = 0.594 +- 0.227 (in-sample avg dev_std = 0.485)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.379
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.431
NEC for r=0.9 class 0 = 0.681 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 1 = 0.177 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 2 = 0.649 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 3 = 0.662 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 4 = 0.62 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 5 = 0.653 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 6 = 0.639 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 7 = 0.521 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 8 = 0.693 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 class 9 = 0.634 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 all KL = 0.712 +- 0.306 (in-sample avg dev_std = 0.464)
NEC for r=0.9 all L1 = 0.586 +- 0.254 (in-sample avg dev_std = 0.464)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.382
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.458
NEC for r=1.0 class 0 = 0.674 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 1 = 0.21 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 2 = 0.619 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 3 = 0.643 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 4 = 0.622 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 5 = 0.651 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 6 = 0.632 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 7 = 0.525 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 8 = 0.679 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 class 9 = 0.637 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 all KL = 0.692 +- 0.298 (in-sample avg dev_std = 0.450)
NEC for r=1.0 all L1 = 0.582 +- 0.246 (in-sample avg dev_std = 0.450)
model_dirname= repr_GSATGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatmitig_readoutweightedmitig_explscoreshardavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Mon Sep 23 10:14:50 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  weighted
[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mUsing no mitigation (None)
[0mUsing mitigation_expl_scores: hard
[1;34mDEBUG[0m: 09/23/2024 10:14:51 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 190...
[0m[1;37mINFO[0m: [1mCheckpoint 190: 
-----------------------------------
Train ACCURACY: 0.3832
Train Loss: 3.2781
ID Validation ACCURACY: 0.3854
ID Validation Loss: 3.3077
ID Test ACCURACY: 0.3756
ID Test Loss: 3.3515
OOD Validation ACCURACY: 0.2441
OOD Validation Loss: 6.2321
OOD Test ACCURACY: 0.1736
OOD Test Loss: 8.2136

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 105...
[0m[1;37mINFO[0m: [1mCheckpoint 105: 
-----------------------------------
Train ACCURACY: 0.3804
Train Loss: 3.3525
ID Validation ACCURACY: 0.3771
ID Validation Loss: 3.3880
ID Test ACCURACY: 0.3827
ID Test Loss: 3.3628
OOD Validation ACCURACY: 0.3297
OOD Validation Loss: 4.5238
OOD Test ACCURACY: 0.2149
OOD Test Loss: 6.5520

[0m[1;37mINFO[0m: [1mChartInfo 0.3756 0.1736 0.3827 0.2149 0.3771 0.3297[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.3 = nan
WIoU for r=0.3 = nan
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=0.9 = nan
WIoU for r=0.9 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.097
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.087
SUFF++ for r=0.3 class 0 = 0.37 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 1 = 0.37 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 2 = 0.344 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 3 = 0.357 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 4 = 0.338 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 5 = 0.349 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 6 = 0.366 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 7 = 0.361 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 8 = 0.405 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 class 9 = 0.338 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 all KL = 0.288 +- 0.193 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.3 all L1 = 0.36 +- 0.122 (in-sample avg dev_std = 0.549)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.268
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.222
SUFF++ for r=0.6 class 0 = 0.251 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 1 = 0.449 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 2 = 0.279 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 3 = 0.323 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 4 = 0.335 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 5 = 0.295 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 6 = 0.303 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 7 = 0.484 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 8 = 0.314 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 class 9 = 0.313 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 all KL = 0.139 +- 0.196 (in-sample avg dev_std = 0.587)
SUFF++ for r=0.6 all L1 = 0.338 +- 0.172 (in-sample avg dev_std = 0.587)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.404
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.286
SUFF++ for r=0.9 class 0 = 0.293 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 1 = 0.417 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 2 = 0.294 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 3 = 0.286 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 4 = 0.273 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 5 = 0.28 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 6 = 0.337 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 7 = 0.493 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 8 = 0.268 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 class 9 = 0.312 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 all KL = 0.139 +- 0.222 (in-sample avg dev_std = 0.604)
SUFF++ for r=0.9 all L1 = 0.329 +- 0.186 (in-sample avg dev_std = 0.604)


ratio=1.0



Zero intervened samples, skipping weight=1.0


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.3


Computing with masking

Model Accuracy of binarized graphs for r=0.3 =  0.097
Model XAI F1 of binarized graphs for r=0.3 =  nan
Model XAI WIoU of binarized graphs for r=0.3 =  nan
len(reference) = 800
Effective ratio: 0.300 +- 0.000
Model Accuracy over intervened graphs for r=0.3 =  0.091
NEC for r=0.3 class 0 = 0.529 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 1 = 0.437 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 2 = 0.537 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 3 = 0.547 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 4 = 0.579 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 5 = 0.518 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 6 = 0.522 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 7 = 0.546 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 8 = 0.412 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 class 9 = 0.591 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 all KL = 0.528 +- 0.281 (in-sample avg dev_std = 0.390)
NEC for r=0.3 all L1 = 0.521 +- 0.221 (in-sample avg dev_std = 0.390)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.268
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.24
NEC for r=0.6 class 0 = 0.713 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 1 = 0.378 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 2 = 0.688 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 3 = 0.613 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 4 = 0.653 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 5 = 0.687 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 6 = 0.679 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 7 = 0.513 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 8 = 0.549 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 class 9 = 0.703 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 all KL = 0.769 +- 0.281 (in-sample avg dev_std = 0.528)
NEC for r=0.6 all L1 = 0.612 +- 0.230 (in-sample avg dev_std = 0.528)


ratio=0.9


Computing with masking

Model Accuracy of binarized graphs for r=0.9 =  0.404
Model XAI F1 of binarized graphs for r=0.9 =  nan
Model XAI WIoU of binarized graphs for r=0.9 =  nan
len(reference) = 800
Effective ratio: 0.900 +- 0.000
Model Accuracy over intervened graphs for r=0.9 =  0.43
NEC for r=0.9 class 0 = 0.583 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 1 = 0.198 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 2 = 0.683 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 3 = 0.672 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 4 = 0.631 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 5 = 0.707 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 6 = 0.697 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 7 = 0.546 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 8 = 0.579 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 class 9 = 0.648 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 all KL = 0.755 +- 0.322 (in-sample avg dev_std = 0.506)
NEC for r=0.9 all L1 = 0.587 +- 0.269 (in-sample avg dev_std = 0.506)


ratio=1.0


Computing with masking

Model Accuracy of binarized graphs for r=1.0 =  0.401
Model XAI F1 of binarized graphs for r=1.0 =  nan
Model XAI WIoU of binarized graphs for r=1.0 =  nan
len(reference) = 800
Effective ratio: 1.000 +- 0.000
Model Accuracy over intervened graphs for r=1.0 =  0.449
NEC for r=1.0 class 0 = 0.561 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 1 = 0.236 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 2 = 0.658 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 3 = 0.662 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 4 = 0.617 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 5 = 0.702 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 6 = 0.682 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 7 = 0.556 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 8 = 0.587 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 class 9 = 0.646 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 all KL = 0.735 +- 0.311 (in-sample avg dev_std = 0.477)
NEC for r=1.0 all L1 = 0.584 +- 0.260 (in-sample avg dev_std = 0.477)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.273, 0.174, 0.164, 1.0], 'all_L1': [0.374, 0.356, 0.379, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.376, 0.199, 0.232, 1.0], 'all_L1': [0.459, 0.334, 0.356, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.222, 0.166, 0.18, 1.0], 'all_L1': [0.329, 0.355, 0.371, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.317, 0.112, 0.171, 1.0], 'all_L1': [0.352, 0.303, 0.342, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.288, 0.139, 0.139, 1.0], 'all_L1': [0.36, 0.338, 0.329, 1.0], 0: [1.0], 1: [1.0], 2: [1.0], 3: [1.0], 4: [1.0], 5: [1.0], 6: [1.0], 7: [1.0], 8: [1.0], 9: [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.562, 0.703, 0.682, 0.662], 'all_L1': [0.531, 0.557, 0.519, 0.524]}), defaultdict(<class 'list'>, {'all_KL': [0.465, 0.769, 0.702, 0.668], 'all_L1': [0.462, 0.636, 0.576, 0.563]}), defaultdict(<class 'list'>, {'all_KL': [0.58, 0.762, 0.725, 0.695], 'all_L1': [0.55, 0.618, 0.592, 0.582]}), defaultdict(<class 'list'>, {'all_KL': [0.542, 0.729, 0.712, 0.692], 'all_L1': [0.557, 0.594, 0.586, 0.582]}), defaultdict(<class 'list'>, {'all_KL': [0.528, 0.769, 0.755, 0.735], 'all_L1': [0.521, 0.612, 0.587, 0.584]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.375 +- 0.045, 0.337 +- 0.019, 0.355 +- 0.018, 1.000 +- 0.000
suff++ class all_KL  =  0.295 +- 0.051, 0.158 +- 0.030, 0.177 +- 0.031, 1.000 +- 0.000
suff++_acc_int  =  0.099 +- 0.008, 0.216 +- 0.017, 0.280 +- 0.012
nec class all_L1  =  0.524 +- 0.034, 0.603 +- 0.027, 0.572 +- 0.027, 0.567 +- 0.023
nec class all_KL  =  0.535 +- 0.039, 0.746 +- 0.026, 0.715 +- 0.024, 0.690 +- 0.026
nec_acc_int  =  0.109 +- 0.015, 0.256 +- 0.012, 0.407 +- 0.026, 0.428 +- 0.028


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.449 +- 0.008, 0.470 +- 0.015, 0.464 +- 0.011, 0.783 +- 0.011
Faith. Armon (L1)= 		  =  0.434 +- 0.016, 0.432 +- 0.016, 0.438 +- 0.011, 0.723 +- 0.019
Faith. GMean (L1)= 	  =  0.441 +- 0.012, 0.451 +- 0.015, 0.450 +- 0.010, 0.753 +- 0.015
Faith. Aritm (KL)= 		  =  0.415 +- 0.010, 0.452 +- 0.022, 0.446 +- 0.014, 0.845 +- 0.013
Faith. Armon (KL)= 		  =  0.375 +- 0.032, 0.259 +- 0.041, 0.282 +- 0.038, 0.817 +- 0.018
Faith. GMean (KL)= 	  =  0.395 +- 0.021, 0.342 +- 0.035, 0.354 +- 0.028, 0.831 +- 0.015
Computed for split load_split = id



Completed in  0:35:20.748431  for GSATGIN GOODCMNIST/color



DONE GSAT GOODCMNIST/color ALL
DONE all :)
