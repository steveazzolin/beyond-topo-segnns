nohup: ignoring input
Time to compute metrics for random explanations!
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:53:55 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 07:53:55 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ACCURACY: 0.6192
Train Loss: 0.8793
ID Validation ACCURACY: 0.6427
ID Validation Loss: 0.8548
ID Test ACCURACY: 0.6190
ID Test Loss: 0.8785
OOD Validation ACCURACY: 0.4187
OOD Validation Loss: 1.1603
OOD Test ACCURACY: 0.4730
OOD Test Loss: 1.9920

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 55...
[0m[1;37mINFO[0m: [1mCheckpoint 55: 
-----------------------------------
Train ACCURACY: 0.4834
Train Loss: 1.3233
ID Validation ACCURACY: 0.4867
ID Validation Loss: 1.3383
ID Test ACCURACY: 0.4863
ID Test Loss: 1.2866
OOD Validation ACCURACY: 0.6313
OOD Validation Loss: 0.8829
OOD Test ACCURACY: 0.4813
OOD Test Loss: 1.8714

[0m[1;37mINFO[0m: [1mChartInfo 0.6190 0.4730 0.4863 0.4813 0.4867 0.6313[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.299
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.313
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.245
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.249


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.655
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29869625
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.415
SUFF++ for r=0.8 class 0 = 0.624 +- 0.211 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 class 1 = 0.53 +- 0.211 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 class 2 = 0.632 +- 0.211 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 all KL = 0.747 +- 0.211 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 all L1 = 0.595 +- 0.155 (in-sample avg dev_std = 0.308)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.494
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24471374999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.388
SUFF++ for r=0.8 class 0 = 0.627 +- 0.219 (in-sample avg dev_std = 0.338)
SUFF++ for r=0.8 class 1 = 0.61 +- 0.219 (in-sample avg dev_std = 0.338)
SUFF++ for r=0.8 class 2 = 0.587 +- 0.219 (in-sample avg dev_std = 0.338)
SUFF++ for r=0.8 all KL = 0.657 +- 0.219 (in-sample avg dev_std = 0.338)
SUFF++ for r=0.8 all L1 = 0.608 +- 0.164 (in-sample avg dev_std = 0.338)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.655
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29869625
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.38
NEC for r=0.8 class 0 = 0.395 +- 0.230 (in-sample avg dev_std = 0.357)
NEC for r=0.8 class 1 = 0.494 +- 0.230 (in-sample avg dev_std = 0.357)
NEC for r=0.8 class 2 = 0.407 +- 0.230 (in-sample avg dev_std = 0.357)
NEC for r=0.8 all KL = 0.294 +- 0.230 (in-sample avg dev_std = 0.357)
NEC for r=0.8 all L1 = 0.432 +- 0.160 (in-sample avg dev_std = 0.357)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.494
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24471374999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.394
NEC for r=0.8 class 0 = 0.38 +- 0.231 (in-sample avg dev_std = 0.362)
NEC for r=0.8 class 1 = 0.38 +- 0.231 (in-sample avg dev_std = 0.362)
NEC for r=0.8 class 2 = 0.41 +- 0.231 (in-sample avg dev_std = 0.362)
NEC for r=0.8 all KL = 0.356 +- 0.231 (in-sample avg dev_std = 0.362)
NEC for r=0.8 all L1 = 0.39 +- 0.181 (in-sample avg dev_std = 0.362)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:54:52 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 07:54:52 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 133...
[0m[1;37mINFO[0m: [1mCheckpoint 133: 
-----------------------------------
Train ACCURACY: 0.9237
Train Loss: 0.3588
ID Validation ACCURACY: 0.9287
ID Validation Loss: 0.3522
ID Test ACCURACY: 0.9200
ID Test Loss: 0.3948
OOD Validation ACCURACY: 0.9140
OOD Validation Loss: 0.4234
OOD Test ACCURACY: 0.6333
OOD Test Loss: 4.8335

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.9002
Train Loss: 0.4180
ID Validation ACCURACY: 0.9057
ID Validation Loss: 0.4083
ID Test ACCURACY: 0.8980
ID Test Loss: 0.4580
OOD Validation ACCURACY: 0.9200
OOD Validation Loss: 0.3940
OOD Test ACCURACY: 0.5167
OOD Test Loss: 2.2389

[0m[1;37mINFO[0m: [1mChartInfo 0.9200 0.6333 0.8980 0.5167 0.9057 0.9200[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.297
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.309
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.248
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.253


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.934
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29709625
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.541
SUFF++ for r=0.8 class 0 = 0.411 +- 0.319 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 class 1 = 0.706 +- 0.319 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 class 2 = 0.364 +- 0.319 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 all KL = 0.46 +- 0.319 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 all L1 = 0.495 +- 0.242 (in-sample avg dev_std = 0.495)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.649
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24844000000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.48
SUFF++ for r=0.8 class 0 = 0.573 +- 0.337 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 class 1 = 0.79 +- 0.337 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 class 2 = 0.522 +- 0.337 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 all KL = 0.421 +- 0.337 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 all L1 = 0.631 +- 0.274 (in-sample avg dev_std = 0.475)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.934
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29709625
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.542
NEC for r=0.8 class 0 = 0.602 +- 0.303 (in-sample avg dev_std = 0.575)
NEC for r=0.8 class 1 = 0.312 +- 0.303 (in-sample avg dev_std = 0.575)
NEC for r=0.8 class 2 = 0.628 +- 0.303 (in-sample avg dev_std = 0.575)
NEC for r=0.8 all KL = 0.562 +- 0.303 (in-sample avg dev_std = 0.575)
NEC for r=0.8 all L1 = 0.513 +- 0.223 (in-sample avg dev_std = 0.575)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.649
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24844000000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.464
NEC for r=0.8 class 0 = 0.387 +- 0.352 (in-sample avg dev_std = 0.495)
NEC for r=0.8 class 1 = 0.185 +- 0.352 (in-sample avg dev_std = 0.495)
NEC for r=0.8 class 2 = 0.458 +- 0.352 (in-sample avg dev_std = 0.495)
NEC for r=0.8 all KL = 0.499 +- 0.352 (in-sample avg dev_std = 0.495)
NEC for r=0.8 all L1 = 0.341 +- 0.280 (in-sample avg dev_std = 0.495)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:55:46 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 07:55:46 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 79...
[0m[1;37mINFO[0m: [1mCheckpoint 79: 
-----------------------------------
Train ACCURACY: 0.8680
Train Loss: 0.4829
ID Validation ACCURACY: 0.8710
ID Validation Loss: 0.4693
ID Test ACCURACY: 0.8680
ID Test Loss: 0.4894
OOD Validation ACCURACY: 0.8327
OOD Validation Loss: 0.5758
OOD Test ACCURACY: 0.4990
OOD Test Loss: 3.3324

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 68...
[0m[1;37mINFO[0m: [1mCheckpoint 68: 
-----------------------------------
Train ACCURACY: 0.8033
Train Loss: 0.5322
ID Validation ACCURACY: 0.8120
ID Validation Loss: 0.5122
ID Test ACCURACY: 0.8153
ID Test Loss: 0.5128
OOD Validation ACCURACY: 0.9143
OOD Validation Loss: 0.5427
OOD Test ACCURACY: 0.5627
OOD Test Loss: 1.2968

[0m[1;37mINFO[0m: [1mChartInfo 0.8680 0.4990 0.8153 0.5627 0.8120 0.9143[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.293
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.307
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.242
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29338749999999997
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.474
SUFF++ for r=0.8 class 0 = 0.352 +- 0.301 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 class 1 = 0.706 +- 0.301 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 class 2 = 0.332 +- 0.301 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 all KL = 0.458 +- 0.301 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 all L1 = 0.465 +- 0.244 (in-sample avg dev_std = 0.446)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.496
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24224
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.387
SUFF++ for r=0.8 class 0 = 0.431 +- 0.300 (in-sample avg dev_std = 0.571)
SUFF++ for r=0.8 class 1 = 0.633 +- 0.300 (in-sample avg dev_std = 0.571)
SUFF++ for r=0.8 class 2 = 0.468 +- 0.300 (in-sample avg dev_std = 0.571)
SUFF++ for r=0.8 all KL = 0.407 +- 0.300 (in-sample avg dev_std = 0.571)
SUFF++ for r=0.8 all L1 = 0.513 +- 0.209 (in-sample avg dev_std = 0.571)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.881
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.29338749999999997
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.509
NEC for r=0.8 class 0 = 0.634 +- 0.264 (in-sample avg dev_std = 0.548)
NEC for r=0.8 class 1 = 0.331 +- 0.264 (in-sample avg dev_std = 0.548)
NEC for r=0.8 class 2 = 0.634 +- 0.264 (in-sample avg dev_std = 0.548)
NEC for r=0.8 all KL = 0.552 +- 0.264 (in-sample avg dev_std = 0.548)
NEC for r=0.8 all L1 = 0.532 +- 0.206 (in-sample avg dev_std = 0.548)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.496
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24224
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.405
NEC for r=0.8 class 0 = 0.386 +- 0.234 (in-sample avg dev_std = 0.391)
NEC for r=0.8 class 1 = 0.246 +- 0.234 (in-sample avg dev_std = 0.391)
NEC for r=0.8 class 2 = 0.402 +- 0.234 (in-sample avg dev_std = 0.391)
NEC for r=0.8 all KL = 0.372 +- 0.234 (in-sample avg dev_std = 0.391)
NEC for r=0.8 all L1 = 0.344 +- 0.239 (in-sample avg dev_std = 0.391)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.747], 'all_L1': [0.595]}), defaultdict(<class 'list'>, {'all_KL': [0.46], 'all_L1': [0.495]}), defaultdict(<class 'list'>, {'all_KL': [0.458], 'all_L1': [0.465]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.294], 'all_L1': [0.432]}), defaultdict(<class 'list'>, {'all_KL': [0.562], 'all_L1': [0.513]}), defaultdict(<class 'list'>, {'all_KL': [0.552], 'all_L1': [0.532]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.657], 'all_L1': [0.608]}), defaultdict(<class 'list'>, {'all_KL': [0.421], 'all_L1': [0.631]}), defaultdict(<class 'list'>, {'all_KL': [0.407], 'all_L1': [0.513]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.356], 'all_L1': [0.39]}), defaultdict(<class 'list'>, {'all_KL': [0.499], 'all_L1': [0.341]}), defaultdict(<class 'list'>, {'all_KL': [0.372], 'all_L1': [0.344]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.518 +- 0.056
suff++ class all_KL  =  0.555 +- 0.136
suff++_acc_int  =  0.477 +- 0.052
nec class all_L1  =  0.492 +- 0.043
nec class all_KL  =  0.469 +- 0.124
nec_acc_int  =  0.477 +- 0.070

Eval split test
suff++ class all_L1  =  0.584 +- 0.051
suff++ class all_KL  =  0.495 +- 0.115
suff++_acc_int  =  0.418 +- 0.043
nec class all_L1  =  0.358 +- 0.022
nec class all_KL  =  0.409 +- 0.064
nec_acc_int  =  0.421 +- 0.031


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.505 +- 0.006
Faith. Armon (L1)= 		  =  0.500 +- 0.003
Faith. GMean (L1)= 	  =  0.503 +- 0.004
Faith. Aritm (KL)= 		  =  0.512 +- 0.006
Faith. Armon (KL)= 		  =  0.476 +- 0.038
Faith. GMean (KL)= 	  =  0.493 +- 0.018

Eval split test
Faith. Aritm (L1)= 		  =  0.471 +- 0.031
Faith. Armon (L1)= 		  =  0.443 +- 0.026
Faith. GMean (L1)= 	  =  0.457 +- 0.028
Faith. Aritm (KL)= 		  =  0.452 +- 0.048
Faith. Armon (KL)= 		  =  0.436 +- 0.033
Faith. GMean (KL)= 	  =  0.444 +- 0.040
Computed for split load_split = id



Completed in  0:02:47.351094  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:56:55 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:56:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 07:56:57 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 146...
[0m[1;37mINFO[0m: [1mCheckpoint 146: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0762
ID Validation ACCURACY: 0.8615
ID Validation Loss: 0.8340
ID Test ACCURACY: 0.8593
ID Test Loss: 0.7822
OOD Validation ACCURACY: 0.8238
OOD Validation Loss: 2.7555
OOD Test ACCURACY: 0.6532
OOD Test Loss: 15.7757

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 100...
[0m[1;37mINFO[0m: [1mCheckpoint 100: 
-----------------------------------
Train ACCURACY: 0.9482
Train Loss: 0.0806
ID Validation ACCURACY: 0.8523
ID Validation Loss: 0.9562
ID Test ACCURACY: 0.8555
ID Test Loss: 0.8378
OOD Validation ACCURACY: 0.8527
OOD Validation Loss: 3.1390
OOD Test ACCURACY: 0.8097
OOD Test Loss: 9.3985

[0m[1;37mINFO[0m: [1mChartInfo 0.8593 0.6532 0.8555 0.8097 0.8523 0.8527[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 07:56:58 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.856
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.843
SUFF++ for r=0.8 class 0.0 = 0.842 +- 0.286 (in-sample avg dev_std = 0.337)
SUFF++ for r=0.8 class 1.0 = 0.944 +- 0.286 (in-sample avg dev_std = 0.337)
SUFF++ for r=0.8 all KL = 0.833 +- 0.286 (in-sample avg dev_std = 0.337)
SUFF++ for r=0.8 all L1 = 0.901 +- 0.182 (in-sample avg dev_std = 0.337)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.666
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.625
SUFF++ for r=0.8 class 0.0 = 0.892 +- 0.453 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.8 class 1.0 = 0.686 +- 0.453 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.8 all KL = 0.6 +- 0.453 (in-sample avg dev_std = 0.545)
SUFF++ for r=0.8 all L1 = 0.785 +- 0.268 (in-sample avg dev_std = 0.545)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.856
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.853
NEC for r=0.8 class 0.0 = 0.128 +- 0.237 (in-sample avg dev_std = 0.199)
NEC for r=0.8 class 1.0 = 0.043 +- 0.237 (in-sample avg dev_std = 0.199)
NEC for r=0.8 all KL = 0.11 +- 0.237 (in-sample avg dev_std = 0.199)
NEC for r=0.8 all L1 = 0.078 +- 0.185 (in-sample avg dev_std = 0.199)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.673
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.669
NEC for r=0.8 class 0.0 = 0.106 +- 0.409 (in-sample avg dev_std = 0.431)
NEC for r=0.8 class 1.0 = 0.203 +- 0.409 (in-sample avg dev_std = 0.431)
NEC for r=0.8 all KL = 0.297 +- 0.409 (in-sample avg dev_std = 0.431)
NEC for r=0.8 all L1 = 0.156 +- 0.245 (in-sample avg dev_std = 0.431)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:57:50 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 07:57:51 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 183...
[0m[1;37mINFO[0m: [1mCheckpoint 183: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8629
ID Validation Loss: 1.2999
ID Test ACCURACY: 0.8585
ID Test Loss: 1.1992
OOD Validation ACCURACY: 0.6298
OOD Validation Loss: 15.9629
OOD Test ACCURACY: 0.4839
OOD Test Loss: 201.9750

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 101...
[0m[1;37mINFO[0m: [1mCheckpoint 101: 
-----------------------------------
Train ACCURACY: 0.9483
Train Loss: 0.0774
ID Validation ACCURACY: 0.8534
ID Validation Loss: 0.7641
ID Test ACCURACY: 0.8512
ID Test Loss: 0.7085
OOD Validation ACCURACY: 0.8540
OOD Validation Loss: 1.8423
OOD Test ACCURACY: 0.7524
OOD Test Loss: 4.8235

[0m[1;37mINFO[0m: [1mChartInfo 0.8585 0.4839 0.8512 0.7524 0.8534 0.8540[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 07:57:52 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.832
SUFF++ for r=0.8 class 0.0 = 0.835 +- 0.362 (in-sample avg dev_std = 0.389)
SUFF++ for r=0.8 class 1.0 = 0.933 +- 0.362 (in-sample avg dev_std = 0.389)
SUFF++ for r=0.8 all KL = 0.76 +- 0.362 (in-sample avg dev_std = 0.389)
SUFF++ for r=0.8 all L1 = 0.892 +- 0.183 (in-sample avg dev_std = 0.389)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.484
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.484
SUFF++ for r=0.8 class 0.0 = 1.0 +- 0.012 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 class 1.0 = 1.0 +- 0.012 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 all KL = 1.0 +- 0.012 (in-sample avg dev_std = 0.001)
SUFF++ for r=0.8 all L1 = 1.0 +- 0.001 (in-sample avg dev_std = 0.001)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.847
NEC for r=0.8 class 0.0 = 0.124 +- 0.279 (in-sample avg dev_std = 0.231)
NEC for r=0.8 class 1.0 = 0.046 +- 0.279 (in-sample avg dev_std = 0.231)
NEC for r=0.8 all KL = 0.129 +- 0.279 (in-sample avg dev_std = 0.231)
NEC for r=0.8 all L1 = 0.079 +- 0.188 (in-sample avg dev_std = 0.231)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.484
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.484
NEC for r=0.8 class 0.0 = 0.0 +- 0.040 (in-sample avg dev_std = 0.021)
NEC for r=0.8 class 1.0 = 0.001 +- 0.040 (in-sample avg dev_std = 0.021)
NEC for r=0.8 all KL = 0.003 +- 0.040 (in-sample avg dev_std = 0.021)
NEC for r=0.8 all L1 = 0.001 +- 0.007 (in-sample avg dev_std = 0.021)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:58:38 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 199...
[0m[1;37mINFO[0m: [1mCheckpoint 199: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8619
ID Validation Loss: 0.8658
ID Test ACCURACY: 0.8593
ID Test Loss: 0.9042
OOD Validation ACCURACY: 0.6146
OOD Validation Loss: 17.0850
OOD Test ACCURACY: 0.5161
OOD Test Loss: 124.0430

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 0.9448
Train Loss: 0.0896
ID Validation ACCURACY: 0.8455
ID Validation Loss: 0.5447
ID Test ACCURACY: 0.8432
ID Test Loss: 0.5905
OOD Validation ACCURACY: 0.8541
OOD Validation Loss: 0.8881
OOD Test ACCURACY: 0.7646
OOD Test Loss: 1.8261

[0m[1;37mINFO[0m: [1mChartInfo 0.8593 0.5161 0.8432 0.7646 0.8455 0.8541[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 07:58:39 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.835
SUFF++ for r=0.8 class 0.0 = 0.839 +- 0.368 (in-sample avg dev_std = 0.403)
SUFF++ for r=0.8 class 1.0 = 0.922 +- 0.368 (in-sample avg dev_std = 0.403)
SUFF++ for r=0.8 all KL = 0.75 +- 0.368 (in-sample avg dev_std = 0.403)
SUFF++ for r=0.8 all L1 = 0.888 +- 0.186 (in-sample avg dev_std = 0.403)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.516
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.516
SUFF++ for r=0.8 class 0.0 = 1.0 +- 0.013 (in-sample avg dev_std = 0.003)
SUFF++ for r=0.8 class 1.0 = 1.0 +- 0.013 (in-sample avg dev_std = 0.003)
SUFF++ for r=0.8 all KL = 0.999 +- 0.013 (in-sample avg dev_std = 0.003)
SUFF++ for r=0.8 all L1 = 1.0 +- 0.002 (in-sample avg dev_std = 0.003)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.85
NEC for r=0.8 class 0.0 = 0.081 +- 0.253 (in-sample avg dev_std = 0.248)
NEC for r=0.8 class 1.0 = 0.061 +- 0.253 (in-sample avg dev_std = 0.248)
NEC for r=0.8 all KL = 0.101 +- 0.253 (in-sample avg dev_std = 0.248)
NEC for r=0.8 all L1 = 0.07 +- 0.186 (in-sample avg dev_std = 0.248)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.516
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.516
NEC for r=0.8 class 0.0 = 0.0 +- 0.005 (in-sample avg dev_std = 0.002)
NEC for r=0.8 class 1.0 = 0.0 +- 0.005 (in-sample avg dev_std = 0.002)
NEC for r=0.8 all KL = 0.0 +- 0.005 (in-sample avg dev_std = 0.002)
NEC for r=0.8 all L1 = 0.0 +- 0.001 (in-sample avg dev_std = 0.002)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.833], 'all_L1': [0.901]}), defaultdict(<class 'list'>, {'all_KL': [0.76], 'all_L1': [0.892]}), defaultdict(<class 'list'>, {'all_KL': [0.75], 'all_L1': [0.888]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.11], 'all_L1': [0.078]}), defaultdict(<class 'list'>, {'all_KL': [0.129], 'all_L1': [0.079]}), defaultdict(<class 'list'>, {'all_KL': [0.101], 'all_L1': [0.07]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.6], 'all_L1': [0.785]}), defaultdict(<class 'list'>, {'all_KL': [1.0], 'all_L1': [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.999], 'all_L1': [1.0]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.297], 'all_L1': [0.156]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.001]}), defaultdict(<class 'list'>, {'all_KL': [0.0], 'all_L1': [0.0]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.894 +- 0.005
suff++ class all_KL  =  0.781 +- 0.037
suff++_acc_int  =  0.836 +- 0.005
nec class all_L1  =  0.076 +- 0.004
nec class all_KL  =  0.113 +- 0.012
nec_acc_int  =  0.850 +- 0.003

Eval split test
suff++ class all_L1  =  0.928 +- 0.101
suff++ class all_KL  =  0.866 +- 0.188
suff++_acc_int  =  0.542 +- 0.060
nec class all_L1  =  0.052 +- 0.073
nec class all_KL  =  0.100 +- 0.139
nec_acc_int  =  0.556 +- 0.081


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.485 +- 0.004
Faith. Armon (L1)= 		  =  0.139 +- 0.007
Faith. GMean (L1)= 	  =  0.260 +- 0.008
Faith. Aritm (KL)= 		  =  0.447 +- 0.019
Faith. Armon (KL)= 		  =  0.198 +- 0.018
Faith. GMean (KL)= 	  =  0.297 +- 0.016

Eval split test
Faith. Aritm (L1)= 		  =  0.490 +- 0.014
Faith. Armon (L1)= 		  =  0.087 +- 0.122
Faith. GMean (L1)= 	  =  0.127 +- 0.158
Faith. Aritm (KL)= 		  =  0.483 +- 0.025
Faith. Armon (KL)= 		  =  0.134 +- 0.186
Faith. GMean (KL)= 	  =  0.159 +- 0.187
Computed for split load_split = id



Completed in  0:02:31.458082  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 19:59:40 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 07:59:40 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 08:00:15 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 08:00:25 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 08:00:37 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 08:00:54 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:01:10 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.8993
Train Loss: 0.3214
ID Validation ROC-AUC: 0.8837
ID Validation Loss: 0.3373
ID Test ROC-AUC: 0.8843
ID Test Loss: 0.3454
OOD Validation ROC-AUC: 0.6640
OOD Validation Loss: 0.3458
OOD Test ROC-AUC: 0.7067
OOD Test Loss: 0.5877

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ROC-AUC: 0.8741
Train Loss: 0.3395
ID Validation ROC-AUC: 0.8720
ID Validation Loss: 0.3474
ID Test ROC-AUC: 0.8700
ID Test Loss: 0.3479
OOD Validation ROC-AUC: 0.7037
OOD Validation Loss: 0.3123
OOD Test ROC-AUC: 0.7160
OOD Test Loss: 0.5638

[0m[1;37mINFO[0m: [1mChartInfo 0.8843 0.7067 0.8700 0.7160 0.8720 0.7037[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 08:01:11 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 08:01:21 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.398
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.436
SUFF++ for r=0.6 class 0.0 = 0.892 +- 0.023 (in-sample avg dev_std = 0.108)
SUFF++ for r=0.6 class 1.0 = 0.901 +- 0.023 (in-sample avg dev_std = 0.108)
SUFF++ for r=0.6 all KL = 0.983 +- 0.023 (in-sample avg dev_std = 0.108)
SUFF++ for r=0.6 all L1 = 0.9 +- 0.047 (in-sample avg dev_std = 0.108)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.468
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.473
SUFF++ for r=0.6 class 0.0 = 0.9 +- 0.030 (in-sample avg dev_std = 0.109)
SUFF++ for r=0.6 class 1.0 = 0.897 +- 0.030 (in-sample avg dev_std = 0.109)
SUFF++ for r=0.6 all KL = 0.982 +- 0.030 (in-sample avg dev_std = 0.109)
SUFF++ for r=0.6 all L1 = 0.898 +- 0.047 (in-sample avg dev_std = 0.109)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.397
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.383
NEC for r=0.6 class 0.0 = 0.025 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 class 1.0 = 0.025 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all L1 = 0.025 +- 0.018 (in-sample avg dev_std = 0.019)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.468
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.459
NEC for r=0.6 class 0.0 = 0.023 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 class 1.0 = 0.025 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all KL = 0.001 +- 0.003 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all L1 = 0.025 +- 0.018 (in-sample avg dev_std = 0.019)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 20:02:33 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 08:02:33 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 08:03:06 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 08:03:16 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 08:03:28 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 08:03:46 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 18...
[0m[1;37mINFO[0m: [1mCheckpoint 18: 
-----------------------------------
Train ROC-AUC: 0.9106
Train Loss: 0.2882
ID Validation ROC-AUC: 0.8956
ID Validation Loss: 0.3124
ID Test ROC-AUC: 0.8957
ID Test Loss: 0.3130
OOD Validation ROC-AUC: 0.6611
OOD Validation Loss: 0.3429
OOD Test ROC-AUC: 0.7016
OOD Test Loss: 0.5746

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ROC-AUC: 0.8841
Train Loss: 0.3105
ID Validation ROC-AUC: 0.8731
ID Validation Loss: 0.3250
ID Test ROC-AUC: 0.8715
ID Test Loss: 0.3287
OOD Validation ROC-AUC: 0.6962
OOD Validation Loss: 0.3128
OOD Test ROC-AUC: 0.7163
OOD Test Loss: 0.5402

[0m[1;37mINFO[0m: [1mChartInfo 0.8957 0.7016 0.8715 0.7163 0.8731 0.6962[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 08:04:03 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 08:04:12 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.352
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.434
SUFF++ for r=0.6 class 0.0 = 0.912 +- 0.028 (in-sample avg dev_std = 0.084)
SUFF++ for r=0.6 class 1.0 = 0.934 +- 0.028 (in-sample avg dev_std = 0.084)
SUFF++ for r=0.6 all KL = 0.99 +- 0.028 (in-sample avg dev_std = 0.084)
SUFF++ for r=0.6 all L1 = 0.931 +- 0.045 (in-sample avg dev_std = 0.084)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.437
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.473
SUFF++ for r=0.6 class 0.0 = 0.921 +- 0.029 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 class 1.0 = 0.927 +- 0.029 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 all KL = 0.99 +- 0.029 (in-sample avg dev_std = 0.080)
SUFF++ for r=0.6 all L1 = 0.926 +- 0.045 (in-sample avg dev_std = 0.080)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.353
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.348
NEC for r=0.6 class 0.0 = 0.024 +- 0.007 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.019 +- 0.007 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.001 +- 0.007 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.02 +- 0.019 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.437
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.438
NEC for r=0.6 class 0.0 = 0.023 +- 0.004 (in-sample avg dev_std = 0.018)
NEC for r=0.6 class 1.0 = 0.022 +- 0.004 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all KL = 0.001 +- 0.004 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all L1 = 0.022 +- 0.020 (in-sample avg dev_std = 0.018)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 20:05:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 08:05:21 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 08:05:55 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:07 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:18 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:34 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 69...
[0m[1;37mINFO[0m: [1mCheckpoint 69: 
-----------------------------------
Train ROC-AUC: 0.9695
Train Loss: 0.2138
ID Validation ROC-AUC: 0.9127
ID Validation Loss: 0.3345
ID Test ROC-AUC: 0.9168
ID Test Loss: 0.3404
OOD Validation ROC-AUC: 0.6338
OOD Validation Loss: 0.4146
OOD Test ROC-AUC: 0.6908
OOD Test Loss: 0.6400

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 5...
[0m[1;37mINFO[0m: [1mCheckpoint 5: 
-----------------------------------
Train ROC-AUC: 0.8656
Train Loss: 0.2766
ID Validation ROC-AUC: 0.8601
ID Validation Loss: 0.2825
ID Test ROC-AUC: 0.8664
ID Test Loss: 0.2815
OOD Validation ROC-AUC: 0.6911
OOD Validation Loss: 0.2740
OOD Test ROC-AUC: 0.7053
OOD Test Loss: 0.4338

[0m[1;37mINFO[0m: [1mChartInfo 0.9168 0.6908 0.8664 0.7053 0.8601 0.6911[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 08:06:49 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 08:06:58 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.784
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.691
SUFF++ for r=0.6 class 0.0 = 0.986 +- 0.013 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 class 1.0 = 0.995 +- 0.013 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 all KL = 0.997 +- 0.013 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 all L1 = 0.994 +- 0.024 (in-sample avg dev_std = 0.012)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.686
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.629
SUFF++ for r=0.6 class 0.0 = 0.978 +- 0.015 (in-sample avg dev_std = 0.014)
SUFF++ for r=0.6 class 1.0 = 0.992 +- 0.015 (in-sample avg dev_std = 0.014)
SUFF++ for r=0.6 all KL = 0.995 +- 0.015 (in-sample avg dev_std = 0.014)
SUFF++ for r=0.6 all L1 = 0.99 +- 0.031 (in-sample avg dev_std = 0.014)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.784
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.783
NEC for r=0.6 class 0.0 = 0.012 +- 0.011 (in-sample avg dev_std = 0.010)
NEC for r=0.6 class 1.0 = 0.004 +- 0.011 (in-sample avg dev_std = 0.010)
NEC for r=0.6 all KL = 0.001 +- 0.011 (in-sample avg dev_std = 0.010)
NEC for r=0.6 all L1 = 0.005 +- 0.024 (in-sample avg dev_std = 0.010)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.687
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.683
NEC for r=0.6 class 0.0 = 0.016 +- 0.011 (in-sample avg dev_std = 0.010)
NEC for r=0.6 class 1.0 = 0.006 +- 0.011 (in-sample avg dev_std = 0.010)
NEC for r=0.6 all KL = 0.002 +- 0.011 (in-sample avg dev_std = 0.010)
NEC for r=0.6 all L1 = 0.007 +- 0.026 (in-sample avg dev_std = 0.010)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.983], 'all_L1': [0.9]}), defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.931]}), defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.994]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.02]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.005]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.982], 'all_L1': [0.898]}), defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.926]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.99]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.022]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.007]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.942 +- 0.039
suff++ class all_KL  =  0.990 +- 0.006
suff++_acc_int  =  0.521 +- 0.121
nec class all_L1  =  0.017 +- 0.008
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.505 +- 0.197

Eval split test
suff++ class all_L1  =  0.938 +- 0.039
suff++ class all_KL  =  0.989 +- 0.005
suff++_acc_int  =  0.525 +- 0.074
nec class all_L1  =  0.018 +- 0.008
nec class all_KL  =  0.001 +- 0.000
nec_acc_int  =  0.527 +- 0.111


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.479 +- 0.015
Faith. Armon (L1)= 		  =  0.033 +- 0.016
Faith. GMean (L1)= 	  =  0.119 +- 0.035
Faith. Aritm (KL)= 		  =  0.495 +- 0.003
Faith. Armon (KL)= 		  =  0.002 +- 0.000
Faith. GMean (KL)= 	  =  0.031 +- 0.000

Eval split test
Faith. Aritm (L1)= 		  =  0.478 +- 0.015
Faith. Armon (L1)= 		  =  0.035 +- 0.015
Faith. GMean (L1)= 	  =  0.125 +- 0.030
Faith. Aritm (KL)= 		  =  0.495 +- 0.003
Faith. Armon (KL)= 		  =  0.003 +- 0.001
Faith. GMean (KL)= 	  =  0.036 +- 0.006
Computed for split load_split = id



Completed in  0:08:30.572219  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 20:08:23 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:23 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:08:24 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 46...
[0m[1;37mINFO[0m: [1mCheckpoint 46: 
-----------------------------------
Train ACCURACY: 0.3282
Train Loss: 2.3518
ID Validation ACCURACY: 0.3286
ID Validation Loss: 2.3614
ID Test ACCURACY: 0.3219
ID Test Loss: 2.3849
OOD Validation ACCURACY: 0.2846
OOD Validation Loss: 3.8053
OOD Test ACCURACY: 0.1441
OOD Test Loss: 9.3433

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 44...
[0m[1;37mINFO[0m: [1mCheckpoint 44: 
-----------------------------------
Train ACCURACY: 0.3185
Train Loss: 2.4273
ID Validation ACCURACY: 0.3143
ID Validation Loss: 2.4540
ID Test ACCURACY: 0.3124
ID Test Loss: 2.4743
OOD Validation ACCURACY: 0.3104
OOD Validation Loss: 2.7678
OOD Test ACCURACY: 0.1749
OOD Test Loss: 4.9128

[0m[1;37mINFO[0m: [1mChartInfo 0.3219 0.1441 0.3124 0.1749 0.3143 0.3104[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.319
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.241
SUFF++ for r=0.6 class 0 = 0.387 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 1 = 0.585 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 2 = 0.319 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 3 = 0.311 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 4 = 0.332 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 5 = 0.321 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 6 = 0.334 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 7 = 0.314 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 8 = 0.305 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 9 = 0.357 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 all KL = 0.195 +- 0.161 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 all L1 = 0.359 +- 0.120 (in-sample avg dev_std = 0.593)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.139
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.192
SUFF++ for r=0.6 class 0 = 0.23 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 1 = 0.8 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 2 = 0.262 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 3 = 0.306 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 4 = 0.388 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 5 = 0.318 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 6 = 0.341 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 7 = 0.345 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 8 = 0.349 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 class 9 = 0.48 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 all KL = 0.126 +- 0.250 (in-sample avg dev_std = 0.534)
SUFF++ for r=0.6 all L1 = 0.386 +- 0.244 (in-sample avg dev_std = 0.534)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.319
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.327
NEC for r=0.6 class 0 = 0.387 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 1 = 0.051 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 2 = 0.4 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 3 = 0.4 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 4 = 0.494 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 5 = 0.456 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 6 = 0.5 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 7 = 0.556 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 8 = 0.466 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 9 = 0.505 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 all KL = 0.335 +- 0.254 (in-sample avg dev_std = 0.212)
NEC for r=0.6 all L1 = 0.416 +- 0.204 (in-sample avg dev_std = 0.212)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.136
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.21
NEC for r=0.6 class 0 = 0.675 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 1 = 0.052 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 2 = 0.609 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 3 = 0.529 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 4 = 0.411 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 5 = 0.474 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 6 = 0.502 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 7 = 0.449 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 8 = 0.418 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 9 = 0.3 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all KL = 0.561 +- 0.399 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all L1 = 0.439 +- 0.335 (in-sample avg dev_std = 0.204)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 20:14:02 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:14:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ACCURACY: 0.2758
Train Loss: 2.1313
ID Validation ACCURACY: 0.2723
ID Validation Loss: 2.1460
ID Test ACCURACY: 0.2776
ID Test Loss: 2.1540
OOD Validation ACCURACY: 0.2164
OOD Validation Loss: 2.4175
OOD Test ACCURACY: 0.1419
OOD Test Loss: 3.0432

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.2489
Train Loss: 2.1703
ID Validation ACCURACY: 0.2416
ID Validation Loss: 2.1907
ID Test ACCURACY: 0.2473
ID Test Loss: 2.1887
OOD Validation ACCURACY: 0.2927
OOD Validation Loss: 2.0351
OOD Test ACCURACY: 0.1840
OOD Test Loss: 2.6631

[0m[1;37mINFO[0m: [1mChartInfo 0.2776 0.1419 0.2473 0.1840 0.2416 0.2927[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.27
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.185
SUFF++ for r=0.6 class 0 = 0.548 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 1 = 0.348 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 2 = 0.556 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 3 = 0.564 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 4 = 0.531 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 5 = 0.547 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 6 = 0.523 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 7 = 0.526 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 8 = 0.557 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 class 9 = 0.51 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 all KL = 0.622 +- 0.189 (in-sample avg dev_std = 0.245)
SUFF++ for r=0.6 all L1 = 0.519 +- 0.095 (in-sample avg dev_std = 0.245)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.139
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.19
SUFF++ for r=0.6 class 0 = 0.497 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 1 = 0.423 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 2 = 0.462 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 3 = 0.456 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 4 = 0.4 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 5 = 0.464 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 6 = 0.412 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 7 = 0.391 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 8 = 0.447 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 class 9 = 0.404 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 all KL = 0.47 +- 0.204 (in-sample avg dev_std = 0.164)
SUFF++ for r=0.6 all L1 = 0.436 +- 0.099 (in-sample avg dev_std = 0.164)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.273
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.268
NEC for r=0.6 class 0 = 0.355 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 1 = 0.542 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 2 = 0.324 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 3 = 0.307 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 4 = 0.356 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 5 = 0.345 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 6 = 0.365 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 7 = 0.374 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 8 = 0.335 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 class 9 = 0.388 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 all KL = 0.215 +- 0.166 (in-sample avg dev_std = 0.118)
NEC for r=0.6 all L1 = 0.371 +- 0.130 (in-sample avg dev_std = 0.118)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.14
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.211
NEC for r=0.6 class 0 = 0.482 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 1 = 0.5 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 2 = 0.525 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 3 = 0.507 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 4 = 0.57 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 5 = 0.504 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 6 = 0.561 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 7 = 0.589 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 8 = 0.531 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 class 9 = 0.561 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 all KL = 0.432 +- 0.171 (in-sample avg dev_std = 0.099)
NEC for r=0.6 all L1 = 0.532 +- 0.115 (in-sample avg dev_std = 0.099)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 20:19:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 08:19:46 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 08:19:47 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.3600
Train Loss: 1.8524
ID Validation ACCURACY: 0.3517
ID Validation Loss: 1.8640
ID Test ACCURACY: 0.3516
ID Test Loss: 1.8567
OOD Validation ACCURACY: 0.3029
OOD Validation Loss: 2.2550
OOD Test ACCURACY: 0.1203
OOD Test Loss: 3.8776

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.3600
Train Loss: 1.8524
ID Validation ACCURACY: 0.3517
ID Validation Loss: 1.8640
ID Test ACCURACY: 0.3516
ID Test Loss: 1.8567
OOD Validation ACCURACY: 0.3029
OOD Validation Loss: 2.2550
OOD Test ACCURACY: 0.1203
OOD Test Loss: 3.8776

[0m[1;37mINFO[0m: [1mChartInfo 0.3516 0.1203 0.3516 0.1203 0.3517 0.3029[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.341
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.246
SUFF++ for r=0.6 class 0 = 0.584 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 1 = 0.404 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 2 = 0.344 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 3 = 0.316 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 4 = 0.377 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 5 = 0.36 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 6 = 0.365 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 7 = 0.374 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 8 = 0.316 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 class 9 = 0.393 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 all KL = 0.309 +- 0.218 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.6 all L1 = 0.383 +- 0.121 (in-sample avg dev_std = 0.524)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.108
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.173
SUFF++ for r=0.6 class 0 = 0.357 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 1 = 0.379 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 2 = 0.43 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 3 = 0.387 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 4 = 0.446 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 5 = 0.335 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 6 = 0.456 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 7 = 0.375 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 8 = 0.389 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 class 9 = 0.409 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 all KL = 0.318 +- 0.259 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.6 all L1 = 0.397 +- 0.183 (in-sample avg dev_std = 0.487)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.343
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.327
NEC for r=0.6 class 0 = 0.321 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 1 = 0.292 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 2 = 0.415 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 3 = 0.435 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 4 = 0.479 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 5 = 0.443 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 6 = 0.477 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 7 = 0.481 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 8 = 0.47 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 class 9 = 0.462 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 all KL = 0.304 +- 0.193 (in-sample avg dev_std = 0.212)
NEC for r=0.6 all L1 = 0.425 +- 0.153 (in-sample avg dev_std = 0.212)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.109
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.178
NEC for r=0.6 class 0 = 0.566 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 1 = 0.44 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 2 = 0.44 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 3 = 0.489 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 4 = 0.416 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 5 = 0.514 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 6 = 0.421 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 7 = 0.543 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 8 = 0.471 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 9 = 0.432 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 all KL = 0.417 +- 0.263 (in-sample avg dev_std = 0.270)
NEC for r=0.6 all L1 = 0.473 +- 0.218 (in-sample avg dev_std = 0.270)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.195], 'all_L1': [0.359]}), defaultdict(<class 'list'>, {'all_KL': [0.622], 'all_L1': [0.519]}), defaultdict(<class 'list'>, {'all_KL': [0.309], 'all_L1': [0.383]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.335], 'all_L1': [0.416]}), defaultdict(<class 'list'>, {'all_KL': [0.215], 'all_L1': [0.371]}), defaultdict(<class 'list'>, {'all_KL': [0.304], 'all_L1': [0.425]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.126], 'all_L1': [0.386]}), defaultdict(<class 'list'>, {'all_KL': [0.47], 'all_L1': [0.436]}), defaultdict(<class 'list'>, {'all_KL': [0.318], 'all_L1': [0.397]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.561], 'all_L1': [0.439]}), defaultdict(<class 'list'>, {'all_KL': [0.432], 'all_L1': [0.532]}), defaultdict(<class 'list'>, {'all_KL': [0.417], 'all_L1': [0.473]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.420 +- 0.070
suff++ class all_KL  =  0.375 +- 0.181
suff++_acc_int  =  0.224 +- 0.028
nec class all_L1  =  0.404 +- 0.024
nec class all_KL  =  0.285 +- 0.051
nec_acc_int  =  0.307 +- 0.028

Eval split test
suff++ class all_L1  =  0.406 +- 0.021
suff++ class all_KL  =  0.305 +- 0.141
suff++_acc_int  =  0.185 +- 0.009
nec class all_L1  =  0.481 +- 0.038
nec class all_KL  =  0.470 +- 0.065
nec_acc_int  =  0.200 +- 0.015


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.412 +- 0.024
Faith. Armon (L1)= 		  =  0.407 +- 0.020
Faith. GMean (L1)= 	  =  0.410 +- 0.022
Faith. Aritm (KL)= 		  =  0.330 +- 0.065
Faith. Armon (KL)= 		  =  0.291 +- 0.032
Faith. GMean (KL)= 	  =  0.309 +- 0.045

Eval split test
Faith. Aritm (L1)= 		  =  0.444 +- 0.030
Faith. Armon (L1)= 		  =  0.441 +- 0.029
Faith. GMean (L1)= 	  =  0.442 +- 0.029
Faith. Aritm (KL)= 		  =  0.387 +- 0.046
Faith. Armon (KL)= 		  =  0.339 +- 0.101
Faith. GMean (KL)= 	  =  0.360 +- 0.075
Computed for split load_split = id



Completed in  0:17:06.684991  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color
DONE all :)
