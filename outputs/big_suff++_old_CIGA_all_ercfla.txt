Time to compute metrics!
The PID of this script is: 2316893

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:02:12 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:12 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 133...
[0m[1;37mINFO[0m: [1mCheckpoint 133: 
-----------------------------------
Train ACCURACY: 0.4515
Train Loss: 2.0116
ID Validation ACCURACY: 0.4543
ID Validation Loss: 2.0095
ID Test ACCURACY: 0.4537
ID Test Loss: 1.9627
OOD Validation ACCURACY: 0.3667
OOD Validation Loss: 1.7075
OOD Test ACCURACY: 0.4907
OOD Test Loss: 20.5069

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.3873
Train Loss: 2.8169
ID Validation ACCURACY: 0.4017
ID Validation Loss: 2.8787
ID Test ACCURACY: 0.3987
ID Test Loss: 2.7126
OOD Validation ACCURACY: 0.4830
OOD Validation Loss: 1.4196
OOD Test ACCURACY: 0.3533
OOD Test Loss: 4.9358

[0m[1;37mINFO[0m: [1mChartInfo 0.4537 0.4907 0.3987 0.3533 0.4017 0.4830[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.308
WIoU for r=0.8 = 0.207


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.498
Model XAI F1 of binarized graphs for r=0.8 =  0.30768375
Model XAI WIoU of binarized graphs for r=0.8 =  0.20650500000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.405
SUFF++ for r=0.8 class 0 = 0.407 +- 0.305 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.8 class 1 = 0.648 +- 0.305 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.8 class 2 = 0.512 +- 0.305 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.8 all KL = 0.466 +- 0.305 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.8 all L1 = 0.525 +- 0.222 (in-sample avg dev_std = 0.556)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.498
Model XAI F1 of binarized graphs for r=0.8 =  0.30768375
Model XAI WIoU of binarized graphs for r=0.8 =  0.20650500000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.401
NEC for r=0.8 class 0 = 0.546 +- 0.292 (in-sample avg dev_std = 0.501)
NEC for r=0.8 class 1 = 0.397 +- 0.292 (in-sample avg dev_std = 0.501)
NEC for r=0.8 class 2 = 0.511 +- 0.292 (in-sample avg dev_std = 0.501)
NEC for r=0.8 all KL = 0.536 +- 0.292 (in-sample avg dev_std = 0.501)
NEC for r=0.8 all L1 = 0.483 +- 0.187 (in-sample avg dev_std = 0.501)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:02:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:02:37 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 0.5550
Train Loss: 1.0486
ID Validation ACCURACY: 0.5667
ID Validation Loss: 1.0182
ID Test ACCURACY: 0.5497
ID Test Loss: 1.0586
OOD Validation ACCURACY: 0.2923
OOD Validation Loss: 1.8056
OOD Test ACCURACY: 0.3420
OOD Test Loss: 2.4359

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 156...
[0m[1;37mINFO[0m: [1mCheckpoint 156: 
-----------------------------------
Train ACCURACY: 0.3756
Train Loss: 1.3215
ID Validation ACCURACY: 0.3880
ID Validation Loss: 1.2841
ID Test ACCURACY: 0.3630
ID Test Loss: 1.3120
OOD Validation ACCURACY: 0.4910
OOD Validation Loss: 1.0719
OOD Test ACCURACY: 0.4573
OOD Test Loss: 1.3658

[0m[1;37mINFO[0m: [1mChartInfo 0.5497 0.3420 0.3630 0.4573 0.3880 0.4910[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.257
WIoU for r=0.8 = 0.177


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.343
Model XAI F1 of binarized graphs for r=0.8 =  0.2569825
Model XAI WIoU of binarized graphs for r=0.8 =  0.1766075
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.345
SUFF++ for r=0.8 class 0 = 0.828 +- 0.162 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.8 class 1 = 0.787 +- 0.162 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.8 class 2 = 0.797 +- 0.162 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.8 all KL = 0.869 +- 0.162 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.8 all L1 = 0.803 +- 0.173 (in-sample avg dev_std = 0.229)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.343
Model XAI F1 of binarized graphs for r=0.8 =  0.2569825
Model XAI WIoU of binarized graphs for r=0.8 =  0.1766075
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.351
NEC for r=0.8 class 0 = 0.159 +- 0.105 (in-sample avg dev_std = 0.221)
NEC for r=0.8 class 1 = 0.13 +- 0.105 (in-sample avg dev_std = 0.221)
NEC for r=0.8 class 2 = 0.208 +- 0.105 (in-sample avg dev_std = 0.221)
NEC for r=0.8 all KL = 0.107 +- 0.105 (in-sample avg dev_std = 0.221)
NEC for r=0.8 all L1 = 0.165 +- 0.121 (in-sample avg dev_std = 0.221)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:03:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:00 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.3922
Train Loss: 2.5243
ID Validation ACCURACY: 0.4000
ID Validation Loss: 2.6128
ID Test ACCURACY: 0.4023
ID Test Loss: 2.4780
OOD Validation ACCURACY: 0.3473
OOD Validation Loss: 1.5777
OOD Test ACCURACY: 0.4613
OOD Test Loss: 12.1111

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 105...
[0m[1;37mINFO[0m: [1mCheckpoint 105: 
-----------------------------------
Train ACCURACY: 0.3601
Train Loss: 2.8396
ID Validation ACCURACY: 0.3580
ID Validation Loss: 2.9725
ID Test ACCURACY: 0.3643
ID Test Loss: 2.7594
OOD Validation ACCURACY: 0.4673
OOD Validation Loss: 1.2057
OOD Test ACCURACY: 0.3347
OOD Test Loss: 10.3926

[0m[1;37mINFO[0m: [1mChartInfo 0.4023 0.4613 0.3643 0.3347 0.3580 0.4673[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.311
WIoU for r=0.8 = 0.212


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.454
Model XAI F1 of binarized graphs for r=0.8 =  0.31054375
Model XAI WIoU of binarized graphs for r=0.8 =  0.21237000000000003
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.381
SUFF++ for r=0.8 class 0 = 0.794 +- 0.141 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.8 class 1 = 0.852 +- 0.141 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.8 class 2 = 0.844 +- 0.141 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.8 all KL = 0.901 +- 0.141 (in-sample avg dev_std = 0.168)
SUFF++ for r=0.8 all L1 = 0.83 +- 0.204 (in-sample avg dev_std = 0.168)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.454
Model XAI F1 of binarized graphs for r=0.8 =  0.31054375
Model XAI WIoU of binarized graphs for r=0.8 =  0.21237000000000003
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.283
NEC for r=0.8 class 0 = 0.217 +- 0.297 (in-sample avg dev_std = 0.339)
NEC for r=0.8 class 1 = 0.295 +- 0.297 (in-sample avg dev_std = 0.339)
NEC for r=0.8 class 2 = 0.186 +- 0.297 (in-sample avg dev_std = 0.339)
NEC for r=0.8 all KL = 0.231 +- 0.297 (in-sample avg dev_std = 0.339)
NEC for r=0.8 all L1 = 0.233 +- 0.268 (in-sample avg dev_std = 0.339)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:03:23 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:23 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 191...
[0m[1;37mINFO[0m: [1mCheckpoint 191: 
-----------------------------------
Train ACCURACY: 0.4899
Train Loss: 2.0486
ID Validation ACCURACY: 0.4887
ID Validation Loss: 2.1187
ID Test ACCURACY: 0.4963
ID Test Loss: 1.9416
OOD Validation ACCURACY: 0.3943
OOD Validation Loss: 1.1884
OOD Test ACCURACY: 0.4750
OOD Test Loss: 32.6168

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 43...
[0m[1;37mINFO[0m: [1mCheckpoint 43: 
-----------------------------------
Train ACCURACY: 0.3959
Train Loss: 2.6359
ID Validation ACCURACY: 0.4100
ID Validation Loss: 2.6788
ID Test ACCURACY: 0.4033
ID Test Loss: 2.6084
OOD Validation ACCURACY: 0.5240
OOD Validation Loss: 1.4202
OOD Test ACCURACY: 0.4643
OOD Test Loss: 2.3969

[0m[1;37mINFO[0m: [1mChartInfo 0.4963 0.4750 0.4033 0.4643 0.4100 0.5240[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.323
WIoU for r=0.8 = 0.219


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.472
Model XAI F1 of binarized graphs for r=0.8 =  0.32297
Model XAI WIoU of binarized graphs for r=0.8 =  0.21888
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.383
SUFF++ for r=0.8 class 0 = 0.827 +- 0.201 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 class 1 = 0.852 +- 0.201 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 class 2 = 0.769 +- 0.201 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 all KL = 0.849 +- 0.201 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 all L1 = 0.816 +- 0.234 (in-sample avg dev_std = 0.273)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.472
Model XAI F1 of binarized graphs for r=0.8 =  0.32297
Model XAI WIoU of binarized graphs for r=0.8 =  0.21888
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.325
NEC for r=0.8 class 0 = 0.258 +- 0.358 (in-sample avg dev_std = 0.361)
NEC for r=0.8 class 1 = 0.278 +- 0.358 (in-sample avg dev_std = 0.361)
NEC for r=0.8 class 2 = 0.27 +- 0.358 (in-sample avg dev_std = 0.361)
NEC for r=0.8 all KL = 0.275 +- 0.358 (in-sample avg dev_std = 0.361)
NEC for r=0.8 all L1 = 0.269 +- 0.311 (in-sample avg dev_std = 0.361)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:03:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:03:45 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 42...
[0m[1;37mINFO[0m: [1mCheckpoint 42: 
-----------------------------------
Train ACCURACY: 0.5243
Train Loss: 1.6300
ID Validation ACCURACY: 0.5290
ID Validation Loss: 1.6217
ID Test ACCURACY: 0.5153
ID Test Loss: 1.6486
OOD Validation ACCURACY: 0.3360
OOD Validation Loss: 2.9312
OOD Test ACCURACY: 0.5983
OOD Test Loss: 1.1300

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.3722
Train Loss: 2.1560
ID Validation ACCURACY: 0.3763
ID Validation Loss: 2.0934
ID Test ACCURACY: 0.3590
ID Test Loss: 2.1927
OOD Validation ACCURACY: 0.4270
OOD Validation Loss: 1.7656
OOD Test ACCURACY: 0.5453
OOD Test Loss: 1.2780

[0m[1;37mINFO[0m: [1mChartInfo 0.5153 0.5983 0.3590 0.5453 0.3763 0.4270[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.251
WIoU for r=0.8 = 0.181


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.591
Model XAI F1 of binarized graphs for r=0.8 =  0.25069874999999997
Model XAI WIoU of binarized graphs for r=0.8 =  0.18060125000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.322
SUFF++ for r=0.8 class 0 = 0.722 +- 0.261 (in-sample avg dev_std = 0.276)
SUFF++ for r=0.8 class 1 = 0.633 +- 0.261 (in-sample avg dev_std = 0.276)
SUFF++ for r=0.8 class 2 = 0.477 +- 0.261 (in-sample avg dev_std = 0.276)
SUFF++ for r=0.8 all KL = 0.718 +- 0.261 (in-sample avg dev_std = 0.276)
SUFF++ for r=0.8 all L1 = 0.61 +- 0.183 (in-sample avg dev_std = 0.276)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.591
Model XAI F1 of binarized graphs for r=0.8 =  0.25069874999999997
Model XAI WIoU of binarized graphs for r=0.8 =  0.18060125000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.455
NEC for r=0.8 class 0 = 0.222 +- 0.253 (in-sample avg dev_std = 0.291)
NEC for r=0.8 class 1 = 0.22 +- 0.253 (in-sample avg dev_std = 0.291)
NEC for r=0.8 class 2 = 0.467 +- 0.253 (in-sample avg dev_std = 0.291)
NEC for r=0.8 all KL = 0.206 +- 0.253 (in-sample avg dev_std = 0.291)
NEC for r=0.8 all L1 = 0.303 +- 0.192 (in-sample avg dev_std = 0.291)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.466], 'all_L1': [0.525]}), defaultdict(<class 'list'>, {'all_KL': [0.869], 'all_L1': [0.803]}), defaultdict(<class 'list'>, {'all_KL': [0.901], 'all_L1': [0.83]}), defaultdict(<class 'list'>, {'all_KL': [0.849], 'all_L1': [0.816]}), defaultdict(<class 'list'>, {'all_KL': [0.718], 'all_L1': [0.61]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.536], 'all_L1': [0.483]}), defaultdict(<class 'list'>, {'all_KL': [0.107], 'all_L1': [0.165]}), defaultdict(<class 'list'>, {'all_KL': [0.231], 'all_L1': [0.233]}), defaultdict(<class 'list'>, {'all_KL': [0.275], 'all_L1': [0.269]}), defaultdict(<class 'list'>, {'all_KL': [0.206], 'all_L1': [0.303]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.717 +- 0.125
suff++ class all_KL  =  0.761 +- 0.160
suff++_acc_int  =  0.367 +- 0.030
nec class all_L1  =  0.291 +- 0.107
nec class all_KL  =  0.271 +- 0.144
nec_acc_int  =  0.363 +- 0.060


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.504 +- 0.031
Faith. Armon (L1)= 		  =  0.390 +- 0.074
Faith. GMean (L1)= 	  =  0.441 +- 0.046
Faith. Aritm (KL)= 		  =  0.516 +- 0.041
Faith. Armon (KL)= 		  =  0.358 +- 0.103
Faith. GMean (KL)= 	  =  0.426 +- 0.072
Computed for split load_split = id



Completed in  0:01:57.691953  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis all mitig

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:04:23 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:04:24 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 98...
[0m[1;37mINFO[0m: [1mCheckpoint 98: 
-----------------------------------
Train ACCURACY: 0.2955
Train Loss: 2.1309
ID Validation ACCURACY: 0.2990
ID Validation Loss: 2.1134
ID Test ACCURACY: 0.2890
ID Test Loss: 2.1875
OOD Validation ACCURACY: 0.2706
OOD Validation Loss: 2.4837
OOD Test ACCURACY: 0.2086
OOD Test Loss: 4.6387

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.2797
Train Loss: 2.1018
ID Validation ACCURACY: 0.2841
ID Validation Loss: 2.0829
ID Test ACCURACY: 0.2744
ID Test Loss: 2.1502
OOD Validation ACCURACY: 0.2713
OOD Validation Loss: 2.2479
OOD Test ACCURACY: 0.1836
OOD Test Loss: 3.7387

[0m[1;37mINFO[0m: [1mChartInfo 0.2890 0.2086 0.2744 0.1836 0.2841 0.2713[0mGOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.192
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.169
SUFF++ for r=0.6 class 0 = 0.378 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 1 = 0.427 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 2 = 0.465 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 3 = 0.451 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 4 = 0.507 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 5 = 0.459 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 6 = 0.489 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 7 = 0.482 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 8 = 0.453 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 class 9 = 0.502 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 all KL = 0.466 +- 0.177 (in-sample avg dev_std = 0.503)
SUFF++ for r=0.6 all L1 = 0.46 +- 0.104 (in-sample avg dev_std = 0.503)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.198
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.277
NEC for r=0.6 class 0 = 0.622 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 1 = 0.537 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 2 = 0.589 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 3 = 0.588 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 4 = 0.619 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 5 = 0.598 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 6 = 0.618 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 7 = 0.641 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 8 = 0.61 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 9 = 0.608 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all KL = 0.673 +- 0.218 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all L1 = 0.602 +- 0.137 (in-sample avg dev_std = 0.214)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:06:57 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:58 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:06:59 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.4261
Train Loss: 1.5957
ID Validation ACCURACY: 0.4141
ID Validation Loss: 1.6418
ID Test ACCURACY: 0.4197
ID Test Loss: 1.6257
OOD Validation ACCURACY: 0.3443
OOD Validation Loss: 2.1405
OOD Test ACCURACY: 0.2526
OOD Test Loss: 2.5709

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 98...
[0m[1;37mINFO[0m: [1mCheckpoint 98: 
-----------------------------------
Train ACCURACY: 0.3952
Train Loss: 1.7198
ID Validation ACCURACY: 0.3889
ID Validation Loss: 1.7556
ID Test ACCURACY: 0.3903
ID Test Loss: 1.7504
OOD Validation ACCURACY: 0.3587
OOD Validation Loss: 2.0597
OOD Test ACCURACY: 0.2260
OOD Test Loss: 2.7268

[0m[1;37mINFO[0m: [1mChartInfo 0.4197 0.2526 0.3903 0.2260 0.3889 0.3587[0mGOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.254
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.23
SUFF++ for r=0.6 class 0 = 0.433 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 1 = 0.467 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 2 = 0.443 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 3 = 0.449 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 4 = 0.464 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 5 = 0.45 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 6 = 0.436 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 7 = 0.467 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 8 = 0.46 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 class 9 = 0.418 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 all KL = 0.452 +- 0.217 (in-sample avg dev_std = 0.454)
SUFF++ for r=0.6 all L1 = 0.449 +- 0.131 (in-sample avg dev_std = 0.454)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.244
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.266
NEC for r=0.6 class 0 = 0.569 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 1 = 0.343 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 2 = 0.552 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 3 = 0.517 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 4 = 0.484 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 5 = 0.565 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 6 = 0.554 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 7 = 0.512 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 8 = 0.566 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 9 = 0.488 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 all KL = 0.443 +- 0.229 (in-sample avg dev_std = 0.264)
NEC for r=0.6 all L1 = 0.513 +- 0.182 (in-sample avg dev_std = 0.264)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:09:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:36 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:09:37 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 96...
[0m[1;37mINFO[0m: [1mCheckpoint 96: 
-----------------------------------
Train ACCURACY: 0.3742
Train Loss: 1.7882
ID Validation ACCURACY: 0.3836
ID Validation Loss: 1.7873
ID Test ACCURACY: 0.3733
ID Test Loss: 1.8005
OOD Validation ACCURACY: 0.3237
OOD Validation Loss: 2.1818
OOD Test ACCURACY: 0.1913
OOD Test Loss: 3.2680

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.3845
Train Loss: 1.7577
ID Validation ACCURACY: 0.3803
ID Validation Loss: 1.7604
ID Test ACCURACY: 0.3764
ID Test Loss: 1.7715
OOD Validation ACCURACY: 0.3377
OOD Validation Loss: 2.0441
OOD Test ACCURACY: 0.1530
OOD Test Loss: 4.8804

[0m[1;37mINFO[0m: [1mChartInfo 0.3733 0.1913 0.3764 0.1530 0.3803 0.3377[0mGOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.183
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.163
SUFF++ for r=0.6 class 0 = 0.383 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 1 = 0.627 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 2 = 0.466 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 3 = 0.448 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 4 = 0.475 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 5 = 0.473 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 6 = 0.466 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 7 = 0.524 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 8 = 0.471 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 class 9 = 0.491 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 all KL = 0.47 +- 0.222 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.6 all L1 = 0.484 +- 0.173 (in-sample avg dev_std = 0.449)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.177
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.28
NEC for r=0.6 class 0 = 0.634 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 1 = 0.286 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 2 = 0.638 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 3 = 0.604 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 4 = 0.582 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 5 = 0.601 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 6 = 0.589 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 7 = 0.606 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 8 = 0.584 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 class 9 = 0.539 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 all KL = 0.558 +- 0.255 (in-sample avg dev_std = 0.277)
NEC for r=0.6 all L1 = 0.563 +- 0.206 (in-sample avg dev_std = 0.277)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:12:10 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:11 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:12:12 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 88...
[0m[1;37mINFO[0m: [1mCheckpoint 88: 
-----------------------------------
Train ACCURACY: 0.4318
Train Loss: 1.6191
ID Validation ACCURACY: 0.4249
ID Validation Loss: 1.6400
ID Test ACCURACY: 0.4277
ID Test Loss: 1.6411
OOD Validation ACCURACY: 0.3363
OOD Validation Loss: 2.1037
OOD Test ACCURACY: 0.2310
OOD Test Loss: 2.6250

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 83...
[0m[1;37mINFO[0m: [1mCheckpoint 83: 
-----------------------------------
Train ACCURACY: 0.4277
Train Loss: 1.6056
ID Validation ACCURACY: 0.4169
ID Validation Loss: 1.6171
ID Test ACCURACY: 0.4223
ID Test Loss: 1.6322
OOD Validation ACCURACY: 0.3510
OOD Validation Loss: 1.9996
OOD Test ACCURACY: 0.2556
OOD Test Loss: 2.4187

[0m[1;37mINFO[0m: [1mChartInfo 0.4277 0.2310 0.4223 0.2556 0.4169 0.3510[0mGOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.228
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.206
SUFF++ for r=0.6 class 0 = 0.428 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 1 = 0.401 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 2 = 0.44 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 3 = 0.463 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 4 = 0.427 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 5 = 0.455 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 6 = 0.426 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 7 = 0.461 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 8 = 0.46 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 9 = 0.39 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 all KL = 0.443 +- 0.203 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 all L1 = 0.435 +- 0.120 (in-sample avg dev_std = 0.490)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.224
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.257
NEC for r=0.6 class 0 = 0.569 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 1 = 0.358 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 2 = 0.519 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 3 = 0.494 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 4 = 0.489 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 5 = 0.515 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 6 = 0.533 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 7 = 0.516 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 8 = 0.514 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 class 9 = 0.509 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 all KL = 0.407 +- 0.228 (in-sample avg dev_std = 0.260)
NEC for r=0.6 all L1 = 0.5 +- 0.176 (in-sample avg dev_std = 0.260)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:14:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:46 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:46 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:46 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:46 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing  5  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:14:47 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.4111
Train Loss: 1.6693
ID Validation ACCURACY: 0.4090
ID Validation Loss: 1.6845
ID Test ACCURACY: 0.4119
ID Test Loss: 1.6805
OOD Validation ACCURACY: 0.3519
OOD Validation Loss: 2.0506
OOD Test ACCURACY: 0.2664
OOD Test Loss: 2.5518

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.4111
Train Loss: 1.6693
ID Validation ACCURACY: 0.4090
ID Validation Loss: 1.6845
ID Test ACCURACY: 0.4119
ID Test Loss: 1.6805
OOD Validation ACCURACY: 0.3519
OOD Validation Loss: 2.0506
OOD Test ACCURACY: 0.2664
OOD Test Loss: 2.5518

[0m[1;37mINFO[0m: [1mChartInfo 0.4119 0.2664 0.4119 0.2664 0.4090 0.3519[0mGOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.274
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.244
SUFF++ for r=0.6 class 0 = 0.465 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 1 = 0.797 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 2 = 0.521 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 3 = 0.495 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 4 = 0.504 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 5 = 0.498 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 6 = 0.505 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 7 = 0.483 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 8 = 0.491 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 class 9 = 0.501 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 all KL = 0.575 +- 0.192 (in-sample avg dev_std = 0.412)
SUFF++ for r=0.6 all L1 = 0.529 +- 0.145 (in-sample avg dev_std = 0.412)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.275
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.312
NEC for r=0.6 class 0 = 0.619 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 1 = 0.114 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 2 = 0.559 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 3 = 0.528 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 4 = 0.501 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 5 = 0.569 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 6 = 0.505 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 7 = 0.479 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 8 = 0.558 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 class 9 = 0.484 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 all KL = 0.434 +- 0.256 (in-sample avg dev_std = 0.259)
NEC for r=0.6 all L1 = 0.487 +- 0.201 (in-sample avg dev_std = 0.259)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.466], 'all_L1': [0.46]}), defaultdict(<class 'list'>, {'all_KL': [0.452], 'all_L1': [0.449]}), defaultdict(<class 'list'>, {'all_KL': [0.47], 'all_L1': [0.484]}), defaultdict(<class 'list'>, {'all_KL': [0.443], 'all_L1': [0.435]}), defaultdict(<class 'list'>, {'all_KL': [0.575], 'all_L1': [0.529]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.673], 'all_L1': [0.602]}), defaultdict(<class 'list'>, {'all_KL': [0.443], 'all_L1': [0.513]}), defaultdict(<class 'list'>, {'all_KL': [0.558], 'all_L1': [0.563]}), defaultdict(<class 'list'>, {'all_KL': [0.407], 'all_L1': [0.5]}), defaultdict(<class 'list'>, {'all_KL': [0.434], 'all_L1': [0.487]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.471 +- 0.033
suff++ class all_KL  =  0.481 +- 0.048
suff++_acc_int  =  0.202 +- 0.032
nec class all_L1  =  0.533 +- 0.043
nec class all_KL  =  0.503 +- 0.100
nec_acc_int  =  0.278 +- 0.019


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.502 +- 0.024
Faith. Armon (L1)= 		  =  0.499 +- 0.023
Faith. GMean (L1)= 	  =  0.500 +- 0.024
Faith. Aritm (KL)= 		  =  0.492 +- 0.051
Faith. Armon (KL)= 		  =  0.485 +- 0.045
Faith. GMean (KL)= 	  =  0.489 +- 0.048
Computed for split load_split = id



Completed in  0:13:00.210520  for CIGAGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color all mitig

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:17:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/18/2024 12:17:35 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/18/2024 12:18:08 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/18/2024 12:18:18 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/18/2024 12:18:29 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/18/2024 12:18:46 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:02 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 150...
[0m[1;37mINFO[0m: [1mCheckpoint 150: 
-----------------------------------
Train ROC-AUC: 0.9480
Train Loss: 0.3695
ID Validation ROC-AUC: 0.9027
ID Validation Loss: 0.4699
ID Test ROC-AUC: 0.9084
ID Test Loss: 0.4696
OOD Validation ROC-AUC: 0.5843
OOD Validation Loss: 0.5387
OOD Test ROC-AUC: 0.6552
OOD Test Loss: 0.8904

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ROC-AUC: 0.9099
Train Loss: 0.2545
ID Validation ROC-AUC: 0.8783
ID Validation Loss: 0.2915
ID Test ROC-AUC: 0.8887
ID Test Loss: 0.2844
OOD Validation ROC-AUC: 0.6509
OOD Validation Loss: 0.3384
OOD Test ROC-AUC: 0.6766
OOD Test Loss: 0.5363

[0m[1;37mINFO[0m: [1mChartInfo 0.9084 0.6552 0.8887 0.6766 0.8783 0.6509[0mLBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/18/2024 12:19:03 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.544
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.541
SUFF++ for r=0.6 class 0.0 = 0.753 +- 0.143 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.6 class 1.0 = 0.741 +- 0.143 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.6 all KL = 0.863 +- 0.143 (in-sample avg dev_std = 0.229)
SUFF++ for r=0.6 all L1 = 0.743 +- 0.161 (in-sample avg dev_std = 0.229)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.511
NEC for r=0.6 class 0.0 = 0.222 +- 0.124 (in-sample avg dev_std = 0.195)
NEC for r=0.6 class 1.0 = 0.251 +- 0.124 (in-sample avg dev_std = 0.195)
NEC for r=0.6 all KL = 0.121 +- 0.124 (in-sample avg dev_std = 0.195)
NEC for r=0.6 all L1 = 0.246 +- 0.167 (in-sample avg dev_std = 0.195)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:19:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/18/2024 12:19:37 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/18/2024 12:20:10 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/18/2024 12:20:20 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/18/2024 12:20:31 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/18/2024 12:20:48 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:04 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:05 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 150...
[0m[1;37mINFO[0m: [1mCheckpoint 150: 
-----------------------------------
Train ROC-AUC: 0.9689
Train Loss: 0.1480
ID Validation ROC-AUC: 0.9058
ID Validation Loss: 0.2470
ID Test ROC-AUC: 0.9023
ID Test Loss: 0.2550
OOD Validation ROC-AUC: 0.6055
OOD Validation Loss: 0.6083
OOD Test ROC-AUC: 0.6627
OOD Test Loss: 0.6582

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 10...
[0m[1;37mINFO[0m: [1mCheckpoint 10: 
-----------------------------------
Train ROC-AUC: 0.8477
Train Loss: 0.3118
ID Validation ROC-AUC: 0.8262
ID Validation Loss: 0.3219
ID Test ROC-AUC: 0.8375
ID Test Loss: 0.3183
OOD Validation ROC-AUC: 0.6697
OOD Validation Loss: 0.4002
OOD Test ROC-AUC: 0.6582
OOD Test Loss: 0.4850

[0m[1;37mINFO[0m: [1mChartInfo 0.9023 0.6627 0.8375 0.6582 0.8262 0.6697[0mLBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/18/2024 12:21:05 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.517
SUFF++ for r=0.6 class 0.0 = 1.0 +- 0.049 (in-sample avg dev_std = 0.027)
SUFF++ for r=0.6 class 1.0 = 1.0 +- 0.049 (in-sample avg dev_std = 0.027)
SUFF++ for r=0.6 all KL = 0.998 +- 0.049 (in-sample avg dev_std = 0.027)
SUFF++ for r=0.6 all L1 = 1.0 +- 0.009 (in-sample avg dev_std = 0.027)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.512
NEC for r=0.6 class 0.0 = 0.0 +- 0.000 (in-sample avg dev_std = 0.000)
NEC for r=0.6 class 1.0 = 0.0 +- 0.000 (in-sample avg dev_std = 0.000)
NEC for r=0.6 all KL = 0.0 +- 0.000 (in-sample avg dev_std = 0.000)
NEC for r=0.6 all L1 = 0.0 +- 0.000 (in-sample avg dev_std = 0.000)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:21:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/18/2024 12:21:38 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/18/2024 12:22:10 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/18/2024 12:22:21 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/18/2024 12:22:32 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/18/2024 12:22:49 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 127...
[0m[1;37mINFO[0m: [1mCheckpoint 127: 
-----------------------------------
Train ROC-AUC: 0.9459
Train Loss: 0.2739
ID Validation ROC-AUC: 0.9033
ID Validation Loss: 0.3429
ID Test ROC-AUC: 0.9089
ID Test Loss: 0.3383
OOD Validation ROC-AUC: 0.6038
OOD Validation Loss: 0.4024
OOD Test ROC-AUC: 0.6620
OOD Test Loss: 0.6481

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 14...
[0m[1;37mINFO[0m: [1mCheckpoint 14: 
-----------------------------------
Train ROC-AUC: 0.8786
Train Loss: 0.2942
ID Validation ROC-AUC: 0.8634
ID Validation Loss: 0.3109
ID Test ROC-AUC: 0.8696
ID Test Loss: 0.3099
OOD Validation ROC-AUC: 0.6437
OOD Validation Loss: 0.3274
OOD Test ROC-AUC: 0.6934
OOD Test Loss: 0.5211

[0m[1;37mINFO[0m: [1mChartInfo 0.9089 0.6620 0.8696 0.6934 0.8634 0.6437[0mLBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/18/2024 12:23:06 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.567
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.539
SUFF++ for r=0.6 class 0.0 = 0.793 +- 0.306 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.6 class 1.0 = 0.83 +- 0.306 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.6 all KL = 0.657 +- 0.306 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.6 all L1 = 0.824 +- 0.179 (in-sample avg dev_std = 0.421)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.567
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.512
NEC for r=0.6 class 0.0 = 0.199 +- 0.354 (in-sample avg dev_std = 0.435)
NEC for r=0.6 class 1.0 = 0.193 +- 0.354 (in-sample avg dev_std = 0.435)
NEC for r=0.6 all KL = 0.342 +- 0.354 (in-sample avg dev_std = 0.435)
NEC for r=0.6 all L1 = 0.194 +- 0.216 (in-sample avg dev_std = 0.435)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:23:39 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/18/2024 12:23:39 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/18/2024 12:24:12 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/18/2024 12:24:23 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/18/2024 12:24:34 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/18/2024 12:24:50 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 159...
[0m[1;37mINFO[0m: [1mCheckpoint 159: 
-----------------------------------
Train ROC-AUC: 0.9548
Train Loss: 0.3192
ID Validation ROC-AUC: 0.9041
ID Validation Loss: 0.4545
ID Test ROC-AUC: 0.9006
ID Test Loss: 0.4686
OOD Validation ROC-AUC: 0.5961
OOD Validation Loss: 0.5439
OOD Test ROC-AUC: 0.6685
OOD Test Loss: 0.8654

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.8746
Train Loss: 0.3120
ID Validation ROC-AUC: 0.8599
ID Validation Loss: 0.3287
ID Test ROC-AUC: 0.8641
ID Test Loss: 0.3301
OOD Validation ROC-AUC: 0.6526
OOD Validation Loss: 0.3467
OOD Test ROC-AUC: 0.6783
OOD Test Loss: 0.5797

[0m[1;37mINFO[0m: [1mChartInfo 0.9006 0.6685 0.8641 0.6783 0.8599 0.6526[0mLBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/18/2024 12:25:07 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.521
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.527
SUFF++ for r=0.6 class 0.0 = 0.596 +- 0.290 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 1.0 = 0.616 +- 0.290 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 all KL = 0.461 +- 0.290 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 all L1 = 0.613 +- 0.208 (in-sample avg dev_std = 0.618)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.522
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.518
NEC for r=0.6 class 0.0 = 0.449 +- 0.325 (in-sample avg dev_std = 0.556)
NEC for r=0.6 class 1.0 = 0.403 +- 0.325 (in-sample avg dev_std = 0.556)
NEC for r=0.6 all KL = 0.542 +- 0.325 (in-sample avg dev_std = 0.556)
NEC for r=0.6 all L1 = 0.41 +- 0.245 (in-sample avg dev_std = 0.556)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:25:40 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/18/2024 12:25:40 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/18/2024 12:26:13 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/18/2024 12:26:23 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/18/2024 12:26:34 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/18/2024 12:26:51 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 152...
[0m[1;37mINFO[0m: [1mCheckpoint 152: 
-----------------------------------
Train ROC-AUC: 0.9637
Train Loss: 0.2273
ID Validation ROC-AUC: 0.9083
ID Validation Loss: 0.3490
ID Test ROC-AUC: 0.9104
ID Test Loss: 0.3513
OOD Validation ROC-AUC: 0.5940
OOD Validation Loss: 0.4940
OOD Test ROC-AUC: 0.6574
OOD Test Loss: 0.7667

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 24...
[0m[1;37mINFO[0m: [1mCheckpoint 24: 
-----------------------------------
Train ROC-AUC: 0.8938
Train Loss: 0.2424
ID Validation ROC-AUC: 0.8717
ID Validation Loss: 0.2611
ID Test ROC-AUC: 0.8711
ID Test Loss: 0.2630
OOD Validation ROC-AUC: 0.6590
OOD Validation Loss: 0.3149
OOD Test ROC-AUC: 0.6741
OOD Test Loss: 0.4665

[0m[1;37mINFO[0m: [1mChartInfo 0.9104 0.6574 0.8711 0.6741 0.8717 0.6590[0mLBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/18/2024 12:27:08 AM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.382
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.486
SUFF++ for r=0.6 class 0.0 = 0.828 +- 0.264 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.6 class 1.0 = 0.896 +- 0.264 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.6 all KL = 0.847 +- 0.264 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.6 all L1 = 0.885 +- 0.209 (in-sample avg dev_std = 0.269)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.383
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.437
NEC for r=0.6 class 0.0 = 0.193 +- 0.280 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 1.0 = 0.102 +- 0.280 (in-sample avg dev_std = 0.273)
NEC for r=0.6 all KL = 0.159 +- 0.280 (in-sample avg dev_std = 0.273)
NEC for r=0.6 all L1 = 0.117 +- 0.213 (in-sample avg dev_std = 0.273)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.863], 'all_L1': [0.743]}), defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [1.0]}), defaultdict(<class 'list'>, {'all_KL': [0.657], 'all_L1': [0.824]}), defaultdict(<class 'list'>, {'all_KL': [0.461], 'all_L1': [0.613]}), defaultdict(<class 'list'>, {'all_KL': [0.847], 'all_L1': [0.885]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.121], 'all_L1': [0.246]}), defaultdict(<class 'list'>, {'all_KL': [0.0], 'all_L1': [0.0]}), defaultdict(<class 'list'>, {'all_KL': [0.342], 'all_L1': [0.194]}), defaultdict(<class 'list'>, {'all_KL': [0.542], 'all_L1': [0.41]}), defaultdict(<class 'list'>, {'all_KL': [0.159], 'all_L1': [0.117]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.813 +- 0.131
suff++ class all_KL  =  0.765 +- 0.187
suff++_acc_int  =  0.522 +- 0.020
nec class all_L1  =  0.193 +- 0.136
nec class all_KL  =  0.233 +- 0.190
nec_acc_int  =  0.498 +- 0.030


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.503 +- 0.006
Faith. Armon (L1)= 		  =  0.276 +- 0.166
Faith. GMean (L1)= 	  =  0.330 +- 0.175
Faith. Aritm (KL)= 		  =  0.499 +- 0.004
Faith. Armon (KL)= 		  =  0.286 +- 0.179
Faith. GMean (KL)= 	  =  0.333 +- 0.179
Computed for split load_split = id



Completed in  0:10:07.916048  for CIGAGIN LBAPcore/assay



DONE CIGA LBAPcore/assay all mitig

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:27:52 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:27:53 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 17...
[0m[1;37mINFO[0m: [1mCheckpoint 17: 
-----------------------------------
Train ACCURACY: 0.8550
Train Loss: 0.4111
ID Validation ACCURACY: 0.8098
ID Validation Loss: 0.4890
ID Test ACCURACY: 0.8080
ID Test Loss: 0.5001
OOD Validation ACCURACY: 0.7392
OOD Validation Loss: 0.5336
OOD Test ACCURACY: 0.5942
OOD Test Loss: 0.6023

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 17...
[0m[1;37mINFO[0m: [1mCheckpoint 17: 
-----------------------------------
Train ACCURACY: 0.8550
Train Loss: 0.4111
ID Validation ACCURACY: 0.8098
ID Validation Loss: 0.4890
ID Test ACCURACY: 0.8080
ID Test Loss: 0.5001
OOD Validation ACCURACY: 0.7392
OOD Validation Loss: 0.5336
OOD Test ACCURACY: 0.5942
OOD Test Loss: 0.6023

[0m[1;37mINFO[0m: [1mChartInfo 0.8080 0.5942 0.8080 0.5942 0.8098 0.7392[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.589
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.545
SUFF++ for r=0.8 class 0.0 = 0.927 +- 0.018 (in-sample avg dev_std = 0.057)
SUFF++ for r=0.8 class 1.0 = 0.937 +- 0.018 (in-sample avg dev_std = 0.057)
SUFF++ for r=0.8 all KL = 0.991 +- 0.018 (in-sample avg dev_std = 0.057)
SUFF++ for r=0.8 all L1 = 0.933 +- 0.045 (in-sample avg dev_std = 0.057)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.586
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.575
NEC for r=0.8 class 0.0 = 0.062 +- 0.021 (in-sample avg dev_std = 0.052)
NEC for r=0.8 class 1.0 = 0.066 +- 0.021 (in-sample avg dev_std = 0.052)
NEC for r=0.8 all KL = 0.01 +- 0.021 (in-sample avg dev_std = 0.052)
NEC for r=0.8 all L1 = 0.064 +- 0.047 (in-sample avg dev_std = 0.052)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:28:28 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:28:29 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0754
ID Validation ACCURACY: 0.8604
ID Validation Loss: 1.0229
ID Test ACCURACY: 0.8561
ID Test Loss: 1.1527
OOD Validation ACCURACY: 0.8477
OOD Validation Loss: 3.3872
OOD Test ACCURACY: 0.7903
OOD Test Loss: 4.9558

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 118...
[0m[1;37mINFO[0m: [1mCheckpoint 118: 
-----------------------------------
Train ACCURACY: 0.9484
Train Loss: 0.0768
ID Validation ACCURACY: 0.8532
ID Validation Loss: 1.2979
ID Test ACCURACY: 0.8521
ID Test Loss: 1.5484
OOD Validation ACCURACY: 0.8587
OOD Validation Loss: 3.8430
OOD Test ACCURACY: 0.8120
OOD Test Loss: 5.5150

[0m[1;37mINFO[0m: [1mChartInfo 0.8561 0.7903 0.8521 0.8120 0.8532 0.8587[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.802
SUFF++ for r=0.8 class 0.0 = 0.865 +- 0.302 (in-sample avg dev_std = 0.305)
SUFF++ for r=0.8 class 1.0 = 0.915 +- 0.302 (in-sample avg dev_std = 0.305)
SUFF++ for r=0.8 all KL = 0.824 +- 0.302 (in-sample avg dev_std = 0.305)
SUFF++ for r=0.8 all L1 = 0.891 +- 0.199 (in-sample avg dev_std = 0.305)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.824
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.815
NEC for r=0.8 class 0.0 = 0.117 +- 0.282 (in-sample avg dev_std = 0.264)
NEC for r=0.8 class 1.0 = 0.067 +- 0.282 (in-sample avg dev_std = 0.264)
NEC for r=0.8 all KL = 0.143 +- 0.282 (in-sample avg dev_std = 0.264)
NEC for r=0.8 all L1 = 0.091 +- 0.187 (in-sample avg dev_std = 0.264)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:29:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:04 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 194...
[0m[1;37mINFO[0m: [1mCheckpoint 194: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8623
ID Validation Loss: 2.4682
ID Test ACCURACY: 0.8612
ID Test Loss: 2.9010
OOD Validation ACCURACY: 0.8557
OOD Validation Loss: 5.3706
OOD Test ACCURACY: 0.7949
OOD Test Loss: 8.1971

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 74...
[0m[1;37mINFO[0m: [1mCheckpoint 74: 
-----------------------------------
Train ACCURACY: 0.9484
Train Loss: 0.0805
ID Validation ACCURACY: 0.8544
ID Validation Loss: 0.8464
ID Test ACCURACY: 0.8542
ID Test Loss: 1.0687
OOD Validation ACCURACY: 0.8595
OOD Validation Loss: 2.9836
OOD Test ACCURACY: 0.7982
OOD Test Loss: 3.4505

[0m[1;37mINFO[0m: [1mChartInfo 0.8612 0.7949 0.8542 0.7982 0.8544 0.8595[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.812
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.791
SUFF++ for r=0.8 class 0.0 = 0.825 +- 0.394 (in-sample avg dev_std = 0.415)
SUFF++ for r=0.8 class 1.0 = 0.918 +- 0.394 (in-sample avg dev_std = 0.415)
SUFF++ for r=0.8 all KL = 0.737 +- 0.394 (in-sample avg dev_std = 0.415)
SUFF++ for r=0.8 all L1 = 0.873 +- 0.217 (in-sample avg dev_std = 0.415)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.82
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.791
NEC for r=0.8 class 0.0 = 0.128 +- 0.327 (in-sample avg dev_std = 0.316)
NEC for r=0.8 class 1.0 = 0.056 +- 0.327 (in-sample avg dev_std = 0.316)
NEC for r=0.8 all KL = 0.158 +- 0.327 (in-sample avg dev_std = 0.316)
NEC for r=0.8 all L1 = 0.091 +- 0.204 (in-sample avg dev_std = 0.316)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:29:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:29:38 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 170...
[0m[1;37mINFO[0m: [1mCheckpoint 170: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8623
ID Validation Loss: 1.5974
ID Test ACCURACY: 0.8544
ID Test Loss: 2.0365
OOD Validation ACCURACY: 0.8543
OOD Validation Loss: 3.4836
OOD Test ACCURACY: 0.7864
OOD Test Loss: 7.7740

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 132...
[0m[1;37mINFO[0m: [1mCheckpoint 132: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0755
ID Validation ACCURACY: 0.8536
ID Validation Loss: 1.0924
ID Test ACCURACY: 0.8563
ID Test Loss: 1.3289
OOD Validation ACCURACY: 0.8577
OOD Validation Loss: 2.8081
OOD Test ACCURACY: 0.7972
OOD Test Loss: 3.9612

[0m[1;37mINFO[0m: [1mChartInfo 0.8544 0.7864 0.8563 0.7972 0.8536 0.8577[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.791
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.776
SUFF++ for r=0.8 class 0.0 = 0.799 +- 0.423 (in-sample avg dev_std = 0.458)
SUFF++ for r=0.8 class 1.0 = 0.902 +- 0.423 (in-sample avg dev_std = 0.458)
SUFF++ for r=0.8 all KL = 0.683 +- 0.423 (in-sample avg dev_std = 0.458)
SUFF++ for r=0.8 all L1 = 0.852 +- 0.227 (in-sample avg dev_std = 0.458)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.794
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.789
NEC for r=0.8 class 0.0 = 0.116 +- 0.311 (in-sample avg dev_std = 0.298)
NEC for r=0.8 class 1.0 = 0.053 +- 0.311 (in-sample avg dev_std = 0.298)
NEC for r=0.8 all KL = 0.146 +- 0.311 (in-sample avg dev_std = 0.298)
NEC for r=0.8 all L1 = 0.084 +- 0.192 (in-sample avg dev_std = 0.298)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:30:10 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:11 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:12 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 110...
[0m[1;37mINFO[0m: [1mCheckpoint 110: 
-----------------------------------
Train ACCURACY: 0.9488
Train Loss: 0.0757
ID Validation ACCURACY: 0.8610
ID Validation Loss: 0.8028
ID Test ACCURACY: 0.8600
ID Test Loss: 1.0996
OOD Validation ACCURACY: 0.8599
OOD Validation Loss: 2.7628
OOD Test ACCURACY: 0.8133
OOD Test Loss: 3.9052

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 109...
[0m[1;37mINFO[0m: [1mCheckpoint 109: 
-----------------------------------
Train ACCURACY: 0.9481
Train Loss: 0.0779
ID Validation ACCURACY: 0.8563
ID Validation Loss: 0.7223
ID Test ACCURACY: 0.8585
ID Test Loss: 0.9870
OOD Validation ACCURACY: 0.8610
OOD Validation Loss: 2.2647
OOD Test ACCURACY: 0.8027
OOD Test Loss: 3.2630

[0m[1;37mINFO[0m: [1mChartInfo 0.8600 0.8133 0.8585 0.8027 0.8563 0.8610[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.827
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.813
SUFF++ for r=0.8 class 0.0 = 0.898 +- 0.282 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 class 1.0 = 0.898 +- 0.282 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 all KL = 0.834 +- 0.282 (in-sample avg dev_std = 0.295)
SUFF++ for r=0.8 all L1 = 0.898 +- 0.185 (in-sample avg dev_std = 0.295)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.827
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.817
NEC for r=0.8 class 0.0 = 0.098 +- 0.258 (in-sample avg dev_std = 0.229)
NEC for r=0.8 class 1.0 = 0.067 +- 0.258 (in-sample avg dev_std = 0.229)
NEC for r=0.8 all KL = 0.122 +- 0.258 (in-sample avg dev_std = 0.229)
NEC for r=0.8 all L1 = 0.082 +- 0.178 (in-sample avg dev_std = 0.229)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.933]}), defaultdict(<class 'list'>, {'all_KL': [0.824], 'all_L1': [0.891]}), defaultdict(<class 'list'>, {'all_KL': [0.737], 'all_L1': [0.873]}), defaultdict(<class 'list'>, {'all_KL': [0.683], 'all_L1': [0.852]}), defaultdict(<class 'list'>, {'all_KL': [0.834], 'all_L1': [0.898]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.01], 'all_L1': [0.064]}), defaultdict(<class 'list'>, {'all_KL': [0.143], 'all_L1': [0.091]}), defaultdict(<class 'list'>, {'all_KL': [0.158], 'all_L1': [0.091]}), defaultdict(<class 'list'>, {'all_KL': [0.146], 'all_L1': [0.084]}), defaultdict(<class 'list'>, {'all_KL': [0.122], 'all_L1': [0.082]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.889 +- 0.027
suff++ class all_KL  =  0.814 +- 0.105
suff++_acc_int  =  0.745 +- 0.101
nec class all_L1  =  0.082 +- 0.010
nec class all_KL  =  0.116 +- 0.054
nec_acc_int  =  0.757 +- 0.092


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.486 +- 0.010
Faith. Armon (L1)= 		  =  0.151 +- 0.017
Faith. GMean (L1)= 	  =  0.270 +- 0.014
Faith. Aritm (KL)= 		  =  0.465 +- 0.030
Faith. Armon (KL)= 		  =  0.195 +- 0.089
Faith. GMean (KL)= 	  =  0.284 +- 0.093
Computed for split load_split = id



Completed in  0:02:54.953638  for CIGAGIN GOODSST2/length



DONE CIGA GOODSST2/length all mitig

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat May 18 00:30:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:56 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:58 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:58 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/18/2024 12:30:59 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:01 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:02 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mUsing feature sampling = raw
[0m[1;34mDEBUG[0m: 05/18/2024 12:31:03 AM : [1mConfig model and output the best checkpoint info...
[0m[1;31mERROR[0m: 05/18/2024 12:31:04 AM - utils.py - line 52 : [1mCheckpoint not found at /mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/storage/checkpoints/round1/GOODTwitter_length_covariate/repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingrawmitig_readoutweightedavgedgeattnmean/0.0001lr_0.0001wd/CIGA_0.6_1_0.5/best.ckpt
[0m
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/bin/goodtg", line 33, in <module>
    sys.exit(load_entry_point('graph-ood', 'console_scripts', 'goodtg')())
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 646, in goodtg
    main()
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 546, in main
    evaluate_metric(args)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 306, in evaluate_metric
    pipeline.load_task(load_param=True, load_split=load_split)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 1372, in load_task
    test_score, test_loss = self.config_model('test', load_param=load_param, load_split=load_split)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 1402, in config_model
    print(f'#E#Checkpoint not found at {os.path.abspath(self.config.test_ckpt)}')
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
