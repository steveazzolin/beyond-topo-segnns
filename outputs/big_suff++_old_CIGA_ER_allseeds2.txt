Time to compute metrics!

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 10:53:49 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 10:53:49 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:02 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:04 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:06 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:08 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 10:54:09 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9236
Train Loss: 0.3604
ID Validation ACCURACY: 0.9227
ID Validation Loss: 0.3790
ID Test ACCURACY: 0.9237
ID Test Loss: 0.3648
OOD Validation ACCURACY: 0.4100
OOD Validation Loss: 14.0711
OOD Test ACCURACY: 0.6730
OOD Test Loss: 0.9019

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 0...
[0m[1;37mINFO[0m: [1mCheckpoint 0: 
-----------------------------------
Train ACCURACY: 0.8039
Train Loss: 0.6801
ID Validation ACCURACY: 0.7977
ID Validation Loss: 0.7040
ID Test ACCURACY: 0.8167
ID Test Loss: 0.6746
OOD Validation ACCURACY: 0.9300
OOD Validation Loss: 0.4492
OOD Test ACCURACY: 0.3700
OOD Test Loss: 12.3785

[0m[1;37mINFO[0m: [1mChartInfo 0.9237 0.6730 0.8167 0.3700 0.7977 0.9300[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.575
WIoU for r=0.6 = 0.453
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 0.995
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1012, 1021,  967]))

Gold ratio (id_test) =  tensor(0.3175) +- tensor(0.1645)
F1 for r=0.6 = 0.594
WIoU for r=0.6 = 0.476
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.577
WIoU for r=0.6 = 0.438


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.921
Model XAI F1 of binarized graphs for r=0.6 =  0.57459375
Model XAI WIoU of binarized graphs for r=0.6 =  0.45291375
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.587
SUFF++ for r=0.6 class 0 = 0.471 +- 0.243 (in-sample avg dev_std = 0.658)
SUFF++ for r=0.6 class 1 = 0.457 +- 0.243 (in-sample avg dev_std = 0.658)
SUFF++ for r=0.6 class 2 = 0.56 +- 0.243 (in-sample avg dev_std = 0.658)
SUFF++ for r=0.6 all KL = 0.405 +- 0.243 (in-sample avg dev_std = 0.658)
SUFF++ for r=0.6 all L1 = 0.497 +- 0.175 (in-sample avg dev_std = 0.658)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.469
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.994545
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.693
SUFF++ for r=0.6 class 0 = 0.456 +- 0.359 (in-sample avg dev_std = 0.728)
SUFF++ for r=0.6 class 1 = 0.845 +- 0.359 (in-sample avg dev_std = 0.728)
SUFF++ for r=0.6 class 2 = 0.489 +- 0.359 (in-sample avg dev_std = 0.728)
SUFF++ for r=0.6 all KL = 0.266 +- 0.359 (in-sample avg dev_std = 0.728)
SUFF++ for r=0.6 all L1 = 0.595 +- 0.262 (in-sample avg dev_std = 0.728)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.918
Model XAI F1 of binarized graphs for r=0.6 =  0.5937250000000001
Model XAI WIoU of binarized graphs for r=0.6 =  0.4758962500000001
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.584
SUFF++ for r=0.6 class 0 = 0.47 +- 0.235 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 1 = 0.458 +- 0.235 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 2 = 0.56 +- 0.235 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 all KL = 0.389 +- 0.235 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 all L1 = 0.495 +- 0.172 (in-sample avg dev_std = 0.679)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.65
Model XAI F1 of binarized graphs for r=0.6 =  0.57742125
Model XAI WIoU of binarized graphs for r=0.6 =  0.43822625000000004
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.489
SUFF++ for r=0.6 class 0 = 0.481 +- 0.248 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 class 1 = 0.549 +- 0.248 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 class 2 = 0.373 +- 0.248 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 all KL = 0.387 +- 0.248 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 all L1 = 0.466 +- 0.160 (in-sample avg dev_std = 0.642)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.921
Model XAI F1 of binarized graphs for r=0.6 =  0.57459375
Model XAI WIoU of binarized graphs for r=0.6 =  0.45291375
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.43
NEC for r=0.6 class 0 = 0.577 +- 0.252 (in-sample avg dev_std = 0.592)
NEC for r=0.6 class 1 = 0.615 +- 0.252 (in-sample avg dev_std = 0.592)
NEC for r=0.6 class 2 = 0.663 +- 0.252 (in-sample avg dev_std = 0.592)
NEC for r=0.6 all KL = 0.694 +- 0.252 (in-sample avg dev_std = 0.592)
NEC for r=0.6 all L1 = 0.619 +- 0.152 (in-sample avg dev_std = 0.592)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.469
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.994545
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.383
NEC for r=0.6 class 0 = 0.167 +- 0.307 (in-sample avg dev_std = 0.312)
NEC for r=0.6 class 1 = 0.038 +- 0.307 (in-sample avg dev_std = 0.312)
NEC for r=0.6 class 2 = 0.229 +- 0.307 (in-sample avg dev_std = 0.312)
NEC for r=0.6 all KL = 0.166 +- 0.307 (in-sample avg dev_std = 0.312)
NEC for r=0.6 all L1 = 0.145 +- 0.265 (in-sample avg dev_std = 0.312)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.918
Model XAI F1 of binarized graphs for r=0.6 =  0.5937250000000001
Model XAI WIoU of binarized graphs for r=0.6 =  0.4758962500000001
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.424
NEC for r=0.6 class 0 = 0.579 +- 0.242 (in-sample avg dev_std = 0.601)
NEC for r=0.6 class 1 = 0.633 +- 0.242 (in-sample avg dev_std = 0.601)
NEC for r=0.6 class 2 = 0.68 +- 0.242 (in-sample avg dev_std = 0.601)
NEC for r=0.6 all KL = 0.718 +- 0.242 (in-sample avg dev_std = 0.601)
NEC for r=0.6 all L1 = 0.63 +- 0.146 (in-sample avg dev_std = 0.601)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.65
Model XAI F1 of binarized graphs for r=0.6 =  0.57742125
Model XAI WIoU of binarized graphs for r=0.6 =  0.43822625000000004
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.515
NEC for r=0.6 class 0 = 0.536 +- 0.269 (in-sample avg dev_std = 0.527)
NEC for r=0.6 class 1 = 0.39 +- 0.269 (in-sample avg dev_std = 0.527)
NEC for r=0.6 class 2 = 0.528 +- 0.269 (in-sample avg dev_std = 0.527)
NEC for r=0.6 all KL = 0.546 +- 0.269 (in-sample avg dev_std = 0.527)
NEC for r=0.6 all L1 = 0.486 +- 0.193 (in-sample avg dev_std = 0.527)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 10:55:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:31 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:44 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:46 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:48 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:50 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 10:55:52 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 38...
[0m[1;37mINFO[0m: [1mCheckpoint 38: 
-----------------------------------
Train ACCURACY: 0.8845
Train Loss: 0.5305
ID Validation ACCURACY: 0.8827
ID Validation Loss: 0.5555
ID Test ACCURACY: 0.8790
ID Test Loss: 0.5276
OOD Validation ACCURACY: 0.4290
OOD Validation Loss: 21.8019
OOD Test ACCURACY: 0.5357
OOD Test Loss: 1.5598

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 91...
[0m[1;37mINFO[0m: [1mCheckpoint 91: 
-----------------------------------
Train ACCURACY: 0.7712
Train Loss: 0.6710
ID Validation ACCURACY: 0.7690
ID Validation Loss: 0.6984
ID Test ACCURACY: 0.7687
ID Test Loss: 0.6839
OOD Validation ACCURACY: 0.7957
OOD Validation Loss: 0.6392
OOD Test ACCURACY: 0.4723
OOD Test Loss: 1.6875

[0m[1;37mINFO[0m: [1mChartInfo 0.8790 0.5357 0.7687 0.4723 0.7690 0.7957[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.335
WIoU for r=0.6 = 0.272
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.210
WIoU for r=0.6 = 0.171
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1012, 1021,  967]))

Gold ratio (id_test) =  tensor(0.3175) +- tensor(0.1645)
F1 for r=0.6 = 0.342
WIoU for r=0.6 = 0.280
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.607
WIoU for r=0.6 = 0.547


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.881
Model XAI F1 of binarized graphs for r=0.6 =  0.33491
Model XAI WIoU of binarized graphs for r=0.6 =  0.2715025
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.496
SUFF++ for r=0.6 class 0 = 0.334 +- 0.310 (in-sample avg dev_std = 0.550)
SUFF++ for r=0.6 class 1 = 0.401 +- 0.310 (in-sample avg dev_std = 0.550)
SUFF++ for r=0.6 class 2 = 0.752 +- 0.310 (in-sample avg dev_std = 0.550)
SUFF++ for r=0.6 all KL = 0.432 +- 0.310 (in-sample avg dev_std = 0.550)
SUFF++ for r=0.6 all L1 = 0.501 +- 0.265 (in-sample avg dev_std = 0.550)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.428
Model XAI F1 of binarized graphs for r=0.6 =  0.20993000000000003
Model XAI WIoU of binarized graphs for r=0.6 =  0.17095749999999998
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.378
SUFF++ for r=0.6 class 0 = 0.681 +- 0.351 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.6 class 1 = 0.893 +- 0.351 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.6 class 2 = 0.931 +- 0.351 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.6 all KL = 0.733 +- 0.351 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.6 all L1 = 0.834 +- 0.240 (in-sample avg dev_std = 0.360)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.874
Model XAI F1 of binarized graphs for r=0.6 =  0.34197000000000005
Model XAI WIoU of binarized graphs for r=0.6 =  0.27991125
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.487
SUFF++ for r=0.6 class 0 = 0.324 +- 0.313 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 class 1 = 0.388 +- 0.313 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 class 2 = 0.766 +- 0.313 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 all KL = 0.417 +- 0.313 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 all L1 = 0.489 +- 0.270 (in-sample avg dev_std = 0.562)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  0.60715375
Model XAI WIoU of binarized graphs for r=0.6 =  0.54695
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.555
SUFF++ for r=0.6 class 0 = 0.544 +- 0.298 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.6 class 1 = 0.523 +- 0.298 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.6 class 2 = 0.936 +- 0.298 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.6 all KL = 0.644 +- 0.298 (in-sample avg dev_std = 0.568)
SUFF++ for r=0.6 all L1 = 0.671 +- 0.246 (in-sample avg dev_std = 0.568)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.881
Model XAI F1 of binarized graphs for r=0.6 =  0.33491
Model XAI WIoU of binarized graphs for r=0.6 =  0.2715025
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.453
NEC for r=0.6 class 0 = 0.651 +- 0.306 (in-sample avg dev_std = 0.541)
NEC for r=0.6 class 1 = 0.468 +- 0.306 (in-sample avg dev_std = 0.541)
NEC for r=0.6 class 2 = 0.541 +- 0.306 (in-sample avg dev_std = 0.541)
NEC for r=0.6 all KL = 0.611 +- 0.306 (in-sample avg dev_std = 0.541)
NEC for r=0.6 all L1 = 0.552 +- 0.220 (in-sample avg dev_std = 0.541)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.428
Model XAI F1 of binarized graphs for r=0.6 =  0.20993000000000003
Model XAI WIoU of binarized graphs for r=0.6 =  0.17095749999999998
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.406
NEC for r=0.6 class 0 = 0.218 +- 0.305 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 1 = 0.111 +- 0.305 (in-sample avg dev_std = 0.264)
NEC for r=0.6 class 2 = 0.139 +- 0.305 (in-sample avg dev_std = 0.264)
NEC for r=0.6 all KL = 0.186 +- 0.305 (in-sample avg dev_std = 0.264)
NEC for r=0.6 all L1 = 0.157 +- 0.231 (in-sample avg dev_std = 0.264)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.874
Model XAI F1 of binarized graphs for r=0.6 =  0.34197000000000005
Model XAI WIoU of binarized graphs for r=0.6 =  0.27991125
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.438
NEC for r=0.6 class 0 = 0.68 +- 0.307 (in-sample avg dev_std = 0.553)
NEC for r=0.6 class 1 = 0.461 +- 0.307 (in-sample avg dev_std = 0.553)
NEC for r=0.6 class 2 = 0.553 +- 0.307 (in-sample avg dev_std = 0.553)
NEC for r=0.6 all KL = 0.628 +- 0.307 (in-sample avg dev_std = 0.553)
NEC for r=0.6 all L1 = 0.565 +- 0.221 (in-sample avg dev_std = 0.553)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  0.60715375
Model XAI WIoU of binarized graphs for r=0.6 =  0.54695
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.445
NEC for r=0.6 class 0 = 0.576 +- 0.319 (in-sample avg dev_std = 0.666)
NEC for r=0.6 class 1 = 0.413 +- 0.319 (in-sample avg dev_std = 0.666)
NEC for r=0.6 class 2 = 0.581 +- 0.319 (in-sample avg dev_std = 0.666)
NEC for r=0.6 all KL = 0.622 +- 0.319 (in-sample avg dev_std = 0.666)
NEC for r=0.6 all L1 = 0.526 +- 0.201 (in-sample avg dev_std = 0.666)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 10:57:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 10:57:37 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 10:57:51 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 10:57:53 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 10:57:56 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 10:57:58 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 10:58:00 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 26...
[0m[1;37mINFO[0m: [1mCheckpoint 26: 
-----------------------------------
Train ACCURACY: 0.8712
Train Loss: 0.4857
ID Validation ACCURACY: 0.8610
ID Validation Loss: 0.5151
ID Test ACCURACY: 0.8730
ID Test Loss: 0.4697
OOD Validation ACCURACY: 0.6573
OOD Validation Loss: 2.5816
OOD Test ACCURACY: 0.7480
OOD Test Loss: 0.6393

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 54...
[0m[1;37mINFO[0m: [1mCheckpoint 54: 
-----------------------------------
Train ACCURACY: 0.7893
Train Loss: 0.6406
ID Validation ACCURACY: 0.7840
ID Validation Loss: 0.6862
ID Test ACCURACY: 0.7930
ID Test Loss: 0.6301
OOD Validation ACCURACY: 0.9127
OOD Validation Loss: 0.4331
OOD Test ACCURACY: 0.5323
OOD Test Loss: 1.1321

[0m[1;37mINFO[0m: [1mChartInfo 0.8730 0.7480 0.7930 0.5323 0.7840 0.9127[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.314
WIoU for r=0.6 = 0.249
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.298
WIoU for r=0.6 = 0.261
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1012, 1021,  967]))

Gold ratio (id_test) =  tensor(0.3175) +- tensor(0.1645)
F1 for r=0.6 = 0.321
WIoU for r=0.6 = 0.256
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.555
WIoU for r=0.6 = 0.504


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.841
Model XAI F1 of binarized graphs for r=0.6 =  0.31428875
Model XAI WIoU of binarized graphs for r=0.6 =  0.24884625000000005
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.48
SUFF++ for r=0.6 class 0 = 0.376 +- 0.270 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.6 class 1 = 0.542 +- 0.270 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.6 class 2 = 0.413 +- 0.270 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.6 all KL = 0.386 +- 0.270 (in-sample avg dev_std = 0.567)
SUFF++ for r=0.6 all L1 = 0.444 +- 0.195 (in-sample avg dev_std = 0.567)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.641
Model XAI F1 of binarized graphs for r=0.6 =  0.29753
Model XAI WIoU of binarized graphs for r=0.6 =  0.26136499999999996
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.476
SUFF++ for r=0.6 class 0 = 0.492 +- 0.292 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 1 = 0.525 +- 0.292 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 2 = 0.612 +- 0.292 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 all KL = 0.379 +- 0.292 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 all L1 = 0.543 +- 0.211 (in-sample avg dev_std = 0.628)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.876
Model XAI F1 of binarized graphs for r=0.6 =  0.32116374999999997
Model XAI WIoU of binarized graphs for r=0.6 =  0.25635375
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.488
SUFF++ for r=0.6 class 0 = 0.368 +- 0.280 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 class 1 = 0.558 +- 0.280 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 class 2 = 0.436 +- 0.280 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 all KL = 0.393 +- 0.280 (in-sample avg dev_std = 0.562)
SUFF++ for r=0.6 all L1 = 0.455 +- 0.207 (in-sample avg dev_std = 0.562)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.745
Model XAI F1 of binarized graphs for r=0.6 =  0.55481
Model XAI WIoU of binarized graphs for r=0.6 =  0.50397625
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.651
SUFF++ for r=0.6 class 0 = 0.542 +- 0.206 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 1 = 0.634 +- 0.206 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 class 2 = 0.796 +- 0.206 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 all KL = 0.712 +- 0.206 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.6 all L1 = 0.658 +- 0.173 (in-sample avg dev_std = 0.473)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.841
Model XAI F1 of binarized graphs for r=0.6 =  0.31428875
Model XAI WIoU of binarized graphs for r=0.6 =  0.24884625000000005
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.476
NEC for r=0.6 class 0 = 0.597 +- 0.305 (in-sample avg dev_std = 0.490)
NEC for r=0.6 class 1 = 0.4 +- 0.305 (in-sample avg dev_std = 0.490)
NEC for r=0.6 class 2 = 0.615 +- 0.305 (in-sample avg dev_std = 0.490)
NEC for r=0.6 all KL = 0.566 +- 0.305 (in-sample avg dev_std = 0.490)
NEC for r=0.6 all L1 = 0.537 +- 0.209 (in-sample avg dev_std = 0.490)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.641
Model XAI F1 of binarized graphs for r=0.6 =  0.29753
Model XAI WIoU of binarized graphs for r=0.6 =  0.26136499999999996
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.541
NEC for r=0.6 class 0 = 0.413 +- 0.298 (in-sample avg dev_std = 0.483)
NEC for r=0.6 class 1 = 0.347 +- 0.298 (in-sample avg dev_std = 0.483)
NEC for r=0.6 class 2 = 0.393 +- 0.298 (in-sample avg dev_std = 0.483)
NEC for r=0.6 all KL = 0.402 +- 0.298 (in-sample avg dev_std = 0.483)
NEC for r=0.6 all L1 = 0.385 +- 0.238 (in-sample avg dev_std = 0.483)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.876
Model XAI F1 of binarized graphs for r=0.6 =  0.32116374999999997
Model XAI WIoU of binarized graphs for r=0.6 =  0.25635375
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.481
NEC for r=0.6 class 0 = 0.622 +- 0.309 (in-sample avg dev_std = 0.506)
NEC for r=0.6 class 1 = 0.367 +- 0.309 (in-sample avg dev_std = 0.506)
NEC for r=0.6 class 2 = 0.627 +- 0.309 (in-sample avg dev_std = 0.506)
NEC for r=0.6 all KL = 0.569 +- 0.309 (in-sample avg dev_std = 0.506)
NEC for r=0.6 all L1 = 0.537 +- 0.215 (in-sample avg dev_std = 0.506)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.745
Model XAI F1 of binarized graphs for r=0.6 =  0.55481
Model XAI WIoU of binarized graphs for r=0.6 =  0.50397625
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.497
NEC for r=0.6 class 0 = 0.486 +- 0.340 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 1 = 0.227 +- 0.340 (in-sample avg dev_std = 0.497)
NEC for r=0.6 class 2 = 0.66 +- 0.340 (in-sample avg dev_std = 0.497)
NEC for r=0.6 all KL = 0.415 +- 0.340 (in-sample avg dev_std = 0.497)
NEC for r=0.6 all L1 = 0.462 +- 0.227 (in-sample avg dev_std = 0.497)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 10:59:27 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:27 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:40 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:42 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:44 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:47 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 10:59:49 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 23...
[0m[1;37mINFO[0m: [1mCheckpoint 23: 
-----------------------------------
Train ACCURACY: 0.9014
Train Loss: 0.4323
ID Validation ACCURACY: 0.9013
ID Validation Loss: 0.4485
ID Test ACCURACY: 0.9030
ID Test Loss: 0.4260
OOD Validation ACCURACY: 0.5967
OOD Validation Loss: 8.9459
OOD Test ACCURACY: 0.7107
OOD Test Loss: 0.7527

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.8482
Train Loss: 0.5161
ID Validation ACCURACY: 0.8487
ID Validation Loss: 0.5316
ID Test ACCURACY: 0.8440
ID Test Loss: 0.5144
OOD Validation ACCURACY: 0.7533
OOD Validation Loss: 1.2198
OOD Test ACCURACY: 0.7353
OOD Test Loss: 0.8736

[0m[1;37mINFO[0m: [1mChartInfo 0.9030 0.7107 0.8440 0.7353 0.8487 0.7533[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.475
WIoU for r=0.6 = 0.441
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.342
WIoU for r=0.6 = 0.304
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1012, 1021,  967]))

Gold ratio (id_test) =  tensor(0.3175) +- tensor(0.1645)
F1 for r=0.6 = 0.483
WIoU for r=0.6 = 0.450
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.638
WIoU for r=0.6 = 0.597


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.899
Model XAI F1 of binarized graphs for r=0.6 =  0.47514375000000003
Model XAI WIoU of binarized graphs for r=0.6 =  0.44110249999999995
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.457
SUFF++ for r=0.6 class 0 = 0.357 +- 0.257 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 1 = 0.299 +- 0.257 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 2 = 0.668 +- 0.257 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 all KL = 0.423 +- 0.257 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 all L1 = 0.445 +- 0.221 (in-sample avg dev_std = 0.516)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.61
Model XAI F1 of binarized graphs for r=0.6 =  0.34209500000000004
Model XAI WIoU of binarized graphs for r=0.6 =  0.30359499999999995
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.51
SUFF++ for r=0.6 class 0 = 0.618 +- 0.300 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.6 class 1 = 0.744 +- 0.300 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.6 class 2 = 0.783 +- 0.300 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.6 all KL = 0.674 +- 0.300 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.6 all L1 = 0.715 +- 0.235 (in-sample avg dev_std = 0.442)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.899
Model XAI F1 of binarized graphs for r=0.6 =  0.48291750000000006
Model XAI WIoU of binarized graphs for r=0.6 =  0.44955
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.443
SUFF++ for r=0.6 class 0 = 0.358 +- 0.260 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.6 class 1 = 0.29 +- 0.260 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.6 class 2 = 0.683 +- 0.260 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.6 all KL = 0.41 +- 0.260 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.6 all L1 = 0.44 +- 0.224 (in-sample avg dev_std = 0.519)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.724
Model XAI F1 of binarized graphs for r=0.6 =  0.63822625
Model XAI WIoU of binarized graphs for r=0.6 =  0.59697875
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.642
SUFF++ for r=0.6 class 0 = 0.559 +- 0.236 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.6 class 1 = 0.529 +- 0.236 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.6 class 2 = 0.814 +- 0.236 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.6 all KL = 0.701 +- 0.236 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.6 all L1 = 0.636 +- 0.192 (in-sample avg dev_std = 0.404)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.899
Model XAI F1 of binarized graphs for r=0.6 =  0.47514375000000003
Model XAI WIoU of binarized graphs for r=0.6 =  0.44110249999999995
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.428
NEC for r=0.6 class 0 = 0.595 +- 0.282 (in-sample avg dev_std = 0.508)
NEC for r=0.6 class 1 = 0.499 +- 0.282 (in-sample avg dev_std = 0.508)
NEC for r=0.6 class 2 = 0.612 +- 0.282 (in-sample avg dev_std = 0.508)
NEC for r=0.6 all KL = 0.564 +- 0.282 (in-sample avg dev_std = 0.508)
NEC for r=0.6 all L1 = 0.569 +- 0.165 (in-sample avg dev_std = 0.508)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.61
Model XAI F1 of binarized graphs for r=0.6 =  0.34209500000000004
Model XAI WIoU of binarized graphs for r=0.6 =  0.30359499999999995
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.447
NEC for r=0.6 class 0 = 0.409 +- 0.322 (in-sample avg dev_std = 0.379)
NEC for r=0.6 class 1 = 0.199 +- 0.322 (in-sample avg dev_std = 0.379)
NEC for r=0.6 class 2 = 0.353 +- 0.322 (in-sample avg dev_std = 0.379)
NEC for r=0.6 all KL = 0.324 +- 0.322 (in-sample avg dev_std = 0.379)
NEC for r=0.6 all L1 = 0.321 +- 0.278 (in-sample avg dev_std = 0.379)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.899
Model XAI F1 of binarized graphs for r=0.6 =  0.48291750000000006
Model XAI WIoU of binarized graphs for r=0.6 =  0.44955
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.423
NEC for r=0.6 class 0 = 0.613 +- 0.280 (in-sample avg dev_std = 0.517)
NEC for r=0.6 class 1 = 0.509 +- 0.280 (in-sample avg dev_std = 0.517)
NEC for r=0.6 class 2 = 0.62 +- 0.280 (in-sample avg dev_std = 0.517)
NEC for r=0.6 all KL = 0.586 +- 0.280 (in-sample avg dev_std = 0.517)
NEC for r=0.6 all L1 = 0.58 +- 0.173 (in-sample avg dev_std = 0.517)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.724
Model XAI F1 of binarized graphs for r=0.6 =  0.63822625
Model XAI WIoU of binarized graphs for r=0.6 =  0.59697875
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.516
NEC for r=0.6 class 0 = 0.5 +- 0.271 (in-sample avg dev_std = 0.477)
NEC for r=0.6 class 1 = 0.328 +- 0.271 (in-sample avg dev_std = 0.477)
NEC for r=0.6 class 2 = 0.573 +- 0.271 (in-sample avg dev_std = 0.477)
NEC for r=0.6 all KL = 0.436 +- 0.271 (in-sample avg dev_std = 0.477)
NEC for r=0.6 all L1 = 0.469 +- 0.173 (in-sample avg dev_std = 0.477)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:01:17 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:17 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:30 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:32 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:35 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:36 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:01:38 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.9261
Train Loss: 0.3565
ID Validation ACCURACY: 0.9233
ID Validation Loss: 0.3727
ID Test ACCURACY: 0.9250
ID Test Loss: 0.3602
OOD Validation ACCURACY: 0.4207
OOD Validation Loss: 11.5665
OOD Test ACCURACY: 0.5380
OOD Test Loss: 1.3419

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 0...
[0m[1;37mINFO[0m: [1mCheckpoint 0: 
-----------------------------------
Train ACCURACY: 0.8612
Train Loss: 0.6286
ID Validation ACCURACY: 0.8557
ID Validation Loss: 0.6690
ID Test ACCURACY: 0.8683
ID Test Loss: 0.6245
OOD Validation ACCURACY: 0.9297
OOD Validation Loss: 0.5461
OOD Test ACCURACY: 0.3917
OOD Test Loss: 9.1522

[0m[1;37mINFO[0m: [1mChartInfo 0.9250 0.5380 0.8683 0.3917 0.8557 0.9297[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.566
WIoU for r=0.6 = 0.438
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 0.991
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1012, 1021,  967]))

Gold ratio (id_test) =  tensor(0.3175) +- tensor(0.1645)
F1 for r=0.6 = 0.589
WIoU for r=0.6 = 0.463
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.464
WIoU for r=0.6 = 0.327


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.919
Model XAI F1 of binarized graphs for r=0.6 =  0.56621875
Model XAI WIoU of binarized graphs for r=0.6 =  0.43821124999999994
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.598
SUFF++ for r=0.6 class 0 = 0.439 +- 0.257 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 1 = 0.574 +- 0.257 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 class 2 = 0.546 +- 0.257 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 all KL = 0.453 +- 0.257 (in-sample avg dev_std = 0.593)
SUFF++ for r=0.6 all L1 = 0.521 +- 0.195 (in-sample avg dev_std = 0.593)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.47
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.99100125
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.702
SUFF++ for r=0.6 class 0 = 0.461 +- 0.381 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 1 = 0.853 +- 0.381 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 2 = 0.51 +- 0.381 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 all KL = 0.297 +- 0.381 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 all L1 = 0.607 +- 0.259 (in-sample avg dev_std = 0.697)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.921
Model XAI F1 of binarized graphs for r=0.6 =  0.588905
Model XAI WIoU of binarized graphs for r=0.6 =  0.46289125
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.603
SUFF++ for r=0.6 class 0 = 0.437 +- 0.257 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 1 = 0.563 +- 0.257 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 2 = 0.569 +- 0.257 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 all KL = 0.443 +- 0.257 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 all L1 = 0.522 +- 0.194 (in-sample avg dev_std = 0.614)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.504
Model XAI F1 of binarized graphs for r=0.6 =  0.4639749999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.32664000000000004
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.431
SUFF++ for r=0.6 class 0 = 0.596 +- 0.248 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.6 class 1 = 0.816 +- 0.248 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.6 class 2 = 0.495 +- 0.248 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.6 all KL = 0.643 +- 0.248 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.6 all L1 = 0.632 +- 0.220 (in-sample avg dev_std = 0.372)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.919
Model XAI F1 of binarized graphs for r=0.6 =  0.56621875
Model XAI WIoU of binarized graphs for r=0.6 =  0.43821124999999994
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.423
NEC for r=0.6 class 0 = 0.605 +- 0.233 (in-sample avg dev_std = 0.629)
NEC for r=0.6 class 1 = 0.6 +- 0.233 (in-sample avg dev_std = 0.629)
NEC for r=0.6 class 2 = 0.65 +- 0.233 (in-sample avg dev_std = 0.629)
NEC for r=0.6 all KL = 0.702 +- 0.233 (in-sample avg dev_std = 0.629)
NEC for r=0.6 all L1 = 0.619 +- 0.149 (in-sample avg dev_std = 0.629)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.47
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.99100125
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.379
NEC for r=0.6 class 0 = 0.191 +- 0.328 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 1 = 0.075 +- 0.328 (in-sample avg dev_std = 0.353)
NEC for r=0.6 class 2 = 0.252 +- 0.328 (in-sample avg dev_std = 0.353)
NEC for r=0.6 all KL = 0.202 +- 0.328 (in-sample avg dev_std = 0.353)
NEC for r=0.6 all L1 = 0.173 +- 0.277 (in-sample avg dev_std = 0.353)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.921
Model XAI F1 of binarized graphs for r=0.6 =  0.588905
Model XAI WIoU of binarized graphs for r=0.6 =  0.46289125
len(reference) = 800
Effective ratio: 0.612 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.412
NEC for r=0.6 class 0 = 0.604 +- 0.233 (in-sample avg dev_std = 0.639)
NEC for r=0.6 class 1 = 0.6 +- 0.233 (in-sample avg dev_std = 0.639)
NEC for r=0.6 class 2 = 0.67 +- 0.233 (in-sample avg dev_std = 0.639)
NEC for r=0.6 all KL = 0.718 +- 0.233 (in-sample avg dev_std = 0.639)
NEC for r=0.6 all L1 = 0.624 +- 0.147 (in-sample avg dev_std = 0.639)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.504
Model XAI F1 of binarized graphs for r=0.6 =  0.4639749999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.32664000000000004
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.594
NEC for r=0.6 class 0 = 0.505 +- 0.242 (in-sample avg dev_std = 0.502)
NEC for r=0.6 class 1 = 0.337 +- 0.242 (in-sample avg dev_std = 0.502)
NEC for r=0.6 class 2 = 0.544 +- 0.242 (in-sample avg dev_std = 0.502)
NEC for r=0.6 all KL = 0.508 +- 0.242 (in-sample avg dev_std = 0.502)
NEC for r=0.6 all L1 = 0.464 +- 0.193 (in-sample avg dev_std = 0.502)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.405], 'all_L1': [0.497]}), defaultdict(<class 'list'>, {'all_KL': [0.432], 'all_L1': [0.501]}), defaultdict(<class 'list'>, {'all_KL': [0.386], 'all_L1': [0.444]}), defaultdict(<class 'list'>, {'all_KL': [0.423], 'all_L1': [0.445]}), defaultdict(<class 'list'>, {'all_KL': [0.453], 'all_L1': [0.521]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.694], 'all_L1': [0.619]}), defaultdict(<class 'list'>, {'all_KL': [0.611], 'all_L1': [0.552]}), defaultdict(<class 'list'>, {'all_KL': [0.566], 'all_L1': [0.537]}), defaultdict(<class 'list'>, {'all_KL': [0.564], 'all_L1': [0.569]}), defaultdict(<class 'list'>, {'all_KL': [0.702], 'all_L1': [0.619]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.266], 'all_L1': [0.595]}), defaultdict(<class 'list'>, {'all_KL': [0.733], 'all_L1': [0.834]}), defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.543]}), defaultdict(<class 'list'>, {'all_KL': [0.674], 'all_L1': [0.715]}), defaultdict(<class 'list'>, {'all_KL': [0.297], 'all_L1': [0.607]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.166], 'all_L1': [0.145]}), defaultdict(<class 'list'>, {'all_KL': [0.186], 'all_L1': [0.157]}), defaultdict(<class 'list'>, {'all_KL': [0.402], 'all_L1': [0.385]}), defaultdict(<class 'list'>, {'all_KL': [0.324], 'all_L1': [0.321]}), defaultdict(<class 'list'>, {'all_KL': [0.202], 'all_L1': [0.173]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.389], 'all_L1': [0.495]}), defaultdict(<class 'list'>, {'all_KL': [0.417], 'all_L1': [0.489]}), defaultdict(<class 'list'>, {'all_KL': [0.393], 'all_L1': [0.455]}), defaultdict(<class 'list'>, {'all_KL': [0.41], 'all_L1': [0.44]}), defaultdict(<class 'list'>, {'all_KL': [0.443], 'all_L1': [0.522]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.718], 'all_L1': [0.63]}), defaultdict(<class 'list'>, {'all_KL': [0.628], 'all_L1': [0.565]}), defaultdict(<class 'list'>, {'all_KL': [0.569], 'all_L1': [0.537]}), defaultdict(<class 'list'>, {'all_KL': [0.586], 'all_L1': [0.58]}), defaultdict(<class 'list'>, {'all_KL': [0.718], 'all_L1': [0.624]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.387], 'all_L1': [0.466]}), defaultdict(<class 'list'>, {'all_KL': [0.644], 'all_L1': [0.671]}), defaultdict(<class 'list'>, {'all_KL': [0.712], 'all_L1': [0.658]}), defaultdict(<class 'list'>, {'all_KL': [0.701], 'all_L1': [0.636]}), defaultdict(<class 'list'>, {'all_KL': [0.643], 'all_L1': [0.632]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.546], 'all_L1': [0.486]}), defaultdict(<class 'list'>, {'all_KL': [0.622], 'all_L1': [0.526]}), defaultdict(<class 'list'>, {'all_KL': [0.415], 'all_L1': [0.462]}), defaultdict(<class 'list'>, {'all_KL': [0.436], 'all_L1': [0.469]}), defaultdict(<class 'list'>, {'all_KL': [0.508], 'all_L1': [0.464]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.482 +- 0.031
suff++ class all_KL  =  0.420 +- 0.023
suff++_acc_int  =  0.524 +- 0.058
nec class all_L1  =  0.579 +- 0.034
nec class all_KL  =  0.627 +- 0.060
nec_acc_int  =  0.442 +- 0.020

Eval split val
suff++ class all_L1  =  0.659 +- 0.104
suff++ class all_KL  =  0.470 +- 0.195
suff++_acc_int  =  0.552 +- 0.127
nec class all_L1  =  0.236 +- 0.098
nec class all_KL  =  0.256 +- 0.091
nec_acc_int  =  0.431 +- 0.060

Eval split id_test
suff++ class all_L1  =  0.480 +- 0.029
suff++ class all_KL  =  0.410 +- 0.019
suff++_acc_int  =  0.521 +- 0.062
nec class all_L1  =  0.587 +- 0.035
nec class all_KL  =  0.644 +- 0.064
nec_acc_int  =  0.436 +- 0.024

Eval split test
suff++ class all_L1  =  0.613 +- 0.075
suff++ class all_KL  =  0.617 +- 0.119
suff++_acc_int  =  0.554 +- 0.086
nec class all_L1  =  0.481 +- 0.024
nec class all_KL  =  0.505 +- 0.075
nec_acc_int  =  0.513 +- 0.048


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.530 +- 0.030
Faith. Armon (L1)= 		  =  0.526 +- 0.030
Faith. GMean (L1)= 	  =  0.528 +- 0.030
Faith. Aritm (KL)= 		  =  0.524 +- 0.037
Faith. Armon (KL)= 		  =  0.502 +- 0.031
Faith. GMean (KL)= 	  =  0.513 +- 0.033

Eval split val
Faith. Aritm (L1)= 		  =  0.447 +- 0.058
Faith. Armon (L1)= 		  =  0.332 +- 0.095
Faith. GMean (L1)= 	  =  0.383 +- 0.073
Faith. Aritm (KL)= 		  =  0.363 +- 0.112
Faith. Armon (KL)= 		  =  0.314 +- 0.088
Faith. GMean (KL)= 	  =  0.336 +- 0.095

Eval split id_test
Faith. Aritm (L1)= 		  =  0.534 +- 0.030
Faith. Armon (L1)= 		  =  0.528 +- 0.030
Faith. GMean (L1)= 	  =  0.531 +- 0.030
Faith. Aritm (KL)= 		  =  0.527 +- 0.036
Faith. Armon (KL)= 		  =  0.500 +- 0.028
Faith. GMean (KL)= 	  =  0.513 +- 0.032

Eval split test
Faith. Aritm (L1)= 		  =  0.547 +- 0.040
Faith. Armon (L1)= 		  =  0.537 +- 0.036
Faith. GMean (L1)= 	  =  0.542 +- 0.038
Faith. Aritm (KL)= 		  =  0.561 +- 0.054
Faith. Armon (KL)= 		  =  0.543 +- 0.059
Faith. GMean (KL)= 	  =  0.552 +- 0.056
Computed for split load_split = id



Completed in  0:09:11.703551  for CIGAGIN GOODMotif/basis



DONE CIGA GOODMotif/basis readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:03:13 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:13 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:26 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:27 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:30 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:33 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:03:40 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ACCURACY: 0.8884
Train Loss: 0.4866
ID Validation ACCURACY: 0.8947
ID Validation Loss: 0.4630
ID Test ACCURACY: 0.8917
ID Test Loss: 0.4878
OOD Validation ACCURACY: 0.7910
OOD Validation Loss: 0.6662
OOD Test ACCURACY: 0.5353
OOD Test Loss: 1.1040

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ACCURACY: 0.8884
Train Loss: 0.4866
ID Validation ACCURACY: 0.8947
ID Validation Loss: 0.4630
ID Test ACCURACY: 0.8917
ID Test Loss: 0.4878
OOD Validation ACCURACY: 0.7910
OOD Validation Loss: 0.6662
OOD Test ACCURACY: 0.5353
OOD Test Loss: 1.1040

[0m[1;37mINFO[0m: [1mChartInfo 0.8917 0.5353 0.8917 0.5353 0.8947 0.7910[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.562
WIoU for r=0.8 = 0.473
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.284
WIoU for r=0.8 = 0.247
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1006, 1011,  983]))

Gold ratio (id_test) =  tensor(0.3660) +- tensor(0.1922)
F1 for r=0.8 = 0.577
WIoU for r=0.8 = 0.492
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.138
WIoU for r=0.8 = 0.172


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.89
Model XAI F1 of binarized graphs for r=0.8 =  0.5618962500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.47323125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.765
SUFF++ for r=0.8 class 0 = 0.717 +- 0.264 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.8 class 1 = 0.738 +- 0.264 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.8 class 2 = 0.882 +- 0.264 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.8 all KL = 0.806 +- 0.264 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.8 all L1 = 0.779 +- 0.240 (in-sample avg dev_std = 0.332)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.819
Model XAI F1 of binarized graphs for r=0.8 =  0.28399625
Model XAI WIoU of binarized graphs for r=0.8 =  0.24708375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.678
SUFF++ for r=0.8 class 0 = 0.634 +- 0.175 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 class 1 = 0.677 +- 0.175 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 class 2 = 0.782 +- 0.175 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 all KL = 0.808 +- 0.175 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 all L1 = 0.696 +- 0.183 (in-sample avg dev_std = 0.341)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.896
Model XAI F1 of binarized graphs for r=0.8 =  0.57701875
Model XAI WIoU of binarized graphs for r=0.8 =  0.49168125
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.788
SUFF++ for r=0.8 class 0 = 0.733 +- 0.254 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 class 1 = 0.758 +- 0.254 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 class 2 = 0.885 +- 0.254 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 all KL = 0.815 +- 0.254 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 all L1 = 0.791 +- 0.230 (in-sample avg dev_std = 0.325)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.536
Model XAI F1 of binarized graphs for r=0.8 =  0.1376925
Model XAI WIoU of binarized graphs for r=0.8 =  0.17154125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.526
SUFF++ for r=0.8 class 0 = 0.636 +- 0.190 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.8 class 1 = 0.633 +- 0.190 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.8 class 2 = 0.701 +- 0.190 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.8 all KL = 0.757 +- 0.190 (in-sample avg dev_std = 0.404)
SUFF++ for r=0.8 all L1 = 0.655 +- 0.146 (in-sample avg dev_std = 0.404)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.89
Model XAI F1 of binarized graphs for r=0.8 =  0.5618962500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.47323125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.458
NEC for r=0.8 class 0 = 0.647 +- 0.275 (in-sample avg dev_std = 0.574)
NEC for r=0.8 class 1 = 0.54 +- 0.275 (in-sample avg dev_std = 0.574)
NEC for r=0.8 class 2 = 0.598 +- 0.275 (in-sample avg dev_std = 0.574)
NEC for r=0.8 all KL = 0.679 +- 0.275 (in-sample avg dev_std = 0.574)
NEC for r=0.8 all L1 = 0.596 +- 0.166 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.819
Model XAI F1 of binarized graphs for r=0.8 =  0.28399625
Model XAI WIoU of binarized graphs for r=0.8 =  0.24708375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.445
NEC for r=0.8 class 0 = 0.569 +- 0.263 (in-sample avg dev_std = 0.380)
NEC for r=0.8 class 1 = 0.472 +- 0.263 (in-sample avg dev_std = 0.380)
NEC for r=0.8 class 2 = 0.487 +- 0.263 (in-sample avg dev_std = 0.380)
NEC for r=0.8 all KL = 0.413 +- 0.263 (in-sample avg dev_std = 0.380)
NEC for r=0.8 all L1 = 0.51 +- 0.172 (in-sample avg dev_std = 0.380)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.896
Model XAI F1 of binarized graphs for r=0.8 =  0.57701875
Model XAI WIoU of binarized graphs for r=0.8 =  0.49168125
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.468
NEC for r=0.8 class 0 = 0.655 +- 0.268 (in-sample avg dev_std = 0.559)
NEC for r=0.8 class 1 = 0.54 +- 0.268 (in-sample avg dev_std = 0.559)
NEC for r=0.8 class 2 = 0.594 +- 0.268 (in-sample avg dev_std = 0.559)
NEC for r=0.8 all KL = 0.688 +- 0.268 (in-sample avg dev_std = 0.559)
NEC for r=0.8 all L1 = 0.596 +- 0.168 (in-sample avg dev_std = 0.559)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.536
Model XAI F1 of binarized graphs for r=0.8 =  0.1376925
Model XAI WIoU of binarized graphs for r=0.8 =  0.17154125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.381
NEC for r=0.8 class 0 = 0.423 +- 0.212 (in-sample avg dev_std = 0.206)
NEC for r=0.8 class 1 = 0.399 +- 0.212 (in-sample avg dev_std = 0.206)
NEC for r=0.8 class 2 = 0.382 +- 0.212 (in-sample avg dev_std = 0.206)
NEC for r=0.8 all KL = 0.234 +- 0.212 (in-sample avg dev_std = 0.206)
NEC for r=0.8 all L1 = 0.402 +- 0.167 (in-sample avg dev_std = 0.206)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:06:04 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:04 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:17 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:18 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:20 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:24 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:06:31 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 44...
[0m[1;37mINFO[0m: [1mCheckpoint 44: 
-----------------------------------
Train ACCURACY: 0.8914
Train Loss: 0.5063
ID Validation ACCURACY: 0.8997
ID Validation Loss: 0.4882
ID Test ACCURACY: 0.8967
ID Test Loss: 0.4931
OOD Validation ACCURACY: 0.7990
OOD Validation Loss: 0.6163
OOD Test ACCURACY: 0.5463
OOD Test Loss: 5.1394

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 92...
[0m[1;37mINFO[0m: [1mCheckpoint 92: 
-----------------------------------
Train ACCURACY: 0.8750
Train Loss: 0.5856
ID Validation ACCURACY: 0.8820
ID Validation Loss: 0.5386
ID Test ACCURACY: 0.8793
ID Test Loss: 0.5812
OOD Validation ACCURACY: 0.8227
OOD Validation Loss: 0.6221
OOD Test ACCURACY: 0.5200
OOD Test Loss: 4.5541

[0m[1;37mINFO[0m: [1mChartInfo 0.8967 0.5463 0.8793 0.5200 0.8820 0.8227[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.554
WIoU for r=0.8 = 0.446
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.283
WIoU for r=0.8 = 0.229
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1006, 1011,  983]))

Gold ratio (id_test) =  tensor(0.3660) +- tensor(0.1922)
F1 for r=0.8 = 0.569
WIoU for r=0.8 = 0.464
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.137
WIoU for r=0.8 = 0.127


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.906
Model XAI F1 of binarized graphs for r=0.8 =  0.55353125
Model XAI WIoU of binarized graphs for r=0.8 =  0.44622874999999995
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.756
SUFF++ for r=0.8 class 0 = 0.773 +- 0.291 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.8 class 1 = 0.655 +- 0.291 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.8 class 2 = 0.817 +- 0.291 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.8 all KL = 0.737 +- 0.291 (in-sample avg dev_std = 0.383)
SUFF++ for r=0.8 all L1 = 0.75 +- 0.240 (in-sample avg dev_std = 0.383)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.831
Model XAI F1 of binarized graphs for r=0.8 =  0.2833025
Model XAI WIoU of binarized graphs for r=0.8 =  0.22862625
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.673
SUFF++ for r=0.8 class 0 = 0.607 +- 0.194 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 class 1 = 0.589 +- 0.194 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 class 2 = 0.665 +- 0.194 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 all KL = 0.706 +- 0.194 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 all L1 = 0.62 +- 0.165 (in-sample avg dev_std = 0.416)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.902
Model XAI F1 of binarized graphs for r=0.8 =  0.56871375
Model XAI WIoU of binarized graphs for r=0.8 =  0.4636725
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.775
SUFF++ for r=0.8 class 0 = 0.792 +- 0.289 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 1 = 0.688 +- 0.289 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 2 = 0.807 +- 0.289 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all KL = 0.752 +- 0.289 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all L1 = 0.762 +- 0.235 (in-sample avg dev_std = 0.372)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.543
Model XAI F1 of binarized graphs for r=0.8 =  0.137085
Model XAI WIoU of binarized graphs for r=0.8 =  0.12699625
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.458
SUFF++ for r=0.8 class 0 = 0.606 +- 0.314 (in-sample avg dev_std = 0.537)
SUFF++ for r=0.8 class 1 = 0.557 +- 0.314 (in-sample avg dev_std = 0.537)
SUFF++ for r=0.8 class 2 = 0.597 +- 0.314 (in-sample avg dev_std = 0.537)
SUFF++ for r=0.8 all KL = 0.592 +- 0.314 (in-sample avg dev_std = 0.537)
SUFF++ for r=0.8 all L1 = 0.587 +- 0.120 (in-sample avg dev_std = 0.537)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.906
Model XAI F1 of binarized graphs for r=0.8 =  0.55353125
Model XAI WIoU of binarized graphs for r=0.8 =  0.44622874999999995
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.446
NEC for r=0.8 class 0 = 0.601 +- 0.249 (in-sample avg dev_std = 0.624)
NEC for r=0.8 class 1 = 0.567 +- 0.249 (in-sample avg dev_std = 0.624)
NEC for r=0.8 class 2 = 0.632 +- 0.249 (in-sample avg dev_std = 0.624)
NEC for r=0.8 all KL = 0.757 +- 0.249 (in-sample avg dev_std = 0.624)
NEC for r=0.8 all L1 = 0.601 +- 0.157 (in-sample avg dev_std = 0.624)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.831
Model XAI F1 of binarized graphs for r=0.8 =  0.2833025
Model XAI WIoU of binarized graphs for r=0.8 =  0.22862625
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.434
NEC for r=0.8 class 0 = 0.557 +- 0.259 (in-sample avg dev_std = 0.443)
NEC for r=0.8 class 1 = 0.554 +- 0.259 (in-sample avg dev_std = 0.443)
NEC for r=0.8 class 2 = 0.587 +- 0.259 (in-sample avg dev_std = 0.443)
NEC for r=0.8 all KL = 0.54 +- 0.259 (in-sample avg dev_std = 0.443)
NEC for r=0.8 all L1 = 0.565 +- 0.154 (in-sample avg dev_std = 0.443)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.902
Model XAI F1 of binarized graphs for r=0.8 =  0.56871375
Model XAI WIoU of binarized graphs for r=0.8 =  0.4636725
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.447
NEC for r=0.8 class 0 = 0.601 +- 0.252 (in-sample avg dev_std = 0.618)
NEC for r=0.8 class 1 = 0.57 +- 0.252 (in-sample avg dev_std = 0.618)
NEC for r=0.8 class 2 = 0.625 +- 0.252 (in-sample avg dev_std = 0.618)
NEC for r=0.8 all KL = 0.761 +- 0.252 (in-sample avg dev_std = 0.618)
NEC for r=0.8 all L1 = 0.598 +- 0.161 (in-sample avg dev_std = 0.618)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.543
Model XAI F1 of binarized graphs for r=0.8 =  0.137085
Model XAI WIoU of binarized graphs for r=0.8 =  0.12699625
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.394
NEC for r=0.8 class 0 = 0.485 +- 0.325 (in-sample avg dev_std = 0.347)
NEC for r=0.8 class 1 = 0.497 +- 0.325 (in-sample avg dev_std = 0.347)
NEC for r=0.8 class 2 = 0.469 +- 0.325 (in-sample avg dev_std = 0.347)
NEC for r=0.8 all KL = 0.391 +- 0.325 (in-sample avg dev_std = 0.347)
NEC for r=0.8 all L1 = 0.484 +- 0.197 (in-sample avg dev_std = 0.347)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:08:47 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 11:08:47 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:08:59 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:01 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:03 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:07 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:09:13 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 52...
[0m[1;37mINFO[0m: [1mCheckpoint 52: 
-----------------------------------
Train ACCURACY: 0.8883
Train Loss: 0.4479
ID Validation ACCURACY: 0.8857
ID Validation Loss: 0.4413
ID Test ACCURACY: 0.8940
ID Test Loss: 0.4470
OOD Validation ACCURACY: 0.6337
OOD Validation Loss: 0.7853
OOD Test ACCURACY: 0.3707
OOD Test Loss: 1.3105

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 64...
[0m[1;37mINFO[0m: [1mCheckpoint 64: 
-----------------------------------
Train ACCURACY: 0.8548
Train Loss: 0.5436
ID Validation ACCURACY: 0.8480
ID Validation Loss: 0.5519
ID Test ACCURACY: 0.8573
ID Test Loss: 0.5426
OOD Validation ACCURACY: 0.7287
OOD Validation Loss: 0.9406
OOD Test ACCURACY: 0.3510
OOD Test Loss: 8.4851

[0m[1;37mINFO[0m: [1mChartInfo 0.8940 0.3707 0.8573 0.3510 0.8480 0.7287[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.438
WIoU for r=0.8 = 0.370
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.190
WIoU for r=0.8 = 0.179
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1006, 1011,  983]))

Gold ratio (id_test) =  tensor(0.3660) +- tensor(0.1922)
F1 for r=0.8 = 0.455
WIoU for r=0.8 = 0.386
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.093
WIoU for r=0.8 = 0.109


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.885
Model XAI F1 of binarized graphs for r=0.8 =  0.43842
Model XAI WIoU of binarized graphs for r=0.8 =  0.36997125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.652
SUFF++ for r=0.8 class 0 = 0.373 +- 0.326 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 class 1 = 0.676 +- 0.326 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 class 2 = 0.765 +- 0.326 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 all KL = 0.642 +- 0.326 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 all L1 = 0.603 +- 0.224 (in-sample avg dev_std = 0.341)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.666
Model XAI F1 of binarized graphs for r=0.8 =  0.19045125
Model XAI WIoU of binarized graphs for r=0.8 =  0.17883749999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.485
SUFF++ for r=0.8 class 0 = 0.483 +- 0.227 (in-sample avg dev_std = 0.307)
SUFF++ for r=0.8 class 1 = 0.665 +- 0.227 (in-sample avg dev_std = 0.307)
SUFF++ for r=0.8 class 2 = 0.632 +- 0.227 (in-sample avg dev_std = 0.307)
SUFF++ for r=0.8 all KL = 0.731 +- 0.227 (in-sample avg dev_std = 0.307)
SUFF++ for r=0.8 all L1 = 0.592 +- 0.192 (in-sample avg dev_std = 0.307)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.902
Model XAI F1 of binarized graphs for r=0.8 =  0.45455625
Model XAI WIoU of binarized graphs for r=0.8 =  0.38575624999999997
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.654
SUFF++ for r=0.8 class 0 = 0.373 +- 0.326 (in-sample avg dev_std = 0.356)
SUFF++ for r=0.8 class 1 = 0.67 +- 0.326 (in-sample avg dev_std = 0.356)
SUFF++ for r=0.8 class 2 = 0.747 +- 0.326 (in-sample avg dev_std = 0.356)
SUFF++ for r=0.8 all KL = 0.628 +- 0.326 (in-sample avg dev_std = 0.356)
SUFF++ for r=0.8 all L1 = 0.596 +- 0.223 (in-sample avg dev_std = 0.356)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.376
Model XAI F1 of binarized graphs for r=0.8 =  0.09318749999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.10880374999999999
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.389
SUFF++ for r=0.8 class 0 = 0.596 +- 0.206 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 class 1 = 0.657 +- 0.206 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 class 2 = 0.602 +- 0.206 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 all KL = 0.742 +- 0.206 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 all L1 = 0.619 +- 0.160 (in-sample avg dev_std = 0.360)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.885
Model XAI F1 of binarized graphs for r=0.8 =  0.43842
Model XAI WIoU of binarized graphs for r=0.8 =  0.36997125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.445
NEC for r=0.8 class 0 = 0.599 +- 0.294 (in-sample avg dev_std = 0.517)
NEC for r=0.8 class 1 = 0.452 +- 0.294 (in-sample avg dev_std = 0.517)
NEC for r=0.8 class 2 = 0.599 +- 0.294 (in-sample avg dev_std = 0.517)
NEC for r=0.8 all KL = 0.569 +- 0.294 (in-sample avg dev_std = 0.517)
NEC for r=0.8 all L1 = 0.552 +- 0.181 (in-sample avg dev_std = 0.517)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.666
Model XAI F1 of binarized graphs for r=0.8 =  0.19045125
Model XAI WIoU of binarized graphs for r=0.8 =  0.17883749999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.404
NEC for r=0.8 class 0 = 0.539 +- 0.233 (in-sample avg dev_std = 0.321)
NEC for r=0.8 class 1 = 0.433 +- 0.233 (in-sample avg dev_std = 0.321)
NEC for r=0.8 class 2 = 0.495 +- 0.233 (in-sample avg dev_std = 0.321)
NEC for r=0.8 all KL = 0.358 +- 0.233 (in-sample avg dev_std = 0.321)
NEC for r=0.8 all L1 = 0.489 +- 0.187 (in-sample avg dev_std = 0.321)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.902
Model XAI F1 of binarized graphs for r=0.8 =  0.45455625
Model XAI WIoU of binarized graphs for r=0.8 =  0.38575624999999997
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.458
NEC for r=0.8 class 0 = 0.613 +- 0.303 (in-sample avg dev_std = 0.506)
NEC for r=0.8 class 1 = 0.444 +- 0.303 (in-sample avg dev_std = 0.506)
NEC for r=0.8 class 2 = 0.598 +- 0.303 (in-sample avg dev_std = 0.506)
NEC for r=0.8 all KL = 0.57 +- 0.303 (in-sample avg dev_std = 0.506)
NEC for r=0.8 all L1 = 0.551 +- 0.187 (in-sample avg dev_std = 0.506)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.376
Model XAI F1 of binarized graphs for r=0.8 =  0.09318749999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.10880374999999999
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.352
NEC for r=0.8 class 0 = 0.458 +- 0.309 (in-sample avg dev_std = 0.263)
NEC for r=0.8 class 1 = 0.451 +- 0.309 (in-sample avg dev_std = 0.263)
NEC for r=0.8 class 2 = 0.448 +- 0.309 (in-sample avg dev_std = 0.263)
NEC for r=0.8 all KL = 0.344 +- 0.309 (in-sample avg dev_std = 0.263)
NEC for r=0.8 all L1 = 0.452 +- 0.241 (in-sample avg dev_std = 0.263)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:11:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:31 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:44 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:45 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:48 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:51 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:11:58 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8933
Train Loss: 0.5080
ID Validation ACCURACY: 0.8957
ID Validation Loss: 0.4867
ID Test ACCURACY: 0.8957
ID Test Loss: 0.5108
OOD Validation ACCURACY: 0.8220
OOD Validation Loss: 0.6464
OOD Test ACCURACY: 0.5567
OOD Test Loss: 2.8145

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8933
Train Loss: 0.5080
ID Validation ACCURACY: 0.8957
ID Validation Loss: 0.4867
ID Test ACCURACY: 0.8957
ID Test Loss: 0.5108
OOD Validation ACCURACY: 0.8220
OOD Validation Loss: 0.6464
OOD Test ACCURACY: 0.5567
OOD Test Loss: 2.8145

[0m[1;37mINFO[0m: [1mChartInfo 0.8957 0.5567 0.8957 0.5567 0.8957 0.8220[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.561
WIoU for r=0.8 = 0.485
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.284
WIoU for r=0.8 = 0.257
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1006, 1011,  983]))

Gold ratio (id_test) =  tensor(0.3660) +- tensor(0.1922)
F1 for r=0.8 = 0.577
WIoU for r=0.8 = 0.504
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.138
WIoU for r=0.8 = 0.185


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.897
Model XAI F1 of binarized graphs for r=0.8 =  0.56119
Model XAI WIoU of binarized graphs for r=0.8 =  0.485355
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.763
SUFF++ for r=0.8 class 0 = 0.713 +- 0.277 (in-sample avg dev_std = 0.320)
SUFF++ for r=0.8 class 1 = 0.721 +- 0.277 (in-sample avg dev_std = 0.320)
SUFF++ for r=0.8 class 2 = 0.917 +- 0.277 (in-sample avg dev_std = 0.320)
SUFF++ for r=0.8 all KL = 0.807 +- 0.277 (in-sample avg dev_std = 0.320)
SUFF++ for r=0.8 all L1 = 0.784 +- 0.250 (in-sample avg dev_std = 0.320)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.854
Model XAI F1 of binarized graphs for r=0.8 =  0.28409375000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.2566375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.681
SUFF++ for r=0.8 class 0 = 0.619 +- 0.172 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.8 class 1 = 0.618 +- 0.172 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.8 class 2 = 0.774 +- 0.172 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.8 all KL = 0.788 +- 0.172 (in-sample avg dev_std = 0.328)
SUFF++ for r=0.8 all L1 = 0.669 +- 0.175 (in-sample avg dev_std = 0.328)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.9
Model XAI F1 of binarized graphs for r=0.8 =  0.5767387500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.5042775
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.785
SUFF++ for r=0.8 class 0 = 0.725 +- 0.268 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 class 1 = 0.758 +- 0.268 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 class 2 = 0.92 +- 0.268 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 all KL = 0.818 +- 0.268 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 all L1 = 0.8 +- 0.237 (in-sample avg dev_std = 0.325)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.571
Model XAI F1 of binarized graphs for r=0.8 =  0.13754375
Model XAI WIoU of binarized graphs for r=0.8 =  0.18451499999999998
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.489
SUFF++ for r=0.8 class 0 = 0.594 +- 0.305 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.8 class 1 = 0.588 +- 0.305 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.8 class 2 = 0.623 +- 0.305 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.8 all KL = 0.642 +- 0.305 (in-sample avg dev_std = 0.477)
SUFF++ for r=0.8 all L1 = 0.601 +- 0.117 (in-sample avg dev_std = 0.477)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.897
Model XAI F1 of binarized graphs for r=0.8 =  0.56119
Model XAI WIoU of binarized graphs for r=0.8 =  0.485355
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.465
NEC for r=0.8 class 0 = 0.612 +- 0.268 (in-sample avg dev_std = 0.592)
NEC for r=0.8 class 1 = 0.559 +- 0.268 (in-sample avg dev_std = 0.592)
NEC for r=0.8 class 2 = 0.587 +- 0.268 (in-sample avg dev_std = 0.592)
NEC for r=0.8 all KL = 0.715 +- 0.268 (in-sample avg dev_std = 0.592)
NEC for r=0.8 all L1 = 0.586 +- 0.167 (in-sample avg dev_std = 0.592)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.854
Model XAI F1 of binarized graphs for r=0.8 =  0.28409375000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.2566375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.43
NEC for r=0.8 class 0 = 0.563 +- 0.232 (in-sample avg dev_std = 0.429)
NEC for r=0.8 class 1 = 0.53 +- 0.232 (in-sample avg dev_std = 0.429)
NEC for r=0.8 class 2 = 0.514 +- 0.232 (in-sample avg dev_std = 0.429)
NEC for r=0.8 all KL = 0.474 +- 0.232 (in-sample avg dev_std = 0.429)
NEC for r=0.8 all L1 = 0.536 +- 0.146 (in-sample avg dev_std = 0.429)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.9
Model XAI F1 of binarized graphs for r=0.8 =  0.5767387500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.5042775
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.469
NEC for r=0.8 class 0 = 0.613 +- 0.259 (in-sample avg dev_std = 0.585)
NEC for r=0.8 class 1 = 0.574 +- 0.259 (in-sample avg dev_std = 0.585)
NEC for r=0.8 class 2 = 0.597 +- 0.259 (in-sample avg dev_std = 0.585)
NEC for r=0.8 all KL = 0.73 +- 0.259 (in-sample avg dev_std = 0.585)
NEC for r=0.8 all L1 = 0.595 +- 0.159 (in-sample avg dev_std = 0.585)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.571
Model XAI F1 of binarized graphs for r=0.8 =  0.13754375
Model XAI WIoU of binarized graphs for r=0.8 =  0.18451499999999998
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.412
NEC for r=0.8 class 0 = 0.465 +- 0.332 (in-sample avg dev_std = 0.432)
NEC for r=0.8 class 1 = 0.49 +- 0.332 (in-sample avg dev_std = 0.432)
NEC for r=0.8 class 2 = 0.48 +- 0.332 (in-sample avg dev_std = 0.432)
NEC for r=0.8 all KL = 0.378 +- 0.332 (in-sample avg dev_std = 0.432)
NEC for r=0.8 all L1 = 0.478 +- 0.182 (in-sample avg dev_std = 0.432)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:14:18 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:18 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:29 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:31 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:34 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:37 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:14:44 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 36...
[0m[1;37mINFO[0m: [1mCheckpoint 36: 
-----------------------------------
Train ACCURACY: 0.8711
Train Loss: 0.5166
ID Validation ACCURACY: 0.8697
ID Validation Loss: 0.5030
ID Test ACCURACY: 0.8843
ID Test Loss: 0.5074
OOD Validation ACCURACY: 0.7277
OOD Validation Loss: 0.7653
OOD Test ACCURACY: 0.3917
OOD Test Loss: 1.9534

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8627
Train Loss: 0.6287
ID Validation ACCURACY: 0.8663
ID Validation Loss: 0.5823
ID Test ACCURACY: 0.8713
ID Test Loss: 0.6228
OOD Validation ACCURACY: 0.7327
OOD Validation Loss: 0.7117
OOD Test ACCURACY: 0.4613
OOD Test Loss: 1.5428

[0m[1;37mINFO[0m: [1mChartInfo 0.8843 0.3917 0.8713 0.4613 0.8663 0.7327[0mGOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.430
WIoU for r=0.8 = 0.344
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.182
WIoU for r=0.8 = 0.153
GOODMotif(3000)
Data example from id_test: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([1006, 1011,  983]))

Gold ratio (id_test) =  tensor(0.3660) +- tensor(0.1922)
F1 for r=0.8 = 0.443
WIoU for r=0.8 = 0.358
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.099
WIoU for r=0.8 = 0.109


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.876
Model XAI F1 of binarized graphs for r=0.8 =  0.42980374999999993
Model XAI WIoU of binarized graphs for r=0.8 =  0.34399875
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.635
SUFF++ for r=0.8 class 0 = 0.379 +- 0.358 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 class 1 = 0.718 +- 0.358 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 class 2 = 0.831 +- 0.358 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 all KL = 0.655 +- 0.358 (in-sample avg dev_std = 0.325)
SUFF++ for r=0.8 all L1 = 0.641 +- 0.252 (in-sample avg dev_std = 0.325)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.761
Model XAI F1 of binarized graphs for r=0.8 =  0.18231124999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.1527875
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.54
SUFF++ for r=0.8 class 0 = 0.436 +- 0.291 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 class 1 = 0.612 +- 0.291 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 class 2 = 0.657 +- 0.291 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 all KL = 0.675 +- 0.291 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 all L1 = 0.566 +- 0.210 (in-sample avg dev_std = 0.283)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.886
Model XAI F1 of binarized graphs for r=0.8 =  0.44322375
Model XAI WIoU of binarized graphs for r=0.8 =  0.35837125000000003
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.637
SUFF++ for r=0.8 class 0 = 0.372 +- 0.365 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.8 class 1 = 0.729 +- 0.365 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.8 class 2 = 0.833 +- 0.365 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.8 all KL = 0.646 +- 0.365 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.8 all L1 = 0.644 +- 0.252 (in-sample avg dev_std = 0.347)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.385
Model XAI F1 of binarized graphs for r=0.8 =  0.0994325
Model XAI WIoU of binarized graphs for r=0.8 =  0.10938
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.435
SUFF++ for r=0.8 class 0 = 0.489 +- 0.331 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 class 1 = 0.573 +- 0.331 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 class 2 = 0.493 +- 0.331 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 all KL = 0.608 +- 0.331 (in-sample avg dev_std = 0.310)
SUFF++ for r=0.8 all L1 = 0.519 +- 0.235 (in-sample avg dev_std = 0.310)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.876
Model XAI F1 of binarized graphs for r=0.8 =  0.42980374999999993
Model XAI WIoU of binarized graphs for r=0.8 =  0.34399875
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.462
NEC for r=0.8 class 0 = 0.571 +- 0.319 (in-sample avg dev_std = 0.496)
NEC for r=0.8 class 1 = 0.431 +- 0.319 (in-sample avg dev_std = 0.496)
NEC for r=0.8 class 2 = 0.594 +- 0.319 (in-sample avg dev_std = 0.496)
NEC for r=0.8 all KL = 0.571 +- 0.319 (in-sample avg dev_std = 0.496)
NEC for r=0.8 all L1 = 0.534 +- 0.188 (in-sample avg dev_std = 0.496)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.761
Model XAI F1 of binarized graphs for r=0.8 =  0.18231124999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.1527875
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.434
NEC for r=0.8 class 0 = 0.581 +- 0.289 (in-sample avg dev_std = 0.305)
NEC for r=0.8 class 1 = 0.475 +- 0.289 (in-sample avg dev_std = 0.305)
NEC for r=0.8 class 2 = 0.553 +- 0.289 (in-sample avg dev_std = 0.305)
NEC for r=0.8 all KL = 0.447 +- 0.289 (in-sample avg dev_std = 0.305)
NEC for r=0.8 all L1 = 0.536 +- 0.197 (in-sample avg dev_std = 0.305)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.886
Model XAI F1 of binarized graphs for r=0.8 =  0.44322375
Model XAI WIoU of binarized graphs for r=0.8 =  0.35837125000000003
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.467
NEC for r=0.8 class 0 = 0.573 +- 0.329 (in-sample avg dev_std = 0.493)
NEC for r=0.8 class 1 = 0.423 +- 0.329 (in-sample avg dev_std = 0.493)
NEC for r=0.8 class 2 = 0.592 +- 0.329 (in-sample avg dev_std = 0.493)
NEC for r=0.8 all KL = 0.568 +- 0.329 (in-sample avg dev_std = 0.493)
NEC for r=0.8 all L1 = 0.528 +- 0.190 (in-sample avg dev_std = 0.493)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.385
Model XAI F1 of binarized graphs for r=0.8 =  0.0994325
Model XAI WIoU of binarized graphs for r=0.8 =  0.10938
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.352
NEC for r=0.8 class 0 = 0.481 +- 0.372 (in-sample avg dev_std = 0.191)
NEC for r=0.8 class 1 = 0.492 +- 0.372 (in-sample avg dev_std = 0.191)
NEC for r=0.8 class 2 = 0.484 +- 0.372 (in-sample avg dev_std = 0.191)
NEC for r=0.8 all KL = 0.414 +- 0.372 (in-sample avg dev_std = 0.191)
NEC for r=0.8 all L1 = 0.486 +- 0.290 (in-sample avg dev_std = 0.191)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.806], 'all_L1': [0.779]}), defaultdict(<class 'list'>, {'all_KL': [0.737], 'all_L1': [0.75]}), defaultdict(<class 'list'>, {'all_KL': [0.642], 'all_L1': [0.603]}), defaultdict(<class 'list'>, {'all_KL': [0.807], 'all_L1': [0.784]}), defaultdict(<class 'list'>, {'all_KL': [0.655], 'all_L1': [0.641]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.679], 'all_L1': [0.596]}), defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.601]}), defaultdict(<class 'list'>, {'all_KL': [0.569], 'all_L1': [0.552]}), defaultdict(<class 'list'>, {'all_KL': [0.715], 'all_L1': [0.586]}), defaultdict(<class 'list'>, {'all_KL': [0.571], 'all_L1': [0.534]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.808], 'all_L1': [0.696]}), defaultdict(<class 'list'>, {'all_KL': [0.706], 'all_L1': [0.62]}), defaultdict(<class 'list'>, {'all_KL': [0.731], 'all_L1': [0.592]}), defaultdict(<class 'list'>, {'all_KL': [0.788], 'all_L1': [0.669]}), defaultdict(<class 'list'>, {'all_KL': [0.675], 'all_L1': [0.566]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.413], 'all_L1': [0.51]}), defaultdict(<class 'list'>, {'all_KL': [0.54], 'all_L1': [0.565]}), defaultdict(<class 'list'>, {'all_KL': [0.358], 'all_L1': [0.489]}), defaultdict(<class 'list'>, {'all_KL': [0.474], 'all_L1': [0.536]}), defaultdict(<class 'list'>, {'all_KL': [0.447], 'all_L1': [0.536]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.815], 'all_L1': [0.791]}), defaultdict(<class 'list'>, {'all_KL': [0.752], 'all_L1': [0.762]}), defaultdict(<class 'list'>, {'all_KL': [0.628], 'all_L1': [0.596]}), defaultdict(<class 'list'>, {'all_KL': [0.818], 'all_L1': [0.8]}), defaultdict(<class 'list'>, {'all_KL': [0.646], 'all_L1': [0.644]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.688], 'all_L1': [0.596]}), defaultdict(<class 'list'>, {'all_KL': [0.761], 'all_L1': [0.598]}), defaultdict(<class 'list'>, {'all_KL': [0.57], 'all_L1': [0.551]}), defaultdict(<class 'list'>, {'all_KL': [0.73], 'all_L1': [0.595]}), defaultdict(<class 'list'>, {'all_KL': [0.568], 'all_L1': [0.528]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.655]}), defaultdict(<class 'list'>, {'all_KL': [0.592], 'all_L1': [0.587]}), defaultdict(<class 'list'>, {'all_KL': [0.742], 'all_L1': [0.619]}), defaultdict(<class 'list'>, {'all_KL': [0.642], 'all_L1': [0.601]}), defaultdict(<class 'list'>, {'all_KL': [0.608], 'all_L1': [0.519]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.234], 'all_L1': [0.402]}), defaultdict(<class 'list'>, {'all_KL': [0.391], 'all_L1': [0.484]}), defaultdict(<class 'list'>, {'all_KL': [0.344], 'all_L1': [0.452]}), defaultdict(<class 'list'>, {'all_KL': [0.378], 'all_L1': [0.478]}), defaultdict(<class 'list'>, {'all_KL': [0.414], 'all_L1': [0.486]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.711 +- 0.075
suff++ class all_KL  =  0.729 +- 0.071
suff++_acc_int  =  0.714 +- 0.058
nec class all_L1  =  0.574 +- 0.026
nec class all_KL  =  0.658 +- 0.076
nec_acc_int  =  0.455 +- 0.008

Eval split val
suff++ class all_L1  =  0.629 +- 0.048
suff++ class all_KL  =  0.742 +- 0.050
suff++_acc_int  =  0.611 +- 0.083
nec class all_L1  =  0.527 +- 0.026
nec class all_KL  =  0.446 +- 0.061
nec_acc_int  =  0.430 +- 0.013

Eval split id_test
suff++ class all_L1  =  0.719 +- 0.083
suff++ class all_KL  =  0.732 +- 0.081
suff++_acc_int  =  0.728 +- 0.068
nec class all_L1  =  0.574 +- 0.029
nec class all_KL  =  0.663 +- 0.080
nec_acc_int  =  0.462 +- 0.008

Eval split test
suff++ class all_L1  =  0.596 +- 0.045
suff++ class all_KL  =  0.668 +- 0.068
suff++_acc_int  =  0.459 +- 0.047
nec class all_L1  =  0.460 +- 0.032
nec class all_KL  =  0.352 +- 0.063
nec_acc_int  =  0.378 +- 0.024


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.643 +- 0.049
Faith. Armon (L1)= 		  =  0.634 +- 0.045
Faith. GMean (L1)= 	  =  0.639 +- 0.047
Faith. Aritm (KL)= 		  =  0.694 +- 0.069
Faith. Armon (KL)= 		  =  0.691 +- 0.069
Faith. GMean (KL)= 	  =  0.692 +- 0.069

Eval split val
Faith. Aritm (L1)= 		  =  0.578 +- 0.027
Faith. Armon (L1)= 		  =  0.572 +- 0.024
Faith. GMean (L1)= 	  =  0.575 +- 0.025
Faith. Aritm (KL)= 		  =  0.594 +- 0.035
Faith. Armon (KL)= 		  =  0.554 +- 0.046
Faith. GMean (KL)= 	  =  0.573 +- 0.039

Eval split id_test
Faith. Aritm (L1)= 		  =  0.646 +- 0.055
Faith. Armon (L1)= 		  =  0.637 +- 0.050
Faith. GMean (L1)= 	  =  0.642 +- 0.052
Faith. Aritm (KL)= 		  =  0.698 +- 0.078
Faith. Armon (KL)= 		  =  0.695 +- 0.077
Faith. GMean (KL)= 	  =  0.696 +- 0.077

Eval split test
Faith. Aritm (L1)= 		  =  0.528 +- 0.013
Faith. Armon (L1)= 		  =  0.517 +- 0.014
Faith. GMean (L1)= 	  =  0.523 +- 0.013
Faith. Aritm (KL)= 		  =  0.510 +- 0.018
Faith. Armon (KL)= 		  =  0.453 +- 0.049
Faith. GMean (KL)= 	  =  0.480 +- 0.031
Computed for split load_split = id



Completed in  0:13:47.977952  for CIGAGIN GOODMotif/size



DONE CIGA GOODMotif/size readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:17:13 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:17:13 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.9018
Train Loss: 0.4354
ID Validation ACCURACY: 0.9037
ID Validation Loss: 0.4370
ID Test ACCURACY: 0.8970
ID Test Loss: 0.4710
OOD Validation ACCURACY: 0.7420
OOD Validation Loss: 0.6904
OOD Test ACCURACY: 0.4850
OOD Test Loss: 11.6168

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 89...
[0m[1;37mINFO[0m: [1mCheckpoint 89: 
-----------------------------------
Train ACCURACY: 0.8768
Train Loss: 0.4361
ID Validation ACCURACY: 0.8860
ID Validation Loss: 0.4229
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4687
OOD Validation ACCURACY: 0.8807
OOD Validation Loss: 0.4940
OOD Test ACCURACY: 0.4280
OOD Test Loss: 6.8125

[0m[1;37mINFO[0m: [1mChartInfo 0.8970 0.4850 0.8727 0.4280 0.8860 0.8807[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.395
WIoU for r=0.8 = 0.275
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.202
WIoU for r=0.8 = 0.137
GOODMotif2(3000)
Data example from id_test: Data(edge_index=[2, 56], x=[16, 1], node_gt=[16], edge_gt=[56], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=16)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([ 988,  973, 1039]))

Gold ratio (id_test) =  tensor(0.3032) +- tensor(0.1592)
F1 for r=0.8 = 0.393
WIoU for r=0.8 = 0.276
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.293
WIoU for r=0.8 = 0.163


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.919
Model XAI F1 of binarized graphs for r=0.8 =  0.3951125
Model XAI WIoU of binarized graphs for r=0.8 =  0.27478875
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.578
SUFF++ for r=0.8 class 0 = 0.359 +- 0.303 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 class 1 = 0.633 +- 0.303 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 class 2 = 0.612 +- 0.303 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 all KL = 0.567 +- 0.303 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 all L1 = 0.534 +- 0.214 (in-sample avg dev_std = 0.447)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  0.20218375000000002
Model XAI WIoU of binarized graphs for r=0.8 =  0.1370225
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.456
SUFF++ for r=0.8 class 0 = 0.418 +- 0.250 (in-sample avg dev_std = 0.467)
SUFF++ for r=0.8 class 1 = 0.699 +- 0.250 (in-sample avg dev_std = 0.467)
SUFF++ for r=0.8 class 2 = 0.466 +- 0.250 (in-sample avg dev_std = 0.467)
SUFF++ for r=0.8 all KL = 0.606 +- 0.250 (in-sample avg dev_std = 0.467)
SUFF++ for r=0.8 all L1 = 0.526 +- 0.176 (in-sample avg dev_std = 0.467)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.904
Model XAI F1 of binarized graphs for r=0.8 =  0.39343249999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.2757425
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.574
SUFF++ for r=0.8 class 0 = 0.373 +- 0.295 (in-sample avg dev_std = 0.435)
SUFF++ for r=0.8 class 1 = 0.637 +- 0.295 (in-sample avg dev_std = 0.435)
SUFF++ for r=0.8 class 2 = 0.622 +- 0.295 (in-sample avg dev_std = 0.435)
SUFF++ for r=0.8 all KL = 0.579 +- 0.295 (in-sample avg dev_std = 0.435)
SUFF++ for r=0.8 all L1 = 0.545 +- 0.212 (in-sample avg dev_std = 0.435)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.491
Model XAI F1 of binarized graphs for r=0.8 =  0.29258375000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.16311374999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.426
SUFF++ for r=0.8 class 0 = 0.73 +- 0.370 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 class 1 = 0.777 +- 0.370 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 class 2 = 0.694 +- 0.370 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 all KL = 0.617 +- 0.370 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 all L1 = 0.734 +- 0.281 (in-sample avg dev_std = 0.410)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.919
Model XAI F1 of binarized graphs for r=0.8 =  0.3951125
Model XAI WIoU of binarized graphs for r=0.8 =  0.27478875
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.505
NEC for r=0.8 class 0 = 0.649 +- 0.290 (in-sample avg dev_std = 0.523)
NEC for r=0.8 class 1 = 0.393 +- 0.290 (in-sample avg dev_std = 0.523)
NEC for r=0.8 class 2 = 0.638 +- 0.290 (in-sample avg dev_std = 0.523)
NEC for r=0.8 all KL = 0.553 +- 0.290 (in-sample avg dev_std = 0.523)
NEC for r=0.8 all L1 = 0.559 +- 0.175 (in-sample avg dev_std = 0.523)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  0.20218375000000002
Model XAI WIoU of binarized graphs for r=0.8 =  0.1370225
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.436
NEC for r=0.8 class 0 = 0.625 +- 0.236 (in-sample avg dev_std = 0.471)
NEC for r=0.8 class 1 = 0.431 +- 0.236 (in-sample avg dev_std = 0.471)
NEC for r=0.8 class 2 = 0.657 +- 0.236 (in-sample avg dev_std = 0.471)
NEC for r=0.8 all KL = 0.508 +- 0.236 (in-sample avg dev_std = 0.471)
NEC for r=0.8 all L1 = 0.572 +- 0.146 (in-sample avg dev_std = 0.471)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.904
Model XAI F1 of binarized graphs for r=0.8 =  0.39343249999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.2757425
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.497
NEC for r=0.8 class 0 = 0.642 +- 0.290 (in-sample avg dev_std = 0.526)
NEC for r=0.8 class 1 = 0.384 +- 0.290 (in-sample avg dev_std = 0.526)
NEC for r=0.8 class 2 = 0.629 +- 0.290 (in-sample avg dev_std = 0.526)
NEC for r=0.8 all KL = 0.552 +- 0.290 (in-sample avg dev_std = 0.526)
NEC for r=0.8 all L1 = 0.554 +- 0.182 (in-sample avg dev_std = 0.526)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.491
Model XAI F1 of binarized graphs for r=0.8 =  0.29258375000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.16311374999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.416
NEC for r=0.8 class 0 = 0.414 +- 0.357 (in-sample avg dev_std = 0.559)
NEC for r=0.8 class 1 = 0.245 +- 0.357 (in-sample avg dev_std = 0.559)
NEC for r=0.8 class 2 = 0.484 +- 0.357 (in-sample avg dev_std = 0.559)
NEC for r=0.8 all KL = 0.585 +- 0.357 (in-sample avg dev_std = 0.559)
NEC for r=0.8 all L1 = 0.38 +- 0.274 (in-sample avg dev_std = 0.559)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:18:47 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:18:47 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 13...
[0m[1;37mINFO[0m: [1mCheckpoint 13: 
-----------------------------------
Train ACCURACY: 0.9209
Train Loss: 0.4610
ID Validation ACCURACY: 0.9260
ID Validation Loss: 0.4490
ID Test ACCURACY: 0.9150
ID Test Loss: 0.5098
OOD Validation ACCURACY: 0.8717
OOD Validation Loss: 0.5431
OOD Test ACCURACY: 0.3343
OOD Test Loss: 29.2798

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.8146
Train Loss: 0.6065
ID Validation ACCURACY: 0.8143
ID Validation Loss: 0.6034
ID Test ACCURACY: 0.8157
ID Test Loss: 0.6423
OOD Validation ACCURACY: 0.9270
OOD Validation Loss: 0.4652
OOD Test ACCURACY: 0.3343
OOD Test Loss: 40.5826

[0m[1;37mINFO[0m: [1mChartInfo 0.9150 0.3343 0.8157 0.3343 0.8143 0.9270[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.520
WIoU for r=0.8 = 0.417
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.325
WIoU for r=0.8 = 0.273
GOODMotif2(3000)
Data example from id_test: Data(edge_index=[2, 56], x=[16, 1], node_gt=[16], edge_gt=[56], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=16)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([ 988,  973, 1039]))

Gold ratio (id_test) =  tensor(0.3032) +- tensor(0.1592)
F1 for r=0.8 = 0.513
WIoU for r=0.8 = 0.410
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.346
WIoU for r=0.8 = 0.209


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.936
Model XAI F1 of binarized graphs for r=0.8 =  0.5201175
Model XAI WIoU of binarized graphs for r=0.8 =  0.41655749999999997
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.635
SUFF++ for r=0.8 class 0 = 0.367 +- 0.370 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 class 1 = 0.63 +- 0.370 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 class 2 = 0.898 +- 0.370 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 all KL = 0.591 +- 0.370 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.8 all L1 = 0.631 +- 0.319 (in-sample avg dev_std = 0.447)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.875
Model XAI F1 of binarized graphs for r=0.8 =  0.324805
Model XAI WIoU of binarized graphs for r=0.8 =  0.27318375000000006
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.624
SUFF++ for r=0.8 class 0 = 0.449 +- 0.209 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 class 1 = 0.573 +- 0.209 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 class 2 = 0.871 +- 0.209 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 all KL = 0.701 +- 0.209 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.8 all L1 = 0.632 +- 0.234 (in-sample avg dev_std = 0.390)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.923
Model XAI F1 of binarized graphs for r=0.8 =  0.5125475
Model XAI WIoU of binarized graphs for r=0.8 =  0.40986625000000004
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.625
SUFF++ for r=0.8 class 0 = 0.368 +- 0.373 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.8 class 1 = 0.599 +- 0.373 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.8 class 2 = 0.879 +- 0.373 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.8 all KL = 0.581 +- 0.373 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.8 all L1 = 0.62 +- 0.322 (in-sample avg dev_std = 0.432)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.3457124999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.20945375
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.338
SUFF++ for r=0.8 class 0 = 0.995 +- 0.089 (in-sample avg dev_std = 0.106)
SUFF++ for r=0.8 class 1 = 0.953 +- 0.089 (in-sample avg dev_std = 0.106)
SUFF++ for r=0.8 class 2 = 0.999 +- 0.089 (in-sample avg dev_std = 0.106)
SUFF++ for r=0.8 all KL = 0.971 +- 0.089 (in-sample avg dev_std = 0.106)
SUFF++ for r=0.8 all L1 = 0.982 +- 0.057 (in-sample avg dev_std = 0.106)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.936
Model XAI F1 of binarized graphs for r=0.8 =  0.5201175
Model XAI WIoU of binarized graphs for r=0.8 =  0.41655749999999997
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.495
NEC for r=0.8 class 0 = 0.609 +- 0.256 (in-sample avg dev_std = 0.669)
NEC for r=0.8 class 1 = 0.49 +- 0.256 (in-sample avg dev_std = 0.669)
NEC for r=0.8 class 2 = 0.607 +- 0.256 (in-sample avg dev_std = 0.669)
NEC for r=0.8 all KL = 0.727 +- 0.256 (in-sample avg dev_std = 0.669)
NEC for r=0.8 all L1 = 0.568 +- 0.174 (in-sample avg dev_std = 0.669)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.875
Model XAI F1 of binarized graphs for r=0.8 =  0.324805
Model XAI WIoU of binarized graphs for r=0.8 =  0.27318375000000006
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.492
NEC for r=0.8 class 0 = 0.546 +- 0.243 (in-sample avg dev_std = 0.553)
NEC for r=0.8 class 1 = 0.607 +- 0.243 (in-sample avg dev_std = 0.553)
NEC for r=0.8 class 2 = 0.426 +- 0.243 (in-sample avg dev_std = 0.553)
NEC for r=0.8 all KL = 0.585 +- 0.243 (in-sample avg dev_std = 0.553)
NEC for r=0.8 all L1 = 0.525 +- 0.142 (in-sample avg dev_std = 0.553)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.923
Model XAI F1 of binarized graphs for r=0.8 =  0.5125475
Model XAI WIoU of binarized graphs for r=0.8 =  0.40986625000000004
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.495
NEC for r=0.8 class 0 = 0.613 +- 0.257 (in-sample avg dev_std = 0.656)
NEC for r=0.8 class 1 = 0.467 +- 0.257 (in-sample avg dev_std = 0.656)
NEC for r=0.8 class 2 = 0.598 +- 0.257 (in-sample avg dev_std = 0.656)
NEC for r=0.8 all KL = 0.708 +- 0.257 (in-sample avg dev_std = 0.656)
NEC for r=0.8 all L1 = 0.561 +- 0.182 (in-sample avg dev_std = 0.656)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.3457124999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.20945375
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.399
NEC for r=0.8 class 0 = 0.232 +- 0.462 (in-sample avg dev_std = 0.523)
NEC for r=0.8 class 1 = 0.257 +- 0.462 (in-sample avg dev_std = 0.523)
NEC for r=0.8 class 2 = 0.114 +- 0.462 (in-sample avg dev_std = 0.523)
NEC for r=0.8 all KL = 0.368 +- 0.462 (in-sample avg dev_std = 0.523)
NEC for r=0.8 all L1 = 0.201 +- 0.277 (in-sample avg dev_std = 0.523)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:20:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:20:19 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 48...
[0m[1;37mINFO[0m: [1mCheckpoint 48: 
-----------------------------------
Train ACCURACY: 0.9083
Train Loss: 0.4178
ID Validation ACCURACY: 0.9150
ID Validation Loss: 0.3921
ID Test ACCURACY: 0.9070
ID Test Loss: 0.4430
OOD Validation ACCURACY: 0.7490
OOD Validation Loss: 0.6150
OOD Test ACCURACY: 0.4973
OOD Test Loss: 4.7022

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.8751
Train Loss: 0.4864
ID Validation ACCURACY: 0.8857
ID Validation Loss: 0.4608
ID Test ACCURACY: 0.8740
ID Test Loss: 0.5012
OOD Validation ACCURACY: 0.9083
OOD Validation Loss: 0.4673
OOD Test ACCURACY: 0.5683
OOD Test Loss: 3.0991

[0m[1;37mINFO[0m: [1mChartInfo 0.9070 0.4973 0.8740 0.5683 0.8857 0.9083[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.404
WIoU for r=0.8 = 0.288
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.229
WIoU for r=0.8 = 0.150
GOODMotif2(3000)
Data example from id_test: Data(edge_index=[2, 56], x=[16, 1], node_gt=[16], edge_gt=[56], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=16)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([ 988,  973, 1039]))

Gold ratio (id_test) =  tensor(0.3032) +- tensor(0.1592)
F1 for r=0.8 = 0.395
WIoU for r=0.8 = 0.285
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.331
WIoU for r=0.8 = 0.202


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.929
Model XAI F1 of binarized graphs for r=0.8 =  0.40430124999999995
Model XAI WIoU of binarized graphs for r=0.8 =  0.28827749999999996
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.65
SUFF++ for r=0.8 class 0 = 0.395 +- 0.319 (in-sample avg dev_std = 0.523)
SUFF++ for r=0.8 class 1 = 0.745 +- 0.319 (in-sample avg dev_std = 0.523)
SUFF++ for r=0.8 class 2 = 0.658 +- 0.319 (in-sample avg dev_std = 0.523)
SUFF++ for r=0.8 all KL = 0.528 +- 0.319 (in-sample avg dev_std = 0.523)
SUFF++ for r=0.8 all L1 = 0.6 +- 0.244 (in-sample avg dev_std = 0.523)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.75
Model XAI F1 of binarized graphs for r=0.8 =  0.228505
Model XAI WIoU of binarized graphs for r=0.8 =  0.15032125000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.559
SUFF++ for r=0.8 class 0 = 0.466 +- 0.218 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.8 class 1 = 0.669 +- 0.218 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.8 class 2 = 0.589 +- 0.218 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.8 all KL = 0.622 +- 0.218 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.8 all L1 = 0.574 +- 0.173 (in-sample avg dev_std = 0.462)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.921
Model XAI F1 of binarized graphs for r=0.8 =  0.39516124999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.2854275
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.635
SUFF++ for r=0.8 class 0 = 0.398 +- 0.318 (in-sample avg dev_std = 0.525)
SUFF++ for r=0.8 class 1 = 0.727 +- 0.318 (in-sample avg dev_std = 0.525)
SUFF++ for r=0.8 class 2 = 0.628 +- 0.318 (in-sample avg dev_std = 0.525)
SUFF++ for r=0.8 all KL = 0.514 +- 0.318 (in-sample avg dev_std = 0.525)
SUFF++ for r=0.8 all L1 = 0.585 +- 0.242 (in-sample avg dev_std = 0.525)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.504
Model XAI F1 of binarized graphs for r=0.8 =  0.33074624999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.20179624999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.478
SUFF++ for r=0.8 class 0 = 0.572 +- 0.350 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.8 class 1 = 0.753 +- 0.350 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.8 class 2 = 0.697 +- 0.350 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.8 all KL = 0.519 +- 0.350 (in-sample avg dev_std = 0.514)
SUFF++ for r=0.8 all L1 = 0.676 +- 0.222 (in-sample avg dev_std = 0.514)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.929
Model XAI F1 of binarized graphs for r=0.8 =  0.40430124999999995
Model XAI WIoU of binarized graphs for r=0.8 =  0.28827749999999996
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.515
NEC for r=0.8 class 0 = 0.667 +- 0.344 (in-sample avg dev_std = 0.584)
NEC for r=0.8 class 1 = 0.268 +- 0.344 (in-sample avg dev_std = 0.584)
NEC for r=0.8 class 2 = 0.636 +- 0.344 (in-sample avg dev_std = 0.584)
NEC for r=0.8 all KL = 0.603 +- 0.344 (in-sample avg dev_std = 0.584)
NEC for r=0.8 all L1 = 0.523 +- 0.263 (in-sample avg dev_std = 0.584)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.75
Model XAI F1 of binarized graphs for r=0.8 =  0.228505
Model XAI WIoU of binarized graphs for r=0.8 =  0.15032125000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.492
NEC for r=0.8 class 0 = 0.597 +- 0.206 (in-sample avg dev_std = 0.574)
NEC for r=0.8 class 1 = 0.421 +- 0.206 (in-sample avg dev_std = 0.574)
NEC for r=0.8 class 2 = 0.594 +- 0.206 (in-sample avg dev_std = 0.574)
NEC for r=0.8 all KL = 0.544 +- 0.206 (in-sample avg dev_std = 0.574)
NEC for r=0.8 all L1 = 0.538 +- 0.142 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.921
Model XAI F1 of binarized graphs for r=0.8 =  0.39516124999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.2854275
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.522
NEC for r=0.8 class 0 = 0.661 +- 0.335 (in-sample avg dev_std = 0.597)
NEC for r=0.8 class 1 = 0.27 +- 0.335 (in-sample avg dev_std = 0.597)
NEC for r=0.8 class 2 = 0.626 +- 0.335 (in-sample avg dev_std = 0.597)
NEC for r=0.8 all KL = 0.607 +- 0.335 (in-sample avg dev_std = 0.597)
NEC for r=0.8 all L1 = 0.522 +- 0.255 (in-sample avg dev_std = 0.597)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.504
Model XAI F1 of binarized graphs for r=0.8 =  0.33074624999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.20179624999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.525
NEC for r=0.8 class 0 = 0.44 +- 0.282 (in-sample avg dev_std = 0.608)
NEC for r=0.8 class 1 = 0.484 +- 0.282 (in-sample avg dev_std = 0.608)
NEC for r=0.8 class 2 = 0.357 +- 0.282 (in-sample avg dev_std = 0.608)
NEC for r=0.8 all KL = 0.685 +- 0.282 (in-sample avg dev_std = 0.608)
NEC for r=0.8 all L1 = 0.428 +- 0.175 (in-sample avg dev_std = 0.608)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:21:54 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:21:54 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 140...
[0m[1;37mINFO[0m: [1mCheckpoint 140: 
-----------------------------------
Train ACCURACY: 0.9114
Train Loss: 0.3792
ID Validation ACCURACY: 0.9143
ID Validation Loss: 0.3758
ID Test ACCURACY: 0.9077
ID Test Loss: 0.4035
OOD Validation ACCURACY: 0.8203
OOD Validation Loss: 0.5376
OOD Test ACCURACY: 0.5747
OOD Test Loss: 2.8057

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.8786
Train Loss: 0.4845
ID Validation ACCURACY: 0.8843
ID Validation Loss: 0.4701
ID Test ACCURACY: 0.8797
ID Test Loss: 0.5157
OOD Validation ACCURACY: 0.8787
OOD Validation Loss: 0.4998
OOD Test ACCURACY: 0.5490
OOD Test Loss: 1.6271

[0m[1;37mINFO[0m: [1mChartInfo 0.9077 0.5747 0.8797 0.5490 0.8843 0.8787[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.401
WIoU for r=0.8 = 0.311
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.210
WIoU for r=0.8 = 0.139
GOODMotif2(3000)
Data example from id_test: Data(edge_index=[2, 56], x=[16, 1], node_gt=[16], edge_gt=[56], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=16)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([ 988,  973, 1039]))

Gold ratio (id_test) =  tensor(0.3032) +- tensor(0.1592)
F1 for r=0.8 = 0.397
WIoU for r=0.8 = 0.310
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.409
WIoU for r=0.8 = 0.346


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.92
Model XAI F1 of binarized graphs for r=0.8 =  0.40120249999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.311275
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.636
SUFF++ for r=0.8 class 0 = 0.52 +- 0.293 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.8 class 1 = 0.501 +- 0.293 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.8 class 2 = 0.721 +- 0.293 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.8 all KL = 0.561 +- 0.293 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.8 all L1 = 0.58 +- 0.228 (in-sample avg dev_std = 0.441)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  0.21049625000000002
Model XAI WIoU of binarized graphs for r=0.8 =  0.1393375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.473
SUFF++ for r=0.8 class 0 = 0.401 +- 0.248 (in-sample avg dev_std = 0.452)
SUFF++ for r=0.8 class 1 = 0.621 +- 0.248 (in-sample avg dev_std = 0.452)
SUFF++ for r=0.8 class 2 = 0.415 +- 0.248 (in-sample avg dev_std = 0.452)
SUFF++ for r=0.8 all KL = 0.513 +- 0.248 (in-sample avg dev_std = 0.452)
SUFF++ for r=0.8 all L1 = 0.477 +- 0.170 (in-sample avg dev_std = 0.452)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.913
Model XAI F1 of binarized graphs for r=0.8 =  0.39743500000000004
Model XAI WIoU of binarized graphs for r=0.8 =  0.3102125
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.633
SUFF++ for r=0.8 class 0 = 0.547 +- 0.288 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 class 1 = 0.502 +- 0.288 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 class 2 = 0.706 +- 0.288 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 all KL = 0.571 +- 0.288 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.8 all L1 = 0.588 +- 0.229 (in-sample avg dev_std = 0.433)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.582
Model XAI F1 of binarized graphs for r=0.8 =  0.40937874999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.346325
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.509
SUFF++ for r=0.8 class 0 = 0.337 +- 0.300 (in-sample avg dev_std = 0.558)
SUFF++ for r=0.8 class 1 = 0.445 +- 0.300 (in-sample avg dev_std = 0.558)
SUFF++ for r=0.8 class 2 = 0.482 +- 0.300 (in-sample avg dev_std = 0.558)
SUFF++ for r=0.8 all KL = 0.236 +- 0.300 (in-sample avg dev_std = 0.558)
SUFF++ for r=0.8 all L1 = 0.423 +- 0.222 (in-sample avg dev_std = 0.558)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.92
Model XAI F1 of binarized graphs for r=0.8 =  0.40120249999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.311275
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.515
NEC for r=0.8 class 0 = 0.648 +- 0.309 (in-sample avg dev_std = 0.574)
NEC for r=0.8 class 1 = 0.345 +- 0.309 (in-sample avg dev_std = 0.574)
NEC for r=0.8 class 2 = 0.644 +- 0.309 (in-sample avg dev_std = 0.574)
NEC for r=0.8 all KL = 0.61 +- 0.309 (in-sample avg dev_std = 0.574)
NEC for r=0.8 all L1 = 0.545 +- 0.213 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  0.21049625000000002
Model XAI WIoU of binarized graphs for r=0.8 =  0.1393375
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.526
NEC for r=0.8 class 0 = 0.581 +- 0.224 (in-sample avg dev_std = 0.551)
NEC for r=0.8 class 1 = 0.406 +- 0.224 (in-sample avg dev_std = 0.551)
NEC for r=0.8 class 2 = 0.601 +- 0.224 (in-sample avg dev_std = 0.551)
NEC for r=0.8 all KL = 0.527 +- 0.224 (in-sample avg dev_std = 0.551)
NEC for r=0.8 all L1 = 0.531 +- 0.145 (in-sample avg dev_std = 0.551)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.913
Model XAI F1 of binarized graphs for r=0.8 =  0.39743500000000004
Model XAI WIoU of binarized graphs for r=0.8 =  0.3102125
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.514
NEC for r=0.8 class 0 = 0.65 +- 0.306 (in-sample avg dev_std = 0.572)
NEC for r=0.8 class 1 = 0.328 +- 0.306 (in-sample avg dev_std = 0.572)
NEC for r=0.8 class 2 = 0.632 +- 0.306 (in-sample avg dev_std = 0.572)
NEC for r=0.8 all KL = 0.602 +- 0.306 (in-sample avg dev_std = 0.572)
NEC for r=0.8 all L1 = 0.539 +- 0.211 (in-sample avg dev_std = 0.572)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.582
Model XAI F1 of binarized graphs for r=0.8 =  0.40937874999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.346325
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.436
NEC for r=0.8 class 0 = 0.542 +- 0.273 (in-sample avg dev_std = 0.567)
NEC for r=0.8 class 1 = 0.49 +- 0.273 (in-sample avg dev_std = 0.567)
NEC for r=0.8 class 2 = 0.589 +- 0.273 (in-sample avg dev_std = 0.567)
NEC for r=0.8 all KL = 0.623 +- 0.273 (in-sample avg dev_std = 0.567)
NEC for r=0.8 all L1 = 0.54 +- 0.197 (in-sample avg dev_std = 0.567)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:23:25 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:23:25 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 19...
[0m[1;37mINFO[0m: [1mCheckpoint 19: 
-----------------------------------
Train ACCURACY: 0.9057
Train Loss: 0.4458
ID Validation ACCURACY: 0.9100
ID Validation Loss: 0.4449
ID Test ACCURACY: 0.9023
ID Test Loss: 0.4743
OOD Validation ACCURACY: 0.6823
OOD Validation Loss: 0.7176
OOD Test ACCURACY: 0.3343
OOD Test Loss: 11.3225

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 14...
[0m[1;37mINFO[0m: [1mCheckpoint 14: 
-----------------------------------
Train ACCURACY: 0.8665
Train Loss: 0.5395
ID Validation ACCURACY: 0.8680
ID Validation Loss: 0.5306
ID Test ACCURACY: 0.8620
ID Test Loss: 0.5767
OOD Validation ACCURACY: 0.8170
OOD Validation Loss: 0.6338
OOD Test ACCURACY: 0.4653
OOD Test Loss: 8.5253

[0m[1;37mINFO[0m: [1mChartInfo 0.9023 0.3343 0.8620 0.4653 0.8680 0.8170[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.361
WIoU for r=0.8 = 0.241
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.178
WIoU for r=0.8 = 0.105
GOODMotif2(3000)
Data example from id_test: Data(edge_index=[2, 56], x=[16, 1], node_gt=[16], edge_gt=[56], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=16)
Label distribution from id_test: (tensor([0, 1, 2]), tensor([ 988,  973, 1039]))

Gold ratio (id_test) =  tensor(0.3032) +- tensor(0.1592)
F1 for r=0.8 = 0.353
WIoU for r=0.8 = 0.239
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.293
WIoU for r=0.8 = 0.174


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.918
Model XAI F1 of binarized graphs for r=0.8 =  0.36116125000000004
Model XAI WIoU of binarized graphs for r=0.8 =  0.24129625000000002
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.621
SUFF++ for r=0.8 class 0 = 0.406 +- 0.257 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 class 1 = 0.607 +- 0.257 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 class 2 = 0.779 +- 0.257 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 all KL = 0.667 +- 0.257 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 all L1 = 0.596 +- 0.215 (in-sample avg dev_std = 0.360)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.678
Model XAI F1 of binarized graphs for r=0.8 =  0.178065
Model XAI WIoU of binarized graphs for r=0.8 =  0.10499125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.598
SUFF++ for r=0.8 class 0 = 0.466 +- 0.174 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.8 class 1 = 0.719 +- 0.174 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.8 class 2 = 0.6 +- 0.174 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.8 all KL = 0.724 +- 0.174 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.8 all L1 = 0.594 +- 0.170 (in-sample avg dev_std = 0.286)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.913
Model XAI F1 of binarized graphs for r=0.8 =  0.35313249999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.23899500000000004
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.621
SUFF++ for r=0.8 class 0 = 0.403 +- 0.260 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 class 1 = 0.604 +- 0.260 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 class 2 = 0.75 +- 0.260 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all KL = 0.655 +- 0.260 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all L1 = 0.588 +- 0.212 (in-sample avg dev_std = 0.371)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.29305375
Model XAI WIoU of binarized graphs for r=0.8 =  0.17390250000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.341
SUFF++ for r=0.8 class 0 = 0.825 +- 0.240 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 class 1 = 0.915 +- 0.240 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 class 2 = 0.921 +- 0.240 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 all KL = 0.83 +- 0.240 (in-sample avg dev_std = 0.273)
SUFF++ for r=0.8 all L1 = 0.888 +- 0.146 (in-sample avg dev_std = 0.273)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.918
Model XAI F1 of binarized graphs for r=0.8 =  0.36116125000000004
Model XAI WIoU of binarized graphs for r=0.8 =  0.24129625000000002
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.448
NEC for r=0.8 class 0 = 0.612 +- 0.233 (in-sample avg dev_std = 0.503)
NEC for r=0.8 class 1 = 0.517 +- 0.233 (in-sample avg dev_std = 0.503)
NEC for r=0.8 class 2 = 0.539 +- 0.233 (in-sample avg dev_std = 0.503)
NEC for r=0.8 all KL = 0.543 +- 0.233 (in-sample avg dev_std = 0.503)
NEC for r=0.8 all L1 = 0.556 +- 0.148 (in-sample avg dev_std = 0.503)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.678
Model XAI F1 of binarized graphs for r=0.8 =  0.178065
Model XAI WIoU of binarized graphs for r=0.8 =  0.10499125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.462
NEC for r=0.8 class 0 = 0.604 +- 0.161 (in-sample avg dev_std = 0.427)
NEC for r=0.8 class 1 = 0.548 +- 0.161 (in-sample avg dev_std = 0.427)
NEC for r=0.8 class 2 = 0.49 +- 0.161 (in-sample avg dev_std = 0.427)
NEC for r=0.8 all KL = 0.461 +- 0.161 (in-sample avg dev_std = 0.427)
NEC for r=0.8 all L1 = 0.547 +- 0.101 (in-sample avg dev_std = 0.427)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.913
Model XAI F1 of binarized graphs for r=0.8 =  0.35313249999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.23899500000000004
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.47
NEC for r=0.8 class 0 = 0.604 +- 0.232 (in-sample avg dev_std = 0.510)
NEC for r=0.8 class 1 = 0.517 +- 0.232 (in-sample avg dev_std = 0.510)
NEC for r=0.8 class 2 = 0.533 +- 0.232 (in-sample avg dev_std = 0.510)
NEC for r=0.8 all KL = 0.542 +- 0.232 (in-sample avg dev_std = 0.510)
NEC for r=0.8 all L1 = 0.551 +- 0.147 (in-sample avg dev_std = 0.510)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.29305375
Model XAI WIoU of binarized graphs for r=0.8 =  0.17390250000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.405
NEC for r=0.8 class 0 = 0.22 +- 0.340 (in-sample avg dev_std = 0.422)
NEC for r=0.8 class 1 = 0.306 +- 0.340 (in-sample avg dev_std = 0.422)
NEC for r=0.8 class 2 = 0.177 +- 0.340 (in-sample avg dev_std = 0.422)
NEC for r=0.8 all KL = 0.383 +- 0.340 (in-sample avg dev_std = 0.422)
NEC for r=0.8 all L1 = 0.235 +- 0.226 (in-sample avg dev_std = 0.422)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.567], 'all_L1': [0.534]}), defaultdict(<class 'list'>, {'all_KL': [0.591], 'all_L1': [0.631]}), defaultdict(<class 'list'>, {'all_KL': [0.528], 'all_L1': [0.6]}), defaultdict(<class 'list'>, {'all_KL': [0.561], 'all_L1': [0.58]}), defaultdict(<class 'list'>, {'all_KL': [0.667], 'all_L1': [0.596]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.553], 'all_L1': [0.559]}), defaultdict(<class 'list'>, {'all_KL': [0.727], 'all_L1': [0.568]}), defaultdict(<class 'list'>, {'all_KL': [0.603], 'all_L1': [0.523]}), defaultdict(<class 'list'>, {'all_KL': [0.61], 'all_L1': [0.545]}), defaultdict(<class 'list'>, {'all_KL': [0.543], 'all_L1': [0.556]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.606], 'all_L1': [0.526]}), defaultdict(<class 'list'>, {'all_KL': [0.701], 'all_L1': [0.632]}), defaultdict(<class 'list'>, {'all_KL': [0.622], 'all_L1': [0.574]}), defaultdict(<class 'list'>, {'all_KL': [0.513], 'all_L1': [0.477]}), defaultdict(<class 'list'>, {'all_KL': [0.724], 'all_L1': [0.594]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.508], 'all_L1': [0.572]}), defaultdict(<class 'list'>, {'all_KL': [0.585], 'all_L1': [0.525]}), defaultdict(<class 'list'>, {'all_KL': [0.544], 'all_L1': [0.538]}), defaultdict(<class 'list'>, {'all_KL': [0.527], 'all_L1': [0.531]}), defaultdict(<class 'list'>, {'all_KL': [0.461], 'all_L1': [0.547]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.579], 'all_L1': [0.545]}), defaultdict(<class 'list'>, {'all_KL': [0.581], 'all_L1': [0.62]}), defaultdict(<class 'list'>, {'all_KL': [0.514], 'all_L1': [0.585]}), defaultdict(<class 'list'>, {'all_KL': [0.571], 'all_L1': [0.588]}), defaultdict(<class 'list'>, {'all_KL': [0.655], 'all_L1': [0.588]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.552], 'all_L1': [0.554]}), defaultdict(<class 'list'>, {'all_KL': [0.708], 'all_L1': [0.561]}), defaultdict(<class 'list'>, {'all_KL': [0.607], 'all_L1': [0.522]}), defaultdict(<class 'list'>, {'all_KL': [0.602], 'all_L1': [0.539]}), defaultdict(<class 'list'>, {'all_KL': [0.542], 'all_L1': [0.551]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.617], 'all_L1': [0.734]}), defaultdict(<class 'list'>, {'all_KL': [0.971], 'all_L1': [0.982]}), defaultdict(<class 'list'>, {'all_KL': [0.519], 'all_L1': [0.676]}), defaultdict(<class 'list'>, {'all_KL': [0.236], 'all_L1': [0.423]}), defaultdict(<class 'list'>, {'all_KL': [0.83], 'all_L1': [0.888]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.585], 'all_L1': [0.38]}), defaultdict(<class 'list'>, {'all_KL': [0.368], 'all_L1': [0.201]}), defaultdict(<class 'list'>, {'all_KL': [0.685], 'all_L1': [0.428]}), defaultdict(<class 'list'>, {'all_KL': [0.623], 'all_L1': [0.54]}), defaultdict(<class 'list'>, {'all_KL': [0.383], 'all_L1': [0.235]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.588 +- 0.032
suff++ class all_KL  =  0.583 +- 0.047
suff++_acc_int  =  0.624 +- 0.025
nec class all_L1  =  0.550 +- 0.015
nec class all_KL  =  0.607 +- 0.065
nec_acc_int  =  0.496 +- 0.025

Eval split val
suff++ class all_L1  =  0.561 +- 0.054
suff++ class all_KL  =  0.633 +- 0.075
suff++_acc_int  =  0.542 +- 0.067
nec class all_L1  =  0.543 +- 0.016
nec class all_KL  =  0.525 +- 0.041
nec_acc_int  =  0.481 +- 0.031

Eval split id_test
suff++ class all_L1  =  0.585 +- 0.024
suff++ class all_KL  =  0.580 +- 0.045
suff++_acc_int  =  0.618 +- 0.022
nec class all_L1  =  0.545 +- 0.014
nec class all_KL  =  0.602 +- 0.059
nec_acc_int  =  0.500 +- 0.018

Eval split test
suff++ class all_L1  =  0.741 +- 0.192
suff++ class all_KL  =  0.635 +- 0.255
suff++_acc_int  =  0.418 +- 0.070
nec class all_L1  =  0.357 +- 0.125
nec class all_KL  =  0.529 +- 0.129
nec_acc_int  =  0.436 +- 0.046


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.569 +- 0.018
Faith. Armon (L1)= 		  =  0.568 +- 0.018
Faith. GMean (L1)= 	  =  0.569 +- 0.018
Faith. Aritm (KL)= 		  =  0.595 +- 0.036
Faith. Armon (KL)= 		  =  0.592 +- 0.033
Faith. GMean (KL)= 	  =  0.593 +- 0.035

Eval split val
Faith. Aritm (L1)= 		  =  0.552 +- 0.026
Faith. Armon (L1)= 		  =  0.550 +- 0.025
Faith. GMean (L1)= 	  =  0.551 +- 0.026
Faith. Aritm (KL)= 		  =  0.579 +- 0.041
Faith. Armon (KL)= 		  =  0.571 +- 0.039
Faith. GMean (KL)= 	  =  0.575 +- 0.039

Eval split id_test
Faith. Aritm (L1)= 		  =  0.565 +- 0.014
Faith. Armon (L1)= 		  =  0.564 +- 0.014
Faith. GMean (L1)= 	  =  0.565 +- 0.014
Faith. Aritm (KL)= 		  =  0.591 +- 0.030
Faith. Armon (KL)= 		  =  0.588 +- 0.028
Faith. GMean (KL)= 	  =  0.589 +- 0.029

Eval split test
Faith. Aritm (L1)= 		  =  0.549 +- 0.036
Faith. Armon (L1)= 		  =  0.441 +- 0.075
Faith. GMean (L1)= 	  =  0.489 +- 0.038
Faith. Aritm (KL)= 		  =  0.582 +- 0.080
Faith. Armon (KL)= 		  =  0.518 +- 0.093
Faith. GMean (KL)= 	  =  0.548 +- 0.084
Computed for split load_split = id



Completed in  0:07:43.471753  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:25:08 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:09 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:11 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:12 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:12 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:14 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:25:16 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 62...
[0m[1;37mINFO[0m: [1mCheckpoint 62: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.7058
ID Validation Loss: 1.4156
ID Test ACCURACY: 0.6715
ID Test Loss: 1.6499
OOD Validation ACCURACY: 0.6258
OOD Validation Loss: 1.7398
OOD Test ACCURACY: 0.5635
OOD Test Loss: 2.2884

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 56...
[0m[1;37mINFO[0m: [1mCheckpoint 56: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0002
ID Validation ACCURACY: 0.6841
ID Validation Loss: 1.4339
ID Test ACCURACY: 0.6715
ID Test Loss: 1.5868
OOD Validation ACCURACY: 0.6331
OOD Validation Loss: 1.6768
OOD Test ACCURACY: 0.5553
OOD Test Loss: 2.2147

[0m[1;37mINFO[0m: [1mChartInfo 0.6715 0.5635 0.6715 0.5553 0.6841 0.6331[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_test: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from id_test: (tensor([0, 1, 2]), tensor([138, 251, 165]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.701
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.652
SUFF++ for r=0.6 class 0 = 0.745 +- 0.263 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 class 1 = 0.812 +- 0.263 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 class 2 = 0.719 +- 0.263 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 all KL = 0.695 +- 0.263 (in-sample avg dev_std = 0.441)
SUFF++ for r=0.6 all L1 = 0.77 +- 0.180 (in-sample avg dev_std = 0.441)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.636
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.619
SUFF++ for r=0.6 class 0 = 0.768 +- 0.213 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 class 1 = 0.817 +- 0.213 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 class 2 = 0.729 +- 0.213 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 all KL = 0.785 +- 0.213 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.6 all L1 = 0.786 +- 0.189 (in-sample avg dev_std = 0.366)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.67
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.631
SUFF++ for r=0.6 class 0 = 0.745 +- 0.277 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.6 class 1 = 0.803 +- 0.277 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.6 class 2 = 0.728 +- 0.277 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.6 all KL = 0.681 +- 0.277 (in-sample avg dev_std = 0.447)
SUFF++ for r=0.6 all L1 = 0.766 +- 0.188 (in-sample avg dev_std = 0.447)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.559
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.539
SUFF++ for r=0.6 class 0 = 0.783 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 1 = 0.794 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 2 = 0.733 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all KL = 0.775 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all L1 = 0.776 +- 0.181 (in-sample avg dev_std = 0.371)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.701
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.68
NEC for r=0.6 class 0 = 0.145 +- 0.170 (in-sample avg dev_std = 0.200)
NEC for r=0.6 class 1 = 0.123 +- 0.170 (in-sample avg dev_std = 0.200)
NEC for r=0.6 class 2 = 0.167 +- 0.170 (in-sample avg dev_std = 0.200)
NEC for r=0.6 all KL = 0.098 +- 0.170 (in-sample avg dev_std = 0.200)
NEC for r=0.6 all L1 = 0.14 +- 0.188 (in-sample avg dev_std = 0.200)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.636
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.636
NEC for r=0.6 class 0 = 0.204 +- 0.206 (in-sample avg dev_std = 0.215)
NEC for r=0.6 class 1 = 0.158 +- 0.206 (in-sample avg dev_std = 0.215)
NEC for r=0.6 class 2 = 0.215 +- 0.206 (in-sample avg dev_std = 0.215)
NEC for r=0.6 all KL = 0.137 +- 0.206 (in-sample avg dev_std = 0.215)
NEC for r=0.6 all L1 = 0.182 +- 0.211 (in-sample avg dev_std = 0.215)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.67
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.655
NEC for r=0.6 class 0 = 0.166 +- 0.192 (in-sample avg dev_std = 0.178)
NEC for r=0.6 class 1 = 0.127 +- 0.192 (in-sample avg dev_std = 0.178)
NEC for r=0.6 class 2 = 0.153 +- 0.192 (in-sample avg dev_std = 0.178)
NEC for r=0.6 all KL = 0.111 +- 0.192 (in-sample avg dev_std = 0.178)
NEC for r=0.6 all L1 = 0.144 +- 0.194 (in-sample avg dev_std = 0.178)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.559
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.557
NEC for r=0.6 class 0 = 0.189 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 1 = 0.161 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 2 = 0.223 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all KL = 0.136 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all L1 = 0.183 +- 0.211 (in-sample avg dev_std = 0.214)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:27:15 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:15 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:17 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:17 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:18 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:20 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:27:22 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 103...
[0m[1;37mINFO[0m: [1mCheckpoint 103: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.6986
ID Validation Loss: 1.6698
ID Test ACCURACY: 0.6444
ID Test Loss: 1.7400
OOD Validation ACCURACY: 0.6129
OOD Validation Loss: 1.8537
OOD Test ACCURACY: 0.5367
OOD Test Loss: 2.1560

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 86...
[0m[1;37mINFO[0m: [1mCheckpoint 86: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0002
ID Validation ACCURACY: 0.6823
ID Validation Loss: 1.5415
ID Test ACCURACY: 0.6751
ID Test Loss: 1.6717
OOD Validation ACCURACY: 0.6381
OOD Validation Loss: 1.7231
OOD Test ACCURACY: 0.5628
OOD Test Loss: 2.0130

[0m[1;37mINFO[0m: [1mChartInfo 0.6444 0.5367 0.6751 0.5628 0.6823 0.6381[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_test: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from id_test: (tensor([0, 1, 2]), tensor([138, 251, 165]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.658
SUFF++ for r=0.6 class 0 = 0.73 +- 0.274 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 class 1 = 0.789 +- 0.274 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 class 2 = 0.748 +- 0.274 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 all KL = 0.667 +- 0.274 (in-sample avg dev_std = 0.462)
SUFF++ for r=0.6 all L1 = 0.763 +- 0.186 (in-sample avg dev_std = 0.462)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.619
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.609
SUFF++ for r=0.6 class 0 = 0.748 +- 0.222 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.6 class 1 = 0.787 +- 0.222 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.6 class 2 = 0.755 +- 0.222 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.6 all KL = 0.757 +- 0.222 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.6 all L1 = 0.77 +- 0.182 (in-sample avg dev_std = 0.391)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.644
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.615
SUFF++ for r=0.6 class 0 = 0.727 +- 0.281 (in-sample avg dev_std = 0.450)
SUFF++ for r=0.6 class 1 = 0.783 +- 0.281 (in-sample avg dev_std = 0.450)
SUFF++ for r=0.6 class 2 = 0.741 +- 0.281 (in-sample avg dev_std = 0.450)
SUFF++ for r=0.6 all KL = 0.669 +- 0.281 (in-sample avg dev_std = 0.450)
SUFF++ for r=0.6 all L1 = 0.757 +- 0.193 (in-sample avg dev_std = 0.450)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.526
SUFF++ for r=0.6 class 0 = 0.743 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 class 1 = 0.76 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 class 2 = 0.73 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 all KL = 0.752 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 all L1 = 0.748 +- 0.169 (in-sample avg dev_std = 0.393)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.672
NEC for r=0.6 class 0 = 0.15 +- 0.194 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 1 = 0.138 +- 0.194 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 2 = 0.168 +- 0.194 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all KL = 0.117 +- 0.194 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all L1 = 0.149 +- 0.201 (in-sample avg dev_std = 0.211)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.619
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.62
NEC for r=0.6 class 0 = 0.185 +- 0.206 (in-sample avg dev_std = 0.220)
NEC for r=0.6 class 1 = 0.18 +- 0.206 (in-sample avg dev_std = 0.220)
NEC for r=0.6 class 2 = 0.224 +- 0.206 (in-sample avg dev_std = 0.220)
NEC for r=0.6 all KL = 0.145 +- 0.206 (in-sample avg dev_std = 0.220)
NEC for r=0.6 all L1 = 0.191 +- 0.214 (in-sample avg dev_std = 0.220)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.644
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.647
NEC for r=0.6 class 0 = 0.183 +- 0.207 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 1 = 0.14 +- 0.207 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 2 = 0.169 +- 0.207 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all KL = 0.126 +- 0.207 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all L1 = 0.159 +- 0.211 (in-sample avg dev_std = 0.211)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.523
NEC for r=0.6 class 0 = 0.194 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 class 1 = 0.201 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 class 2 = 0.218 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 all KL = 0.14 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 all L1 = 0.203 +- 0.207 (in-sample avg dev_std = 0.217)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:29:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:21 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:23 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:23 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:24 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:27 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:29:29 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 125...
[0m[1;37mINFO[0m: [1mCheckpoint 125: 
-----------------------------------
Train ACCURACY: 0.9996
Train Loss: 0.0013
ID Validation ACCURACY: 0.6986
ID Validation Loss: 1.7716
ID Test ACCURACY: 0.6390
ID Test Loss: 1.9140
OOD Validation ACCURACY: 0.6392
OOD Validation Loss: 1.9480
OOD Test ACCURACY: 0.5470
OOD Test Loss: 2.3429

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 181...
[0m[1;37mINFO[0m: [1mCheckpoint 181: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6625
ID Validation Loss: 1.8607
ID Test ACCURACY: 0.6462
ID Test Loss: 1.9297
OOD Validation ACCURACY: 0.6538
OOD Validation Loss: 1.9479
OOD Test ACCURACY: 0.5518
OOD Test Loss: 2.4294

[0m[1;37mINFO[0m: [1mChartInfo 0.6390 0.5470 0.6462 0.5518 0.6625 0.6538[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_test: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from id_test: (tensor([0, 1, 2]), tensor([138, 251, 165]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.626
SUFF++ for r=0.6 class 0 = 0.716 +- 0.297 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.6 class 1 = 0.842 +- 0.297 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.6 class 2 = 0.704 +- 0.297 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.6 all KL = 0.688 +- 0.297 (in-sample avg dev_std = 0.440)
SUFF++ for r=0.6 all L1 = 0.773 +- 0.201 (in-sample avg dev_std = 0.440)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.656
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.618
SUFF++ for r=0.6 class 0 = 0.752 +- 0.230 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.6 class 1 = 0.852 +- 0.230 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.6 class 2 = 0.732 +- 0.230 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.6 all KL = 0.797 +- 0.230 (in-sample avg dev_std = 0.347)
SUFF++ for r=0.6 all L1 = 0.801 +- 0.200 (in-sample avg dev_std = 0.347)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.637
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.594
SUFF++ for r=0.6 class 0 = 0.713 +- 0.303 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.6 class 1 = 0.85 +- 0.303 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.6 class 2 = 0.686 +- 0.303 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.6 all KL = 0.682 +- 0.303 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.6 all L1 = 0.767 +- 0.204 (in-sample avg dev_std = 0.457)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.532
SUFF++ for r=0.6 class 0 = 0.78 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 class 1 = 0.812 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 class 2 = 0.755 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 all KL = 0.792 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 all L1 = 0.79 +- 0.185 (in-sample avg dev_std = 0.351)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.664
NEC for r=0.6 class 0 = 0.164 +- 0.198 (in-sample avg dev_std = 0.200)
NEC for r=0.6 class 1 = 0.114 +- 0.198 (in-sample avg dev_std = 0.200)
NEC for r=0.6 class 2 = 0.174 +- 0.198 (in-sample avg dev_std = 0.200)
NEC for r=0.6 all KL = 0.113 +- 0.198 (in-sample avg dev_std = 0.200)
NEC for r=0.6 all L1 = 0.143 +- 0.202 (in-sample avg dev_std = 0.200)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.656
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.645
NEC for r=0.6 class 0 = 0.191 +- 0.190 (in-sample avg dev_std = 0.215)
NEC for r=0.6 class 1 = 0.142 +- 0.190 (in-sample avg dev_std = 0.215)
NEC for r=0.6 class 2 = 0.196 +- 0.190 (in-sample avg dev_std = 0.215)
NEC for r=0.6 all KL = 0.128 +- 0.190 (in-sample avg dev_std = 0.215)
NEC for r=0.6 all L1 = 0.166 +- 0.199 (in-sample avg dev_std = 0.215)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.637
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.633
NEC for r=0.6 class 0 = 0.18 +- 0.199 (in-sample avg dev_std = 0.203)
NEC for r=0.6 class 1 = 0.092 +- 0.199 (in-sample avg dev_std = 0.203)
NEC for r=0.6 class 2 = 0.177 +- 0.199 (in-sample avg dev_std = 0.203)
NEC for r=0.6 all KL = 0.111 +- 0.199 (in-sample avg dev_std = 0.203)
NEC for r=0.6 all L1 = 0.139 +- 0.193 (in-sample avg dev_std = 0.203)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.531
NEC for r=0.6 class 0 = 0.174 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 1 = 0.15 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 2 = 0.179 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all KL = 0.113 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all L1 = 0.163 +- 0.192 (in-sample avg dev_std = 0.204)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:31:28 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:29 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:30 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:31 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:31 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:33 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:31:35 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 34...
[0m[1;37mINFO[0m: [1mCheckpoint 34: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6986
ID Validation Loss: 1.7703
ID Test ACCURACY: 0.6534
ID Test Loss: 1.9993
OOD Validation ACCURACY: 0.6202
OOD Validation Loss: 2.0138
OOD Test ACCURACY: 0.5607
OOD Test Loss: 2.5829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6877
ID Validation Loss: 1.7754
ID Test ACCURACY: 0.6498
ID Test Loss: 1.9549
OOD Validation ACCURACY: 0.6308
OOD Validation Loss: 1.9916
OOD Test ACCURACY: 0.5697
OOD Test Loss: 2.4454

[0m[1;37mINFO[0m: [1mChartInfo 0.6534 0.5607 0.6498 0.5697 0.6877 0.6308[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_test: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from id_test: (tensor([0, 1, 2]), tensor([138, 251, 165]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.669
SUFF++ for r=0.6 class 0 = 0.782 +- 0.278 (in-sample avg dev_std = 0.414)
SUFF++ for r=0.6 class 1 = 0.807 +- 0.278 (in-sample avg dev_std = 0.414)
SUFF++ for r=0.6 class 2 = 0.744 +- 0.278 (in-sample avg dev_std = 0.414)
SUFF++ for r=0.6 all KL = 0.725 +- 0.278 (in-sample avg dev_std = 0.414)
SUFF++ for r=0.6 all L1 = 0.783 +- 0.221 (in-sample avg dev_std = 0.414)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.632
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.621
SUFF++ for r=0.6 class 0 = 0.757 +- 0.258 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.6 class 1 = 0.776 +- 0.258 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.6 class 2 = 0.722 +- 0.258 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.6 all KL = 0.731 +- 0.258 (in-sample avg dev_std = 0.390)
SUFF++ for r=0.6 all L1 = 0.76 +- 0.225 (in-sample avg dev_std = 0.390)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.653
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.623
SUFF++ for r=0.6 class 0 = 0.742 +- 0.282 (in-sample avg dev_std = 0.419)
SUFF++ for r=0.6 class 1 = 0.817 +- 0.282 (in-sample avg dev_std = 0.419)
SUFF++ for r=0.6 class 2 = 0.76 +- 0.282 (in-sample avg dev_std = 0.419)
SUFF++ for r=0.6 all KL = 0.714 +- 0.282 (in-sample avg dev_std = 0.419)
SUFF++ for r=0.6 all L1 = 0.781 +- 0.218 (in-sample avg dev_std = 0.419)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.565
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.55
SUFF++ for r=0.6 class 0 = 0.774 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 class 1 = 0.761 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 class 2 = 0.727 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 all KL = 0.743 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 all L1 = 0.756 +- 0.214 (in-sample avg dev_std = 0.386)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.69
NEC for r=0.6 class 0 = 0.14 +- 0.190 (in-sample avg dev_std = 0.257)
NEC for r=0.6 class 1 = 0.132 +- 0.190 (in-sample avg dev_std = 0.257)
NEC for r=0.6 class 2 = 0.144 +- 0.190 (in-sample avg dev_std = 0.257)
NEC for r=0.6 all KL = 0.12 +- 0.190 (in-sample avg dev_std = 0.257)
NEC for r=0.6 all L1 = 0.137 +- 0.183 (in-sample avg dev_std = 0.257)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.632
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.621
NEC for r=0.6 class 0 = 0.182 +- 0.217 (in-sample avg dev_std = 0.287)
NEC for r=0.6 class 1 = 0.166 +- 0.217 (in-sample avg dev_std = 0.287)
NEC for r=0.6 class 2 = 0.212 +- 0.217 (in-sample avg dev_std = 0.287)
NEC for r=0.6 all KL = 0.157 +- 0.217 (in-sample avg dev_std = 0.287)
NEC for r=0.6 all L1 = 0.18 +- 0.207 (in-sample avg dev_std = 0.287)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.653
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.633
NEC for r=0.6 class 0 = 0.159 +- 0.204 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 1 = 0.124 +- 0.204 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 2 = 0.15 +- 0.204 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all KL = 0.125 +- 0.204 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all L1 = 0.14 +- 0.192 (in-sample avg dev_std = 0.261)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.565
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.551
NEC for r=0.6 class 0 = 0.168 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 class 1 = 0.168 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 class 2 = 0.211 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 all KL = 0.144 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 all L1 = 0.178 +- 0.195 (in-sample avg dev_std = 0.278)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:33:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:31 AM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:33 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:33 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:34 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:36 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:33:38 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0005
ID Validation ACCURACY: 0.6895
ID Validation Loss: 1.4368
ID Test ACCURACY: 0.6408
ID Test Loss: 1.6540
OOD Validation ACCURACY: 0.6028
OOD Validation Loss: 1.6340
OOD Test ACCURACY: 0.5470
OOD Test Loss: 2.1702

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 50...
[0m[1;37mINFO[0m: [1mCheckpoint 50: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0003
ID Validation ACCURACY: 0.6769
ID Validation Loss: 1.5279
ID Test ACCURACY: 0.6354
ID Test Loss: 1.7317
OOD Validation ACCURACY: 0.6409
OOD Validation Loss: 1.6703
OOD Test ACCURACY: 0.5786
OOD Test Loss: 1.9127

[0m[1;37mINFO[0m: [1mChartInfo 0.6408 0.5470 0.6354 0.5786 0.6769 0.6409[0mGOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_test: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from id_test: (tensor([0, 1, 2]), tensor([138, 251, 165]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.688
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.644
SUFF++ for r=0.6 class 0 = 0.713 +- 0.244 (in-sample avg dev_std = 0.430)
SUFF++ for r=0.6 class 1 = 0.774 +- 0.244 (in-sample avg dev_std = 0.430)
SUFF++ for r=0.6 class 2 = 0.756 +- 0.244 (in-sample avg dev_std = 0.430)
SUFF++ for r=0.6 all KL = 0.702 +- 0.244 (in-sample avg dev_std = 0.430)
SUFF++ for r=0.6 all L1 = 0.754 +- 0.177 (in-sample avg dev_std = 0.430)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.61
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.596
SUFF++ for r=0.6 class 0 = 0.728 +- 0.201 (in-sample avg dev_std = 0.385)
SUFF++ for r=0.6 class 1 = 0.757 +- 0.201 (in-sample avg dev_std = 0.385)
SUFF++ for r=0.6 class 2 = 0.743 +- 0.201 (in-sample avg dev_std = 0.385)
SUFF++ for r=0.6 all KL = 0.75 +- 0.201 (in-sample avg dev_std = 0.385)
SUFF++ for r=0.6 all L1 = 0.747 +- 0.175 (in-sample avg dev_std = 0.385)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.609
SUFF++ for r=0.6 class 0 = 0.708 +- 0.248 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.6 class 1 = 0.777 +- 0.248 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.6 class 2 = 0.77 +- 0.248 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.6 all KL = 0.706 +- 0.248 (in-sample avg dev_std = 0.434)
SUFF++ for r=0.6 all L1 = 0.758 +- 0.174 (in-sample avg dev_std = 0.434)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.549
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.529
SUFF++ for r=0.6 class 0 = 0.732 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 class 1 = 0.736 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 class 2 = 0.743 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 all KL = 0.752 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 all L1 = 0.737 +- 0.163 (in-sample avg dev_std = 0.387)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.688
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.682
NEC for r=0.6 class 0 = 0.191 +- 0.194 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 1 = 0.147 +- 0.194 (in-sample avg dev_std = 0.216)
NEC for r=0.6 class 2 = 0.189 +- 0.194 (in-sample avg dev_std = 0.216)
NEC for r=0.6 all KL = 0.122 +- 0.194 (in-sample avg dev_std = 0.216)
NEC for r=0.6 all L1 = 0.169 +- 0.203 (in-sample avg dev_std = 0.216)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.61
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.61
NEC for r=0.6 class 0 = 0.196 +- 0.186 (in-sample avg dev_std = 0.222)
NEC for r=0.6 class 1 = 0.208 +- 0.186 (in-sample avg dev_std = 0.222)
NEC for r=0.6 class 2 = 0.205 +- 0.186 (in-sample avg dev_std = 0.222)
NEC for r=0.6 all KL = 0.142 +- 0.186 (in-sample avg dev_std = 0.222)
NEC for r=0.6 all L1 = 0.204 +- 0.205 (in-sample avg dev_std = 0.222)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 551
Effective ratio: 0.616 +- 0.016
Model Accuracy over intervened graphs for r=0.6 =  0.622
NEC for r=0.6 class 0 = 0.169 +- 0.195 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 1 = 0.175 +- 0.195 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 2 = 0.149 +- 0.195 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all KL = 0.123 +- 0.195 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all L1 = 0.166 +- 0.199 (in-sample avg dev_std = 0.204)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.549
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.533
NEC for r=0.6 class 0 = 0.193 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 class 1 = 0.222 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 class 2 = 0.229 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 all KL = 0.149 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 all L1 = 0.216 +- 0.207 (in-sample avg dev_std = 0.221)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.695], 'all_L1': [0.77]}), defaultdict(<class 'list'>, {'all_KL': [0.667], 'all_L1': [0.763]}), defaultdict(<class 'list'>, {'all_KL': [0.688], 'all_L1': [0.773]}), defaultdict(<class 'list'>, {'all_KL': [0.725], 'all_L1': [0.783]}), defaultdict(<class 'list'>, {'all_KL': [0.702], 'all_L1': [0.754]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.14]}), defaultdict(<class 'list'>, {'all_KL': [0.117], 'all_L1': [0.149]}), defaultdict(<class 'list'>, {'all_KL': [0.113], 'all_L1': [0.143]}), defaultdict(<class 'list'>, {'all_KL': [0.12], 'all_L1': [0.137]}), defaultdict(<class 'list'>, {'all_KL': [0.122], 'all_L1': [0.169]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.785], 'all_L1': [0.786]}), defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.77]}), defaultdict(<class 'list'>, {'all_KL': [0.797], 'all_L1': [0.801]}), defaultdict(<class 'list'>, {'all_KL': [0.731], 'all_L1': [0.76]}), defaultdict(<class 'list'>, {'all_KL': [0.75], 'all_L1': [0.747]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.137], 'all_L1': [0.182]}), defaultdict(<class 'list'>, {'all_KL': [0.145], 'all_L1': [0.191]}), defaultdict(<class 'list'>, {'all_KL': [0.128], 'all_L1': [0.166]}), defaultdict(<class 'list'>, {'all_KL': [0.157], 'all_L1': [0.18]}), defaultdict(<class 'list'>, {'all_KL': [0.142], 'all_L1': [0.204]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.681], 'all_L1': [0.766]}), defaultdict(<class 'list'>, {'all_KL': [0.669], 'all_L1': [0.757]}), defaultdict(<class 'list'>, {'all_KL': [0.682], 'all_L1': [0.767]}), defaultdict(<class 'list'>, {'all_KL': [0.714], 'all_L1': [0.781]}), defaultdict(<class 'list'>, {'all_KL': [0.706], 'all_L1': [0.758]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.111], 'all_L1': [0.144]}), defaultdict(<class 'list'>, {'all_KL': [0.126], 'all_L1': [0.159]}), defaultdict(<class 'list'>, {'all_KL': [0.111], 'all_L1': [0.139]}), defaultdict(<class 'list'>, {'all_KL': [0.125], 'all_L1': [0.14]}), defaultdict(<class 'list'>, {'all_KL': [0.123], 'all_L1': [0.166]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.775], 'all_L1': [0.776]}), defaultdict(<class 'list'>, {'all_KL': [0.752], 'all_L1': [0.748]}), defaultdict(<class 'list'>, {'all_KL': [0.792], 'all_L1': [0.79]}), defaultdict(<class 'list'>, {'all_KL': [0.743], 'all_L1': [0.756]}), defaultdict(<class 'list'>, {'all_KL': [0.752], 'all_L1': [0.737]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.136], 'all_L1': [0.183]}), defaultdict(<class 'list'>, {'all_KL': [0.14], 'all_L1': [0.203]}), defaultdict(<class 'list'>, {'all_KL': [0.113], 'all_L1': [0.163]}), defaultdict(<class 'list'>, {'all_KL': [0.144], 'all_L1': [0.178]}), defaultdict(<class 'list'>, {'all_KL': [0.149], 'all_L1': [0.216]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.769 +- 0.010
suff++ class all_KL  =  0.695 +- 0.019
suff++_acc_int  =  0.650 +- 0.014
nec class all_L1  =  0.148 +- 0.011
nec class all_KL  =  0.114 +- 0.009
nec_acc_int  =  0.678 +- 0.009

Eval split val
suff++ class all_L1  =  0.773 +- 0.019
suff++ class all_KL  =  0.764 +- 0.024
suff++_acc_int  =  0.612 +- 0.009
nec class all_L1  =  0.185 +- 0.013
nec class all_KL  =  0.142 +- 0.010
nec_acc_int  =  0.626 +- 0.013

Eval split id_test
suff++ class all_L1  =  0.766 +- 0.009
suff++ class all_KL  =  0.690 +- 0.017
suff++_acc_int  =  0.615 +- 0.013
nec class all_L1  =  0.150 +- 0.011
nec class all_KL  =  0.119 +- 0.007
nec_acc_int  =  0.638 +- 0.012

Eval split test
suff++ class all_L1  =  0.761 +- 0.019
suff++ class all_KL  =  0.763 +- 0.018
suff++_acc_int  =  0.535 +- 0.009
nec class all_L1  =  0.189 +- 0.019
nec class all_KL  =  0.136 +- 0.012
nec_acc_int  =  0.539 +- 0.013


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.458 +- 0.002
Faith. Armon (L1)= 		  =  0.247 +- 0.015
Faith. GMean (L1)= 	  =  0.336 +- 0.011
Faith. Aritm (KL)= 		  =  0.405 +- 0.011
Faith. Armon (KL)= 		  =  0.196 +- 0.013
Faith. GMean (KL)= 	  =  0.281 +- 0.012

Eval split val
Faith. Aritm (L1)= 		  =  0.479 +- 0.005
Faith. Armon (L1)= 		  =  0.298 +- 0.015
Faith. GMean (L1)= 	  =  0.377 +- 0.009
Faith. Aritm (KL)= 		  =  0.453 +- 0.008
Faith. Armon (KL)= 		  =  0.239 +- 0.012
Faith. GMean (KL)= 	  =  0.329 +- 0.006

Eval split id_test
Faith. Aritm (L1)= 		  =  0.458 +- 0.003
Faith. Armon (L1)= 		  =  0.250 +- 0.015
Faith. GMean (L1)= 	  =  0.338 +- 0.011
Faith. Aritm (KL)= 		  =  0.405 +- 0.010
Faith. Armon (KL)= 		  =  0.203 +- 0.010
Faith. GMean (KL)= 	  =  0.287 +- 0.010

Eval split test
Faith. Aritm (L1)= 		  =  0.475 +- 0.004
Faith. Armon (L1)= 		  =  0.302 +- 0.023
Faith. GMean (L1)= 	  =  0.378 +- 0.015
Faith. Aritm (KL)= 		  =  0.450 +- 0.004
Faith. Armon (KL)= 		  =  0.231 +- 0.018
Faith. GMean (KL)= 	  =  0.322 +- 0.012
Computed for split load_split = id



Completed in  0:10:32.603049  for CIGAvGIN GOODTwitter/length



DONE CIGA GOODTwitter/length readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:35:53 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 05/10/2024 11:35:53 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:35:56 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:00 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:03 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:36:06 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 129...
[0m[1;37mINFO[0m: [1mCheckpoint 129: 
-----------------------------------
Train ROC-AUC: 0.9976
Train Loss: 0.0246
ID Validation ROC-AUC: 0.8451
ID Validation Loss: 0.1961
ID Test ROC-AUC: 0.8004
ID Test Loss: 0.1892
OOD Validation ROC-AUC: 0.7250
OOD Validation Loss: 0.1961
OOD Test ROC-AUC: 0.6668
OOD Test Loss: 0.1584

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 41...
[0m[1;37mINFO[0m: [1mCheckpoint 41: 
-----------------------------------
Train ROC-AUC: 0.9665
Train Loss: 0.0729
ID Validation ROC-AUC: 0.8288
ID Validation Loss: 0.1456
ID Test ROC-AUC: 0.7858
ID Test Loss: 0.1351
OOD Validation ROC-AUC: 0.7799
OOD Validation Loss: 0.1266
OOD Test ROC-AUC: 0.6771
OOD Test Loss: 0.1073

[0m[1;37mINFO[0m: [1mChartInfo 0.8004 0.6668 0.7858 0.6771 0.8288 0.7799[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 05/10/2024 11:36:07 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 05/10/2024 11:36:12 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_test: Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], smiles='Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OCC(C)C)c1', idx=[1], scaffold='S=C(Nc1ccccc1)c1ccoc1', domain_id=[1], env_id=[1], ori_edge_index=[2, 48], node_perm=[23])
Label distribution from id_test: (tensor([0., 1.]), tensor([3976,  136]))
[1;34mDEBUG[0m: 05/10/2024 11:36:14 AM : [1mUnbalanced warning for GOODHIV (id_test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([136, 136]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 05/10/2024 11:36:17 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.647
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.576
SUFF++ for r=0.8 class 0.0 = 0.668 +- 0.164 (in-sample avg dev_std = 0.289)
SUFF++ for r=0.8 class 1.0 = 0.728 +- 0.164 (in-sample avg dev_std = 0.289)
SUFF++ for r=0.8 all KL = 0.84 +- 0.164 (in-sample avg dev_std = 0.289)
SUFF++ for r=0.8 all L1 = 0.698 +- 0.142 (in-sample avg dev_std = 0.289)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.684
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.645
SUFF++ for r=0.8 class 0.0 = 0.691 +- 0.110 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.8 class 1.0 = 0.704 +- 0.110 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.8 all KL = 0.86 +- 0.110 (in-sample avg dev_std = 0.254)
SUFF++ for r=0.8 all L1 = 0.697 +- 0.112 (in-sample avg dev_std = 0.254)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.529
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.497
SUFF++ for r=0.8 class 0.0 = 0.714 +- 0.159 (in-sample avg dev_std = 0.280)
SUFF++ for r=0.8 class 1.0 = 0.712 +- 0.159 (in-sample avg dev_std = 0.280)
SUFF++ for r=0.8 all KL = 0.855 +- 0.159 (in-sample avg dev_std = 0.280)
SUFF++ for r=0.8 all L1 = 0.713 +- 0.123 (in-sample avg dev_std = 0.280)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.562
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.567
SUFF++ for r=0.8 class 0.0 = 0.757 +- 0.096 (in-sample avg dev_std = 0.231)
SUFF++ for r=0.8 class 1.0 = 0.746 +- 0.096 (in-sample avg dev_std = 0.231)
SUFF++ for r=0.8 all KL = 0.887 +- 0.096 (in-sample avg dev_std = 0.231)
SUFF++ for r=0.8 all L1 = 0.752 +- 0.083 (in-sample avg dev_std = 0.231)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.647
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.633
NEC for r=0.8 class 0.0 = 0.246 +- 0.069 (in-sample avg dev_std = 0.091)
NEC for r=0.8 class 1.0 = 0.19 +- 0.069 (in-sample avg dev_std = 0.091)
NEC for r=0.8 all KL = 0.065 +- 0.069 (in-sample avg dev_std = 0.091)
NEC for r=0.8 all L1 = 0.218 +- 0.113 (in-sample avg dev_std = 0.091)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.684
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.678
NEC for r=0.8 class 0.0 = 0.243 +- 0.053 (in-sample avg dev_std = 0.094)
NEC for r=0.8 class 1.0 = 0.23 +- 0.053 (in-sample avg dev_std = 0.094)
NEC for r=0.8 all KL = 0.069 +- 0.053 (in-sample avg dev_std = 0.094)
NEC for r=0.8 all L1 = 0.236 +- 0.097 (in-sample avg dev_std = 0.094)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.529
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.519
NEC for r=0.8 class 0.0 = 0.218 +- 0.066 (in-sample avg dev_std = 0.089)
NEC for r=0.8 class 1.0 = 0.194 +- 0.066 (in-sample avg dev_std = 0.089)
NEC for r=0.8 all KL = 0.058 +- 0.066 (in-sample avg dev_std = 0.089)
NEC for r=0.8 all L1 = 0.206 +- 0.099 (in-sample avg dev_std = 0.089)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.562
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.551
NEC for r=0.8 class 0.0 = 0.198 +- 0.041 (in-sample avg dev_std = 0.089)
NEC for r=0.8 class 1.0 = 0.189 +- 0.041 (in-sample avg dev_std = 0.089)
NEC for r=0.8 all KL = 0.052 +- 0.041 (in-sample avg dev_std = 0.089)
NEC for r=0.8 all L1 = 0.194 +- 0.091 (in-sample avg dev_std = 0.089)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:36:49 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:49 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:52 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:56 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:36:59 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 63...
[0m[1;37mINFO[0m: [1mCheckpoint 63: 
-----------------------------------
Train ROC-AUC: 0.9783
Train Loss: 0.0616
ID Validation ROC-AUC: 0.8402
ID Validation Loss: 0.1441
ID Test ROC-AUC: 0.8095
ID Test Loss: 0.1360
OOD Validation ROC-AUC: 0.7259
OOD Validation Loss: 0.1652
OOD Test ROC-AUC: 0.7195
OOD Test Loss: 0.1190

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 29...
[0m[1;37mINFO[0m: [1mCheckpoint 29: 
-----------------------------------
Train ROC-AUC: 0.8974
Train Loss: 0.1171
ID Validation ROC-AUC: 0.8181
ID Validation Loss: 0.1542
ID Test ROC-AUC: 0.7779
ID Test Loss: 0.1431
OOD Validation ROC-AUC: 0.7900
OOD Validation Loss: 0.1534
OOD Test ROC-AUC: 0.7206
OOD Test Loss: 0.0975

[0m[1;37mINFO[0m: [1mChartInfo 0.8095 0.7195 0.7779 0.7206 0.8181 0.7900[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 05/10/2024 11:37:02 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 05/10/2024 11:37:05 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_test: Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], smiles='Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OCC(C)C)c1', idx=[1], scaffold='S=C(Nc1ccccc1)c1ccoc1', domain_id=[1], env_id=[1], ori_edge_index=[2, 48], node_perm=[23])
Label distribution from id_test: (tensor([0., 1.]), tensor([3976,  136]))
[1;34mDEBUG[0m: 05/10/2024 11:37:08 AM : [1mUnbalanced warning for GOODHIV (id_test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([136, 136]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 05/10/2024 11:37:12 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.646
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.606
SUFF++ for r=0.8 class 0.0 = 0.744 +- 0.084 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.8 class 1.0 = 0.798 +- 0.084 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.8 all KL = 0.916 +- 0.084 (in-sample avg dev_std = 0.232)
SUFF++ for r=0.8 all L1 = 0.771 +- 0.100 (in-sample avg dev_std = 0.232)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.703
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.675
SUFF++ for r=0.8 class 0.0 = 0.754 +- 0.072 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 class 1.0 = 0.8 +- 0.072 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 all KL = 0.922 +- 0.072 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 all L1 = 0.777 +- 0.092 (in-sample avg dev_std = 0.239)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.552
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.517
SUFF++ for r=0.8 class 0.0 = 0.765 +- 0.074 (in-sample avg dev_std = 0.207)
SUFF++ for r=0.8 class 1.0 = 0.792 +- 0.074 (in-sample avg dev_std = 0.207)
SUFF++ for r=0.8 all KL = 0.927 +- 0.074 (in-sample avg dev_std = 0.207)
SUFF++ for r=0.8 all L1 = 0.778 +- 0.092 (in-sample avg dev_std = 0.207)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.525
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.489
SUFF++ for r=0.8 class 0.0 = 0.85 +- 0.029 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.8 class 1.0 = 0.864 +- 0.029 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.8 all KL = 0.973 +- 0.029 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.8 all L1 = 0.857 +- 0.063 (in-sample avg dev_std = 0.129)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.646
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.625
NEC for r=0.8 class 0.0 = 0.218 +- 0.050 (in-sample avg dev_std = 0.051)
NEC for r=0.8 class 1.0 = 0.172 +- 0.050 (in-sample avg dev_std = 0.051)
NEC for r=0.8 all KL = 0.044 +- 0.050 (in-sample avg dev_std = 0.051)
NEC for r=0.8 all L1 = 0.195 +- 0.093 (in-sample avg dev_std = 0.051)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.703
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.693
NEC for r=0.8 class 0.0 = 0.228 +- 0.033 (in-sample avg dev_std = 0.050)
NEC for r=0.8 class 1.0 = 0.172 +- 0.033 (in-sample avg dev_std = 0.050)
NEC for r=0.8 all KL = 0.043 +- 0.033 (in-sample avg dev_std = 0.050)
NEC for r=0.8 all L1 = 0.2 +- 0.087 (in-sample avg dev_std = 0.050)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.552
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.539
NEC for r=0.8 class 0.0 = 0.208 +- 0.042 (in-sample avg dev_std = 0.051)
NEC for r=0.8 class 1.0 = 0.192 +- 0.042 (in-sample avg dev_std = 0.051)
NEC for r=0.8 all KL = 0.044 +- 0.042 (in-sample avg dev_std = 0.051)
NEC for r=0.8 all L1 = 0.2 +- 0.089 (in-sample avg dev_std = 0.051)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.525
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.507
NEC for r=0.8 class 0.0 = 0.149 +- 0.015 (in-sample avg dev_std = 0.049)
NEC for r=0.8 class 1.0 = 0.134 +- 0.015 (in-sample avg dev_std = 0.049)
NEC for r=0.8 all KL = 0.019 +- 0.015 (in-sample avg dev_std = 0.049)
NEC for r=0.8 all L1 = 0.141 +- 0.057 (in-sample avg dev_std = 0.049)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:37:42 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:42 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:46 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:49 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:52 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 82...
[0m[1;37mINFO[0m: [1mCheckpoint 82: 
-----------------------------------
Train ROC-AUC: 0.9911
Train Loss: 0.0431
ID Validation ROC-AUC: 0.8503
ID Validation Loss: 0.1485
ID Test ROC-AUC: 0.8083
ID Test Loss: 0.1421
OOD Validation ROC-AUC: 0.7751
OOD Validation Loss: 0.1501
OOD Test ROC-AUC: 0.6717
OOD Test Loss: 0.1245

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 76...
[0m[1;37mINFO[0m: [1mCheckpoint 76: 
-----------------------------------
Train ROC-AUC: 0.9906
Train Loss: 0.0461
ID Validation ROC-AUC: 0.8310
ID Validation Loss: 0.1652
ID Test ROC-AUC: 0.8173
ID Test Loss: 0.1510
OOD Validation ROC-AUC: 0.7921
OOD Validation Loss: 0.1494
OOD Test ROC-AUC: 0.6742
OOD Test Loss: 0.1246

[0m[1;37mINFO[0m: [1mChartInfo 0.8083 0.6717 0.8173 0.6742 0.8310 0.7921[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 05/10/2024 11:37:55 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 05/10/2024 11:37:59 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_test: Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], smiles='Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OCC(C)C)c1', idx=[1], scaffold='S=C(Nc1ccccc1)c1ccoc1', domain_id=[1], env_id=[1], ori_edge_index=[2, 48], node_perm=[23])
Label distribution from id_test: (tensor([0., 1.]), tensor([3976,  136]))
[1;34mDEBUG[0m: 05/10/2024 11:38:01 AM : [1mUnbalanced warning for GOODHIV (id_test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([136, 136]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 05/10/2024 11:38:04 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.626
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.62
SUFF++ for r=0.8 class 0.0 = 0.886 +- 0.159 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.8 class 1.0 = 0.948 +- 0.159 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.8 all KL = 0.944 +- 0.159 (in-sample avg dev_std = 0.149)
SUFF++ for r=0.8 all L1 = 0.917 +- 0.162 (in-sample avg dev_std = 0.149)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.657
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.641
SUFF++ for r=0.8 class 0.0 = 0.917 +- 0.077 (in-sample avg dev_std = 0.053)
SUFF++ for r=0.8 class 1.0 = 0.965 +- 0.077 (in-sample avg dev_std = 0.053)
SUFF++ for r=0.8 all KL = 0.975 +- 0.077 (in-sample avg dev_std = 0.053)
SUFF++ for r=0.8 all L1 = 0.941 +- 0.111 (in-sample avg dev_std = 0.053)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.574
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.574
SUFF++ for r=0.8 class 0.0 = 0.882 +- 0.174 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.8 class 1.0 = 0.923 +- 0.174 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.8 all KL = 0.933 +- 0.174 (in-sample avg dev_std = 0.167)
SUFF++ for r=0.8 all L1 = 0.902 +- 0.178 (in-sample avg dev_std = 0.167)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.52
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.506
SUFF++ for r=0.8 class 0.0 = 0.797 +- 0.149 (in-sample avg dev_std = 0.203)
SUFF++ for r=0.8 class 1.0 = 0.814 +- 0.149 (in-sample avg dev_std = 0.203)
SUFF++ for r=0.8 all KL = 0.901 +- 0.149 (in-sample avg dev_std = 0.203)
SUFF++ for r=0.8 all L1 = 0.805 +- 0.197 (in-sample avg dev_std = 0.203)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.626
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.631
NEC for r=0.8 class 0.0 = 0.075 +- 0.072 (in-sample avg dev_std = 0.036)
NEC for r=0.8 class 1.0 = 0.032 +- 0.072 (in-sample avg dev_std = 0.036)
NEC for r=0.8 all KL = 0.022 +- 0.072 (in-sample avg dev_std = 0.036)
NEC for r=0.8 all L1 = 0.054 +- 0.119 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.657
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.644
NEC for r=0.8 class 0.0 = 0.064 +- 0.053 (in-sample avg dev_std = 0.023)
NEC for r=0.8 class 1.0 = 0.023 +- 0.053 (in-sample avg dev_std = 0.023)
NEC for r=0.8 all KL = 0.013 +- 0.053 (in-sample avg dev_std = 0.023)
NEC for r=0.8 all L1 = 0.043 +- 0.095 (in-sample avg dev_std = 0.023)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.574
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.578
NEC for r=0.8 class 0.0 = 0.08 +- 0.091 (in-sample avg dev_std = 0.051)
NEC for r=0.8 class 1.0 = 0.055 +- 0.091 (in-sample avg dev_std = 0.051)
NEC for r=0.8 all KL = 0.031 +- 0.091 (in-sample avg dev_std = 0.051)
NEC for r=0.8 all L1 = 0.067 +- 0.136 (in-sample avg dev_std = 0.051)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.52
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.517
NEC for r=0.8 class 0.0 = 0.146 +- 0.064 (in-sample avg dev_std = 0.080)
NEC for r=0.8 class 1.0 = 0.121 +- 0.064 (in-sample avg dev_std = 0.080)
NEC for r=0.8 all KL = 0.039 +- 0.064 (in-sample avg dev_std = 0.080)
NEC for r=0.8 all L1 = 0.133 +- 0.145 (in-sample avg dev_std = 0.080)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:38:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:35 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:38 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:42 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:45 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 98...
[0m[1;37mINFO[0m: [1mCheckpoint 98: 
-----------------------------------
Train ROC-AUC: 0.9945
Train Loss: 0.0377
ID Validation ROC-AUC: 0.8435
ID Validation Loss: 0.1511
ID Test ROC-AUC: 0.8057
ID Test Loss: 0.1429
OOD Validation ROC-AUC: 0.7471
OOD Validation Loss: 0.1620
OOD Test ROC-AUC: 0.6561
OOD Test Loss: 0.1286

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 25...
[0m[1;37mINFO[0m: [1mCheckpoint 25: 
-----------------------------------
Train ROC-AUC: 0.9150
Train Loss: 0.0947
ID Validation ROC-AUC: 0.8005
ID Validation Loss: 0.1372
ID Test ROC-AUC: 0.7726
ID Test Loss: 0.1264
OOD Validation ROC-AUC: 0.7902
OOD Validation Loss: 0.1138
OOD Test ROC-AUC: 0.6440
OOD Test Loss: 0.0949

[0m[1;37mINFO[0m: [1mChartInfo 0.8057 0.6561 0.7726 0.6440 0.8005 0.7902[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 05/10/2024 11:38:47 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 05/10/2024 11:38:51 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_test: Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], smiles='Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OCC(C)C)c1', idx=[1], scaffold='S=C(Nc1ccccc1)c1ccoc1', domain_id=[1], env_id=[1], ori_edge_index=[2, 48], node_perm=[23])
Label distribution from id_test: (tensor([0., 1.]), tensor([3976,  136]))
[1;34mDEBUG[0m: 05/10/2024 11:38:54 AM : [1mUnbalanced warning for GOODHIV (id_test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([136, 136]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 05/10/2024 11:38:57 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.669
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.639
SUFF++ for r=0.8 class 0.0 = 0.769 +- 0.141 (in-sample avg dev_std = 0.159)
SUFF++ for r=0.8 class 1.0 = 0.856 +- 0.141 (in-sample avg dev_std = 0.159)
SUFF++ for r=0.8 all KL = 0.921 +- 0.141 (in-sample avg dev_std = 0.159)
SUFF++ for r=0.8 all L1 = 0.813 +- 0.160 (in-sample avg dev_std = 0.159)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.719
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.688
SUFF++ for r=0.8 class 0.0 = 0.742 +- 0.115 (in-sample avg dev_std = 0.143)
SUFF++ for r=0.8 class 1.0 = 0.858 +- 0.115 (in-sample avg dev_std = 0.143)
SUFF++ for r=0.8 all KL = 0.926 +- 0.115 (in-sample avg dev_std = 0.143)
SUFF++ for r=0.8 all L1 = 0.8 +- 0.163 (in-sample avg dev_std = 0.143)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.584
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.557
SUFF++ for r=0.8 class 0.0 = 0.79 +- 0.142 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.8 class 1.0 = 0.823 +- 0.142 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.8 all KL = 0.917 +- 0.142 (in-sample avg dev_std = 0.172)
SUFF++ for r=0.8 all L1 = 0.806 +- 0.162 (in-sample avg dev_std = 0.172)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.548
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.527
SUFF++ for r=0.8 class 0.0 = 0.833 +- 0.048 (in-sample avg dev_std = 0.110)
SUFF++ for r=0.8 class 1.0 = 0.848 +- 0.048 (in-sample avg dev_std = 0.110)
SUFF++ for r=0.8 all KL = 0.964 +- 0.048 (in-sample avg dev_std = 0.110)
SUFF++ for r=0.8 all L1 = 0.841 +- 0.108 (in-sample avg dev_std = 0.110)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.669
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.658
NEC for r=0.8 class 0.0 = 0.178 +- 0.100 (in-sample avg dev_std = 0.045)
NEC for r=0.8 class 1.0 = 0.106 +- 0.100 (in-sample avg dev_std = 0.045)
NEC for r=0.8 all KL = 0.045 +- 0.100 (in-sample avg dev_std = 0.045)
NEC for r=0.8 all L1 = 0.142 +- 0.144 (in-sample avg dev_std = 0.045)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.719
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.718
NEC for r=0.8 class 0.0 = 0.225 +- 0.089 (in-sample avg dev_std = 0.044)
NEC for r=0.8 class 1.0 = 0.117 +- 0.089 (in-sample avg dev_std = 0.044)
NEC for r=0.8 all KL = 0.051 +- 0.089 (in-sample avg dev_std = 0.044)
NEC for r=0.8 all L1 = 0.171 +- 0.157 (in-sample avg dev_std = 0.044)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.584
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.587
NEC for r=0.8 class 0.0 = 0.177 +- 0.096 (in-sample avg dev_std = 0.046)
NEC for r=0.8 class 1.0 = 0.134 +- 0.096 (in-sample avg dev_std = 0.046)
NEC for r=0.8 all KL = 0.048 +- 0.096 (in-sample avg dev_std = 0.046)
NEC for r=0.8 all L1 = 0.156 +- 0.151 (in-sample avg dev_std = 0.046)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.548
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.543
NEC for r=0.8 class 0.0 = 0.128 +- 0.034 (in-sample avg dev_std = 0.043)
NEC for r=0.8 class 1.0 = 0.114 +- 0.034 (in-sample avg dev_std = 0.043)
NEC for r=0.8 all KL = 0.02 +- 0.034 (in-sample avg dev_std = 0.043)
NEC for r=0.8 all L1 = 0.121 +- 0.101 (in-sample avg dev_std = 0.043)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:39:27 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:27 AM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:30 AM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:34 AM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:37 AM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ROC-AUC: 0.9763
Train Loss: 0.0754
ID Validation ROC-AUC: 0.8423
ID Validation Loss: 0.1616
ID Test ROC-AUC: 0.8022
ID Test Loss: 0.1618
OOD Validation ROC-AUC: 0.7507
OOD Validation Loss: 0.1758
OOD Test ROC-AUC: 0.7055
OOD Test Loss: 0.1216

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 70...
[0m[1;37mINFO[0m: [1mCheckpoint 70: 
-----------------------------------
Train ROC-AUC: 0.9711
Train Loss: 0.0667
ID Validation ROC-AUC: 0.7891
ID Validation Loss: 0.1858
ID Test ROC-AUC: 0.7767
ID Test Loss: 0.1596
OOD Validation ROC-AUC: 0.7883
OOD Validation Loss: 0.1527
OOD Test ROC-AUC: 0.6633
OOD Test Loss: 0.1257

[0m[1;37mINFO[0m: [1mChartInfo 0.8022 0.7055 0.7767 0.6633 0.7891 0.7883[0mGOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 05/10/2024 11:39:40 AM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 05/10/2024 11:39:44 AM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_test: Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], smiles='Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OCC(C)C)c1', idx=[1], scaffold='S=C(Nc1ccccc1)c1ccoc1', domain_id=[1], env_id=[1], ori_edge_index=[2, 48], node_perm=[23])
Label distribution from id_test: (tensor([0., 1.]), tensor([3976,  136]))
[1;34mDEBUG[0m: 05/10/2024 11:39:46 AM : [1mUnbalanced warning for GOODHIV (id_test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([136, 136]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 05/10/2024 11:39:49 AM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.664
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.646
SUFF++ for r=0.8 class 0.0 = 0.793 +- 0.071 (in-sample avg dev_std = 0.196)
SUFF++ for r=0.8 class 1.0 = 0.835 +- 0.071 (in-sample avg dev_std = 0.196)
SUFF++ for r=0.8 all KL = 0.939 +- 0.071 (in-sample avg dev_std = 0.196)
SUFF++ for r=0.8 all L1 = 0.814 +- 0.092 (in-sample avg dev_std = 0.196)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.747
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.691
SUFF++ for r=0.8 class 0.0 = 0.782 +- 0.047 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.8 class 1.0 = 0.858 +- 0.047 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.8 all KL = 0.951 +- 0.047 (in-sample avg dev_std = 0.174)
SUFF++ for r=0.8 all L1 = 0.82 +- 0.085 (in-sample avg dev_std = 0.174)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.592
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.573
SUFF++ for r=0.8 class 0.0 = 0.805 +- 0.063 (in-sample avg dev_std = 0.189)
SUFF++ for r=0.8 class 1.0 = 0.836 +- 0.063 (in-sample avg dev_std = 0.189)
SUFF++ for r=0.8 all KL = 0.942 +- 0.063 (in-sample avg dev_std = 0.189)
SUFF++ for r=0.8 all L1 = 0.82 +- 0.085 (in-sample avg dev_std = 0.189)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.61
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.583
SUFF++ for r=0.8 class 0.0 = 0.849 +- 0.046 (in-sample avg dev_std = 0.165)
SUFF++ for r=0.8 class 1.0 = 0.86 +- 0.046 (in-sample avg dev_std = 0.165)
SUFF++ for r=0.8 all KL = 0.956 +- 0.046 (in-sample avg dev_std = 0.165)
SUFF++ for r=0.8 all L1 = 0.855 +- 0.058 (in-sample avg dev_std = 0.165)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.664
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.657
NEC for r=0.8 class 0.0 = 0.117 +- 0.015 (in-sample avg dev_std = 0.041)
NEC for r=0.8 class 1.0 = 0.088 +- 0.015 (in-sample avg dev_std = 0.041)
NEC for r=0.8 all KL = 0.013 +- 0.015 (in-sample avg dev_std = 0.041)
NEC for r=0.8 all L1 = 0.102 +- 0.060 (in-sample avg dev_std = 0.041)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.747
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.712
NEC for r=0.8 class 0.0 = 0.131 +- 0.012 (in-sample avg dev_std = 0.040)
NEC for r=0.8 class 1.0 = 0.078 +- 0.012 (in-sample avg dev_std = 0.040)
NEC for r=0.8 all KL = 0.013 +- 0.012 (in-sample avg dev_std = 0.040)
NEC for r=0.8 all L1 = 0.105 +- 0.061 (in-sample avg dev_std = 0.040)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.592
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 272
Effective ratio: 0.808 +- 0.007
Model ROC-AUC over intervened graphs for r=0.8 =  0.583
NEC for r=0.8 class 0.0 = 0.11 +- 0.014 (in-sample avg dev_std = 0.041)
NEC for r=0.8 class 1.0 = 0.094 +- 0.014 (in-sample avg dev_std = 0.041)
NEC for r=0.8 all KL = 0.014 +- 0.014 (in-sample avg dev_std = 0.041)
NEC for r=0.8 all L1 = 0.102 +- 0.054 (in-sample avg dev_std = 0.041)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.61
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.59
NEC for r=0.8 class 0.0 = 0.086 +- 0.007 (in-sample avg dev_std = 0.040)
NEC for r=0.8 class 1.0 = 0.073 +- 0.007 (in-sample avg dev_std = 0.040)
NEC for r=0.8 all KL = 0.009 +- 0.007 (in-sample avg dev_std = 0.040)
NEC for r=0.8 all L1 = 0.079 +- 0.037 (in-sample avg dev_std = 0.040)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.84], 'all_L1': [0.698]}), defaultdict(<class 'list'>, {'all_KL': [0.916], 'all_L1': [0.771]}), defaultdict(<class 'list'>, {'all_KL': [0.944], 'all_L1': [0.917]}), defaultdict(<class 'list'>, {'all_KL': [0.921], 'all_L1': [0.813]}), defaultdict(<class 'list'>, {'all_KL': [0.939], 'all_L1': [0.814]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.065], 'all_L1': [0.218]}), defaultdict(<class 'list'>, {'all_KL': [0.044], 'all_L1': [0.195]}), defaultdict(<class 'list'>, {'all_KL': [0.022], 'all_L1': [0.054]}), defaultdict(<class 'list'>, {'all_KL': [0.045], 'all_L1': [0.142]}), defaultdict(<class 'list'>, {'all_KL': [0.013], 'all_L1': [0.102]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.86], 'all_L1': [0.697]}), defaultdict(<class 'list'>, {'all_KL': [0.922], 'all_L1': [0.777]}), defaultdict(<class 'list'>, {'all_KL': [0.975], 'all_L1': [0.941]}), defaultdict(<class 'list'>, {'all_KL': [0.926], 'all_L1': [0.8]}), defaultdict(<class 'list'>, {'all_KL': [0.951], 'all_L1': [0.82]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.069], 'all_L1': [0.236]}), defaultdict(<class 'list'>, {'all_KL': [0.043], 'all_L1': [0.2]}), defaultdict(<class 'list'>, {'all_KL': [0.013], 'all_L1': [0.043]}), defaultdict(<class 'list'>, {'all_KL': [0.051], 'all_L1': [0.171]}), defaultdict(<class 'list'>, {'all_KL': [0.013], 'all_L1': [0.105]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.855], 'all_L1': [0.713]}), defaultdict(<class 'list'>, {'all_KL': [0.927], 'all_L1': [0.778]}), defaultdict(<class 'list'>, {'all_KL': [0.933], 'all_L1': [0.902]}), defaultdict(<class 'list'>, {'all_KL': [0.917], 'all_L1': [0.806]}), defaultdict(<class 'list'>, {'all_KL': [0.942], 'all_L1': [0.82]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.058], 'all_L1': [0.206]}), defaultdict(<class 'list'>, {'all_KL': [0.044], 'all_L1': [0.2]}), defaultdict(<class 'list'>, {'all_KL': [0.031], 'all_L1': [0.067]}), defaultdict(<class 'list'>, {'all_KL': [0.048], 'all_L1': [0.156]}), defaultdict(<class 'list'>, {'all_KL': [0.014], 'all_L1': [0.102]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.887], 'all_L1': [0.752]}), defaultdict(<class 'list'>, {'all_KL': [0.973], 'all_L1': [0.857]}), defaultdict(<class 'list'>, {'all_KL': [0.901], 'all_L1': [0.805]}), defaultdict(<class 'list'>, {'all_KL': [0.964], 'all_L1': [0.841]}), defaultdict(<class 'list'>, {'all_KL': [0.956], 'all_L1': [0.855]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.052], 'all_L1': [0.194]}), defaultdict(<class 'list'>, {'all_KL': [0.019], 'all_L1': [0.141]}), defaultdict(<class 'list'>, {'all_KL': [0.039], 'all_L1': [0.133]}), defaultdict(<class 'list'>, {'all_KL': [0.02], 'all_L1': [0.121]}), defaultdict(<class 'list'>, {'all_KL': [0.009], 'all_L1': [0.079]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.803 +- 0.071
suff++ class all_KL  =  0.912 +- 0.038
suff++_acc_int  =  0.618 +- 0.025
nec class all_L1  =  0.142 +- 0.060
nec class all_KL  =  0.038 +- 0.018
nec_acc_int  =  0.641 +- 0.014

Eval split val
suff++ class all_L1  =  0.807 +- 0.079
suff++ class all_KL  =  0.927 +- 0.038
suff++_acc_int  =  0.668 +- 0.021
nec class all_L1  =  0.151 +- 0.069
nec class all_KL  =  0.038 +- 0.022
nec_acc_int  =  0.689 +- 0.026

Eval split id_test
suff++ class all_L1  =  0.804 +- 0.061
suff++ class all_KL  =  0.915 +- 0.031
suff++_acc_int  =  0.544 +- 0.031
nec class all_L1  =  0.146 +- 0.054
nec class all_KL  =  0.039 +- 0.015
nec_acc_int  =  0.561 +- 0.027

Eval split test
suff++ class all_L1  =  0.822 +- 0.040
suff++ class all_KL  =  0.936 +- 0.035
suff++_acc_int  =  0.534 +- 0.036
nec class all_L1  =  0.134 +- 0.037
nec class all_KL  =  0.028 +- 0.016
nec_acc_int  =  0.541 +- 0.029


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.472 +- 0.012
Faith. Armon (L1)= 		  =  0.234 +- 0.085
Faith. GMean (L1)= 	  =  0.326 +- 0.064
Faith. Aritm (KL)= 		  =  0.475 +- 0.011
Faith. Armon (KL)= 		  =  0.072 +- 0.034
Faith. GMean (KL)= 	  =  0.179 +- 0.045

Eval split val
Faith. Aritm (L1)= 		  =  0.479 +- 0.012
Faith. Armon (L1)= 		  =  0.244 +- 0.098
Faith. GMean (L1)= 	  =  0.333 +- 0.077
Faith. Aritm (KL)= 		  =  0.482 +- 0.010
Faith. Armon (KL)= 		  =  0.072 +- 0.040
Faith. GMean (KL)= 	  =  0.177 +- 0.055

Eval split id_test
Faith. Aritm (L1)= 		  =  0.475 +- 0.012
Faith. Armon (L1)= 		  =  0.241 +- 0.077
Faith. GMean (L1)= 	  =  0.333 +- 0.057
Faith. Aritm (KL)= 		  =  0.477 +- 0.010
Faith. Armon (KL)= 		  =  0.074 +- 0.028
Faith. GMean (KL)= 	  =  0.184 +- 0.039

Eval split test
Faith. Aritm (L1)= 		  =  0.478 +- 0.012
Faith. Armon (L1)= 		  =  0.227 +- 0.053
Faith. GMean (L1)= 	  =  0.327 +- 0.040
Faith. Aritm (KL)= 		  =  0.482 +- 0.011
Faith. Armon (KL)= 		  =  0.053 +- 0.029
Faith. GMean (KL)= 	  =  0.154 +- 0.043
Computed for split load_split = id



Completed in  0:04:29.298546  for CIGAvGIN GOODHIV/scaffold



DONE CIGA GOODHIV/scaffold readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:40:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:40:36 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 99...
[0m[1;37mINFO[0m: [1mCheckpoint 99: 
-----------------------------------
Train ACCURACY: 0.6868
Train Loss: 1.1340
ID Validation ACCURACY: 0.6349
ID Validation Loss: 1.4523
ID Test ACCURACY: 0.6443
ID Test Loss: 1.4493
OOD Validation ACCURACY: 0.5267
OOD Validation Loss: 2.4705
OOD Test ACCURACY: 0.2393
OOD Test Loss: 50.3153

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 85...
[0m[1;37mINFO[0m: [1mCheckpoint 85: 
-----------------------------------
Train ACCURACY: 0.6818
Train Loss: 1.0209
ID Validation ACCURACY: 0.6344
ID Validation Loss: 1.2867
ID Test ACCURACY: 0.6417
ID Test Loss: 1.2611
OOD Validation ACCURACY: 0.5907
OOD Validation Loss: 1.6844
OOD Test ACCURACY: 0.2530
OOD Test Loss: 30.3575

[0m[1;37mINFO[0m: [1mChartInfo 0.6443 0.2393 0.6417 0.2530 0.6344 0.5907[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_test: Data(x=[75, 3], edge_index=[2, 1351], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1351], node_perm=[75])
Label distribution from id_test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([657, 814, 676, 706, 676, 663, 688, 748, 702, 670]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.696
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.316
SUFF++ for r=0.6 class 0 = 0.334 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 1 = 0.959 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 2 = 0.23 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 3 = 0.248 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 4 = 0.226 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 5 = 0.26 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 6 = 0.208 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 7 = 0.239 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 8 = 0.261 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 9 = 0.261 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 all KL = 0.116 +- 0.296 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 all L1 = 0.331 +- 0.239 (in-sample avg dev_std = 0.686)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.504
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.281
SUFF++ for r=0.6 class 0 = 0.323 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 1 = 0.942 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 2 = 0.244 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 3 = 0.221 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 4 = 0.271 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 5 = 0.237 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 6 = 0.247 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 7 = 0.257 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 8 = 0.256 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 9 = 0.302 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 all KL = 0.142 +- 0.323 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 all L1 = 0.34 +- 0.258 (in-sample avg dev_std = 0.649)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.683
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.317
SUFF++ for r=0.6 class 0 = 0.34 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 1 = 0.928 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 2 = 0.229 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 3 = 0.232 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 4 = 0.227 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 5 = 0.25 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 6 = 0.206 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 7 = 0.246 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 8 = 0.245 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 class 9 = 0.248 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 all KL = 0.11 +- 0.289 (in-sample avg dev_std = 0.679)
SUFF++ for r=0.6 all L1 = 0.326 +- 0.236 (in-sample avg dev_std = 0.679)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.168
SUFF++ for r=0.6 class 0 = 0.487 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 1 = 0.99 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 2 = 0.511 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 3 = 0.529 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 4 = 0.774 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 5 = 0.549 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 6 = 0.799 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 7 = 0.709 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 8 = 0.666 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 class 9 = 0.873 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 all KL = 0.575 +- 0.459 (in-sample avg dev_std = 0.496)
SUFF++ for r=0.6 all L1 = 0.691 +- 0.350 (in-sample avg dev_std = 0.496)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.696
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.509
NEC for r=0.6 class 0 = 0.101 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 1 = 0.039 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 2 = 0.543 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 3 = 0.647 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 4 = 0.469 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 5 = 0.517 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 6 = 0.577 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 7 = 0.389 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 8 = 0.651 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 class 9 = 0.484 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 all KL = 0.54 +- 0.353 (in-sample avg dev_std = 0.388)
NEC for r=0.6 all L1 = 0.436 +- 0.293 (in-sample avg dev_std = 0.388)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.504
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.423
NEC for r=0.6 class 0 = 0.254 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 1 = 0.049 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 2 = 0.553 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 3 = 0.667 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 4 = 0.554 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 5 = 0.672 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 6 = 0.583 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 7 = 0.502 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 8 = 0.641 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 class 9 = 0.484 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 all KL = 0.612 +- 0.347 (in-sample avg dev_std = 0.437)
NEC for r=0.6 all L1 = 0.488 +- 0.286 (in-sample avg dev_std = 0.437)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.683
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.501
NEC for r=0.6 class 0 = 0.106 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 1 = 0.06 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 2 = 0.526 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 3 = 0.693 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 4 = 0.529 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 5 = 0.539 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 6 = 0.633 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 7 = 0.334 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 8 = 0.666 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 class 9 = 0.496 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 all KL = 0.573 +- 0.350 (in-sample avg dev_std = 0.397)
NEC for r=0.6 all L1 = 0.452 +- 0.291 (in-sample avg dev_std = 0.397)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.272
NEC for r=0.6 class 0 = 0.494 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 1 = 0.017 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 2 = 0.689 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 3 = 0.592 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 4 = 0.495 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 5 = 0.609 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 6 = 0.527 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 7 = 0.558 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 8 = 0.503 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 class 9 = 0.296 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 all KL = 0.664 +- 0.422 (in-sample avg dev_std = 0.393)
NEC for r=0.6 all L1 = 0.473 +- 0.357 (in-sample avg dev_std = 0.393)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 11:50:41 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 11:50:42 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.3406
Train Loss: 2.8099
ID Validation ACCURACY: 0.3361
ID Validation Loss: 2.8348
ID Test ACCURACY: 0.3410
ID Test Loss: 2.8208
OOD Validation ACCURACY: 0.3401
OOD Validation Loss: 3.0656
OOD Test ACCURACY: 0.2519
OOD Test Loss: 4.4899

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ACCURACY: 0.3406
Train Loss: 2.8099
ID Validation ACCURACY: 0.3361
ID Validation Loss: 2.8348
ID Test ACCURACY: 0.3410
ID Test Loss: 2.8208
OOD Validation ACCURACY: 0.3401
OOD Validation Loss: 3.0656
OOD Test ACCURACY: 0.2519
OOD Test Loss: 4.4899

[0m[1;37mINFO[0m: [1mChartInfo 0.3410 0.2519 0.3410 0.2519 0.3361 0.3401[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_test: Data(x=[75, 3], edge_index=[2, 1351], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1351], node_perm=[75])
Label distribution from id_test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([657, 814, 676, 706, 676, 663, 688, 748, 702, 670]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.334
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.226
SUFF++ for r=0.6 class 0 = 0.419 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 1 = 0.431 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 2 = 0.28 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 3 = 0.275 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 4 = 0.33 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 5 = 0.299 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 6 = 0.313 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 7 = 0.31 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 8 = 0.267 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 9 = 0.356 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 all KL = 0.107 +- 0.162 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 all L1 = 0.329 +- 0.119 (in-sample avg dev_std = 0.631)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.351
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.224
SUFF++ for r=0.6 class 0 = 0.313 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 1 = 0.672 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 2 = 0.271 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 3 = 0.28 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 4 = 0.342 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 5 = 0.285 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 6 = 0.306 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 7 = 0.319 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 8 = 0.27 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 class 9 = 0.331 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 all KL = 0.132 +- 0.254 (in-sample avg dev_std = 0.662)
SUFF++ for r=0.6 all L1 = 0.345 +- 0.165 (in-sample avg dev_std = 0.662)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.35
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.207
SUFF++ for r=0.6 class 0 = 0.376 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 1 = 0.37 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 2 = 0.293 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 3 = 0.277 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 4 = 0.326 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 5 = 0.282 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 6 = 0.312 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 7 = 0.297 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 8 = 0.27 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 class 9 = 0.358 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 all KL = 0.098 +- 0.136 (in-sample avg dev_std = 0.631)
SUFF++ for r=0.6 all L1 = 0.316 +- 0.094 (in-sample avg dev_std = 0.631)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.208
SUFF++ for r=0.6 class 0 = 0.262 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 1 = 0.805 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 2 = 0.237 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 3 = 0.251 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 4 = 0.205 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 5 = 0.249 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 6 = 0.223 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 7 = 0.258 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 8 = 0.24 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 class 9 = 0.248 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 all KL = 0.139 +- 0.288 (in-sample avg dev_std = 0.509)
SUFF++ for r=0.6 all L1 = 0.304 +- 0.219 (in-sample avg dev_std = 0.509)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.334
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.237
NEC for r=0.6 class 0 = 0.18 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 1 = 0.438 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 2 = 0.56 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 3 = 0.554 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 4 = 0.308 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 5 = 0.567 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 6 = 0.426 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 7 = 0.448 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 8 = 0.479 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 9 = 0.249 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 all KL = 0.361 +- 0.266 (in-sample avg dev_std = 0.234)
NEC for r=0.6 all L1 = 0.422 +- 0.242 (in-sample avg dev_std = 0.234)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.351
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.249
NEC for r=0.6 class 0 = 0.294 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 1 = 0.448 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 2 = 0.604 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 3 = 0.575 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 4 = 0.365 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 5 = 0.56 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 6 = 0.43 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 7 = 0.444 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 8 = 0.519 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 9 = 0.282 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all KL = 0.397 +- 0.261 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all L1 = 0.45 +- 0.225 (in-sample avg dev_std = 0.261)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.35
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.226
NEC for r=0.6 class 0 = 0.155 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 1 = 0.435 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 2 = 0.586 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 3 = 0.573 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 4 = 0.292 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 5 = 0.558 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 6 = 0.477 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 7 = 0.489 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 8 = 0.519 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 class 9 = 0.314 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 all KL = 0.378 +- 0.266 (in-sample avg dev_std = 0.223)
NEC for r=0.6 all L1 = 0.442 +- 0.244 (in-sample avg dev_std = 0.223)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.241
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.228
NEC for r=0.6 class 0 = 0.538 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 1 = 0.229 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 2 = 0.518 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 3 = 0.521 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 4 = 0.497 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 5 = 0.527 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 6 = 0.523 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 7 = 0.522 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 8 = 0.533 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 class 9 = 0.523 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 all KL = 0.429 +- 0.222 (in-sample avg dev_std = 0.273)
NEC for r=0.6 all L1 = 0.49 +- 0.182 (in-sample avg dev_std = 0.273)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:00:49 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:00:50 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:00:51 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 90...
[0m[1;37mINFO[0m: [1mCheckpoint 90: 
-----------------------------------
Train ACCURACY: 0.7166
Train Loss: 0.8744
ID Validation ACCURACY: 0.6586
ID Validation Loss: 1.1864
ID Test ACCURACY: 0.6627
ID Test Loss: 1.1704
OOD Validation ACCURACY: 0.5016
OOD Validation Loss: 2.0563
OOD Test ACCURACY: 0.2053
OOD Test Loss: 7.1389

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 29...
[0m[1;37mINFO[0m: [1mCheckpoint 29: 
-----------------------------------
Train ACCURACY: 0.5742
Train Loss: 1.2504
ID Validation ACCURACY: 0.5544
ID Validation Loss: 1.3208
ID Test ACCURACY: 0.5559
ID Test Loss: 1.3027
OOD Validation ACCURACY: 0.5757
OOD Validation Loss: 1.2810
OOD Test ACCURACY: 0.2889
OOD Test Loss: 2.5977

[0m[1;37mINFO[0m: [1mChartInfo 0.6627 0.2053 0.5559 0.2889 0.5544 0.5757[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_test: Data(x=[75, 3], edge_index=[2, 1351], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1351], node_perm=[75])
Label distribution from id_test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([657, 814, 676, 706, 676, 663, 688, 748, 702, 670]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.694
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.275
SUFF++ for r=0.6 class 0 = 0.337 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 1 = 0.817 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 2 = 0.245 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 3 = 0.209 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 4 = 0.228 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 5 = 0.23 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 6 = 0.215 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 7 = 0.3 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 8 = 0.199 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 class 9 = 0.233 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 all KL = 0.104 +- 0.229 (in-sample avg dev_std = 0.569)
SUFF++ for r=0.6 all L1 = 0.309 +- 0.225 (in-sample avg dev_std = 0.569)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.507
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.21
SUFF++ for r=0.6 class 0 = 0.275 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 1 = 0.754 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 2 = 0.226 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 3 = 0.202 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 4 = 0.242 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 5 = 0.216 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 6 = 0.231 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 7 = 0.251 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 8 = 0.224 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 9 = 0.264 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 all KL = 0.096 +- 0.214 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 all L1 = 0.296 +- 0.209 (in-sample avg dev_std = 0.565)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.695
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.275
SUFF++ for r=0.6 class 0 = 0.365 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 1 = 0.783 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 2 = 0.26 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 3 = 0.204 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 4 = 0.231 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 5 = 0.224 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 6 = 0.193 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 7 = 0.346 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 8 = 0.193 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 9 = 0.254 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all KL = 0.111 +- 0.235 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all L1 = 0.314 +- 0.228 (in-sample avg dev_std = 0.554)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.188
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.187
SUFF++ for r=0.6 class 0 = 0.304 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 1 = 0.76 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 2 = 0.308 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 3 = 0.338 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 4 = 0.355 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 5 = 0.352 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 6 = 0.396 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 7 = 0.405 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 8 = 0.381 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 class 9 = 0.468 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all KL = 0.255 +- 0.305 (in-sample avg dev_std = 0.528)
SUFF++ for r=0.6 all L1 = 0.41 +- 0.254 (in-sample avg dev_std = 0.528)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.694
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.402
NEC for r=0.6 class 0 = 0.714 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 1 = 0.285 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 2 = 0.656 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 3 = 0.619 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 4 = 0.582 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 5 = 0.654 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 6 = 0.69 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 7 = 0.624 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 8 = 0.568 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 class 9 = 0.651 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 all KL = 0.757 +- 0.261 (in-sample avg dev_std = 0.455)
NEC for r=0.6 all L1 = 0.599 +- 0.217 (in-sample avg dev_std = 0.455)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.507
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.397
NEC for r=0.6 class 0 = 0.643 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 1 = 0.172 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 2 = 0.625 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 3 = 0.627 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 4 = 0.591 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 5 = 0.641 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 6 = 0.631 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 7 = 0.612 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 8 = 0.619 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 class 9 = 0.612 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 all KL = 0.7 +- 0.265 (in-sample avg dev_std = 0.441)
NEC for r=0.6 all L1 = 0.571 +- 0.224 (in-sample avg dev_std = 0.441)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.695
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.414
NEC for r=0.6 class 0 = 0.697 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 1 = 0.322 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 2 = 0.609 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 3 = 0.633 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 4 = 0.565 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 5 = 0.673 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 6 = 0.653 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 7 = 0.638 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 8 = 0.545 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 class 9 = 0.631 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 all KL = 0.751 +- 0.252 (in-sample avg dev_std = 0.459)
NEC for r=0.6 all L1 = 0.591 +- 0.215 (in-sample avg dev_std = 0.459)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.188
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.168
NEC for r=0.6 class 0 = 0.591 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 1 = 0.185 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 2 = 0.594 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 3 = 0.528 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 4 = 0.487 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 5 = 0.571 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 6 = 0.463 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 7 = 0.48 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 8 = 0.477 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 class 9 = 0.366 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 all KL = 0.508 +- 0.303 (in-sample avg dev_std = 0.376)
NEC for r=0.6 all L1 = 0.471 +- 0.247 (in-sample avg dev_std = 0.376)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:11:26 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:27 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:27 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:27 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:11:28 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 94...
[0m[1;37mINFO[0m: [1mCheckpoint 94: 
-----------------------------------
Train ACCURACY: 0.4384
Train Loss: 2.1899
ID Validation ACCURACY: 0.4306
ID Validation Loss: 2.2851
ID Test ACCURACY: 0.4244
ID Test Loss: 2.2523
OOD Validation ACCURACY: 0.3311
OOD Validation Loss: 3.7102
OOD Test ACCURACY: 0.2449
OOD Test Loss: 7.9565

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.4047
Train Loss: 1.8306
ID Validation ACCURACY: 0.3959
ID Validation Loss: 1.8921
ID Test ACCURACY: 0.4023
ID Test Loss: 1.8483
OOD Validation ACCURACY: 0.3679
OOD Validation Loss: 2.1088
OOD Test ACCURACY: 0.2666
OOD Test Loss: 3.2335

[0m[1;37mINFO[0m: [1mChartInfo 0.4244 0.2449 0.4023 0.2666 0.3959 0.3679[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_test: Data(x=[75, 3], edge_index=[2, 1351], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1351], node_perm=[75])
Label distribution from id_test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([657, 814, 676, 706, 676, 663, 688, 748, 702, 670]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.217
SUFF++ for r=0.6 class 0 = 0.267 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 1 = 0.861 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 2 = 0.244 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 3 = 0.26 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 4 = 0.235 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 5 = 0.273 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 6 = 0.228 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 7 = 0.256 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 8 = 0.239 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 class 9 = 0.261 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 all KL = 0.134 +- 0.278 (in-sample avg dev_std = 0.606)
SUFF++ for r=0.6 all L1 = 0.32 +- 0.218 (in-sample avg dev_std = 0.606)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.322
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.184
SUFF++ for r=0.6 class 0 = 0.363 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 1 = 0.866 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 2 = 0.262 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 3 = 0.29 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 4 = 0.247 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 5 = 0.275 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 6 = 0.305 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 7 = 0.349 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 8 = 0.317 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 class 9 = 0.338 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 all KL = 0.182 +- 0.293 (in-sample avg dev_std = 0.628)
SUFF++ for r=0.6 all L1 = 0.369 +- 0.246 (in-sample avg dev_std = 0.628)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.214
SUFF++ for r=0.6 class 0 = 0.25 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 1 = 0.814 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 2 = 0.241 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 3 = 0.257 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 4 = 0.242 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 5 = 0.257 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 6 = 0.253 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 7 = 0.243 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 8 = 0.251 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 class 9 = 0.267 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 all KL = 0.124 +- 0.265 (in-sample avg dev_std = 0.614)
SUFF++ for r=0.6 all L1 = 0.317 +- 0.212 (in-sample avg dev_std = 0.614)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.239
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.18
SUFF++ for r=0.6 class 0 = 0.353 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 1 = 0.89 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 2 = 0.212 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 3 = 0.222 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 4 = 0.256 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 5 = 0.27 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 6 = 0.264 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 7 = 0.27 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 8 = 0.221 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 class 9 = 0.325 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 all KL = 0.153 +- 0.320 (in-sample avg dev_std = 0.549)
SUFF++ for r=0.6 all L1 = 0.335 +- 0.273 (in-sample avg dev_std = 0.549)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.39
NEC for r=0.6 class 0 = 0.197 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 1 = 0.271 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 2 = 0.608 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 3 = 0.561 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 4 = 0.56 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 5 = 0.581 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 6 = 0.584 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 7 = 0.55 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 8 = 0.549 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 9 = 0.522 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 all KL = 0.526 +- 0.273 (in-sample avg dev_std = 0.364)
NEC for r=0.6 all L1 = 0.494 +- 0.235 (in-sample avg dev_std = 0.364)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.322
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.336
NEC for r=0.6 class 0 = 0.152 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 1 = 0.155 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 2 = 0.511 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 3 = 0.517 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 4 = 0.541 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 5 = 0.465 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 6 = 0.504 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 7 = 0.585 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 8 = 0.589 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 class 9 = 0.562 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 all KL = 0.531 +- 0.319 (in-sample avg dev_std = 0.402)
NEC for r=0.6 all L1 = 0.456 +- 0.268 (in-sample avg dev_std = 0.402)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.434
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.394
NEC for r=0.6 class 0 = 0.163 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 1 = 0.309 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 2 = 0.596 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 3 = 0.596 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 4 = 0.551 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 5 = 0.575 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 6 = 0.565 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 7 = 0.598 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 8 = 0.497 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 class 9 = 0.514 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 all KL = 0.525 +- 0.276 (in-sample avg dev_std = 0.365)
NEC for r=0.6 all L1 = 0.495 +- 0.235 (in-sample avg dev_std = 0.365)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.239
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.236
NEC for r=0.6 class 0 = 0.406 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 1 = 0.056 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 2 = 0.575 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 3 = 0.564 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 4 = 0.475 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 5 = 0.558 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 6 = 0.51 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 7 = 0.562 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 8 = 0.574 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 class 9 = 0.44 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 all KL = 0.583 +- 0.365 (in-sample avg dev_std = 0.442)
NEC for r=0.6 all L1 = 0.467 +- 0.299 (in-sample avg dev_std = 0.442)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:22:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:38 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:38 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:38 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:22:39 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 88...
[0m[1;37mINFO[0m: [1mCheckpoint 88: 
-----------------------------------
Train ACCURACY: 0.4462
Train Loss: 2.1957
ID Validation ACCURACY: 0.4340
ID Validation Loss: 2.2713
ID Test ACCURACY: 0.4387
ID Test Loss: 2.2521
OOD Validation ACCURACY: 0.3690
OOD Validation Loss: 3.1579
OOD Test ACCURACY: 0.2166
OOD Test Loss: 6.3998

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 75...
[0m[1;37mINFO[0m: [1mCheckpoint 75: 
-----------------------------------
Train ACCURACY: 0.4253
Train Loss: 2.0693
ID Validation ACCURACY: 0.4229
ID Validation Loss: 2.1638
ID Test ACCURACY: 0.4240
ID Test Loss: 2.1236
OOD Validation ACCURACY: 0.3857
OOD Validation Loss: 2.7142
OOD Test ACCURACY: 0.2043
OOD Test Loss: 5.5404

[0m[1;37mINFO[0m: [1mChartInfo 0.4387 0.2166 0.4240 0.2043 0.4229 0.3857[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_test: Data(x=[75, 3], edge_index=[2, 1351], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1351], node_perm=[75])
Label distribution from id_test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([657, 814, 676, 706, 676, 663, 688, 748, 702, 670]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.466
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.206
SUFF++ for r=0.6 class 0 = 0.231 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 1 = 0.475 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 2 = 0.223 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 3 = 0.251 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 4 = 0.254 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 5 = 0.246 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 6 = 0.288 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 7 = 0.291 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 8 = 0.259 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 class 9 = 0.379 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 all KL = 0.117 +- 0.178 (in-sample avg dev_std = 0.565)
SUFF++ for r=0.6 all L1 = 0.292 +- 0.137 (in-sample avg dev_std = 0.565)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.366
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.211
SUFF++ for r=0.6 class 0 = 0.311 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 1 = 0.477 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 2 = 0.241 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 3 = 0.246 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 4 = 0.342 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 5 = 0.302 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 6 = 0.333 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 7 = 0.305 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 8 = 0.253 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 class 9 = 0.393 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 all KL = 0.138 +- 0.180 (in-sample avg dev_std = 0.618)
SUFF++ for r=0.6 all L1 = 0.323 +- 0.139 (in-sample avg dev_std = 0.618)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.43
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.203
SUFF++ for r=0.6 class 0 = 0.216 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 1 = 0.433 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 2 = 0.233 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 3 = 0.243 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 4 = 0.27 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 5 = 0.241 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 6 = 0.273 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 7 = 0.271 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 8 = 0.259 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 9 = 0.373 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 all KL = 0.107 +- 0.165 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 all L1 = 0.284 +- 0.128 (in-sample avg dev_std = 0.564)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.195
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.155
SUFF++ for r=0.6 class 0 = 0.339 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 1 = 0.394 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 2 = 0.243 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 3 = 0.271 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 4 = 0.268 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 5 = 0.251 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 6 = 0.274 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 7 = 0.279 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 8 = 0.269 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 class 9 = 0.289 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 all KL = 0.114 +- 0.171 (in-sample avg dev_std = 0.602)
SUFF++ for r=0.6 all L1 = 0.289 +- 0.129 (in-sample avg dev_std = 0.602)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.466
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.362
NEC for r=0.6 class 0 = 0.287 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 1 = 0.425 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 2 = 0.6 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 3 = 0.493 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 4 = 0.56 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 5 = 0.578 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 6 = 0.602 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 7 = 0.534 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 8 = 0.578 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 class 9 = 0.502 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 all KL = 0.559 +- 0.266 (in-sample avg dev_std = 0.364)
NEC for r=0.6 all L1 = 0.513 +- 0.212 (in-sample avg dev_std = 0.364)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.366
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.32
NEC for r=0.6 class 0 = 0.249 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 1 = 0.421 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 2 = 0.583 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 3 = 0.575 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 4 = 0.515 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 5 = 0.563 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 6 = 0.497 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 7 = 0.581 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 8 = 0.597 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 class 9 = 0.45 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 all KL = 0.572 +- 0.290 (in-sample avg dev_std = 0.405)
NEC for r=0.6 all L1 = 0.502 +- 0.225 (in-sample avg dev_std = 0.405)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.43
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.351
NEC for r=0.6 class 0 = 0.24 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 1 = 0.414 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 2 = 0.567 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 3 = 0.514 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 4 = 0.552 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 5 = 0.59 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 6 = 0.592 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 7 = 0.529 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 8 = 0.563 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 class 9 = 0.498 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 all KL = 0.537 +- 0.280 (in-sample avg dev_std = 0.355)
NEC for r=0.6 all L1 = 0.505 +- 0.224 (in-sample avg dev_std = 0.355)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.195
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.177
NEC for r=0.6 class 0 = 0.472 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 1 = 0.346 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 2 = 0.536 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 3 = 0.551 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 4 = 0.466 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 5 = 0.548 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 6 = 0.521 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 7 = 0.547 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 8 = 0.58 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 class 9 = 0.471 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 all KL = 0.557 +- 0.334 (in-sample avg dev_std = 0.425)
NEC for r=0.6 all L1 = 0.502 +- 0.247 (in-sample avg dev_std = 0.425)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.116], 'all_L1': [0.331]}), defaultdict(<class 'list'>, {'all_KL': [0.107], 'all_L1': [0.329]}), defaultdict(<class 'list'>, {'all_KL': [0.104], 'all_L1': [0.309]}), defaultdict(<class 'list'>, {'all_KL': [0.134], 'all_L1': [0.32]}), defaultdict(<class 'list'>, {'all_KL': [0.117], 'all_L1': [0.292]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.54], 'all_L1': [0.436]}), defaultdict(<class 'list'>, {'all_KL': [0.361], 'all_L1': [0.422]}), defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.599]}), defaultdict(<class 'list'>, {'all_KL': [0.526], 'all_L1': [0.494]}), defaultdict(<class 'list'>, {'all_KL': [0.559], 'all_L1': [0.513]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.142], 'all_L1': [0.34]}), defaultdict(<class 'list'>, {'all_KL': [0.132], 'all_L1': [0.345]}), defaultdict(<class 'list'>, {'all_KL': [0.096], 'all_L1': [0.296]}), defaultdict(<class 'list'>, {'all_KL': [0.182], 'all_L1': [0.369]}), defaultdict(<class 'list'>, {'all_KL': [0.138], 'all_L1': [0.323]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.612], 'all_L1': [0.488]}), defaultdict(<class 'list'>, {'all_KL': [0.397], 'all_L1': [0.45]}), defaultdict(<class 'list'>, {'all_KL': [0.7], 'all_L1': [0.571]}), defaultdict(<class 'list'>, {'all_KL': [0.531], 'all_L1': [0.456]}), defaultdict(<class 'list'>, {'all_KL': [0.572], 'all_L1': [0.502]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.11], 'all_L1': [0.326]}), defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.316]}), defaultdict(<class 'list'>, {'all_KL': [0.111], 'all_L1': [0.314]}), defaultdict(<class 'list'>, {'all_KL': [0.124], 'all_L1': [0.317]}), defaultdict(<class 'list'>, {'all_KL': [0.107], 'all_L1': [0.284]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.573], 'all_L1': [0.452]}), defaultdict(<class 'list'>, {'all_KL': [0.378], 'all_L1': [0.442]}), defaultdict(<class 'list'>, {'all_KL': [0.751], 'all_L1': [0.591]}), defaultdict(<class 'list'>, {'all_KL': [0.525], 'all_L1': [0.495]}), defaultdict(<class 'list'>, {'all_KL': [0.537], 'all_L1': [0.505]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.575], 'all_L1': [0.691]}), defaultdict(<class 'list'>, {'all_KL': [0.139], 'all_L1': [0.304]}), defaultdict(<class 'list'>, {'all_KL': [0.255], 'all_L1': [0.41]}), defaultdict(<class 'list'>, {'all_KL': [0.153], 'all_L1': [0.335]}), defaultdict(<class 'list'>, {'all_KL': [0.114], 'all_L1': [0.289]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.664], 'all_L1': [0.473]}), defaultdict(<class 'list'>, {'all_KL': [0.429], 'all_L1': [0.49]}), defaultdict(<class 'list'>, {'all_KL': [0.508], 'all_L1': [0.471]}), defaultdict(<class 'list'>, {'all_KL': [0.583], 'all_L1': [0.467]}), defaultdict(<class 'list'>, {'all_KL': [0.557], 'all_L1': [0.502]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.316 +- 0.014
suff++ class all_KL  =  0.116 +- 0.010
suff++_acc_int  =  0.248 +- 0.041
nec class all_L1  =  0.493 +- 0.063
nec class all_KL  =  0.549 +- 0.126
nec_acc_int  =  0.380 +- 0.087

Eval split val
suff++ class all_L1  =  0.335 +- 0.024
suff++ class all_KL  =  0.138 +- 0.027
suff++_acc_int  =  0.222 +- 0.032
nec class all_L1  =  0.493 +- 0.043
nec class all_KL  =  0.562 +- 0.100
nec_acc_int  =  0.345 +- 0.061

Eval split id_test
suff++ class all_L1  =  0.311 +- 0.014
suff++ class all_KL  =  0.110 +- 0.008
suff++_acc_int  =  0.243 +- 0.045
nec class all_L1  =  0.497 +- 0.053
nec class all_KL  =  0.553 +- 0.119
nec_acc_int  =  0.377 +- 0.090

Eval split test
suff++ class all_L1  =  0.406 +- 0.149
suff++ class all_KL  =  0.247 +- 0.171
suff++_acc_int  =  0.180 +- 0.018
nec class all_L1  =  0.481 +- 0.013
nec class all_KL  =  0.548 +- 0.078
nec_acc_int  =  0.216 +- 0.039


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.404 +- 0.027
Faith. Armon (L1)= 		  =  0.383 +- 0.014
Faith. GMean (L1)= 	  =  0.393 +- 0.020
Faith. Aritm (KL)= 		  =  0.332 +- 0.062
Faith. Armon (KL)= 		  =  0.189 +- 0.016
Faith. GMean (KL)= 	  =  0.250 +- 0.029

Eval split val
Faith. Aritm (L1)= 		  =  0.414 +- 0.011
Faith. Armon (L1)= 		  =  0.396 +- 0.007
Faith. GMean (L1)= 	  =  0.405 +- 0.006
Faith. Aritm (KL)= 		  =  0.350 +- 0.046
Faith. Armon (KL)= 		  =  0.218 +- 0.034
Faith. GMean (KL)= 	  =  0.275 +- 0.029

Eval split id_test
Faith. Aritm (L1)= 		  =  0.404 +- 0.026
Faith. Armon (L1)= 		  =  0.381 +- 0.016
Faith. GMean (L1)= 	  =  0.393 +- 0.020
Faith. Aritm (KL)= 		  =  0.331 +- 0.061
Faith. Armon (KL)= 		  =  0.183 +- 0.015
Faith. GMean (KL)= 	  =  0.245 +- 0.031

Eval split test
Faith. Aritm (L1)= 		  =  0.443 +- 0.071
Faith. Armon (L1)= 		  =  0.426 +- 0.072
Faith. GMean (L1)= 	  =  0.435 +- 0.072
Faith. Aritm (KL)= 		  =  0.398 +- 0.116
Faith. Armon (KL)= 		  =  0.319 +- 0.157
Faith. GMean (KL)= 	  =  0.355 +- 0.138
Computed for split load_split = id



Completed in  0:52:34.053138  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:33:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/10/2024 12:33:21 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 12:33:55 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:07 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:20 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:36 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:34:53 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 64...
[0m[1;37mINFO[0m: [1mCheckpoint 64: 
-----------------------------------
Train ROC-AUC: 0.9925
Train Loss: 0.0975
ID Validation ROC-AUC: 0.9178
ID Validation Loss: 0.3060
ID Test ROC-AUC: 0.9170
ID Test Loss: 0.3147
OOD Validation ROC-AUC: 0.6215
OOD Validation Loss: 0.5097
OOD Test ROC-AUC: 0.6702
OOD Test Loss: 0.7230

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ROC-AUC: 0.8878
Train Loss: 0.2467
ID Validation ROC-AUC: 0.8703
ID Validation Loss: 0.2604
ID Test ROC-AUC: 0.8722
ID Test Loss: 0.2607
OOD Validation ROC-AUC: 0.6813
OOD Validation Loss: 0.3373
OOD Test ROC-AUC: 0.6909
OOD Test Loss: 0.4560

[0m[1;37mINFO[0m: [1mChartInfo 0.9170 0.6702 0.8722 0.6909 0.8703 0.6813[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 05/10/2024 12:34:55 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 05/10/2024 12:35:20 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11683)
Data example from id_test: Data(x=[33, 39], edge_index=[2, 72], edge_attr=[72, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 72], node_perm=[33])
Label distribution from id_test: (tensor([0., 1.]), tensor([ 1386, 10297]))
[1;34mDEBUG[0m: 05/10/2024 12:35:44 PM : [1mUnbalanced warning for LBAPcore (id_test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/10/2024 12:36:11 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.784
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.706
SUFF++ for r=0.6 class 0.0 = 0.986 +- 0.009 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 class 1.0 = 0.998 +- 0.009 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 all KL = 0.998 +- 0.009 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 all L1 = 0.996 +- 0.022 (in-sample avg dev_std = 0.012)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.581
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 799
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.579
SUFF++ for r=0.6 class 0.0 = 0.984 +- 0.011 (in-sample avg dev_std = 0.014)
SUFF++ for r=0.6 class 1.0 = 0.997 +- 0.011 (in-sample avg dev_std = 0.014)
SUFF++ for r=0.6 all KL = 0.998 +- 0.011 (in-sample avg dev_std = 0.014)
SUFF++ for r=0.6 all L1 = 0.996 +- 0.024 (in-sample avg dev_std = 0.014)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.751
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.668
SUFF++ for r=0.6 class 0.0 = 0.983 +- 0.014 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 class 1.0 = 0.996 +- 0.014 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 all KL = 0.998 +- 0.014 (in-sample avg dev_std = 0.012)
SUFF++ for r=0.6 all L1 = 0.995 +- 0.029 (in-sample avg dev_std = 0.012)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.645
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.607
SUFF++ for r=0.6 class 0.0 = 0.986 +- 0.013 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 class 1.0 = 0.995 +- 0.013 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all KL = 0.997 +- 0.013 (in-sample avg dev_std = 0.013)
SUFF++ for r=0.6 all L1 = 0.994 +- 0.027 (in-sample avg dev_std = 0.013)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.784
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.774
NEC for r=0.6 class 0.0 = 0.011 +- 0.005 (in-sample avg dev_std = 0.006)
NEC for r=0.6 class 1.0 = 0.002 +- 0.005 (in-sample avg dev_std = 0.006)
NEC for r=0.6 all KL = 0.001 +- 0.005 (in-sample avg dev_std = 0.006)
NEC for r=0.6 all L1 = 0.003 +- 0.017 (in-sample avg dev_std = 0.006)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.587
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.602
NEC for r=0.6 class 0.0 = 0.015 +- 0.007 (in-sample avg dev_std = 0.007)
NEC for r=0.6 class 1.0 = 0.002 +- 0.007 (in-sample avg dev_std = 0.007)
NEC for r=0.6 all KL = 0.001 +- 0.007 (in-sample avg dev_std = 0.007)
NEC for r=0.6 all L1 = 0.003 +- 0.023 (in-sample avg dev_std = 0.007)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.751
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.745
NEC for r=0.6 class 0.0 = 0.013 +- 0.007 (in-sample avg dev_std = 0.009)
NEC for r=0.6 class 1.0 = 0.002 +- 0.007 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all KL = 0.001 +- 0.007 (in-sample avg dev_std = 0.009)
NEC for r=0.6 all L1 = 0.003 +- 0.021 (in-sample avg dev_std = 0.009)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.645
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.64
NEC for r=0.6 class 0.0 = 0.009 +- 0.005 (in-sample avg dev_std = 0.010)
NEC for r=0.6 class 1.0 = 0.003 +- 0.005 (in-sample avg dev_std = 0.010)
NEC for r=0.6 all KL = 0.001 +- 0.005 (in-sample avg dev_std = 0.010)
NEC for r=0.6 all L1 = 0.004 +- 0.017 (in-sample avg dev_std = 0.010)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:38:49 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/10/2024 12:38:50 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 12:39:25 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 12:39:37 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 12:39:50 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:06 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ROC-AUC: 0.9942
Train Loss: 0.0721
ID Validation ROC-AUC: 0.9179
ID Validation Loss: 0.2928
ID Test ROC-AUC: 0.9171
ID Test Loss: 0.2997
OOD Validation ROC-AUC: 0.6280
OOD Validation Loss: 0.5802
OOD Test ROC-AUC: 0.6756
OOD Test Loss: 0.7411

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 2...
[0m[1;37mINFO[0m: [1mCheckpoint 2: 
-----------------------------------
Train ROC-AUC: 0.8761
Train Loss: 0.2569
ID Validation ROC-AUC: 0.8739
ID Validation Loss: 0.2600
ID Test ROC-AUC: 0.8719
ID Test Loss: 0.2637
OOD Validation ROC-AUC: 0.6912
OOD Validation Loss: 0.2846
OOD Test ROC-AUC: 0.7078
OOD Test Loss: 0.4249

[0m[1;37mINFO[0m: [1mChartInfo 0.9171 0.6756 0.8719 0.7078 0.8739 0.6912[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 05/10/2024 12:40:23 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 05/10/2024 12:40:49 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11683)
Data example from id_test: Data(x=[33, 39], edge_index=[2, 72], edge_attr=[72, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 72], node_perm=[33])
Label distribution from id_test: (tensor([0., 1.]), tensor([ 1386, 10297]))
[1;34mDEBUG[0m: 05/10/2024 12:41:14 PM : [1mUnbalanced warning for LBAPcore (id_test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/10/2024 12:41:39 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.753
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.714
SUFF++ for r=0.6 class 0.0 = 0.951 +- 0.011 (in-sample avg dev_std = 0.029)
SUFF++ for r=0.6 class 1.0 = 0.976 +- 0.011 (in-sample avg dev_std = 0.029)
SUFF++ for r=0.6 all KL = 0.995 +- 0.011 (in-sample avg dev_std = 0.029)
SUFF++ for r=0.6 all L1 = 0.973 +- 0.032 (in-sample avg dev_std = 0.029)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.526
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.535
SUFF++ for r=0.6 class 0.0 = 0.942 +- 0.013 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 class 1.0 = 0.964 +- 0.013 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 all KL = 0.993 +- 0.013 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 all L1 = 0.962 +- 0.041 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.716
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.679
SUFF++ for r=0.6 class 0.0 = 0.955 +- 0.008 (in-sample avg dev_std = 0.027)
SUFF++ for r=0.6 class 1.0 = 0.976 +- 0.008 (in-sample avg dev_std = 0.027)
SUFF++ for r=0.6 all KL = 0.995 +- 0.008 (in-sample avg dev_std = 0.027)
SUFF++ for r=0.6 all L1 = 0.973 +- 0.031 (in-sample avg dev_std = 0.027)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.634
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.599
SUFF++ for r=0.6 class 0.0 = 0.948 +- 0.011 (in-sample avg dev_std = 0.035)
SUFF++ for r=0.6 class 1.0 = 0.966 +- 0.011 (in-sample avg dev_std = 0.035)
SUFF++ for r=0.6 all KL = 0.993 +- 0.011 (in-sample avg dev_std = 0.035)
SUFF++ for r=0.6 all L1 = 0.963 +- 0.039 (in-sample avg dev_std = 0.035)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.753
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.74
NEC for r=0.6 class 0.0 = 0.035 +- 0.005 (in-sample avg dev_std = 0.012)
NEC for r=0.6 class 1.0 = 0.015 +- 0.005 (in-sample avg dev_std = 0.012)
NEC for r=0.6 all KL = 0.002 +- 0.005 (in-sample avg dev_std = 0.012)
NEC for r=0.6 all L1 = 0.017 +- 0.025 (in-sample avg dev_std = 0.012)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.526
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.548
NEC for r=0.6 class 0.0 = 0.043 +- 0.008 (in-sample avg dev_std = 0.017)
NEC for r=0.6 class 1.0 = 0.025 +- 0.008 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all KL = 0.003 +- 0.008 (in-sample avg dev_std = 0.017)
NEC for r=0.6 all L1 = 0.027 +- 0.035 (in-sample avg dev_std = 0.017)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.716
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.703
NEC for r=0.6 class 0.0 = 0.033 +- 0.005 (in-sample avg dev_std = 0.012)
NEC for r=0.6 class 1.0 = 0.016 +- 0.005 (in-sample avg dev_std = 0.012)
NEC for r=0.6 all KL = 0.002 +- 0.005 (in-sample avg dev_std = 0.012)
NEC for r=0.6 all L1 = 0.018 +- 0.025 (in-sample avg dev_std = 0.012)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.634
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.633
NEC for r=0.6 class 0.0 = 0.037 +- 0.006 (in-sample avg dev_std = 0.015)
NEC for r=0.6 class 1.0 = 0.023 +- 0.006 (in-sample avg dev_std = 0.015)
NEC for r=0.6 all KL = 0.003 +- 0.006 (in-sample avg dev_std = 0.015)
NEC for r=0.6 all L1 = 0.025 +- 0.031 (in-sample avg dev_std = 0.015)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:44:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/10/2024 12:44:21 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 12:44:58 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:09 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:20 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:36 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ROC-AUC: 0.9750
Train Loss: 0.1417
ID Validation ROC-AUC: 0.9182
ID Validation Loss: 0.2388
ID Test ROC-AUC: 0.9183
ID Test Loss: 0.2437
OOD Validation ROC-AUC: 0.6336
OOD Validation Loss: 0.4159
OOD Test ROC-AUC: 0.6692
OOD Test Loss: 0.5811

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8791
Train Loss: 0.2903
ID Validation ROC-AUC: 0.8687
ID Validation Loss: 0.2995
ID Test ROC-AUC: 0.8711
ID Test Loss: 0.2998
OOD Validation ROC-AUC: 0.6926
OOD Validation Loss: 0.2862
OOD Test ROC-AUC: 0.7124
OOD Test Loss: 0.4811

[0m[1;37mINFO[0m: [1mChartInfo 0.9183 0.6692 0.8711 0.7124 0.8687 0.6926[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 05/10/2024 12:45:54 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 05/10/2024 12:46:19 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11683)
Data example from id_test: Data(x=[33, 39], edge_index=[2, 72], edge_attr=[72, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 72], node_perm=[33])
Label distribution from id_test: (tensor([0., 1.]), tensor([ 1386, 10297]))
[1;34mDEBUG[0m: 05/10/2024 12:46:41 PM : [1mUnbalanced warning for LBAPcore (id_test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/10/2024 12:47:08 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.8
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.726
SUFF++ for r=0.6 class 0.0 = 0.911 +- 0.018 (in-sample avg dev_std = 0.037)
SUFF++ for r=0.6 class 1.0 = 0.967 +- 0.018 (in-sample avg dev_std = 0.037)
SUFF++ for r=0.6 all KL = 0.988 +- 0.018 (in-sample avg dev_std = 0.037)
SUFF++ for r=0.6 all L1 = 0.96 +- 0.052 (in-sample avg dev_std = 0.037)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.601
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 799
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.557
SUFF++ for r=0.6 class 0.0 = 0.931 +- 0.016 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 class 1.0 = 0.956 +- 0.016 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 all KL = 0.987 +- 0.016 (in-sample avg dev_std = 0.036)
SUFF++ for r=0.6 all L1 = 0.954 +- 0.051 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.765
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.695
SUFF++ for r=0.6 class 0.0 = 0.922 +- 0.017 (in-sample avg dev_std = 0.033)
SUFF++ for r=0.6 class 1.0 = 0.966 +- 0.017 (in-sample avg dev_std = 0.033)
SUFF++ for r=0.6 all KL = 0.989 +- 0.017 (in-sample avg dev_std = 0.033)
SUFF++ for r=0.6 all L1 = 0.961 +- 0.052 (in-sample avg dev_std = 0.033)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.674
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.635
SUFF++ for r=0.6 class 0.0 = 0.918 +- 0.020 (in-sample avg dev_std = 0.045)
SUFF++ for r=0.6 class 1.0 = 0.951 +- 0.020 (in-sample avg dev_std = 0.045)
SUFF++ for r=0.6 all KL = 0.984 +- 0.020 (in-sample avg dev_std = 0.045)
SUFF++ for r=0.6 all L1 = 0.946 +- 0.061 (in-sample avg dev_std = 0.045)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.8
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.78
NEC for r=0.6 class 0.0 = 0.062 +- 0.008 (in-sample avg dev_std = 0.018)
NEC for r=0.6 class 1.0 = 0.02 +- 0.008 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all KL = 0.004 +- 0.008 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all L1 = 0.025 +- 0.039 (in-sample avg dev_std = 0.018)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.604
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.596
NEC for r=0.6 class 0.0 = 0.05 +- 0.009 (in-sample avg dev_std = 0.018)
NEC for r=0.6 class 1.0 = 0.028 +- 0.009 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all KL = 0.005 +- 0.009 (in-sample avg dev_std = 0.018)
NEC for r=0.6 all L1 = 0.03 +- 0.039 (in-sample avg dev_std = 0.018)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.765
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.739
NEC for r=0.6 class 0.0 = 0.056 +- 0.008 (in-sample avg dev_std = 0.019)
NEC for r=0.6 class 1.0 = 0.021 +- 0.008 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all KL = 0.004 +- 0.008 (in-sample avg dev_std = 0.019)
NEC for r=0.6 all L1 = 0.025 +- 0.039 (in-sample avg dev_std = 0.019)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.674
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.668
NEC for r=0.6 class 0.0 = 0.062 +- 0.012 (in-sample avg dev_std = 0.022)
NEC for r=0.6 class 1.0 = 0.033 +- 0.012 (in-sample avg dev_std = 0.022)
NEC for r=0.6 all KL = 0.007 +- 0.012 (in-sample avg dev_std = 0.022)
NEC for r=0.6 all L1 = 0.038 +- 0.050 (in-sample avg dev_std = 0.022)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:49:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/10/2024 12:49:51 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 12:50:26 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 12:50:36 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 12:50:47 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:05 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:51:24 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:51:25 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ROC-AUC: 0.9750
Train Loss: 0.1492
ID Validation ROC-AUC: 0.9191
ID Validation Loss: 0.2545
ID Test ROC-AUC: 0.9158
ID Test Loss: 0.2649
OOD Validation ROC-AUC: 0.6267
OOD Validation Loss: 0.4135
OOD Test ROC-AUC: 0.6863
OOD Test Loss: 0.5801

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 6...
[0m[1;37mINFO[0m: [1mCheckpoint 6: 
-----------------------------------
Train ROC-AUC: 0.9012
Train Loss: 0.2347
ID Validation ROC-AUC: 0.8845
ID Validation Loss: 0.2520
ID Test ROC-AUC: 0.8857
ID Test Loss: 0.2551
OOD Validation ROC-AUC: 0.6780
OOD Validation Loss: 0.3280
OOD Test ROC-AUC: 0.7060
OOD Test Loss: 0.4756

[0m[1;37mINFO[0m: [1mChartInfo 0.9158 0.6863 0.8857 0.7060 0.8845 0.6780[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 05/10/2024 12:51:25 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 05/10/2024 12:51:49 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11683)
Data example from id_test: Data(x=[33, 39], edge_index=[2, 72], edge_attr=[72, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 72], node_perm=[33])
Label distribution from id_test: (tensor([0., 1.]), tensor([ 1386, 10297]))
[1;34mDEBUG[0m: 05/10/2024 12:52:14 PM : [1mUnbalanced warning for LBAPcore (id_test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/10/2024 12:52:41 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.472
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.533
SUFF++ for r=0.6 class 0.0 = 0.873 +- 0.027 (in-sample avg dev_std = 0.114)
SUFF++ for r=0.6 class 1.0 = 0.899 +- 0.027 (in-sample avg dev_std = 0.114)
SUFF++ for r=0.6 all KL = 0.976 +- 0.027 (in-sample avg dev_std = 0.114)
SUFF++ for r=0.6 all L1 = 0.896 +- 0.044 (in-sample avg dev_std = 0.114)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.337
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 799
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.395
SUFF++ for r=0.6 class 0.0 = 0.88 +- 0.025 (in-sample avg dev_std = 0.111)
SUFF++ for r=0.6 class 1.0 = 0.896 +- 0.025 (in-sample avg dev_std = 0.111)
SUFF++ for r=0.6 all KL = 0.976 +- 0.025 (in-sample avg dev_std = 0.111)
SUFF++ for r=0.6 all L1 = 0.895 +- 0.045 (in-sample avg dev_std = 0.111)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.444
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.51
SUFF++ for r=0.6 class 0.0 = 0.871 +- 0.024 (in-sample avg dev_std = 0.110)
SUFF++ for r=0.6 class 1.0 = 0.901 +- 0.024 (in-sample avg dev_std = 0.110)
SUFF++ for r=0.6 all KL = 0.977 +- 0.024 (in-sample avg dev_std = 0.110)
SUFF++ for r=0.6 all L1 = 0.897 +- 0.045 (in-sample avg dev_std = 0.110)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.497
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.508
SUFF++ for r=0.6 class 0.0 = 0.884 +- 0.026 (in-sample avg dev_std = 0.115)
SUFF++ for r=0.6 class 1.0 = 0.892 +- 0.026 (in-sample avg dev_std = 0.115)
SUFF++ for r=0.6 all KL = 0.975 +- 0.026 (in-sample avg dev_std = 0.115)
SUFF++ for r=0.6 all L1 = 0.89 +- 0.045 (in-sample avg dev_std = 0.115)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.472
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.485
NEC for r=0.6 class 0.0 = 0.058 +- 0.005 (in-sample avg dev_std = 0.035)
NEC for r=0.6 class 1.0 = 0.057 +- 0.005 (in-sample avg dev_std = 0.035)
NEC for r=0.6 all KL = 0.005 +- 0.005 (in-sample avg dev_std = 0.035)
NEC for r=0.6 all L1 = 0.057 +- 0.022 (in-sample avg dev_std = 0.035)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.332
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.345
NEC for r=0.6 class 0.0 = 0.068 +- 0.005 (in-sample avg dev_std = 0.036)
NEC for r=0.6 class 1.0 = 0.056 +- 0.005 (in-sample avg dev_std = 0.036)
NEC for r=0.6 all KL = 0.005 +- 0.005 (in-sample avg dev_std = 0.036)
NEC for r=0.6 all L1 = 0.057 +- 0.025 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.444
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.442
NEC for r=0.6 class 0.0 = 0.058 +- 0.006 (in-sample avg dev_std = 0.036)
NEC for r=0.6 class 1.0 = 0.056 +- 0.006 (in-sample avg dev_std = 0.036)
NEC for r=0.6 all KL = 0.005 +- 0.006 (in-sample avg dev_std = 0.036)
NEC for r=0.6 all L1 = 0.056 +- 0.022 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.497
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.5
NEC for r=0.6 class 0.0 = 0.057 +- 0.005 (in-sample avg dev_std = 0.037)
NEC for r=0.6 class 1.0 = 0.057 +- 0.005 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all KL = 0.006 +- 0.005 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all L1 = 0.057 +- 0.024 (in-sample avg dev_std = 0.037)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 12:55:20 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 05/10/2024 12:55:20 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/10/2024 12:55:56 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:09 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:22 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:40 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 12:56:57 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 38...
[0m[1;37mINFO[0m: [1mCheckpoint 38: 
-----------------------------------
Train ROC-AUC: 0.9770
Train Loss: 0.1340
ID Validation ROC-AUC: 0.9225
ID Validation Loss: 0.2679
ID Test ROC-AUC: 0.9232
ID Test Loss: 0.2743
OOD Validation ROC-AUC: 0.6385
OOD Validation Loss: 0.5014
OOD Test ROC-AUC: 0.6848
OOD Test Loss: 0.6911

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 7...
[0m[1;37mINFO[0m: [1mCheckpoint 7: 
-----------------------------------
Train ROC-AUC: 0.8995
Train Loss: 0.2412
ID Validation ROC-AUC: 0.8839
ID Validation Loss: 0.2545
ID Test ROC-AUC: 0.8857
ID Test Loss: 0.2576
OOD Validation ROC-AUC: 0.6922
OOD Validation Loss: 0.2925
OOD Test ROC-AUC: 0.7003
OOD Test Loss: 0.4517

[0m[1;37mINFO[0m: [1mChartInfo 0.9232 0.6848 0.8857 0.7003 0.8839 0.6922[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 05/10/2024 12:56:58 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 05/10/2024 12:57:22 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11683)
Data example from id_test: Data(x=[33, 39], edge_index=[2, 72], edge_attr=[72, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 72], node_perm=[33])
Label distribution from id_test: (tensor([0., 1.]), tensor([ 1386, 10297]))
[1;34mDEBUG[0m: 05/10/2024 12:57:43 PM : [1mUnbalanced warning for LBAPcore (id_test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 05/10/2024 12:58:10 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.764
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.678
SUFF++ for r=0.6 class 0.0 = 0.858 +- 0.041 (in-sample avg dev_std = 0.074)
SUFF++ for r=0.6 class 1.0 = 0.946 +- 0.041 (in-sample avg dev_std = 0.074)
SUFF++ for r=0.6 all KL = 0.976 +- 0.041 (in-sample avg dev_std = 0.074)
SUFF++ for r=0.6 all L1 = 0.936 +- 0.085 (in-sample avg dev_std = 0.074)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.535
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.492
SUFF++ for r=0.6 class 0.0 = 0.917 +- 0.035 (in-sample avg dev_std = 0.070)
SUFF++ for r=0.6 class 1.0 = 0.933 +- 0.035 (in-sample avg dev_std = 0.070)
SUFF++ for r=0.6 all KL = 0.977 +- 0.035 (in-sample avg dev_std = 0.070)
SUFF++ for r=0.6 all L1 = 0.932 +- 0.077 (in-sample avg dev_std = 0.070)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.721
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.682
SUFF++ for r=0.6 class 0.0 = 0.865 +- 0.038 (in-sample avg dev_std = 0.069)
SUFF++ for r=0.6 class 1.0 = 0.943 +- 0.038 (in-sample avg dev_std = 0.069)
SUFF++ for r=0.6 all KL = 0.977 +- 0.038 (in-sample avg dev_std = 0.069)
SUFF++ for r=0.6 all L1 = 0.934 +- 0.085 (in-sample avg dev_std = 0.069)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.651
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.613
SUFF++ for r=0.6 class 0.0 = 0.863 +- 0.050 (in-sample avg dev_std = 0.091)
SUFF++ for r=0.6 class 1.0 = 0.927 +- 0.050 (in-sample avg dev_std = 0.091)
SUFF++ for r=0.6 all KL = 0.968 +- 0.050 (in-sample avg dev_std = 0.091)
SUFF++ for r=0.6 all L1 = 0.917 +- 0.102 (in-sample avg dev_std = 0.091)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.764
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.764
NEC for r=0.6 class 0.0 = 0.079 +- 0.012 (in-sample avg dev_std = 0.037)
NEC for r=0.6 class 1.0 = 0.03 +- 0.012 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all KL = 0.007 +- 0.012 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all L1 = 0.035 +- 0.045 (in-sample avg dev_std = 0.037)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.535
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.547
NEC for r=0.6 class 0.0 = 0.048 +- 0.012 (in-sample avg dev_std = 0.037)
NEC for r=0.6 class 1.0 = 0.037 +- 0.012 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all KL = 0.007 +- 0.012 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all L1 = 0.038 +- 0.041 (in-sample avg dev_std = 0.037)



--------------------------------------------------


#D#Computing NEC over id_test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.721
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.723
NEC for r=0.6 class 0.0 = 0.074 +- 0.011 (in-sample avg dev_std = 0.037)
NEC for r=0.6 class 1.0 = 0.03 +- 0.011 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all KL = 0.006 +- 0.011 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all L1 = 0.035 +- 0.042 (in-sample avg dev_std = 0.037)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.651
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.66
NEC for r=0.6 class 0.0 = 0.075 +- 0.017 (in-sample avg dev_std = 0.047)
NEC for r=0.6 class 1.0 = 0.041 +- 0.017 (in-sample avg dev_std = 0.047)
NEC for r=0.6 all KL = 0.009 +- 0.017 (in-sample avg dev_std = 0.047)
NEC for r=0.6 all L1 = 0.046 +- 0.057 (in-sample avg dev_std = 0.047)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [0.996]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.973]}), defaultdict(<class 'list'>, {'all_KL': [0.988], 'all_L1': [0.96]}), defaultdict(<class 'list'>, {'all_KL': [0.976], 'all_L1': [0.896]}), defaultdict(<class 'list'>, {'all_KL': [0.976], 'all_L1': [0.936]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.003]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.017]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.057]}), defaultdict(<class 'list'>, {'all_KL': [0.007], 'all_L1': [0.035]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [0.996]}), defaultdict(<class 'list'>, {'all_KL': [0.993], 'all_L1': [0.962]}), defaultdict(<class 'list'>, {'all_KL': [0.987], 'all_L1': [0.954]}), defaultdict(<class 'list'>, {'all_KL': [0.976], 'all_L1': [0.895]}), defaultdict(<class 'list'>, {'all_KL': [0.977], 'all_L1': [0.932]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.003]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.027]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.03]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.057]}), defaultdict(<class 'list'>, {'all_KL': [0.007], 'all_L1': [0.038]})]

Eval split id_test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.998], 'all_L1': [0.995]}), defaultdict(<class 'list'>, {'all_KL': [0.995], 'all_L1': [0.973]}), defaultdict(<class 'list'>, {'all_KL': [0.989], 'all_L1': [0.961]}), defaultdict(<class 'list'>, {'all_KL': [0.977], 'all_L1': [0.897]}), defaultdict(<class 'list'>, {'all_KL': [0.977], 'all_L1': [0.934]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.003]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.018]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.056]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.035]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.997], 'all_L1': [0.994]}), defaultdict(<class 'list'>, {'all_KL': [0.993], 'all_L1': [0.963]}), defaultdict(<class 'list'>, {'all_KL': [0.984], 'all_L1': [0.946]}), defaultdict(<class 'list'>, {'all_KL': [0.975], 'all_L1': [0.89]}), defaultdict(<class 'list'>, {'all_KL': [0.968], 'all_L1': [0.917]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.001], 'all_L1': [0.004]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.025]}), defaultdict(<class 'list'>, {'all_KL': [0.007], 'all_L1': [0.038]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.057]}), defaultdict(<class 'list'>, {'all_KL': [0.009], 'all_L1': [0.046]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.952 +- 0.034
suff++ class all_KL  =  0.987 +- 0.009
suff++_acc_int  =  0.671 +- 0.071
nec class all_L1  =  0.027 +- 0.018
nec class all_KL  =  0.004 +- 0.002
nec_acc_int  =  0.708 +- 0.113

Eval split val
suff++ class all_L1  =  0.948 +- 0.033
suff++ class all_KL  =  0.986 +- 0.009
suff++_acc_int  =  0.512 +- 0.065
nec class all_L1  =  0.031 +- 0.017
nec class all_KL  =  0.004 +- 0.002
nec_acc_int  =  0.528 +- 0.094

Eval split id_test
suff++ class all_L1  =  0.952 +- 0.034
suff++ class all_KL  =  0.987 +- 0.009
suff++_acc_int  =  0.647 +- 0.069
nec class all_L1  =  0.027 +- 0.018
nec class all_KL  =  0.004 +- 0.002
nec_acc_int  =  0.670 +- 0.115

Eval split test
suff++ class all_L1  =  0.942 +- 0.036
suff++ class all_KL  =  0.983 +- 0.011
suff++_acc_int  =  0.592 +- 0.044
nec class all_L1  =  0.034 +- 0.018
nec class all_KL  =  0.005 +- 0.003
nec_acc_int  =  0.621 +- 0.061


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.490 +- 0.008
Faith. Armon (L1)= 		  =  0.053 +- 0.034
Faith. GMean (L1)= 	  =  0.149 +- 0.057
Faith. Aritm (KL)= 		  =  0.495 +- 0.004
Faith. Armon (KL)= 		  =  0.008 +- 0.004
Faith. GMean (KL)= 	  =  0.058 +- 0.018

Eval split val
Faith. Aritm (L1)= 		  =  0.489 +- 0.008
Faith. Armon (L1)= 		  =  0.059 +- 0.033
Faith. GMean (L1)= 	  =  0.160 +- 0.057
Faith. Aritm (KL)= 		  =  0.495 +- 0.003
Faith. Armon (KL)= 		  =  0.008 +- 0.004
Faith. GMean (KL)= 	  =  0.062 +- 0.018

Eval split id_test
Faith. Aritm (L1)= 		  =  0.490 +- 0.008
Faith. Armon (L1)= 		  =  0.053 +- 0.033
Faith. GMean (L1)= 	  =  0.149 +- 0.056
Faith. Aritm (KL)= 		  =  0.495 +- 0.004
Faith. Armon (KL)= 		  =  0.007 +- 0.004
Faith. GMean (KL)= 	  =  0.057 +- 0.017

Eval split test
Faith. Aritm (L1)= 		  =  0.488 +- 0.009
Faith. Armon (L1)= 		  =  0.065 +- 0.034
Faith. GMean (L1)= 	  =  0.168 +- 0.057
Faith. Aritm (KL)= 		  =  0.494 +- 0.004
Faith. Armon (KL)= 		  =  0.010 +- 0.006
Faith. GMean (KL)= 	  =  0.068 +- 0.022
Computed for split load_split = id



Completed in  0:27:27.542253  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 13:01:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 01:01:01 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 174...
[0m[1;37mINFO[0m: [1mCheckpoint 174: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8570
ID Validation Loss: 1.0776
ID Test ACCURACY: 0.8538
ID Test Loss: 1.1832
OOD Validation ACCURACY: 0.8127
OOD Validation Loss: 1.9445
OOD Test ACCURACY: 0.7096
OOD Test Loss: 3.2829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.9462
Train Loss: 0.0859
ID Validation ACCURACY: 0.8468
ID Validation Loss: 0.5292
ID Test ACCURACY: 0.8487
ID Test Loss: 0.5857
OOD Validation ACCURACY: 0.8539
OOD Validation Loss: 0.6808
OOD Test ACCURACY: 0.8169
OOD Test Loss: 0.7504

[0m[1;37mINFO[0m: [1mChartInfo 0.8538 0.7096 0.8487 0.8169 0.8468 0.8539[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 05/10/2024 01:01:03 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(5301)
Data example from id_test: Data(x=[5, 768], edge_index=[2, 8], y=[1, 1], idx=[1], sentence_tokens=[5], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_test: (tensor([0., 1.]), tensor([2181, 3120]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.846
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.84
SUFF++ for r=0.8 class 0.0 = 0.829 +- 0.327 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 class 1.0 = 0.945 +- 0.327 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all KL = 0.799 +- 0.327 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all L1 = 0.897 +- 0.185 (in-sample avg dev_std = 0.371)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.827
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.793
SUFF++ for r=0.8 class 0.0 = 0.792 +- 0.300 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 class 1.0 = 0.96 +- 0.300 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 all KL = 0.809 +- 0.300 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 all L1 = 0.879 +- 0.202 (in-sample avg dev_std = 0.333)



--------------------------------------------------


#D#Computing SUFF++ over id_test across ratios (random_expl=False)


ratio=0.8


[1;31mERROR[0m: 05/10/2024 01:01:42 PM - utils.py - line 87 : [1mTraceback (most recent call last):
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/bin/goodtg", line 33, in <module>
    sys.exit(load_entry_point('graph-ood', 'console_scripts', 'goodtg')())
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 636, in goodtg
    main()
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 536, in main
    evaluate_metric(args)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 331, in evaluate_metric
    score, acc_int, results = pipeline.compute_metric_ratio(
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/pipelines/basic_pipeline.py", line 879, in compute_metric_ratio
    causal_subgraphs_r[ratio][j],
IndexError: list index out of range
[0mTime to compute metrics!

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 14:49:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:49:47 PM : [1mConfig model and output the best checkpoint info...
[0m[1;31mERROR[0m: 05/10/2024 02:49:47 PM - utils.py - line 52 : [1mCUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[0m
  File "/mnt/cimec-storage6/users/steve.azzolin/venv/leci/bin/goodtg", line 33, in <module>
    sys.exit(load_entry_point('graph-ood', 'console_scripts', 'goodtg')())
  File "/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/kernel/main.py", line 639, in goodtg
    print(f'#E#{e}')
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Time to compute metrics!

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 14:56:26 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:56:28 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 174...
[0m[1;37mINFO[0m: [1mCheckpoint 174: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8570
ID Validation Loss: 1.0776
ID Test ACCURACY: 0.8538
ID Test Loss: 1.1832
OOD Validation ACCURACY: 0.8127
OOD Validation Loss: 1.9445
OOD Test ACCURACY: 0.7096
OOD Test Loss: 3.2829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.9462
Train Loss: 0.0859
ID Validation ACCURACY: 0.8468
ID Validation Loss: 0.5292
ID Test ACCURACY: 0.8487
ID Test Loss: 0.5857
OOD Validation ACCURACY: 0.8539
OOD Validation Loss: 0.6808
OOD Test ACCURACY: 0.8169
OOD Test Loss: 0.7504

[0m[1;37mINFO[0m: [1mChartInfo 0.8538 0.7096 0.8487 0.8169 0.8468 0.8539[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 05/10/2024 02:56:29 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.846
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.84
SUFF++ for r=0.8 class 0.0 = 0.829 +- 0.327 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 class 1.0 = 0.945 +- 0.327 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all KL = 0.799 +- 0.327 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.8 all L1 = 0.897 +- 0.185 (in-sample avg dev_std = 0.371)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.827
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.793
SUFF++ for r=0.8 class 0.0 = 0.792 +- 0.300 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 class 1.0 = 0.96 +- 0.300 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 all KL = 0.809 +- 0.300 (in-sample avg dev_std = 0.333)
SUFF++ for r=0.8 all L1 = 0.879 +- 0.202 (in-sample avg dev_std = 0.333)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.671
SUFF++ for r=0.8 class 0.0 = 0.782 +- 0.284 (in-sample avg dev_std = 0.298)
SUFF++ for r=0.8 class 1.0 = 0.972 +- 0.284 (in-sample avg dev_std = 0.298)
SUFF++ for r=0.8 all KL = 0.836 +- 0.284 (in-sample avg dev_std = 0.298)
SUFF++ for r=0.8 all L1 = 0.88 +- 0.215 (in-sample avg dev_std = 0.298)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.846
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.848
NEC for r=0.8 class 0.0 = 0.105 +- 0.246 (in-sample avg dev_std = 0.251)
NEC for r=0.8 class 1.0 = 0.046 +- 0.246 (in-sample avg dev_std = 0.251)
NEC for r=0.8 all KL = 0.098 +- 0.246 (in-sample avg dev_std = 0.251)
NEC for r=0.8 all L1 = 0.071 +- 0.177 (in-sample avg dev_std = 0.251)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.827
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.808
NEC for r=0.8 class 0.0 = 0.117 +- 0.207 (in-sample avg dev_std = 0.209)
NEC for r=0.8 class 1.0 = 0.031 +- 0.207 (in-sample avg dev_std = 0.209)
NEC for r=0.8 all KL = 0.084 +- 0.207 (in-sample avg dev_std = 0.209)
NEC for r=0.8 all L1 = 0.072 +- 0.168 (in-sample avg dev_std = 0.209)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.705
NEC for r=0.8 class 0.0 = 0.173 +- 0.240 (in-sample avg dev_std = 0.246)
NEC for r=0.8 class 1.0 = 0.023 +- 0.240 (in-sample avg dev_std = 0.246)
NEC for r=0.8 all KL = 0.109 +- 0.240 (in-sample avg dev_std = 0.246)
NEC for r=0.8 all L1 = 0.095 +- 0.200 (in-sample avg dev_std = 0.246)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 14:57:54 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 193...
[0m[1;37mINFO[0m: [1mCheckpoint 193: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8621
ID Validation Loss: 1.0883
ID Test ACCURACY: 0.8557
ID Test Loss: 1.1549
OOD Validation ACCURACY: 0.8230
OOD Validation Loss: 1.8365
OOD Test ACCURACY: 0.7082
OOD Test Loss: 3.4909

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 125...
[0m[1;37mINFO[0m: [1mCheckpoint 125: 
-----------------------------------
Train ACCURACY: 0.9435
Train Loss: 0.0905
ID Validation ACCURACY: 0.8459
ID Validation Loss: 0.6688
ID Test ACCURACY: 0.8398
ID Test Loss: 0.7578
OOD Validation ACCURACY: 0.8575
OOD Validation Loss: 0.7711
OOD Test ACCURACY: 0.8105
OOD Test Loss: 0.8099

[0m[1;37mINFO[0m: [1mChartInfo 0.8557 0.7082 0.8398 0.8105 0.8459 0.8575[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 05/10/2024 02:57:55 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.852
SUFF++ for r=0.8 class 0.0 = 0.837 +- 0.312 (in-sample avg dev_std = 0.340)
SUFF++ for r=0.8 class 1.0 = 0.953 +- 0.312 (in-sample avg dev_std = 0.340)
SUFF++ for r=0.8 all KL = 0.83 +- 0.312 (in-sample avg dev_std = 0.340)
SUFF++ for r=0.8 all L1 = 0.905 +- 0.191 (in-sample avg dev_std = 0.340)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.82
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.791
SUFF++ for r=0.8 class 0.0 = 0.798 +- 0.291 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 class 1.0 = 0.963 +- 0.291 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 all KL = 0.821 +- 0.291 (in-sample avg dev_std = 0.323)
SUFF++ for r=0.8 all L1 = 0.884 +- 0.200 (in-sample avg dev_std = 0.323)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.672
SUFF++ for r=0.8 class 0.0 = 0.793 +- 0.269 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 class 1.0 = 0.974 +- 0.269 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 all KL = 0.851 +- 0.269 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 all L1 = 0.887 +- 0.213 (in-sample avg dev_std = 0.283)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.854
NEC for r=0.8 class 0.0 = 0.104 +- 0.210 (in-sample avg dev_std = 0.184)
NEC for r=0.8 class 1.0 = 0.024 +- 0.210 (in-sample avg dev_std = 0.184)
NEC for r=0.8 all KL = 0.077 +- 0.210 (in-sample avg dev_std = 0.184)
NEC for r=0.8 all L1 = 0.058 +- 0.160 (in-sample avg dev_std = 0.184)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.82
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.818
NEC for r=0.8 class 0.0 = 0.125 +- 0.199 (in-sample avg dev_std = 0.209)
NEC for r=0.8 class 1.0 = 0.029 +- 0.199 (in-sample avg dev_std = 0.209)
NEC for r=0.8 all KL = 0.083 +- 0.199 (in-sample avg dev_std = 0.209)
NEC for r=0.8 all L1 = 0.075 +- 0.170 (in-sample avg dev_std = 0.209)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.714
NEC for r=0.8 class 0.0 = 0.156 +- 0.216 (in-sample avg dev_std = 0.237)
NEC for r=0.8 class 1.0 = 0.023 +- 0.216 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all KL = 0.097 +- 0.216 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all L1 = 0.088 +- 0.185 (in-sample avg dev_std = 0.237)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 14:59:22 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0773
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.5674
ID Test ACCURACY: 0.8602
ID Test Loss: 0.6145
OOD Validation ACCURACY: 0.8564
OOD Validation Loss: 0.7958
OOD Test ACCURACY: 0.8034
OOD Test Loss: 0.9273

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0773
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.5674
ID Test ACCURACY: 0.8602
ID Test Loss: 0.6145
OOD Validation ACCURACY: 0.8564
OOD Validation Loss: 0.7958
OOD Test ACCURACY: 0.8034
OOD Test Loss: 0.9273

[0m[1;37mINFO[0m: [1mChartInfo 0.8602 0.8034 0.8602 0.8034 0.8581 0.8564[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 05/10/2024 02:59:24 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.853
SUFF++ for r=0.8 class 0.0 = 0.904 +- 0.196 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 class 1.0 = 0.927 +- 0.196 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 all KL = 0.889 +- 0.196 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 all L1 = 0.917 +- 0.147 (in-sample avg dev_std = 0.253)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.855
SUFF++ for r=0.8 class 0.0 = 0.903 +- 0.163 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.8 class 1.0 = 0.911 +- 0.163 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.8 all KL = 0.908 +- 0.163 (in-sample avg dev_std = 0.234)
SUFF++ for r=0.8 all L1 = 0.908 +- 0.156 (in-sample avg dev_std = 0.234)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.81
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.799
SUFF++ for r=0.8 class 0.0 = 0.877 +- 0.164 (in-sample avg dev_std = 0.224)
SUFF++ for r=0.8 class 1.0 = 0.919 +- 0.164 (in-sample avg dev_std = 0.224)
SUFF++ for r=0.8 all KL = 0.908 +- 0.164 (in-sample avg dev_std = 0.224)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.161 (in-sample avg dev_std = 0.224)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.851
NEC for r=0.8 class 0.0 = 0.077 +- 0.191 (in-sample avg dev_std = 0.161)
NEC for r=0.8 class 1.0 = 0.083 +- 0.191 (in-sample avg dev_std = 0.161)
NEC for r=0.8 all KL = 0.082 +- 0.191 (in-sample avg dev_std = 0.161)
NEC for r=0.8 all L1 = 0.081 +- 0.169 (in-sample avg dev_std = 0.161)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.86
NEC for r=0.8 class 0.0 = 0.074 +- 0.131 (in-sample avg dev_std = 0.156)
NEC for r=0.8 class 1.0 = 0.07 +- 0.131 (in-sample avg dev_std = 0.156)
NEC for r=0.8 all KL = 0.054 +- 0.131 (in-sample avg dev_std = 0.156)
NEC for r=0.8 all L1 = 0.072 +- 0.147 (in-sample avg dev_std = 0.156)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.81
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.808
NEC for r=0.8 class 0.0 = 0.098 +- 0.151 (in-sample avg dev_std = 0.181)
NEC for r=0.8 class 1.0 = 0.079 +- 0.151 (in-sample avg dev_std = 0.181)
NEC for r=0.8 all KL = 0.07 +- 0.151 (in-sample avg dev_std = 0.181)
NEC for r=0.8 all L1 = 0.088 +- 0.156 (in-sample avg dev_std = 0.181)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 15:00:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 192...
[0m[1;37mINFO[0m: [1mCheckpoint 192: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8600
ID Validation Loss: 1.1678
ID Test ACCURACY: 0.8613
ID Test Loss: 1.2094
OOD Validation ACCURACY: 0.8373
OOD Validation Loss: 1.8807
OOD Test ACCURACY: 0.7372
OOD Test Loss: 3.1834

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 145...
[0m[1;37mINFO[0m: [1mCheckpoint 145: 
-----------------------------------
Train ACCURACY: 0.9451
Train Loss: 0.0856
ID Validation ACCURACY: 0.8491
ID Validation Loss: 0.7959
ID Test ACCURACY: 0.8461
ID Test Loss: 0.9122
OOD Validation ACCURACY: 0.8587
OOD Validation Loss: 0.9326
OOD Test ACCURACY: 0.8134
OOD Test Loss: 1.1555

[0m[1;37mINFO[0m: [1mChartInfo 0.8613 0.7372 0.8461 0.8134 0.8491 0.8587[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 05/10/2024 03:00:38 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.85
SUFF++ for r=0.8 class 0.0 = 0.842 +- 0.313 (in-sample avg dev_std = 0.340)
SUFF++ for r=0.8 class 1.0 = 0.963 +- 0.313 (in-sample avg dev_std = 0.340)
SUFF++ for r=0.8 all KL = 0.819 +- 0.313 (in-sample avg dev_std = 0.340)
SUFF++ for r=0.8 all L1 = 0.912 +- 0.167 (in-sample avg dev_std = 0.340)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.836
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.812
SUFF++ for r=0.8 class 0.0 = 0.823 +- 0.286 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 class 1.0 = 0.968 +- 0.286 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 all KL = 0.836 +- 0.286 (in-sample avg dev_std = 0.312)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.189 (in-sample avg dev_std = 0.312)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.711
SUFF++ for r=0.8 class 0.0 = 0.813 +- 0.274 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.8 class 1.0 = 0.981 +- 0.274 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.8 all KL = 0.851 +- 0.274 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.8 all L1 = 0.9 +- 0.189 (in-sample avg dev_std = 0.296)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.863
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.867
NEC for r=0.8 class 0.0 = 0.099 +- 0.223 (in-sample avg dev_std = 0.212)
NEC for r=0.8 class 1.0 = 0.033 +- 0.223 (in-sample avg dev_std = 0.212)
NEC for r=0.8 all KL = 0.08 +- 0.223 (in-sample avg dev_std = 0.212)
NEC for r=0.8 all L1 = 0.06 +- 0.169 (in-sample avg dev_std = 0.212)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.836
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.837
NEC for r=0.8 class 0.0 = 0.115 +- 0.202 (in-sample avg dev_std = 0.212)
NEC for r=0.8 class 1.0 = 0.023 +- 0.202 (in-sample avg dev_std = 0.212)
NEC for r=0.8 all KL = 0.083 +- 0.202 (in-sample avg dev_std = 0.212)
NEC for r=0.8 all L1 = 0.067 +- 0.160 (in-sample avg dev_std = 0.212)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.74
NEC for r=0.8 class 0.0 = 0.143 +- 0.223 (in-sample avg dev_std = 0.222)
NEC for r=0.8 class 1.0 = 0.022 +- 0.223 (in-sample avg dev_std = 0.222)
NEC for r=0.8 all KL = 0.098 +- 0.223 (in-sample avg dev_std = 0.222)
NEC for r=0.8 all L1 = 0.081 +- 0.177 (in-sample avg dev_std = 0.222)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Fri May 10 15:01:46 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 135...
[0m[1;37mINFO[0m: [1mCheckpoint 135: 
-----------------------------------
Train ACCURACY: 0.9480
Train Loss: 0.0788
ID Validation ACCURACY: 0.8570
ID Validation Loss: 0.7811
ID Test ACCURACY: 0.8510
ID Test Loss: 0.7955
OOD Validation ACCURACY: 0.8215
OOD Validation Loss: 1.3083
OOD Test ACCURACY: 0.7393
OOD Test Loss: 2.3604

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ACCURACY: 0.8942
Train Loss: 0.1948
ID Validation ACCURACY: 0.8127
ID Validation Loss: 0.4206
ID Test ACCURACY: 0.8148
ID Test Loss: 0.4362
OOD Validation ACCURACY: 0.8560
OOD Validation Loss: 0.4250
OOD Test ACCURACY: 0.8085
OOD Test Loss: 0.5168

[0m[1;37mINFO[0m: [1mChartInfo 0.8510 0.7393 0.8148 0.8085 0.8127 0.8560[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 05/10/2024 03:01:47 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.877
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.841
SUFF++ for r=0.8 class 0.0 = 0.8 +- 0.263 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 class 1.0 = 0.962 +- 0.263 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 all KL = 0.842 +- 0.263 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.8 all L1 = 0.895 +- 0.179 (in-sample avg dev_std = 0.326)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.829
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.801
SUFF++ for r=0.8 class 0.0 = 0.81 +- 0.219 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.8 class 1.0 = 0.957 +- 0.219 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.8 all KL = 0.863 +- 0.219 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.8 all L1 = 0.887 +- 0.181 (in-sample avg dev_std = 0.272)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.748
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.711
SUFF++ for r=0.8 class 0.0 = 0.798 +- 0.213 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 class 1.0 = 0.974 +- 0.213 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 all KL = 0.879 +- 0.213 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 all L1 = 0.889 +- 0.187 (in-sample avg dev_std = 0.253)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.877
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.867
NEC for r=0.8 class 0.0 = 0.083 +- 0.145 (in-sample avg dev_std = 0.151)
NEC for r=0.8 class 1.0 = 0.031 +- 0.145 (in-sample avg dev_std = 0.151)
NEC for r=0.8 all KL = 0.052 +- 0.145 (in-sample avg dev_std = 0.151)
NEC for r=0.8 all L1 = 0.053 +- 0.125 (in-sample avg dev_std = 0.151)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.829
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.835
NEC for r=0.8 class 0.0 = 0.115 +- 0.156 (in-sample avg dev_std = 0.167)
NEC for r=0.8 class 1.0 = 0.032 +- 0.156 (in-sample avg dev_std = 0.167)
NEC for r=0.8 all KL = 0.063 +- 0.156 (in-sample avg dev_std = 0.167)
NEC for r=0.8 all L1 = 0.072 +- 0.150 (in-sample avg dev_std = 0.167)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.748
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.747
NEC for r=0.8 class 0.0 = 0.162 +- 0.177 (in-sample avg dev_std = 0.191)
NEC for r=0.8 class 1.0 = 0.023 +- 0.177 (in-sample avg dev_std = 0.191)
NEC for r=0.8 all KL = 0.082 +- 0.177 (in-sample avg dev_std = 0.191)
NEC for r=0.8 all L1 = 0.09 +- 0.171 (in-sample avg dev_std = 0.191)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.799], 'all_L1': [0.897]}), defaultdict(<class 'list'>, {'all_KL': [0.83], 'all_L1': [0.905]}), defaultdict(<class 'list'>, {'all_KL': [0.889], 'all_L1': [0.917]}), defaultdict(<class 'list'>, {'all_KL': [0.819], 'all_L1': [0.912]}), defaultdict(<class 'list'>, {'all_KL': [0.842], 'all_L1': [0.895]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.071]}), defaultdict(<class 'list'>, {'all_KL': [0.077], 'all_L1': [0.058]}), defaultdict(<class 'list'>, {'all_KL': [0.082], 'all_L1': [0.081]}), defaultdict(<class 'list'>, {'all_KL': [0.08], 'all_L1': [0.06]}), defaultdict(<class 'list'>, {'all_KL': [0.052], 'all_L1': [0.053]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.809], 'all_L1': [0.879]}), defaultdict(<class 'list'>, {'all_KL': [0.821], 'all_L1': [0.884]}), defaultdict(<class 'list'>, {'all_KL': [0.908], 'all_L1': [0.908]}), defaultdict(<class 'list'>, {'all_KL': [0.836], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.863], 'all_L1': [0.887]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.084], 'all_L1': [0.072]}), defaultdict(<class 'list'>, {'all_KL': [0.083], 'all_L1': [0.075]}), defaultdict(<class 'list'>, {'all_KL': [0.054], 'all_L1': [0.072]}), defaultdict(<class 'list'>, {'all_KL': [0.083], 'all_L1': [0.067]}), defaultdict(<class 'list'>, {'all_KL': [0.063], 'all_L1': [0.072]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.836], 'all_L1': [0.88]}), defaultdict(<class 'list'>, {'all_KL': [0.851], 'all_L1': [0.887]}), defaultdict(<class 'list'>, {'all_KL': [0.908], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.851], 'all_L1': [0.9]}), defaultdict(<class 'list'>, {'all_KL': [0.879], 'all_L1': [0.889]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.109], 'all_L1': [0.095]}), defaultdict(<class 'list'>, {'all_KL': [0.097], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.07], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.081]}), defaultdict(<class 'list'>, {'all_KL': [0.082], 'all_L1': [0.09]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.905 +- 0.008
suff++ class all_KL  =  0.836 +- 0.030
suff++_acc_int  =  0.847 +- 0.005
nec class all_L1  =  0.065 +- 0.010
nec class all_KL  =  0.078 +- 0.015
nec_acc_int  =  0.857 +- 0.008

Eval split val
suff++ class all_L1  =  0.891 +- 0.011
suff++ class all_KL  =  0.847 +- 0.035
suff++_acc_int  =  0.810 +- 0.024
nec class all_L1  =  0.072 +- 0.003
nec class all_KL  =  0.073 +- 0.012
nec_acc_int  =  0.832 +- 0.018

Eval split test
suff++ class all_L1  =  0.891 +- 0.008
suff++ class all_KL  =  0.865 +- 0.026
suff++_acc_int  =  0.713 +- 0.047
nec class all_L1  =  0.088 +- 0.004
nec class all_KL  =  0.091 +- 0.014
nec_acc_int  =  0.743 +- 0.036


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.485 +- 0.008
Faith. Armon (L1)= 		  =  0.120 +- 0.018
Faith. GMean (L1)= 	  =  0.241 +- 0.019
Faith. Aritm (KL)= 		  =  0.457 +- 0.015
Faith. Armon (KL)= 		  =  0.142 +- 0.025
Faith. GMean (KL)= 	  =  0.254 +- 0.024

Eval split val
Faith. Aritm (L1)= 		  =  0.481 +- 0.005
Faith. Armon (L1)= 		  =  0.133 +- 0.004
Faith. GMean (L1)= 	  =  0.253 +- 0.004
Faith. Aritm (KL)= 		  =  0.460 +- 0.012
Faith. Armon (KL)= 		  =  0.135 +- 0.021
Faith. GMean (KL)= 	  =  0.248 +- 0.017

Eval split test
Faith. Aritm (L1)= 		  =  0.490 +- 0.002
Faith. Armon (L1)= 		  =  0.161 +- 0.007
Faith. GMean (L1)= 	  =  0.281 +- 0.006
Faith. Aritm (KL)= 		  =  0.478 +- 0.006
Faith. Armon (KL)= 		  =  0.165 +- 0.022
Faith. GMean (KL)= 	  =  0.280 +- 0.017
Computed for split load_split = id



Completed in  0:06:32.431326  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length readout
DONE all :)
