nohup: ignoring input
Time to compute metrics!

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:33:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:31 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:44 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:46 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:48 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:50 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:33:52 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 0.8933
Train Loss: 0.4689
ID Validation ACCURACY: 0.8867
ID Validation Loss: 0.4937
ID Test ACCURACY: 0.9010
ID Test Loss: 0.4439
OOD Validation ACCURACY: 0.6550
OOD Validation Loss: 1.5131
OOD Test ACCURACY: 0.3787
OOD Test Loss: 2.5757

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ACCURACY: 0.5774
Train Loss: 1.3209
ID Validation ACCURACY: 0.5740
ID Validation Loss: 1.3483
ID Test ACCURACY: 0.5860
ID Test Loss: 1.2983
OOD Validation ACCURACY: 0.9227
OOD Validation Loss: 0.4906
OOD Test ACCURACY: 0.3487
OOD Test Loss: 9.0129

[0m[1;37mINFO[0m: [1mChartInfo 0.9010 0.3787 0.5860 0.3487 0.5740 0.9227[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
Label distribution from train: (tensor([0, 1, 2]), tensor([5874, 6198, 5928]))

Gold ratio (train) =  tensor(0.3042) +- tensor(0.1620)
F1 for r=0.6 = 0.554
WIoU for r=0.6 = 0.669
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.552
WIoU for r=0.6 = 0.675
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.795
WIoU for r=0.6 = 0.996
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.447
WIoU for r=0.6 = 0.352


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.911
Model XAI F1 of binarized graphs for r=0.6 =  0.55371
Model XAI WIoU of binarized graphs for r=0.6 =  0.6687150000000002
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.612
SUFF++ for r=0.6 class 0 = 0.39 +- 0.273 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 1 = 0.552 +- 0.273 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 2 = 0.61 +- 0.273 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all KL = 0.473 +- 0.273 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all L1 = 0.519 +- 0.204 (in-sample avg dev_std = 0.554)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.887
Model XAI F1 of binarized graphs for r=0.6 =  0.55154375
Model XAI WIoU of binarized graphs for r=0.6 =  0.67477125
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.617
SUFF++ for r=0.6 class 0 = 0.41 +- 0.273 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 1 = 0.549 +- 0.273 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 2 = 0.604 +- 0.273 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 all KL = 0.475 +- 0.273 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 all L1 = 0.524 +- 0.203 (in-sample avg dev_std = 0.559)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.654
Model XAI F1 of binarized graphs for r=0.6 =  0.7952750000000001
Model XAI WIoU of binarized graphs for r=0.6 =  0.9961325
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.674
SUFF++ for r=0.6 class 0 = 0.792 +- 0.336 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 1 = 0.548 +- 0.336 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 2 = 0.696 +- 0.336 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 all KL = 0.507 +- 0.336 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 all L1 = 0.68 +- 0.202 (in-sample avg dev_std = 0.595)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.377
Model XAI F1 of binarized graphs for r=0.6 =  0.44746250000000004
Model XAI WIoU of binarized graphs for r=0.6 =  0.3520587500000001
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.4
SUFF++ for r=0.6 class 0 = 0.759 +- 0.259 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.6 class 1 = 0.853 +- 0.259 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.6 class 2 = 0.662 +- 0.259 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.6 all KL = 0.677 +- 0.259 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.6 all L1 = 0.756 +- 0.193 (in-sample avg dev_std = 0.453)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.911
Model XAI F1 of binarized graphs for r=0.6 =  0.55371
Model XAI WIoU of binarized graphs for r=0.6 =  0.6687150000000002
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.459
NEC for r=0.6 class 0 = 0.594 +- 0.261 (in-sample avg dev_std = 0.619)
NEC for r=0.6 class 1 = 0.554 +- 0.261 (in-sample avg dev_std = 0.619)
NEC for r=0.6 class 2 = 0.641 +- 0.261 (in-sample avg dev_std = 0.619)
NEC for r=0.6 all KL = 0.659 +- 0.261 (in-sample avg dev_std = 0.619)
NEC for r=0.6 all L1 = 0.596 +- 0.159 (in-sample avg dev_std = 0.619)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.887
Model XAI F1 of binarized graphs for r=0.6 =  0.55154375
Model XAI WIoU of binarized graphs for r=0.6 =  0.67477125
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.449
NEC for r=0.6 class 0 = 0.599 +- 0.264 (in-sample avg dev_std = 0.622)
NEC for r=0.6 class 1 = 0.538 +- 0.264 (in-sample avg dev_std = 0.622)
NEC for r=0.6 class 2 = 0.653 +- 0.264 (in-sample avg dev_std = 0.622)
NEC for r=0.6 all KL = 0.661 +- 0.264 (in-sample avg dev_std = 0.622)
NEC for r=0.6 all L1 = 0.597 +- 0.162 (in-sample avg dev_std = 0.622)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.654
Model XAI F1 of binarized graphs for r=0.6 =  0.7952750000000001
Model XAI WIoU of binarized graphs for r=0.6 =  0.9961325
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.409
NEC for r=0.6 class 0 = 0.375 +- 0.326 (in-sample avg dev_std = 0.562)
NEC for r=0.6 class 1 = 0.269 +- 0.326 (in-sample avg dev_std = 0.562)
NEC for r=0.6 class 2 = 0.541 +- 0.326 (in-sample avg dev_std = 0.562)
NEC for r=0.6 all KL = 0.464 +- 0.326 (in-sample avg dev_std = 0.562)
NEC for r=0.6 all L1 = 0.396 +- 0.219 (in-sample avg dev_std = 0.562)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.377
Model XAI F1 of binarized graphs for r=0.6 =  0.44746250000000004
Model XAI WIoU of binarized graphs for r=0.6 =  0.3520587500000001
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.421
NEC for r=0.6 class 0 = 0.304 +- 0.282 (in-sample avg dev_std = 0.431)
NEC for r=0.6 class 1 = 0.148 +- 0.282 (in-sample avg dev_std = 0.431)
NEC for r=0.6 class 2 = 0.364 +- 0.282 (in-sample avg dev_std = 0.431)
NEC for r=0.6 all KL = 0.322 +- 0.282 (in-sample avg dev_std = 0.431)
NEC for r=0.6 all L1 = 0.274 +- 0.219 (in-sample avg dev_std = 0.431)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:35:15 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:15 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:28 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:30 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:32 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:33 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:35:35 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 86...
[0m[1;37mINFO[0m: [1mCheckpoint 86: 
-----------------------------------
Train ACCURACY: 0.8799
Train Loss: 0.4851
ID Validation ACCURACY: 0.8757
ID Validation Loss: 0.5174
ID Test ACCURACY: 0.8873
ID Test Loss: 0.4680
OOD Validation ACCURACY: 0.6670
OOD Validation Loss: 0.9447
OOD Test ACCURACY: 0.4367
OOD Test Loss: 3.0736

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 58...
[0m[1;37mINFO[0m: [1mCheckpoint 58: 
-----------------------------------
Train ACCURACY: 0.8507
Train Loss: 0.4884
ID Validation ACCURACY: 0.8450
ID Validation Loss: 0.5152
ID Test ACCURACY: 0.8610
ID Test Loss: 0.4707
OOD Validation ACCURACY: 0.9313
OOD Validation Loss: 0.4447
OOD Test ACCURACY: 0.3857
OOD Test Loss: 4.2847

[0m[1;37mINFO[0m: [1mChartInfo 0.8873 0.4367 0.8610 0.3857 0.8450 0.9313[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
Label distribution from train: (tensor([0, 1, 2]), tensor([5874, 6198, 5928]))

Gold ratio (train) =  tensor(0.3042) +- tensor(0.1620)
F1 for r=0.6 = 0.583
WIoU for r=0.6 = 0.676
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.583
WIoU for r=0.6 = 0.685
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 1.000
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.503
WIoU for r=0.6 = 0.392


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.895
Model XAI F1 of binarized graphs for r=0.6 =  0.5825875
Model XAI WIoU of binarized graphs for r=0.6 =  0.6759175000000001
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.643
SUFF++ for r=0.6 class 0 = 0.46 +- 0.302 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 1 = 0.633 +- 0.302 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 class 2 = 0.579 +- 0.302 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 all KL = 0.464 +- 0.302 (in-sample avg dev_std = 0.575)
SUFF++ for r=0.6 all L1 = 0.559 +- 0.217 (in-sample avg dev_std = 0.575)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.877
Model XAI F1 of binarized graphs for r=0.6 =  0.5827899999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.68527375
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.647
SUFF++ for r=0.6 class 0 = 0.477 +- 0.294 (in-sample avg dev_std = 0.572)
SUFF++ for r=0.6 class 1 = 0.637 +- 0.294 (in-sample avg dev_std = 0.572)
SUFF++ for r=0.6 class 2 = 0.585 +- 0.294 (in-sample avg dev_std = 0.572)
SUFF++ for r=0.6 all KL = 0.47 +- 0.294 (in-sample avg dev_std = 0.572)
SUFF++ for r=0.6 all L1 = 0.568 +- 0.207 (in-sample avg dev_std = 0.572)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.665
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  1.0
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.682
SUFF++ for r=0.6 class 0 = 0.729 +- 0.374 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 class 1 = 0.579 +- 0.374 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 class 2 = 0.56 +- 0.374 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 all KL = 0.496 +- 0.374 (in-sample avg dev_std = 0.642)
SUFF++ for r=0.6 all L1 = 0.623 +- 0.229 (in-sample avg dev_std = 0.642)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.449
Model XAI F1 of binarized graphs for r=0.6 =  0.5030225
Model XAI WIoU of binarized graphs for r=0.6 =  0.39182
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.405
SUFF++ for r=0.6 class 0 = 0.768 +- 0.281 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 1 = 0.903 +- 0.281 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 2 = 0.762 +- 0.281 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all KL = 0.758 +- 0.281 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all L1 = 0.809 +- 0.225 (in-sample avg dev_std = 0.371)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.895
Model XAI F1 of binarized graphs for r=0.6 =  0.5825875
Model XAI WIoU of binarized graphs for r=0.6 =  0.6759175000000001
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.471
NEC for r=0.6 class 0 = 0.578 +- 0.264 (in-sample avg dev_std = 0.660)
NEC for r=0.6 class 1 = 0.546 +- 0.264 (in-sample avg dev_std = 0.660)
NEC for r=0.6 class 2 = 0.653 +- 0.264 (in-sample avg dev_std = 0.660)
NEC for r=0.6 all KL = 0.704 +- 0.264 (in-sample avg dev_std = 0.660)
NEC for r=0.6 all L1 = 0.592 +- 0.171 (in-sample avg dev_std = 0.660)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.877
Model XAI F1 of binarized graphs for r=0.6 =  0.5827899999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.68527375
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.46
NEC for r=0.6 class 0 = 0.581 +- 0.267 (in-sample avg dev_std = 0.661)
NEC for r=0.6 class 1 = 0.548 +- 0.267 (in-sample avg dev_std = 0.661)
NEC for r=0.6 class 2 = 0.653 +- 0.267 (in-sample avg dev_std = 0.661)
NEC for r=0.6 all KL = 0.707 +- 0.267 (in-sample avg dev_std = 0.661)
NEC for r=0.6 all L1 = 0.595 +- 0.171 (in-sample avg dev_std = 0.661)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.665
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  1.0
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.494
NEC for r=0.6 class 0 = 0.345 +- 0.323 (in-sample avg dev_std = 0.542)
NEC for r=0.6 class 1 = 0.382 +- 0.323 (in-sample avg dev_std = 0.542)
NEC for r=0.6 class 2 = 0.512 +- 0.323 (in-sample avg dev_std = 0.542)
NEC for r=0.6 all KL = 0.487 +- 0.323 (in-sample avg dev_std = 0.542)
NEC for r=0.6 all L1 = 0.413 +- 0.229 (in-sample avg dev_std = 0.542)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.449
Model XAI F1 of binarized graphs for r=0.6 =  0.5030225
Model XAI WIoU of binarized graphs for r=0.6 =  0.39182
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.447
NEC for r=0.6 class 0 = 0.365 +- 0.336 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 1 = 0.09 +- 0.336 (in-sample avg dev_std = 0.463)
NEC for r=0.6 class 2 = 0.422 +- 0.336 (in-sample avg dev_std = 0.463)
NEC for r=0.6 all KL = 0.378 +- 0.336 (in-sample avg dev_std = 0.463)
NEC for r=0.6 all L1 = 0.296 +- 0.270 (in-sample avg dev_std = 0.463)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:36:54 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:36:54 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:07 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:09 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:11 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:13 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:37:15 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 90...
[0m[1;37mINFO[0m: [1mCheckpoint 90: 
-----------------------------------
Train ACCURACY: 0.9108
Train Loss: 0.4126
ID Validation ACCURACY: 0.9080
ID Validation Loss: 0.4411
ID Test ACCURACY: 0.9153
ID Test Loss: 0.4096
OOD Validation ACCURACY: 0.9047
OOD Validation Loss: 0.4491
OOD Test ACCURACY: 0.4210
OOD Test Loss: 3.6186

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 42...
[0m[1;37mINFO[0m: [1mCheckpoint 42: 
-----------------------------------
Train ACCURACY: 0.8849
Train Loss: 0.5208
ID Validation ACCURACY: 0.8813
ID Validation Loss: 0.5566
ID Test ACCURACY: 0.8920
ID Test Loss: 0.5012
OOD Validation ACCURACY: 0.9313
OOD Validation Loss: 0.4367
OOD Test ACCURACY: 0.3817
OOD Test Loss: 5.1552

[0m[1;37mINFO[0m: [1mChartInfo 0.9153 0.4210 0.8920 0.3817 0.8813 0.9313[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
Label distribution from train: (tensor([0, 1, 2]), tensor([5874, 6198, 5928]))

Gold ratio (train) =  tensor(0.3042) +- tensor(0.1620)
F1 for r=0.6 = 0.568
WIoU for r=0.6 = 0.646
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.564
WIoU for r=0.6 = 0.647
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.795
WIoU for r=0.6 = 0.992
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.457
WIoU for r=0.6 = 0.354


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.934
Model XAI F1 of binarized graphs for r=0.6 =  0.56769
Model XAI WIoU of binarized graphs for r=0.6 =  0.6459024999999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.654
SUFF++ for r=0.6 class 0 = 0.426 +- 0.296 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 1 = 0.597 +- 0.296 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 2 = 0.675 +- 0.296 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 all KL = 0.471 +- 0.296 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 all L1 = 0.567 +- 0.231 (in-sample avg dev_std = 0.577)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.906
Model XAI F1 of binarized graphs for r=0.6 =  0.56384375
Model XAI WIoU of binarized graphs for r=0.6 =  0.64702125
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.648
SUFF++ for r=0.6 class 0 = 0.414 +- 0.289 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.6 class 1 = 0.581 +- 0.289 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.6 class 2 = 0.675 +- 0.289 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.6 all KL = 0.46 +- 0.289 (in-sample avg dev_std = 0.601)
SUFF++ for r=0.6 all L1 = 0.56 +- 0.224 (in-sample avg dev_std = 0.601)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.902
Model XAI F1 of binarized graphs for r=0.6 =  0.79503625
Model XAI WIoU of binarized graphs for r=0.6 =  0.99223875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.781
SUFF++ for r=0.6 class 0 = 0.706 +- 0.362 (in-sample avg dev_std = 0.560)
SUFF++ for r=0.6 class 1 = 0.772 +- 0.362 (in-sample avg dev_std = 0.560)
SUFF++ for r=0.6 class 2 = 0.647 +- 0.362 (in-sample avg dev_std = 0.560)
SUFF++ for r=0.6 all KL = 0.647 +- 0.362 (in-sample avg dev_std = 0.560)
SUFF++ for r=0.6 all L1 = 0.708 +- 0.207 (in-sample avg dev_std = 0.560)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.429
Model XAI F1 of binarized graphs for r=0.6 =  0.45746
Model XAI WIoU of binarized graphs for r=0.6 =  0.3537675
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.371
SUFF++ for r=0.6 class 0 = 0.801 +- 0.298 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 class 1 = 0.916 +- 0.298 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 class 2 = 0.815 +- 0.298 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 all KL = 0.789 +- 0.298 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 all L1 = 0.843 +- 0.221 (in-sample avg dev_std = 0.344)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.934
Model XAI F1 of binarized graphs for r=0.6 =  0.56769
Model XAI WIoU of binarized graphs for r=0.6 =  0.6459024999999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.46
NEC for r=0.6 class 0 = 0.621 +- 0.263 (in-sample avg dev_std = 0.661)
NEC for r=0.6 class 1 = 0.538 +- 0.263 (in-sample avg dev_std = 0.661)
NEC for r=0.6 class 2 = 0.639 +- 0.263 (in-sample avg dev_std = 0.661)
NEC for r=0.6 all KL = 0.704 +- 0.263 (in-sample avg dev_std = 0.661)
NEC for r=0.6 all L1 = 0.598 +- 0.162 (in-sample avg dev_std = 0.661)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.906
Model XAI F1 of binarized graphs for r=0.6 =  0.56384375
Model XAI WIoU of binarized graphs for r=0.6 =  0.64702125
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.455
NEC for r=0.6 class 0 = 0.622 +- 0.256 (in-sample avg dev_std = 0.648)
NEC for r=0.6 class 1 = 0.538 +- 0.256 (in-sample avg dev_std = 0.648)
NEC for r=0.6 class 2 = 0.642 +- 0.256 (in-sample avg dev_std = 0.648)
NEC for r=0.6 all KL = 0.704 +- 0.256 (in-sample avg dev_std = 0.648)
NEC for r=0.6 all L1 = 0.601 +- 0.163 (in-sample avg dev_std = 0.648)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.902
Model XAI F1 of binarized graphs for r=0.6 =  0.79503625
Model XAI WIoU of binarized graphs for r=0.6 =  0.99223875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.534
NEC for r=0.6 class 0 = 0.505 +- 0.289 (in-sample avg dev_std = 0.654)
NEC for r=0.6 class 1 = 0.415 +- 0.289 (in-sample avg dev_std = 0.654)
NEC for r=0.6 class 2 = 0.579 +- 0.289 (in-sample avg dev_std = 0.654)
NEC for r=0.6 all KL = 0.578 +- 0.289 (in-sample avg dev_std = 0.654)
NEC for r=0.6 all L1 = 0.5 +- 0.177 (in-sample avg dev_std = 0.654)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.429
Model XAI F1 of binarized graphs for r=0.6 =  0.45746
Model XAI WIoU of binarized graphs for r=0.6 =  0.3537675
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.427
NEC for r=0.6 class 0 = 0.331 +- 0.329 (in-sample avg dev_std = 0.448)
NEC for r=0.6 class 1 = 0.094 +- 0.329 (in-sample avg dev_std = 0.448)
NEC for r=0.6 class 2 = 0.367 +- 0.329 (in-sample avg dev_std = 0.448)
NEC for r=0.6 all KL = 0.351 +- 0.329 (in-sample avg dev_std = 0.448)
NEC for r=0.6 all L1 = 0.267 +- 0.266 (in-sample avg dev_std = 0.448)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:38:34 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:34 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:47 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:49 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:51 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:53 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:38:55 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.8929
Train Loss: 0.4784
ID Validation ACCURACY: 0.8877
ID Validation Loss: 0.5235
ID Test ACCURACY: 0.9007
ID Test Loss: 0.4658
OOD Validation ACCURACY: 0.8657
OOD Validation Loss: 0.4939
OOD Test ACCURACY: 0.4123
OOD Test Loss: 4.0821

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 0...
[0m[1;37mINFO[0m: [1mCheckpoint 0: 
-----------------------------------
Train ACCURACY: 0.4968
Train Loss: 1.1639
ID Validation ACCURACY: 0.4947
ID Validation Loss: 1.2213
ID Test ACCURACY: 0.4953
ID Test Loss: 1.1575
OOD Validation ACCURACY: 0.9307
OOD Validation Loss: 0.4981
OOD Test ACCURACY: 0.3283
OOD Test Loss: 9.9458

[0m[1;37mINFO[0m: [1mChartInfo 0.9007 0.4123 0.4953 0.3283 0.4947 0.9307[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
Label distribution from train: (tensor([0, 1, 2]), tensor([5874, 6198, 5928]))

Gold ratio (train) =  tensor(0.3042) +- tensor(0.1620)
F1 for r=0.6 = 0.581
WIoU for r=0.6 = 0.687
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.581
WIoU for r=0.6 = 0.696
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 1.000
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.471
WIoU for r=0.6 = 0.357


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.91
Model XAI F1 of binarized graphs for r=0.6 =  0.5808337499999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.6872912499999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.672
SUFF++ for r=0.6 class 0 = 0.418 +- 0.335 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 1 = 0.711 +- 0.335 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 2 = 0.666 +- 0.335 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 all KL = 0.486 +- 0.335 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 all L1 = 0.601 +- 0.262 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.887
Model XAI F1 of binarized graphs for r=0.6 =  0.5811375
Model XAI WIoU of binarized graphs for r=0.6 =  0.69626375
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.668
SUFF++ for r=0.6 class 0 = 0.413 +- 0.337 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.6 class 1 = 0.705 +- 0.337 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.6 class 2 = 0.685 +- 0.337 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.6 all KL = 0.486 +- 0.337 (in-sample avg dev_std = 0.579)
SUFF++ for r=0.6 all L1 = 0.604 +- 0.261 (in-sample avg dev_std = 0.579)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.868
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9999875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.781
SUFF++ for r=0.6 class 0 = 0.692 +- 0.406 (in-sample avg dev_std = 0.633)
SUFF++ for r=0.6 class 1 = 0.744 +- 0.406 (in-sample avg dev_std = 0.633)
SUFF++ for r=0.6 class 2 = 0.656 +- 0.406 (in-sample avg dev_std = 0.633)
SUFF++ for r=0.6 all KL = 0.599 +- 0.406 (in-sample avg dev_std = 0.633)
SUFF++ for r=0.6 all L1 = 0.697 +- 0.252 (in-sample avg dev_std = 0.633)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.416
Model XAI F1 of binarized graphs for r=0.6 =  0.47081125
Model XAI WIoU of binarized graphs for r=0.6 =  0.35654375
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.387
SUFF++ for r=0.6 class 0 = 0.802 +- 0.289 (in-sample avg dev_std = 0.367)
SUFF++ for r=0.6 class 1 = 0.91 +- 0.289 (in-sample avg dev_std = 0.367)
SUFF++ for r=0.6 class 2 = 0.781 +- 0.289 (in-sample avg dev_std = 0.367)
SUFF++ for r=0.6 all KL = 0.774 +- 0.289 (in-sample avg dev_std = 0.367)
SUFF++ for r=0.6 all L1 = 0.83 +- 0.223 (in-sample avg dev_std = 0.367)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.91
Model XAI F1 of binarized graphs for r=0.6 =  0.5808337499999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.6872912499999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.432
NEC for r=0.6 class 0 = 0.653 +- 0.232 (in-sample avg dev_std = 0.711)
NEC for r=0.6 class 1 = 0.574 +- 0.232 (in-sample avg dev_std = 0.711)
NEC for r=0.6 class 2 = 0.669 +- 0.232 (in-sample avg dev_std = 0.711)
NEC for r=0.6 all KL = 0.802 +- 0.232 (in-sample avg dev_std = 0.711)
NEC for r=0.6 all L1 = 0.631 +- 0.160 (in-sample avg dev_std = 0.711)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.887
Model XAI F1 of binarized graphs for r=0.6 =  0.5811375
Model XAI WIoU of binarized graphs for r=0.6 =  0.69626375
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.414
NEC for r=0.6 class 0 = 0.667 +- 0.235 (in-sample avg dev_std = 0.696)
NEC for r=0.6 class 1 = 0.586 +- 0.235 (in-sample avg dev_std = 0.696)
NEC for r=0.6 class 2 = 0.673 +- 0.235 (in-sample avg dev_std = 0.696)
NEC for r=0.6 all KL = 0.807 +- 0.235 (in-sample avg dev_std = 0.696)
NEC for r=0.6 all L1 = 0.642 +- 0.162 (in-sample avg dev_std = 0.696)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.868
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9999875
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.505
NEC for r=0.6 class 0 = 0.598 +- 0.279 (in-sample avg dev_std = 0.774)
NEC for r=0.6 class 1 = 0.477 +- 0.279 (in-sample avg dev_std = 0.774)
NEC for r=0.6 class 2 = 0.561 +- 0.279 (in-sample avg dev_std = 0.774)
NEC for r=0.6 all KL = 0.704 +- 0.279 (in-sample avg dev_std = 0.774)
NEC for r=0.6 all L1 = 0.546 +- 0.181 (in-sample avg dev_std = 0.774)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.416
Model XAI F1 of binarized graphs for r=0.6 =  0.47081125
Model XAI WIoU of binarized graphs for r=0.6 =  0.35654375
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.45
NEC for r=0.6 class 0 = 0.36 +- 0.346 (in-sample avg dev_std = 0.464)
NEC for r=0.6 class 1 = 0.086 +- 0.346 (in-sample avg dev_std = 0.464)
NEC for r=0.6 class 2 = 0.397 +- 0.346 (in-sample avg dev_std = 0.464)
NEC for r=0.6 all KL = 0.392 +- 0.346 (in-sample avg dev_std = 0.464)
NEC for r=0.6 all L1 = 0.285 +- 0.272 (in-sample avg dev_std = 0.464)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:40:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:14 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:27 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:29 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:31 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:33 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:40:34 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 16...
[0m[1;37mINFO[0m: [1mCheckpoint 16: 
-----------------------------------
Train ACCURACY: 0.8887
Train Loss: 0.4924
ID Validation ACCURACY: 0.8847
ID Validation Loss: 0.5324
ID Test ACCURACY: 0.8977
ID Test Loss: 0.4691
OOD Validation ACCURACY: 0.7167
OOD Validation Loss: 1.3877
OOD Test ACCURACY: 0.3873
OOD Test Loss: 3.5672

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 45...
[0m[1;37mINFO[0m: [1mCheckpoint 45: 
-----------------------------------
Train ACCURACY: 0.8089
Train Loss: 0.6907
ID Validation ACCURACY: 0.8030
ID Validation Loss: 0.7150
ID Test ACCURACY: 0.8183
ID Test Loss: 0.6665
OOD Validation ACCURACY: 0.9317
OOD Validation Loss: 0.4273
OOD Test ACCURACY: 0.4333
OOD Test Loss: 4.3285

[0m[1;37mINFO[0m: [1mChartInfo 0.8977 0.3873 0.8183 0.4333 0.8030 0.9317[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
Label distribution from train: (tensor([0, 1, 2]), tensor([5874, 6198, 5928]))

Gold ratio (train) =  tensor(0.3042) +- tensor(0.1620)
F1 for r=0.6 = 0.585
WIoU for r=0.6 = 0.657
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([ 959, 1004, 1037]))

Gold ratio (id_val) =  tensor(0.3071) +- tensor(0.1673)
F1 for r=0.6 = 0.584
WIoU for r=0.6 = 0.660
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 34], x=[17, 1], node_gt=[17], edge_gt=[34], y=[1], env_id=[1], ori_edge_index=[2, 34], node_perm=[17], num_nodes=17)
Label distribution from val: (tensor([0, 1, 2]), tensor([1007,  991, 1002]))

Gold ratio (val) =  tensor(0.4137) +- tensor(0.0836)
F1 for r=0.6 = 0.798
WIoU for r=0.6 = 1.000
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.505
WIoU for r=0.6 = 0.395


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.9
Model XAI F1 of binarized graphs for r=0.6 =  0.5849449999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.6572274999999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.656
SUFF++ for r=0.6 class 0 = 0.4 +- 0.330 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 1 = 0.759 +- 0.330 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 class 2 = 0.66 +- 0.330 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 all KL = 0.53 +- 0.330 (in-sample avg dev_std = 0.531)
SUFF++ for r=0.6 all L1 = 0.609 +- 0.261 (in-sample avg dev_std = 0.531)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.887
Model XAI F1 of binarized graphs for r=0.6 =  0.5835112499999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.6602475
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.653
SUFF++ for r=0.6 class 0 = 0.421 +- 0.327 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 1 = 0.741 +- 0.327 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 2 = 0.649 +- 0.327 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 all KL = 0.514 +- 0.327 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 all L1 = 0.607 +- 0.253 (in-sample avg dev_std = 0.559)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.731
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9997787500000002
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.719
SUFF++ for r=0.6 class 0 = 0.726 +- 0.425 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 1 = 0.536 +- 0.425 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 class 2 = 0.697 +- 0.425 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 all KL = 0.455 +- 0.425 (in-sample avg dev_std = 0.686)
SUFF++ for r=0.6 all L1 = 0.654 +- 0.253 (in-sample avg dev_std = 0.686)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.396
Model XAI F1 of binarized graphs for r=0.6 =  0.5046725
Model XAI WIoU of binarized graphs for r=0.6 =  0.39466625
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.401
SUFF++ for r=0.6 class 0 = 0.849 +- 0.276 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 class 1 = 0.941 +- 0.276 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 class 2 = 0.727 +- 0.276 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 all KL = 0.757 +- 0.276 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 all L1 = 0.837 +- 0.199 (in-sample avg dev_std = 0.387)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.9
Model XAI F1 of binarized graphs for r=0.6 =  0.5849449999999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.6572274999999999
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.447
NEC for r=0.6 class 0 = 0.627 +- 0.248 (in-sample avg dev_std = 0.701)
NEC for r=0.6 class 1 = 0.537 +- 0.248 (in-sample avg dev_std = 0.701)
NEC for r=0.6 class 2 = 0.674 +- 0.248 (in-sample avg dev_std = 0.701)
NEC for r=0.6 all KL = 0.743 +- 0.248 (in-sample avg dev_std = 0.701)
NEC for r=0.6 all L1 = 0.612 +- 0.164 (in-sample avg dev_std = 0.701)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.887
Model XAI F1 of binarized graphs for r=0.6 =  0.5835112499999999
Model XAI WIoU of binarized graphs for r=0.6 =  0.6602475
len(reference) = 800
Effective ratio: 0.611 +- 0.007
Model Accuracy over intervened graphs for r=0.6 =  0.431
NEC for r=0.6 class 0 = 0.633 +- 0.250 (in-sample avg dev_std = 0.688)
NEC for r=0.6 class 1 = 0.543 +- 0.250 (in-sample avg dev_std = 0.688)
NEC for r=0.6 class 2 = 0.682 +- 0.250 (in-sample avg dev_std = 0.688)
NEC for r=0.6 all KL = 0.748 +- 0.250 (in-sample avg dev_std = 0.688)
NEC for r=0.6 all L1 = 0.62 +- 0.168 (in-sample avg dev_std = 0.688)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.731
Model XAI F1 of binarized graphs for r=0.6 =  0.7975175
Model XAI WIoU of binarized graphs for r=0.6 =  0.9997787500000002
len(reference) = 800
Effective ratio: 0.614 +- 0.009
Model Accuracy over intervened graphs for r=0.6 =  0.422
NEC for r=0.6 class 0 = 0.457 +- 0.374 (in-sample avg dev_std = 0.693)
NEC for r=0.6 class 1 = 0.299 +- 0.374 (in-sample avg dev_std = 0.693)
NEC for r=0.6 class 2 = 0.581 +- 0.374 (in-sample avg dev_std = 0.693)
NEC for r=0.6 all KL = 0.594 +- 0.374 (in-sample avg dev_std = 0.693)
NEC for r=0.6 all L1 = 0.446 +- 0.256 (in-sample avg dev_std = 0.693)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.396
Model XAI F1 of binarized graphs for r=0.6 =  0.5046725
Model XAI WIoU of binarized graphs for r=0.6 =  0.39466625
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.446
NEC for r=0.6 class 0 = 0.241 +- 0.343 (in-sample avg dev_std = 0.468)
NEC for r=0.6 class 1 = 0.083 +- 0.343 (in-sample avg dev_std = 0.468)
NEC for r=0.6 class 2 = 0.457 +- 0.343 (in-sample avg dev_std = 0.468)
NEC for r=0.6 all KL = 0.353 +- 0.343 (in-sample avg dev_std = 0.468)
NEC for r=0.6 all L1 = 0.264 +- 0.270 (in-sample avg dev_std = 0.468)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.473], 'all_L1': [0.519]}), defaultdict(<class 'list'>, {'all_KL': [0.464], 'all_L1': [0.559]}), defaultdict(<class 'list'>, {'all_KL': [0.471], 'all_L1': [0.567]}), defaultdict(<class 'list'>, {'all_KL': [0.486], 'all_L1': [0.601]}), defaultdict(<class 'list'>, {'all_KL': [0.53], 'all_L1': [0.609]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.659], 'all_L1': [0.596]}), defaultdict(<class 'list'>, {'all_KL': [0.704], 'all_L1': [0.592]}), defaultdict(<class 'list'>, {'all_KL': [0.704], 'all_L1': [0.598]}), defaultdict(<class 'list'>, {'all_KL': [0.802], 'all_L1': [0.631]}), defaultdict(<class 'list'>, {'all_KL': [0.743], 'all_L1': [0.612]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.475], 'all_L1': [0.524]}), defaultdict(<class 'list'>, {'all_KL': [0.47], 'all_L1': [0.568]}), defaultdict(<class 'list'>, {'all_KL': [0.46], 'all_L1': [0.56]}), defaultdict(<class 'list'>, {'all_KL': [0.486], 'all_L1': [0.604]}), defaultdict(<class 'list'>, {'all_KL': [0.514], 'all_L1': [0.607]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.661], 'all_L1': [0.597]}), defaultdict(<class 'list'>, {'all_KL': [0.707], 'all_L1': [0.595]}), defaultdict(<class 'list'>, {'all_KL': [0.704], 'all_L1': [0.601]}), defaultdict(<class 'list'>, {'all_KL': [0.807], 'all_L1': [0.642]}), defaultdict(<class 'list'>, {'all_KL': [0.748], 'all_L1': [0.62]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.507], 'all_L1': [0.68]}), defaultdict(<class 'list'>, {'all_KL': [0.496], 'all_L1': [0.623]}), defaultdict(<class 'list'>, {'all_KL': [0.647], 'all_L1': [0.708]}), defaultdict(<class 'list'>, {'all_KL': [0.599], 'all_L1': [0.697]}), defaultdict(<class 'list'>, {'all_KL': [0.455], 'all_L1': [0.654]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.464], 'all_L1': [0.396]}), defaultdict(<class 'list'>, {'all_KL': [0.487], 'all_L1': [0.413]}), defaultdict(<class 'list'>, {'all_KL': [0.578], 'all_L1': [0.5]}), defaultdict(<class 'list'>, {'all_KL': [0.704], 'all_L1': [0.546]}), defaultdict(<class 'list'>, {'all_KL': [0.594], 'all_L1': [0.446]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.677], 'all_L1': [0.756]}), defaultdict(<class 'list'>, {'all_KL': [0.758], 'all_L1': [0.809]}), defaultdict(<class 'list'>, {'all_KL': [0.789], 'all_L1': [0.843]}), defaultdict(<class 'list'>, {'all_KL': [0.774], 'all_L1': [0.83]}), defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.837]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.322], 'all_L1': [0.274]}), defaultdict(<class 'list'>, {'all_KL': [0.378], 'all_L1': [0.296]}), defaultdict(<class 'list'>, {'all_KL': [0.351], 'all_L1': [0.267]}), defaultdict(<class 'list'>, {'all_KL': [0.392], 'all_L1': [0.285]}), defaultdict(<class 'list'>, {'all_KL': [0.353], 'all_L1': [0.264]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.571 +- 0.032
suff++ class all_KL  =  0.485 +- 0.024
suff++_acc_int  =  0.647 +- 0.020
nec class all_L1  =  0.606 +- 0.014
nec class all_KL  =  0.722 +- 0.048
nec_acc_int  =  0.454 +- 0.013

Eval split id_val
suff++ class all_L1  =  0.573 +- 0.031
suff++ class all_KL  =  0.481 +- 0.019
suff++_acc_int  =  0.647 +- 0.017
nec class all_L1  =  0.611 +- 0.018
nec class all_KL  =  0.725 +- 0.049
nec_acc_int  =  0.442 +- 0.017

Eval split val
suff++ class all_L1  =  0.672 +- 0.031
suff++ class all_KL  =  0.541 +- 0.071
suff++_acc_int  =  0.727 +- 0.046
nec class all_L1  =  0.460 +- 0.056
nec class all_KL  =  0.565 +- 0.086
nec_acc_int  =  0.473 +- 0.049

Eval split test
suff++ class all_L1  =  0.815 +- 0.032
suff++ class all_KL  =  0.751 +- 0.039
suff++_acc_int  =  0.393 +- 0.013
nec class all_L1  =  0.277 +- 0.012
nec class all_KL  =  0.359 +- 0.024
nec_acc_int  =  0.438 +- 0.012


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.588 +- 0.022
Faith. Armon (L1)= 		  =  0.588 +- 0.023
Faith. GMean (L1)= 	  =  0.588 +- 0.022
Faith. Aritm (KL)= 		  =  0.604 +- 0.031
Faith. Armon (KL)= 		  =  0.580 +- 0.027
Faith. GMean (KL)= 	  =  0.592 +- 0.029

Eval split id_val
Faith. Aritm (L1)= 		  =  0.592 +- 0.023
Faith. Armon (L1)= 		  =  0.591 +- 0.024
Faith. GMean (L1)= 	  =  0.591 +- 0.023
Faith. Aritm (KL)= 		  =  0.603 +- 0.030
Faith. Armon (KL)= 		  =  0.578 +- 0.025
Faith. GMean (KL)= 	  =  0.590 +- 0.027

Eval split val
Faith. Aritm (L1)= 		  =  0.566 +- 0.040
Faith. Armon (L1)= 		  =  0.545 +- 0.046
Faith. GMean (L1)= 	  =  0.556 +- 0.043
Faith. Aritm (KL)= 		  =  0.553 +- 0.067
Faith. Armon (KL)= 		  =  0.550 +- 0.066
Faith. GMean (KL)= 	  =  0.551 +- 0.067

Eval split test
Faith. Aritm (L1)= 		  =  0.546 +- 0.016
Faith. Armon (L1)= 		  =  0.413 +- 0.013
Faith. GMean (L1)= 	  =  0.475 +- 0.012
Faith. Aritm (KL)= 		  =  0.555 +- 0.029
Faith. Armon (KL)= 		  =  0.486 +- 0.028
Faith. GMean (KL)= 	  =  0.519 +- 0.028
Computed for split load_split = id



Completed in  0:08:23.970441  for CIGAGIN GOODMotif/basis



DONE CIGA GOODMotif/basis

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:42:25 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:25 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:37 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:39 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:41 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:44 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:42:51 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 48...
[0m[1;37mINFO[0m: [1mCheckpoint 48: 
-----------------------------------
Train ACCURACY: 0.8899
Train Loss: 0.5257
ID Validation ACCURACY: 0.8960
ID Validation Loss: 0.4847
ID Test ACCURACY: 0.8937
ID Test Loss: 0.5229
OOD Validation ACCURACY: 0.6193
OOD Validation Loss: 1.1712
OOD Test ACCURACY: 0.4250
OOD Test Loss: 6.2644

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 79...
[0m[1;37mINFO[0m: [1mCheckpoint 79: 
-----------------------------------
Train ACCURACY: 0.8699
Train Loss: 0.5291
ID Validation ACCURACY: 0.8790
ID Validation Loss: 0.4662
ID Test ACCURACY: 0.8723
ID Test Loss: 0.5479
OOD Validation ACCURACY: 0.7500
OOD Validation Loss: 0.6795
OOD Test ACCURACY: 0.4680
OOD Test Loss: 3.3662

[0m[1;37mINFO[0m: [1mChartInfo 0.8937 0.4250 0.8723 0.4680 0.8790 0.7500[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
Label distribution from train: (tensor([0, 1, 2]), tensor([5912, 6084, 6004]))

Gold ratio (train) =  tensor(0.3628) +- tensor(0.1901)
F1 for r=0.8 = 0.531
WIoU for r=0.8 = 0.550
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.516
WIoU for r=0.8 = 0.515
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.264
WIoU for r=0.8 = 0.368
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.123
WIoU for r=0.8 = 0.283


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.887
Model XAI F1 of binarized graphs for r=0.8 =  0.5312837499999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.549695
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.705
SUFF++ for r=0.8 class 0 = 0.564 +- 0.374 (in-sample avg dev_std = 0.494)
SUFF++ for r=0.8 class 1 = 0.729 +- 0.374 (in-sample avg dev_std = 0.494)
SUFF++ for r=0.8 class 2 = 0.768 +- 0.374 (in-sample avg dev_std = 0.494)
SUFF++ for r=0.8 all KL = 0.627 +- 0.374 (in-sample avg dev_std = 0.494)
SUFF++ for r=0.8 all L1 = 0.688 +- 0.279 (in-sample avg dev_std = 0.494)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.904
Model XAI F1 of binarized graphs for r=0.8 =  0.51633375
Model XAI WIoU of binarized graphs for r=0.8 =  0.51461125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.709
SUFF++ for r=0.8 class 0 = 0.551 +- 0.370 (in-sample avg dev_std = 0.493)
SUFF++ for r=0.8 class 1 = 0.708 +- 0.370 (in-sample avg dev_std = 0.493)
SUFF++ for r=0.8 class 2 = 0.786 +- 0.370 (in-sample avg dev_std = 0.493)
SUFF++ for r=0.8 all KL = 0.62 +- 0.370 (in-sample avg dev_std = 0.493)
SUFF++ for r=0.8 all L1 = 0.681 +- 0.278 (in-sample avg dev_std = 0.493)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.64
Model XAI F1 of binarized graphs for r=0.8 =  0.26407125
Model XAI WIoU of binarized graphs for r=0.8 =  0.36762
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.592
SUFF++ for r=0.8 class 0 = 0.522 +- 0.289 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 class 1 = 0.607 +- 0.289 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 class 2 = 0.65 +- 0.289 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 all KL = 0.599 +- 0.289 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 all L1 = 0.592 +- 0.198 (in-sample avg dev_std = 0.495)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.43
Model XAI F1 of binarized graphs for r=0.8 =  0.12252750000000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.28341375
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.394
SUFF++ for r=0.8 class 0 = 0.496 +- 0.332 (in-sample avg dev_std = 0.652)
SUFF++ for r=0.8 class 1 = 0.441 +- 0.332 (in-sample avg dev_std = 0.652)
SUFF++ for r=0.8 class 2 = 0.512 +- 0.332 (in-sample avg dev_std = 0.652)
SUFF++ for r=0.8 all KL = 0.386 +- 0.332 (in-sample avg dev_std = 0.652)
SUFF++ for r=0.8 all L1 = 0.482 +- 0.133 (in-sample avg dev_std = 0.652)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.887
Model XAI F1 of binarized graphs for r=0.8 =  0.5312837499999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.549695
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.455
NEC for r=0.8 class 0 = 0.593 +- 0.264 (in-sample avg dev_std = 0.670)
NEC for r=0.8 class 1 = 0.578 +- 0.264 (in-sample avg dev_std = 0.670)
NEC for r=0.8 class 2 = 0.603 +- 0.264 (in-sample avg dev_std = 0.670)
NEC for r=0.8 all KL = 0.742 +- 0.264 (in-sample avg dev_std = 0.670)
NEC for r=0.8 all L1 = 0.591 +- 0.186 (in-sample avg dev_std = 0.670)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.904
Model XAI F1 of binarized graphs for r=0.8 =  0.51633375
Model XAI WIoU of binarized graphs for r=0.8 =  0.51461125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.458
NEC for r=0.8 class 0 = 0.583 +- 0.263 (in-sample avg dev_std = 0.669)
NEC for r=0.8 class 1 = 0.591 +- 0.263 (in-sample avg dev_std = 0.669)
NEC for r=0.8 class 2 = 0.604 +- 0.263 (in-sample avg dev_std = 0.669)
NEC for r=0.8 all KL = 0.744 +- 0.263 (in-sample avg dev_std = 0.669)
NEC for r=0.8 all L1 = 0.593 +- 0.182 (in-sample avg dev_std = 0.669)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.64
Model XAI F1 of binarized graphs for r=0.8 =  0.26407125
Model XAI WIoU of binarized graphs for r=0.8 =  0.36762
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.417
NEC for r=0.8 class 0 = 0.395 +- 0.313 (in-sample avg dev_std = 0.385)
NEC for r=0.8 class 1 = 0.449 +- 0.313 (in-sample avg dev_std = 0.385)
NEC for r=0.8 class 2 = 0.469 +- 0.313 (in-sample avg dev_std = 0.385)
NEC for r=0.8 all KL = 0.398 +- 0.313 (in-sample avg dev_std = 0.385)
NEC for r=0.8 all L1 = 0.437 +- 0.244 (in-sample avg dev_std = 0.385)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.43
Model XAI F1 of binarized graphs for r=0.8 =  0.12252750000000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.28341375
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.381
NEC for r=0.8 class 0 = 0.359 +- 0.305 (in-sample avg dev_std = 0.253)
NEC for r=0.8 class 1 = 0.393 +- 0.305 (in-sample avg dev_std = 0.253)
NEC for r=0.8 class 2 = 0.37 +- 0.305 (in-sample avg dev_std = 0.253)
NEC for r=0.8 all KL = 0.337 +- 0.305 (in-sample avg dev_std = 0.253)
NEC for r=0.8 all L1 = 0.374 +- 0.263 (in-sample avg dev_std = 0.253)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:44:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:44:59 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:11 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:13 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:15 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:18 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:45:24 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 44...
[0m[1;37mINFO[0m: [1mCheckpoint 44: 
-----------------------------------
Train ACCURACY: 0.8903
Train Loss: 0.5320
ID Validation ACCURACY: 0.8943
ID Validation Loss: 0.4871
ID Test ACCURACY: 0.8910
ID Test Loss: 0.5314
OOD Validation ACCURACY: 0.6970
OOD Validation Loss: 0.9031
OOD Test ACCURACY: 0.5007
OOD Test Loss: 6.0511

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ACCURACY: 0.8831
Train Loss: 0.5175
ID Validation ACCURACY: 0.8833
ID Validation Loss: 0.4767
ID Test ACCURACY: 0.8860
ID Test Loss: 0.5135
OOD Validation ACCURACY: 0.7723
OOD Validation Loss: 0.6911
OOD Test ACCURACY: 0.4477
OOD Test Loss: 2.0349

[0m[1;37mINFO[0m: [1mChartInfo 0.8910 0.5007 0.8860 0.4477 0.8833 0.7723[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
Label distribution from train: (tensor([0, 1, 2]), tensor([5912, 6084, 6004]))

Gold ratio (train) =  tensor(0.3628) +- tensor(0.1901)
F1 for r=0.8 = 0.518
WIoU for r=0.8 = 0.525
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.502
WIoU for r=0.8 = 0.494
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.260
WIoU for r=0.8 = 0.364
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.122
WIoU for r=0.8 = 0.280


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.875
Model XAI F1 of binarized graphs for r=0.8 =  0.51803875
Model XAI WIoU of binarized graphs for r=0.8 =  0.5247225
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.675
SUFF++ for r=0.8 class 0 = 0.534 +- 0.384 (in-sample avg dev_std = 0.461)
SUFF++ for r=0.8 class 1 = 0.704 +- 0.384 (in-sample avg dev_std = 0.461)
SUFF++ for r=0.8 class 2 = 0.762 +- 0.384 (in-sample avg dev_std = 0.461)
SUFF++ for r=0.8 all KL = 0.599 +- 0.384 (in-sample avg dev_std = 0.461)
SUFF++ for r=0.8 all L1 = 0.667 +- 0.288 (in-sample avg dev_std = 0.461)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.9
Model XAI F1 of binarized graphs for r=0.8 =  0.5018312500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.49427
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.679
SUFF++ for r=0.8 class 0 = 0.517 +- 0.382 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 class 1 = 0.688 +- 0.382 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 class 2 = 0.768 +- 0.382 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 all KL = 0.586 +- 0.382 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 all L1 = 0.657 +- 0.289 (in-sample avg dev_std = 0.468)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.712
Model XAI F1 of binarized graphs for r=0.8 =  0.25982
Model XAI WIoU of binarized graphs for r=0.8 =  0.36369500000000005
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.625
SUFF++ for r=0.8 class 0 = 0.542 +- 0.274 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 class 1 = 0.564 +- 0.274 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 class 2 = 0.707 +- 0.274 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 all KL = 0.64 +- 0.274 (in-sample avg dev_std = 0.468)
SUFF++ for r=0.8 all L1 = 0.603 +- 0.197 (in-sample avg dev_std = 0.468)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.482
Model XAI F1 of binarized graphs for r=0.8 =  0.1218825
Model XAI WIoU of binarized graphs for r=0.8 =  0.28028
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.426
SUFF++ for r=0.8 class 0 = 0.584 +- 0.316 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.8 class 1 = 0.551 +- 0.316 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.8 class 2 = 0.575 +- 0.316 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.8 all KL = 0.526 +- 0.316 (in-sample avg dev_std = 0.442)
SUFF++ for r=0.8 all L1 = 0.57 +- 0.161 (in-sample avg dev_std = 0.442)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.875
Model XAI F1 of binarized graphs for r=0.8 =  0.51803875
Model XAI WIoU of binarized graphs for r=0.8 =  0.5247225
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.452
NEC for r=0.8 class 0 = 0.601 +- 0.249 (in-sample avg dev_std = 0.689)
NEC for r=0.8 class 1 = 0.59 +- 0.249 (in-sample avg dev_std = 0.689)
NEC for r=0.8 class 2 = 0.603 +- 0.249 (in-sample avg dev_std = 0.689)
NEC for r=0.8 all KL = 0.761 +- 0.249 (in-sample avg dev_std = 0.689)
NEC for r=0.8 all L1 = 0.598 +- 0.174 (in-sample avg dev_std = 0.689)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.9
Model XAI F1 of binarized graphs for r=0.8 =  0.5018312500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.49427
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.462
NEC for r=0.8 class 0 = 0.578 +- 0.257 (in-sample avg dev_std = 0.685)
NEC for r=0.8 class 1 = 0.593 +- 0.257 (in-sample avg dev_std = 0.685)
NEC for r=0.8 class 2 = 0.6 +- 0.257 (in-sample avg dev_std = 0.685)
NEC for r=0.8 all KL = 0.751 +- 0.257 (in-sample avg dev_std = 0.685)
NEC for r=0.8 all L1 = 0.591 +- 0.176 (in-sample avg dev_std = 0.685)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.712
Model XAI F1 of binarized graphs for r=0.8 =  0.25982
Model XAI WIoU of binarized graphs for r=0.8 =  0.36369500000000005
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.423
NEC for r=0.8 class 0 = 0.495 +- 0.302 (in-sample avg dev_std = 0.421)
NEC for r=0.8 class 1 = 0.456 +- 0.302 (in-sample avg dev_std = 0.421)
NEC for r=0.8 class 2 = 0.546 +- 0.302 (in-sample avg dev_std = 0.421)
NEC for r=0.8 all KL = 0.472 +- 0.302 (in-sample avg dev_std = 0.421)
NEC for r=0.8 all L1 = 0.498 +- 0.216 (in-sample avg dev_std = 0.421)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.482
Model XAI F1 of binarized graphs for r=0.8 =  0.1218825
Model XAI WIoU of binarized graphs for r=0.8 =  0.28028
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.39
NEC for r=0.8 class 0 = 0.406 +- 0.316 (in-sample avg dev_std = 0.237)
NEC for r=0.8 class 1 = 0.34 +- 0.316 (in-sample avg dev_std = 0.237)
NEC for r=0.8 class 2 = 0.362 +- 0.316 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all KL = 0.308 +- 0.316 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all L1 = 0.37 +- 0.255 (in-sample avg dev_std = 0.237)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:47:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:32 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:43 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:45 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:47 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:50 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:47:56 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 64...
[0m[1;37mINFO[0m: [1mCheckpoint 64: 
-----------------------------------
Train ACCURACY: 0.8937
Train Loss: 0.5698
ID Validation ACCURACY: 0.9000
ID Validation Loss: 0.5288
ID Test ACCURACY: 0.8943
ID Test Loss: 0.5798
OOD Validation ACCURACY: 0.6730
OOD Validation Loss: 1.3851
OOD Test ACCURACY: 0.4563
OOD Test Loss: 4.4608

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 20...
[0m[1;37mINFO[0m: [1mCheckpoint 20: 
-----------------------------------
Train ACCURACY: 0.8583
Train Loss: 0.7164
ID Validation ACCURACY: 0.8650
ID Validation Loss: 0.6880
ID Test ACCURACY: 0.8517
ID Test Loss: 0.7220
OOD Validation ACCURACY: 0.7457
OOD Validation Loss: 1.4504
OOD Test ACCURACY: 0.4083
OOD Test Loss: 6.2487

[0m[1;37mINFO[0m: [1mChartInfo 0.8943 0.4563 0.8517 0.4083 0.8650 0.7457[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
Label distribution from train: (tensor([0, 1, 2]), tensor([5912, 6084, 6004]))

Gold ratio (train) =  tensor(0.3628) +- tensor(0.1901)
F1 for r=0.8 = 0.531
WIoU for r=0.8 = 0.533
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.516
WIoU for r=0.8 = 0.501
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.260
WIoU for r=0.8 = 0.365
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.121
WIoU for r=0.8 = 0.280


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.887
Model XAI F1 of binarized graphs for r=0.8 =  0.5311625
Model XAI WIoU of binarized graphs for r=0.8 =  0.53295375
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.698
SUFF++ for r=0.8 class 0 = 0.57 +- 0.376 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.8 class 1 = 0.747 +- 0.376 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.8 class 2 = 0.804 +- 0.376 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.8 all KL = 0.641 +- 0.376 (in-sample avg dev_std = 0.449)
SUFF++ for r=0.8 all L1 = 0.708 +- 0.291 (in-sample avg dev_std = 0.449)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.905
Model XAI F1 of binarized graphs for r=0.8 =  0.51593375
Model XAI WIoU of binarized graphs for r=0.8 =  0.50095
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.709
SUFF++ for r=0.8 class 0 = 0.546 +- 0.369 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 class 1 = 0.752 +- 0.369 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 class 2 = 0.815 +- 0.369 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 all KL = 0.64 +- 0.369 (in-sample avg dev_std = 0.443)
SUFF++ for r=0.8 all L1 = 0.703 +- 0.291 (in-sample avg dev_std = 0.443)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.68
Model XAI F1 of binarized graphs for r=0.8 =  0.26014624999999997
Model XAI WIoU of binarized graphs for r=0.8 =  0.3648875
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.623
SUFF++ for r=0.8 class 0 = 0.587 +- 0.323 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.8 class 1 = 0.629 +- 0.323 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.8 class 2 = 0.692 +- 0.323 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.8 all KL = 0.618 +- 0.323 (in-sample avg dev_std = 0.487)
SUFF++ for r=0.8 all L1 = 0.635 +- 0.215 (in-sample avg dev_std = 0.487)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.461
Model XAI F1 of binarized graphs for r=0.8 =  0.1207675
Model XAI WIoU of binarized graphs for r=0.8 =  0.2801725
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.435
SUFF++ for r=0.8 class 0 = 0.502 +- 0.314 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 class 1 = 0.485 +- 0.314 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 class 2 = 0.512 +- 0.314 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 all KL = 0.479 +- 0.314 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 all L1 = 0.499 +- 0.184 (in-sample avg dev_std = 0.475)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.887
Model XAI F1 of binarized graphs for r=0.8 =  0.5311625
Model XAI WIoU of binarized graphs for r=0.8 =  0.53295375
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.448
NEC for r=0.8 class 0 = 0.596 +- 0.243 (in-sample avg dev_std = 0.703)
NEC for r=0.8 class 1 = 0.606 +- 0.243 (in-sample avg dev_std = 0.703)
NEC for r=0.8 class 2 = 0.61 +- 0.243 (in-sample avg dev_std = 0.703)
NEC for r=0.8 all KL = 0.791 +- 0.243 (in-sample avg dev_std = 0.703)
NEC for r=0.8 all L1 = 0.604 +- 0.187 (in-sample avg dev_std = 0.703)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.905
Model XAI F1 of binarized graphs for r=0.8 =  0.51593375
Model XAI WIoU of binarized graphs for r=0.8 =  0.50095
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.447
NEC for r=0.8 class 0 = 0.576 +- 0.250 (in-sample avg dev_std = 0.705)
NEC for r=0.8 class 1 = 0.628 +- 0.250 (in-sample avg dev_std = 0.705)
NEC for r=0.8 class 2 = 0.611 +- 0.250 (in-sample avg dev_std = 0.705)
NEC for r=0.8 all KL = 0.789 +- 0.250 (in-sample avg dev_std = 0.705)
NEC for r=0.8 all L1 = 0.605 +- 0.188 (in-sample avg dev_std = 0.705)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.68
Model XAI F1 of binarized graphs for r=0.8 =  0.26014624999999997
Model XAI WIoU of binarized graphs for r=0.8 =  0.3648875
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.414
NEC for r=0.8 class 0 = 0.445 +- 0.349 (in-sample avg dev_std = 0.438)
NEC for r=0.8 class 1 = 0.486 +- 0.349 (in-sample avg dev_std = 0.438)
NEC for r=0.8 class 2 = 0.496 +- 0.349 (in-sample avg dev_std = 0.438)
NEC for r=0.8 all KL = 0.484 +- 0.349 (in-sample avg dev_std = 0.438)
NEC for r=0.8 all L1 = 0.475 +- 0.271 (in-sample avg dev_std = 0.438)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.461
Model XAI F1 of binarized graphs for r=0.8 =  0.1207675
Model XAI WIoU of binarized graphs for r=0.8 =  0.2801725
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.381
NEC for r=0.8 class 0 = 0.48 +- 0.363 (in-sample avg dev_std = 0.267)
NEC for r=0.8 class 1 = 0.443 +- 0.363 (in-sample avg dev_std = 0.267)
NEC for r=0.8 class 2 = 0.464 +- 0.363 (in-sample avg dev_std = 0.267)
NEC for r=0.8 all KL = 0.44 +- 0.363 (in-sample avg dev_std = 0.267)
NEC for r=0.8 all L1 = 0.462 +- 0.287 (in-sample avg dev_std = 0.267)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:50:02 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:02 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:13 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:15 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:17 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:20 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:50:26 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 27...
[0m[1;37mINFO[0m: [1mCheckpoint 27: 
-----------------------------------
Train ACCURACY: 0.8753
Train Loss: 0.5158
ID Validation ACCURACY: 0.8807
ID Validation Loss: 0.4789
ID Test ACCURACY: 0.8773
ID Test Loss: 0.5165
OOD Validation ACCURACY: 0.6493
OOD Validation Loss: 1.1427
OOD Test ACCURACY: 0.4040
OOD Test Loss: 2.8675

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.8559
Train Loss: 0.5232
ID Validation ACCURACY: 0.8650
ID Validation Loss: 0.4809
ID Test ACCURACY: 0.8633
ID Test Loss: 0.5159
OOD Validation ACCURACY: 0.7123
OOD Validation Loss: 0.8312
OOD Test ACCURACY: 0.3543
OOD Test Loss: 8.7485

[0m[1;37mINFO[0m: [1mChartInfo 0.8773 0.4040 0.8633 0.3543 0.8650 0.7123[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
Label distribution from train: (tensor([0, 1, 2]), tensor([5912, 6084, 6004]))

Gold ratio (train) =  tensor(0.3628) +- tensor(0.1901)
F1 for r=0.8 = 0.554
WIoU for r=0.8 = 0.592
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.539
WIoU for r=0.8 = 0.564
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.280
WIoU for r=0.8 = 0.418
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.136
WIoU for r=0.8 = 0.483


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.859
Model XAI F1 of binarized graphs for r=0.8 =  0.5539287500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.59166625
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.74
SUFF++ for r=0.8 class 0 = 0.699 +- 0.298 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 class 1 = 0.583 +- 0.298 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 class 2 = 0.876 +- 0.298 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 all KL = 0.714 +- 0.298 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 all L1 = 0.719 +- 0.246 (in-sample avg dev_std = 0.387)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.87
Model XAI F1 of binarized graphs for r=0.8 =  0.5392025
Model XAI WIoU of binarized graphs for r=0.8 =  0.56392125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.743
SUFF++ for r=0.8 class 0 = 0.669 +- 0.291 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 class 1 = 0.589 +- 0.291 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 class 2 = 0.872 +- 0.291 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 all KL = 0.715 +- 0.291 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 all L1 = 0.711 +- 0.243 (in-sample avg dev_std = 0.396)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.647
Model XAI F1 of binarized graphs for r=0.8 =  0.28031
Model XAI WIoU of binarized graphs for r=0.8 =  0.41801125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.598
SUFF++ for r=0.8 class 0 = 0.568 +- 0.301 (in-sample avg dev_std = 0.464)
SUFF++ for r=0.8 class 1 = 0.565 +- 0.301 (in-sample avg dev_std = 0.464)
SUFF++ for r=0.8 class 2 = 0.658 +- 0.301 (in-sample avg dev_std = 0.464)
SUFF++ for r=0.8 all KL = 0.638 +- 0.301 (in-sample avg dev_std = 0.464)
SUFF++ for r=0.8 all L1 = 0.596 +- 0.221 (in-sample avg dev_std = 0.464)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.393
Model XAI F1 of binarized graphs for r=0.8 =  0.13596125
Model XAI WIoU of binarized graphs for r=0.8 =  0.48281875
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.426
SUFF++ for r=0.8 class 0 = 0.593 +- 0.300 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 class 1 = 0.647 +- 0.300 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 class 2 = 0.598 +- 0.300 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 all KL = 0.552 +- 0.300 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 all L1 = 0.613 +- 0.148 (in-sample avg dev_std = 0.507)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.859
Model XAI F1 of binarized graphs for r=0.8 =  0.5539287500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.59166625
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.424
NEC for r=0.8 class 0 = 0.618 +- 0.264 (in-sample avg dev_std = 0.610)
NEC for r=0.8 class 1 = 0.571 +- 0.264 (in-sample avg dev_std = 0.610)
NEC for r=0.8 class 2 = 0.607 +- 0.264 (in-sample avg dev_std = 0.610)
NEC for r=0.8 all KL = 0.698 +- 0.264 (in-sample avg dev_std = 0.610)
NEC for r=0.8 all L1 = 0.599 +- 0.166 (in-sample avg dev_std = 0.610)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.87
Model XAI F1 of binarized graphs for r=0.8 =  0.5392025
Model XAI WIoU of binarized graphs for r=0.8 =  0.56392125
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.425
NEC for r=0.8 class 0 = 0.6 +- 0.262 (in-sample avg dev_std = 0.609)
NEC for r=0.8 class 1 = 0.589 +- 0.262 (in-sample avg dev_std = 0.609)
NEC for r=0.8 class 2 = 0.608 +- 0.262 (in-sample avg dev_std = 0.609)
NEC for r=0.8 all KL = 0.695 +- 0.262 (in-sample avg dev_std = 0.609)
NEC for r=0.8 all L1 = 0.599 +- 0.164 (in-sample avg dev_std = 0.609)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.647
Model XAI F1 of binarized graphs for r=0.8 =  0.28031
Model XAI WIoU of binarized graphs for r=0.8 =  0.41801125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.408
NEC for r=0.8 class 0 = 0.432 +- 0.267 (in-sample avg dev_std = 0.391)
NEC for r=0.8 class 1 = 0.445 +- 0.267 (in-sample avg dev_std = 0.391)
NEC for r=0.8 class 2 = 0.503 +- 0.267 (in-sample avg dev_std = 0.391)
NEC for r=0.8 all KL = 0.397 +- 0.267 (in-sample avg dev_std = 0.391)
NEC for r=0.8 all L1 = 0.459 +- 0.211 (in-sample avg dev_std = 0.391)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.393
Model XAI F1 of binarized graphs for r=0.8 =  0.13596125
Model XAI WIoU of binarized graphs for r=0.8 =  0.48281875
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.364
NEC for r=0.8 class 0 = 0.312 +- 0.284 (in-sample avg dev_std = 0.245)
NEC for r=0.8 class 1 = 0.317 +- 0.284 (in-sample avg dev_std = 0.245)
NEC for r=0.8 class 2 = 0.335 +- 0.284 (in-sample avg dev_std = 0.245)
NEC for r=0.8 all KL = 0.269 +- 0.284 (in-sample avg dev_std = 0.245)
NEC for r=0.8 all L1 = 0.321 +- 0.265 (in-sample avg dev_std = 0.245)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:52:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:31 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:43 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:45 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:47 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:50 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:52:56 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 37...
[0m[1;37mINFO[0m: [1mCheckpoint 37: 
-----------------------------------
Train ACCURACY: 0.8598
Train Loss: 0.5187
ID Validation ACCURACY: 0.8710
ID Validation Loss: 0.4867
ID Test ACCURACY: 0.8650
ID Test Loss: 0.5182
OOD Validation ACCURACY: 0.6850
OOD Validation Loss: 0.7691
OOD Test ACCURACY: 0.3613
OOD Test Loss: 3.5062

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 48...
[0m[1;37mINFO[0m: [1mCheckpoint 48: 
-----------------------------------
Train ACCURACY: 0.8180
Train Loss: 0.7026
ID Validation ACCURACY: 0.8267
ID Validation Loss: 0.6541
ID Test ACCURACY: 0.8193
ID Test Loss: 0.6900
OOD Validation ACCURACY: 0.8000
OOD Validation Loss: 0.6826
OOD Test ACCURACY: 0.4330
OOD Test Loss: 1.3960

[0m[1;37mINFO[0m: [1mChartInfo 0.8650 0.3613 0.8193 0.4330 0.8267 0.8000[0mGOODMotif(18000)
Data example from train: Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
Label distribution from train: (tensor([0, 1, 2]), tensor([5912, 6084, 6004]))

Gold ratio (train) =  tensor(0.3628) +- tensor(0.1901)
F1 for r=0.8 = 0.571
WIoU for r=0.8 = 0.721
GOODMotif(3000)
Data example from id_val: Data(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1019,  973, 1008]))

Gold ratio (id_val) =  tensor(0.3486) +- tensor(0.1769)
F1 for r=0.8 = 0.560
WIoU for r=0.8 = 0.704
GOODMotif(3000)
Data example from val: Data(edge_index=[2, 42], x=[20, 1], node_gt=[20], edge_gt=[42], y=[1], env_id=[1], ori_edge_index=[2, 42], node_perm=[20], num_nodes=20)
Label distribution from val: (tensor([0, 1, 2]), tensor([1025, 1009,  966]))

Gold ratio (val) =  tensor(0.1390) +- tensor(0.0663)
F1 for r=0.8 = 0.285
WIoU for r=0.8 = 0.591
GOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.138
WIoU for r=0.8 = 0.357


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  0.5709912500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.721185
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.772
SUFF++ for r=0.8 class 0 = 0.768 +- 0.226 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 class 1 = 0.734 +- 0.226 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 class 2 = 0.839 +- 0.226 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 all KL = 0.833 +- 0.226 (in-sample avg dev_std = 0.303)
SUFF++ for r=0.8 all L1 = 0.78 +- 0.208 (in-sample avg dev_std = 0.303)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.869
Model XAI F1 of binarized graphs for r=0.8 =  0.55966
Model XAI WIoU of binarized graphs for r=0.8 =  0.70362375
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.778
SUFF++ for r=0.8 class 0 = 0.742 +- 0.234 (in-sample avg dev_std = 0.311)
SUFF++ for r=0.8 class 1 = 0.733 +- 0.234 (in-sample avg dev_std = 0.311)
SUFF++ for r=0.8 class 2 = 0.837 +- 0.234 (in-sample avg dev_std = 0.311)
SUFF++ for r=0.8 all KL = 0.824 +- 0.234 (in-sample avg dev_std = 0.311)
SUFF++ for r=0.8 all L1 = 0.771 +- 0.211 (in-sample avg dev_std = 0.311)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.692
Model XAI F1 of binarized graphs for r=0.8 =  0.28490125
Model XAI WIoU of binarized graphs for r=0.8 =  0.59116
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.575
SUFF++ for r=0.8 class 0 = 0.632 +- 0.176 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 class 1 = 0.65 +- 0.176 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 class 2 = 0.674 +- 0.176 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 all KL = 0.777 +- 0.176 (in-sample avg dev_std = 0.360)
SUFF++ for r=0.8 all L1 = 0.652 +- 0.168 (in-sample avg dev_std = 0.360)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.364
Model XAI F1 of binarized graphs for r=0.8 =  0.13843
Model XAI WIoU of binarized graphs for r=0.8 =  0.35728499999999996
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.409
SUFF++ for r=0.8 class 0 = 0.602 +- 0.328 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 class 1 = 0.595 +- 0.328 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 class 2 = 0.588 +- 0.328 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 all KL = 0.562 +- 0.328 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 all L1 = 0.595 +- 0.183 (in-sample avg dev_std = 0.527)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  0.5709912500000001
Model XAI WIoU of binarized graphs for r=0.8 =  0.721185
len(reference) = 800
Effective ratio: 0.813 +- 0.012
Model Accuracy over intervened graphs for r=0.8 =  0.449
NEC for r=0.8 class 0 = 0.651 +- 0.325 (in-sample avg dev_std = 0.534)
NEC for r=0.8 class 1 = 0.387 +- 0.325 (in-sample avg dev_std = 0.534)
NEC for r=0.8 class 2 = 0.617 +- 0.325 (in-sample avg dev_std = 0.534)
NEC for r=0.8 all KL = 0.595 +- 0.325 (in-sample avg dev_std = 0.534)
NEC for r=0.8 all L1 = 0.55 +- 0.229 (in-sample avg dev_std = 0.534)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.869
Model XAI F1 of binarized graphs for r=0.8 =  0.55966
Model XAI WIoU of binarized graphs for r=0.8 =  0.70362375
len(reference) = 800
Effective ratio: 0.813 +- 0.011
Model Accuracy over intervened graphs for r=0.8 =  0.468
NEC for r=0.8 class 0 = 0.618 +- 0.325 (in-sample avg dev_std = 0.542)
NEC for r=0.8 class 1 = 0.38 +- 0.325 (in-sample avg dev_std = 0.542)
NEC for r=0.8 class 2 = 0.618 +- 0.325 (in-sample avg dev_std = 0.542)
NEC for r=0.8 all KL = 0.584 +- 0.325 (in-sample avg dev_std = 0.542)
NEC for r=0.8 all L1 = 0.541 +- 0.230 (in-sample avg dev_std = 0.542)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.692
Model XAI F1 of binarized graphs for r=0.8 =  0.28490125
Model XAI WIoU of binarized graphs for r=0.8 =  0.59116
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.452
NEC for r=0.8 class 0 = 0.563 +- 0.312 (in-sample avg dev_std = 0.345)
NEC for r=0.8 class 1 = 0.421 +- 0.312 (in-sample avg dev_std = 0.345)
NEC for r=0.8 class 2 = 0.604 +- 0.312 (in-sample avg dev_std = 0.345)
NEC for r=0.8 all KL = 0.448 +- 0.312 (in-sample avg dev_std = 0.345)
NEC for r=0.8 all L1 = 0.529 +- 0.228 (in-sample avg dev_std = 0.345)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.364
Model XAI F1 of binarized graphs for r=0.8 =  0.13843
Model XAI WIoU of binarized graphs for r=0.8 =  0.35728499999999996
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.352
NEC for r=0.8 class 0 = 0.449 +- 0.391 (in-sample avg dev_std = 0.200)
NEC for r=0.8 class 1 = 0.44 +- 0.391 (in-sample avg dev_std = 0.200)
NEC for r=0.8 class 2 = 0.46 +- 0.391 (in-sample avg dev_std = 0.200)
NEC for r=0.8 all KL = 0.371 +- 0.391 (in-sample avg dev_std = 0.200)
NEC for r=0.8 all L1 = 0.449 +- 0.291 (in-sample avg dev_std = 0.200)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.627], 'all_L1': [0.688]}), defaultdict(<class 'list'>, {'all_KL': [0.599], 'all_L1': [0.667]}), defaultdict(<class 'list'>, {'all_KL': [0.641], 'all_L1': [0.708]}), defaultdict(<class 'list'>, {'all_KL': [0.714], 'all_L1': [0.719]}), defaultdict(<class 'list'>, {'all_KL': [0.833], 'all_L1': [0.78]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.742], 'all_L1': [0.591]}), defaultdict(<class 'list'>, {'all_KL': [0.761], 'all_L1': [0.598]}), defaultdict(<class 'list'>, {'all_KL': [0.791], 'all_L1': [0.604]}), defaultdict(<class 'list'>, {'all_KL': [0.698], 'all_L1': [0.599]}), defaultdict(<class 'list'>, {'all_KL': [0.595], 'all_L1': [0.55]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.62], 'all_L1': [0.681]}), defaultdict(<class 'list'>, {'all_KL': [0.586], 'all_L1': [0.657]}), defaultdict(<class 'list'>, {'all_KL': [0.64], 'all_L1': [0.703]}), defaultdict(<class 'list'>, {'all_KL': [0.715], 'all_L1': [0.711]}), defaultdict(<class 'list'>, {'all_KL': [0.824], 'all_L1': [0.771]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.744], 'all_L1': [0.593]}), defaultdict(<class 'list'>, {'all_KL': [0.751], 'all_L1': [0.591]}), defaultdict(<class 'list'>, {'all_KL': [0.789], 'all_L1': [0.605]}), defaultdict(<class 'list'>, {'all_KL': [0.695], 'all_L1': [0.599]}), defaultdict(<class 'list'>, {'all_KL': [0.584], 'all_L1': [0.541]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.599], 'all_L1': [0.592]}), defaultdict(<class 'list'>, {'all_KL': [0.64], 'all_L1': [0.603]}), defaultdict(<class 'list'>, {'all_KL': [0.618], 'all_L1': [0.635]}), defaultdict(<class 'list'>, {'all_KL': [0.638], 'all_L1': [0.596]}), defaultdict(<class 'list'>, {'all_KL': [0.777], 'all_L1': [0.652]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.398], 'all_L1': [0.437]}), defaultdict(<class 'list'>, {'all_KL': [0.472], 'all_L1': [0.498]}), defaultdict(<class 'list'>, {'all_KL': [0.484], 'all_L1': [0.475]}), defaultdict(<class 'list'>, {'all_KL': [0.397], 'all_L1': [0.459]}), defaultdict(<class 'list'>, {'all_KL': [0.448], 'all_L1': [0.529]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.386], 'all_L1': [0.482]}), defaultdict(<class 'list'>, {'all_KL': [0.526], 'all_L1': [0.57]}), defaultdict(<class 'list'>, {'all_KL': [0.479], 'all_L1': [0.499]}), defaultdict(<class 'list'>, {'all_KL': [0.552], 'all_L1': [0.613]}), defaultdict(<class 'list'>, {'all_KL': [0.562], 'all_L1': [0.595]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.337], 'all_L1': [0.374]}), defaultdict(<class 'list'>, {'all_KL': [0.308], 'all_L1': [0.37]}), defaultdict(<class 'list'>, {'all_KL': [0.44], 'all_L1': [0.462]}), defaultdict(<class 'list'>, {'all_KL': [0.269], 'all_L1': [0.321]}), defaultdict(<class 'list'>, {'all_KL': [0.371], 'all_L1': [0.449]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.712 +- 0.038
suff++ class all_KL  =  0.683 +- 0.084
suff++_acc_int  =  0.718 +- 0.034
nec class all_L1  =  0.588 +- 0.020
nec class all_KL  =  0.717 +- 0.068
nec_acc_int  =  0.446 +- 0.011

Eval split id_val
suff++ class all_L1  =  0.705 +- 0.038
suff++ class all_KL  =  0.677 +- 0.085
suff++_acc_int  =  0.724 +- 0.034
nec class all_L1  =  0.586 +- 0.023
nec class all_KL  =  0.713 +- 0.071
nec_acc_int  =  0.452 +- 0.015

Eval split val
suff++ class all_L1  =  0.616 +- 0.024
suff++ class all_KL  =  0.654 +- 0.063
suff++_acc_int  =  0.603 +- 0.019
nec class all_L1  =  0.480 +- 0.032
nec class all_KL  =  0.440 +- 0.036
nec_acc_int  =  0.423 +- 0.015

Eval split test
suff++ class all_L1  =  0.552 +- 0.052
suff++ class all_KL  =  0.501 +- 0.064
suff++_acc_int  =  0.418 +- 0.015
nec class all_L1  =  0.395 +- 0.053
nec class all_KL  =  0.345 +- 0.058
nec_acc_int  =  0.374 +- 0.014


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.650 +- 0.012
Faith. Armon (L1)= 		  =  0.643 +- 0.009
Faith. GMean (L1)= 	  =  0.647 +- 0.010
Faith. Aritm (KL)= 		  =  0.700 +- 0.015
Faith. Armon (KL)= 		  =  0.692 +- 0.015
Faith. GMean (KL)= 	  =  0.696 +- 0.014

Eval split id_val
Faith. Aritm (L1)= 		  =  0.645 +- 0.013
Faith. Armon (L1)= 		  =  0.639 +- 0.011
Faith. GMean (L1)= 	  =  0.642 +- 0.011
Faith. Aritm (KL)= 		  =  0.695 +- 0.017
Faith. Armon (KL)= 		  =  0.686 +- 0.018
Faith. GMean (KL)= 	  =  0.690 +- 0.017

Eval split val
Faith. Aritm (L1)= 		  =  0.548 +- 0.026
Faith. Armon (L1)= 		  =  0.539 +- 0.028
Faith. GMean (L1)= 	  =  0.543 +- 0.027
Faith. Aritm (KL)= 		  =  0.547 +- 0.039
Faith. Armon (KL)= 		  =  0.524 +- 0.035
Faith. GMean (KL)= 	  =  0.536 +- 0.036

Eval split test
Faith. Aritm (L1)= 		  =  0.474 +- 0.030
Faith. Armon (L1)= 		  =  0.457 +- 0.035
Faith. GMean (L1)= 	  =  0.465 +- 0.032
Faith. Aritm (KL)= 		  =  0.423 +- 0.038
Faith. Armon (KL)= 		  =  0.403 +- 0.042
Faith. GMean (KL)= 	  =  0.413 +- 0.039
Computed for split load_split = id



Completed in  0:12:39.585504  for CIGAGIN GOODMotif/size



DONE CIGA GOODMotif/size

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:55:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:55:35 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 105...
[0m[1;37mINFO[0m: [1mCheckpoint 105: 
-----------------------------------
Train ACCURACY: 0.9248
Train Loss: 0.3767
ID Validation ACCURACY: 0.9243
ID Validation Loss: 0.3771
ID Test ACCURACY: 0.9193
ID Test Loss: 0.4310
OOD Validation ACCURACY: 0.9260
OOD Validation Loss: 0.3978
OOD Test ACCURACY: 0.4087
OOD Test Loss: 27.1216

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 105...
[0m[1;37mINFO[0m: [1mCheckpoint 105: 
-----------------------------------
Train ACCURACY: 0.9248
Train Loss: 0.3767
ID Validation ACCURACY: 0.9243
ID Validation Loss: 0.3771
ID Test ACCURACY: 0.9193
ID Test Loss: 0.4310
OOD Validation ACCURACY: 0.9260
OOD Validation Loss: 0.3978
OOD Test ACCURACY: 0.4087
OOD Test Loss: 27.1216

[0m[1;37mINFO[0m: [1mChartInfo 0.9193 0.4087 0.9193 0.4087 0.9243 0.9260[0mGOODMotif2(18000)
Data example from train: Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from train: (tensor([0, 1, 2]), tensor([5925, 6060, 6015]))

Gold ratio (train) =  tensor(0.3124) +- tensor(0.1674)
F1 for r=0.8 = 0.517
WIoU for r=0.8 = 0.647
GOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.516
WIoU for r=0.8 = 0.652
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.307
WIoU for r=0.8 = 0.893
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.343
WIoU for r=0.8 = 0.231


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.926
Model XAI F1 of binarized graphs for r=0.8 =  0.51688125
Model XAI WIoU of binarized graphs for r=0.8 =  0.6474575
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.692
SUFF++ for r=0.8 class 0 = 0.473 +- 0.313 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 class 1 = 0.559 +- 0.313 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 class 2 = 0.878 +- 0.313 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 all KL = 0.602 +- 0.313 (in-sample avg dev_std = 0.527)
SUFF++ for r=0.8 all L1 = 0.637 +- 0.253 (in-sample avg dev_std = 0.527)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.51646625
Model XAI WIoU of binarized graphs for r=0.8 =  0.65170375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.715
SUFF++ for r=0.8 class 0 = 0.467 +- 0.310 (in-sample avg dev_std = 0.510)
SUFF++ for r=0.8 class 1 = 0.604 +- 0.310 (in-sample avg dev_std = 0.510)
SUFF++ for r=0.8 class 2 = 0.89 +- 0.310 (in-sample avg dev_std = 0.510)
SUFF++ for r=0.8 all KL = 0.623 +- 0.310 (in-sample avg dev_std = 0.510)
SUFF++ for r=0.8 all L1 = 0.653 +- 0.252 (in-sample avg dev_std = 0.510)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.928
Model XAI F1 of binarized graphs for r=0.8 =  0.3071675
Model XAI WIoU of binarized graphs for r=0.8 =  0.89316125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.598
SUFF++ for r=0.8 class 0 = 0.427 +- 0.247 (in-sample avg dev_std = 0.482)
SUFF++ for r=0.8 class 1 = 0.514 +- 0.247 (in-sample avg dev_std = 0.482)
SUFF++ for r=0.8 class 2 = 0.78 +- 0.247 (in-sample avg dev_std = 0.482)
SUFF++ for r=0.8 all KL = 0.599 +- 0.247 (in-sample avg dev_std = 0.482)
SUFF++ for r=0.8 all L1 = 0.574 +- 0.228 (in-sample avg dev_std = 0.482)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.34301874999999993
Model XAI WIoU of binarized graphs for r=0.8 =  0.23099499999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.392
SUFF++ for r=0.8 class 0 = 0.787 +- 0.353 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.8 class 1 = 0.853 +- 0.353 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.8 class 2 = 0.973 +- 0.353 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.8 all KL = 0.785 +- 0.353 (in-sample avg dev_std = 0.334)
SUFF++ for r=0.8 all L1 = 0.872 +- 0.247 (in-sample avg dev_std = 0.334)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.926
Model XAI F1 of binarized graphs for r=0.8 =  0.51688125
Model XAI WIoU of binarized graphs for r=0.8 =  0.6474575
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.435
NEC for r=0.8 class 0 = 0.652 +- 0.235 (in-sample avg dev_std = 0.688)
NEC for r=0.8 class 1 = 0.602 +- 0.235 (in-sample avg dev_std = 0.688)
NEC for r=0.8 class 2 = 0.582 +- 0.235 (in-sample avg dev_std = 0.688)
NEC for r=0.8 all KL = 0.723 +- 0.235 (in-sample avg dev_std = 0.688)
NEC for r=0.8 all L1 = 0.612 +- 0.158 (in-sample avg dev_std = 0.688)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.51646625
Model XAI WIoU of binarized graphs for r=0.8 =  0.65170375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.439
NEC for r=0.8 class 0 = 0.655 +- 0.233 (in-sample avg dev_std = 0.699)
NEC for r=0.8 class 1 = 0.611 +- 0.233 (in-sample avg dev_std = 0.699)
NEC for r=0.8 class 2 = 0.562 +- 0.233 (in-sample avg dev_std = 0.699)
NEC for r=0.8 all KL = 0.727 +- 0.233 (in-sample avg dev_std = 0.699)
NEC for r=0.8 all L1 = 0.61 +- 0.158 (in-sample avg dev_std = 0.699)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.928
Model XAI F1 of binarized graphs for r=0.8 =  0.3071675
Model XAI WIoU of binarized graphs for r=0.8 =  0.89316125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.493
NEC for r=0.8 class 0 = 0.584 +- 0.220 (in-sample avg dev_std = 0.608)
NEC for r=0.8 class 1 = 0.573 +- 0.220 (in-sample avg dev_std = 0.608)
NEC for r=0.8 class 2 = 0.471 +- 0.220 (in-sample avg dev_std = 0.608)
NEC for r=0.8 all KL = 0.58 +- 0.220 (in-sample avg dev_std = 0.608)
NEC for r=0.8 all L1 = 0.542 +- 0.142 (in-sample avg dev_std = 0.608)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.34301874999999993
Model XAI WIoU of binarized graphs for r=0.8 =  0.23099499999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.393
NEC for r=0.8 class 0 = 0.267 +- 0.447 (in-sample avg dev_std = 0.495)
NEC for r=0.8 class 1 = 0.253 +- 0.447 (in-sample avg dev_std = 0.495)
NEC for r=0.8 class 2 = 0.143 +- 0.447 (in-sample avg dev_std = 0.495)
NEC for r=0.8 all KL = 0.393 +- 0.447 (in-sample avg dev_std = 0.495)
NEC for r=0.8 all L1 = 0.221 +- 0.288 (in-sample avg dev_std = 0.495)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:57:05 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:57:05 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ACCURACY: 0.8638
Train Loss: 0.5776
ID Validation ACCURACY: 0.8760
ID Validation Loss: 0.5248
ID Test ACCURACY: 0.8623
ID Test Loss: 0.5892
OOD Validation ACCURACY: 0.8237
OOD Validation Loss: 0.5364
OOD Test ACCURACY: 0.6500
OOD Test Loss: 2.0536

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 84...
[0m[1;37mINFO[0m: [1mCheckpoint 84: 
-----------------------------------
Train ACCURACY: 0.8533
Train Loss: 0.5252
ID Validation ACCURACY: 0.8630
ID Validation Loss: 0.4912
ID Test ACCURACY: 0.8500
ID Test Loss: 0.5371
OOD Validation ACCURACY: 0.8757
OOD Validation Loss: 0.4826
OOD Test ACCURACY: 0.4940
OOD Test Loss: 5.1451

[0m[1;37mINFO[0m: [1mChartInfo 0.8623 0.6500 0.8500 0.4940 0.8630 0.8757[0mGOODMotif2(18000)
Data example from train: Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from train: (tensor([0, 1, 2]), tensor([5925, 6060, 6015]))

Gold ratio (train) =  tensor(0.3124) +- tensor(0.1674)
F1 for r=0.8 = 0.516
WIoU for r=0.8 = 0.716
GOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.514
WIoU for r=0.8 = 0.725
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.325
WIoU for r=0.8 = 0.972
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.421
WIoU for r=0.8 = 0.491


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.858
Model XAI F1 of binarized graphs for r=0.8 =  0.5163899999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.7159762500000001
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.727
SUFF++ for r=0.8 class 0 = 0.603 +- 0.326 (in-sample avg dev_std = 0.422)
SUFF++ for r=0.8 class 1 = 0.914 +- 0.326 (in-sample avg dev_std = 0.422)
SUFF++ for r=0.8 class 2 = 0.639 +- 0.326 (in-sample avg dev_std = 0.422)
SUFF++ for r=0.8 all KL = 0.694 +- 0.326 (in-sample avg dev_std = 0.422)
SUFF++ for r=0.8 all L1 = 0.72 +- 0.258 (in-sample avg dev_std = 0.422)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.88
Model XAI F1 of binarized graphs for r=0.8 =  0.5143775
Model XAI WIoU of binarized graphs for r=0.8 =  0.7254775000000001
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.731
SUFF++ for r=0.8 class 0 = 0.599 +- 0.331 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 class 1 = 0.91 +- 0.331 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 class 2 = 0.671 +- 0.331 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 all KL = 0.706 +- 0.331 (in-sample avg dev_std = 0.410)
SUFF++ for r=0.8 all L1 = 0.727 +- 0.264 (in-sample avg dev_std = 0.410)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.815
Model XAI F1 of binarized graphs for r=0.8 =  0.32483249999999997
Model XAI WIoU of binarized graphs for r=0.8 =  0.97190125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.7
SUFF++ for r=0.8 class 0 = 0.621 +- 0.231 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.8 class 1 = 0.824 +- 0.231 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.8 class 2 = 0.584 +- 0.231 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.8 all KL = 0.715 +- 0.231 (in-sample avg dev_std = 0.421)
SUFF++ for r=0.8 all L1 = 0.675 +- 0.194 (in-sample avg dev_std = 0.421)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.642
Model XAI F1 of binarized graphs for r=0.8 =  0.42105750000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.49111374999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.516
SUFF++ for r=0.8 class 0 = 0.654 +- 0.363 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 class 1 = 0.723 +- 0.363 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 class 2 = 0.629 +- 0.363 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 all KL = 0.623 +- 0.363 (in-sample avg dev_std = 0.495)
SUFF++ for r=0.8 all L1 = 0.669 +- 0.200 (in-sample avg dev_std = 0.495)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.858
Model XAI F1 of binarized graphs for r=0.8 =  0.5163899999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.7159762500000001
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.457
NEC for r=0.8 class 0 = 0.593 +- 0.270 (in-sample avg dev_std = 0.711)
NEC for r=0.8 class 1 = 0.454 +- 0.270 (in-sample avg dev_std = 0.711)
NEC for r=0.8 class 2 = 0.663 +- 0.270 (in-sample avg dev_std = 0.711)
NEC for r=0.8 all KL = 0.71 +- 0.270 (in-sample avg dev_std = 0.711)
NEC for r=0.8 all L1 = 0.57 +- 0.205 (in-sample avg dev_std = 0.711)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.88
Model XAI F1 of binarized graphs for r=0.8 =  0.5143775
Model XAI WIoU of binarized graphs for r=0.8 =  0.7254775000000001
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.45
NEC for r=0.8 class 0 = 0.587 +- 0.255 (in-sample avg dev_std = 0.724)
NEC for r=0.8 class 1 = 0.468 +- 0.255 (in-sample avg dev_std = 0.724)
NEC for r=0.8 class 2 = 0.675 +- 0.255 (in-sample avg dev_std = 0.724)
NEC for r=0.8 all KL = 0.728 +- 0.255 (in-sample avg dev_std = 0.724)
NEC for r=0.8 all L1 = 0.576 +- 0.199 (in-sample avg dev_std = 0.724)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.815
Model XAI F1 of binarized graphs for r=0.8 =  0.32483249999999997
Model XAI WIoU of binarized graphs for r=0.8 =  0.97190125
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.502
NEC for r=0.8 class 0 = 0.528 +- 0.226 (in-sample avg dev_std = 0.655)
NEC for r=0.8 class 1 = 0.507 +- 0.226 (in-sample avg dev_std = 0.655)
NEC for r=0.8 class 2 = 0.578 +- 0.226 (in-sample avg dev_std = 0.655)
NEC for r=0.8 all KL = 0.591 +- 0.226 (in-sample avg dev_std = 0.655)
NEC for r=0.8 all L1 = 0.538 +- 0.143 (in-sample avg dev_std = 0.655)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.642
Model XAI F1 of binarized graphs for r=0.8 =  0.42105750000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.49111374999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.521
NEC for r=0.8 class 0 = 0.534 +- 0.317 (in-sample avg dev_std = 0.625)
NEC for r=0.8 class 1 = 0.345 +- 0.317 (in-sample avg dev_std = 0.625)
NEC for r=0.8 class 2 = 0.469 +- 0.317 (in-sample avg dev_std = 0.625)
NEC for r=0.8 all KL = 0.605 +- 0.317 (in-sample avg dev_std = 0.625)
NEC for r=0.8 all L1 = 0.447 +- 0.183 (in-sample avg dev_std = 0.625)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 16:58:33 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 04:58:33 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 17...
[0m[1;37mINFO[0m: [1mCheckpoint 17: 
-----------------------------------
Train ACCURACY: 0.9147
Train Loss: 0.4936
ID Validation ACCURACY: 0.9210
ID Validation Loss: 0.4879
ID Test ACCURACY: 0.9093
ID Test Loss: 0.5467
OOD Validation ACCURACY: 0.8387
OOD Validation Loss: 0.5746
OOD Test ACCURACY: 0.4087
OOD Test Loss: 32.4148

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 0.8200
Train Loss: 0.6115
ID Validation ACCURACY: 0.8157
ID Validation Loss: 0.6203
ID Test ACCURACY: 0.8157
ID Test Loss: 0.6734
OOD Validation ACCURACY: 0.8897
OOD Validation Loss: 0.4646
OOD Test ACCURACY: 0.3343
OOD Test Loss: 29.0064

[0m[1;37mINFO[0m: [1mChartInfo 0.9093 0.4087 0.8157 0.3343 0.8157 0.8897[0mGOODMotif2(18000)
Data example from train: Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from train: (tensor([0, 1, 2]), tensor([5925, 6060, 6015]))

Gold ratio (train) =  tensor(0.3124) +- tensor(0.1674)
F1 for r=0.8 = 0.522
WIoU for r=0.8 = 0.502
GOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.518
WIoU for r=0.8 = 0.501
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.324
WIoU for r=0.8 = 0.261
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.302
WIoU for r=0.8 = 0.200


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.923
Model XAI F1 of binarized graphs for r=0.8 =  0.52203125
Model XAI WIoU of binarized graphs for r=0.8 =  0.50218375
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.656
SUFF++ for r=0.8 class 0 = 0.478 +- 0.314 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.8 class 1 = 0.542 +- 0.314 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.8 class 2 = 0.9 +- 0.314 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.8 all KL = 0.623 +- 0.314 (in-sample avg dev_std = 0.519)
SUFF++ for r=0.8 all L1 = 0.64 +- 0.263 (in-sample avg dev_std = 0.519)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.925
Model XAI F1 of binarized graphs for r=0.8 =  0.51809375
Model XAI WIoU of binarized graphs for r=0.8 =  0.5013487499999999
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.657
SUFF++ for r=0.8 class 0 = 0.449 +- 0.321 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 class 1 = 0.564 +- 0.321 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 class 2 = 0.918 +- 0.321 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 all KL = 0.627 +- 0.321 (in-sample avg dev_std = 0.507)
SUFF++ for r=0.8 all L1 = 0.642 +- 0.269 (in-sample avg dev_std = 0.507)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.831
Model XAI F1 of binarized graphs for r=0.8 =  0.32363625
Model XAI WIoU of binarized graphs for r=0.8 =  0.2613325
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.523
SUFF++ for r=0.8 class 0 = 0.406 +- 0.268 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 class 1 = 0.466 +- 0.268 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 class 2 = 0.892 +- 0.268 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 all KL = 0.643 +- 0.268 (in-sample avg dev_std = 0.396)
SUFF++ for r=0.8 all L1 = 0.59 +- 0.270 (in-sample avg dev_std = 0.396)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.3022025
Model XAI WIoU of binarized graphs for r=0.8 =  0.19976875
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.345
SUFF++ for r=0.8 class 0 = 0.868 +- 0.189 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.8 class 1 = 0.848 +- 0.189 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.8 class 2 = 0.996 +- 0.189 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.8 all KL = 0.922 +- 0.189 (in-sample avg dev_std = 0.132)
SUFF++ for r=0.8 all L1 = 0.904 +- 0.225 (in-sample avg dev_std = 0.132)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.923
Model XAI F1 of binarized graphs for r=0.8 =  0.52203125
Model XAI WIoU of binarized graphs for r=0.8 =  0.50218375
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.459
NEC for r=0.8 class 0 = 0.659 +- 0.250 (in-sample avg dev_std = 0.711)
NEC for r=0.8 class 1 = 0.591 +- 0.250 (in-sample avg dev_std = 0.711)
NEC for r=0.8 class 2 = 0.568 +- 0.250 (in-sample avg dev_std = 0.711)
NEC for r=0.8 all KL = 0.77 +- 0.250 (in-sample avg dev_std = 0.711)
NEC for r=0.8 all L1 = 0.606 +- 0.181 (in-sample avg dev_std = 0.711)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.925
Model XAI F1 of binarized graphs for r=0.8 =  0.51809375
Model XAI WIoU of binarized graphs for r=0.8 =  0.5013487499999999
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.459
NEC for r=0.8 class 0 = 0.647 +- 0.240 (in-sample avg dev_std = 0.718)
NEC for r=0.8 class 1 = 0.599 +- 0.240 (in-sample avg dev_std = 0.718)
NEC for r=0.8 class 2 = 0.565 +- 0.240 (in-sample avg dev_std = 0.718)
NEC for r=0.8 all KL = 0.779 +- 0.240 (in-sample avg dev_std = 0.718)
NEC for r=0.8 all L1 = 0.604 +- 0.170 (in-sample avg dev_std = 0.718)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.831
Model XAI F1 of binarized graphs for r=0.8 =  0.32363625
Model XAI WIoU of binarized graphs for r=0.8 =  0.2613325
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.409
NEC for r=0.8 class 0 = 0.637 +- 0.226 (in-sample avg dev_std = 0.523)
NEC for r=0.8 class 1 = 0.618 +- 0.226 (in-sample avg dev_std = 0.523)
NEC for r=0.8 class 2 = 0.348 +- 0.226 (in-sample avg dev_std = 0.523)
NEC for r=0.8 all KL = 0.598 +- 0.226 (in-sample avg dev_std = 0.523)
NEC for r=0.8 all L1 = 0.533 +- 0.190 (in-sample avg dev_std = 0.523)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.3022025
Model XAI WIoU of binarized graphs for r=0.8 =  0.19976875
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.387
NEC for r=0.8 class 0 = 0.229 +- 0.421 (in-sample avg dev_std = 0.360)
NEC for r=0.8 class 1 = 0.199 +- 0.421 (in-sample avg dev_std = 0.360)
NEC for r=0.8 class 2 = 0.099 +- 0.421 (in-sample avg dev_std = 0.360)
NEC for r=0.8 all KL = 0.319 +- 0.421 (in-sample avg dev_std = 0.360)
NEC for r=0.8 all L1 = 0.175 +- 0.264 (in-sample avg dev_std = 0.360)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:00:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:00:01 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 17...
[0m[1;37mINFO[0m: [1mCheckpoint 17: 
-----------------------------------
Train ACCURACY: 0.9185
Train Loss: 0.4761
ID Validation ACCURACY: 0.9243
ID Validation Loss: 0.4624
ID Test ACCURACY: 0.9137
ID Test Loss: 0.5178
OOD Validation ACCURACY: 0.8713
OOD Validation Loss: 0.5049
OOD Test ACCURACY: 0.4087
OOD Test Loss: 34.3607

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 20...
[0m[1;37mINFO[0m: [1mCheckpoint 20: 
-----------------------------------
Train ACCURACY: 0.9192
Train Loss: 0.4721
ID Validation ACCURACY: 0.9240
ID Validation Loss: 0.4583
ID Test ACCURACY: 0.9157
ID Test Loss: 0.5307
OOD Validation ACCURACY: 0.9143
OOD Validation Loss: 0.4105
OOD Test ACCURACY: 0.4087
OOD Test Loss: 17.9440

[0m[1;37mINFO[0m: [1mChartInfo 0.9137 0.4087 0.9157 0.4087 0.9240 0.9143[0mGOODMotif2(18000)
Data example from train: Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from train: (tensor([0, 1, 2]), tensor([5925, 6060, 6015]))

Gold ratio (train) =  tensor(0.3124) +- tensor(0.1674)
F1 for r=0.8 = 0.524
WIoU for r=0.8 = 0.521
GOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.521
WIoU for r=0.8 = 0.522
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.324
WIoU for r=0.8 = 0.295
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.309
WIoU for r=0.8 = 0.202


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.924
Model XAI F1 of binarized graphs for r=0.8 =  0.52354125
Model XAI WIoU of binarized graphs for r=0.8 =  0.5206025000000001
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.746
SUFF++ for r=0.8 class 0 = 0.565 +- 0.298 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 class 1 = 0.629 +- 0.298 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 class 2 = 0.864 +- 0.298 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 all KL = 0.661 +- 0.298 (in-sample avg dev_std = 0.448)
SUFF++ for r=0.8 all L1 = 0.686 +- 0.245 (in-sample avg dev_std = 0.448)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.5207937499999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.52180375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.745
SUFF++ for r=0.8 class 0 = 0.537 +- 0.305 (in-sample avg dev_std = 0.455)
SUFF++ for r=0.8 class 1 = 0.636 +- 0.305 (in-sample avg dev_std = 0.455)
SUFF++ for r=0.8 class 2 = 0.889 +- 0.305 (in-sample avg dev_std = 0.455)
SUFF++ for r=0.8 all KL = 0.664 +- 0.305 (in-sample avg dev_std = 0.455)
SUFF++ for r=0.8 all L1 = 0.686 +- 0.251 (in-sample avg dev_std = 0.455)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.879
Model XAI F1 of binarized graphs for r=0.8 =  0.32405
Model XAI WIoU of binarized graphs for r=0.8 =  0.2951175
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.529
SUFF++ for r=0.8 class 0 = 0.379 +- 0.278 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 class 1 = 0.47 +- 0.278 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 class 2 = 0.847 +- 0.278 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 all KL = 0.599 +- 0.278 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 all L1 = 0.567 +- 0.261 (in-sample avg dev_std = 0.465)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.405
Model XAI F1 of binarized graphs for r=0.8 =  0.30855
Model XAI WIoU of binarized graphs for r=0.8 =  0.20215000000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.383
SUFF++ for r=0.8 class 0 = 0.871 +- 0.164 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.8 class 1 = 0.904 +- 0.164 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.8 class 2 = 0.973 +- 0.164 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.8 all KL = 0.919 +- 0.164 (in-sample avg dev_std = 0.194)
SUFF++ for r=0.8 all L1 = 0.916 +- 0.178 (in-sample avg dev_std = 0.194)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.924
Model XAI F1 of binarized graphs for r=0.8 =  0.52354125
Model XAI WIoU of binarized graphs for r=0.8 =  0.5206025000000001
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.452
NEC for r=0.8 class 0 = 0.672 +- 0.247 (in-sample avg dev_std = 0.713)
NEC for r=0.8 class 1 = 0.569 +- 0.247 (in-sample avg dev_std = 0.713)
NEC for r=0.8 class 2 = 0.592 +- 0.247 (in-sample avg dev_std = 0.713)
NEC for r=0.8 all KL = 0.774 +- 0.247 (in-sample avg dev_std = 0.713)
NEC for r=0.8 all L1 = 0.611 +- 0.182 (in-sample avg dev_std = 0.713)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.5207937499999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.52180375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.445
NEC for r=0.8 class 0 = 0.677 +- 0.236 (in-sample avg dev_std = 0.722)
NEC for r=0.8 class 1 = 0.577 +- 0.236 (in-sample avg dev_std = 0.722)
NEC for r=0.8 class 2 = 0.59 +- 0.236 (in-sample avg dev_std = 0.722)
NEC for r=0.8 all KL = 0.785 +- 0.236 (in-sample avg dev_std = 0.722)
NEC for r=0.8 all L1 = 0.615 +- 0.173 (in-sample avg dev_std = 0.722)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.879
Model XAI F1 of binarized graphs for r=0.8 =  0.32405
Model XAI WIoU of binarized graphs for r=0.8 =  0.2951175
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.413
NEC for r=0.8 class 0 = 0.693 +- 0.231 (in-sample avg dev_std = 0.557)
NEC for r=0.8 class 1 = 0.608 +- 0.231 (in-sample avg dev_std = 0.557)
NEC for r=0.8 class 2 = 0.382 +- 0.231 (in-sample avg dev_std = 0.557)
NEC for r=0.8 all KL = 0.632 +- 0.231 (in-sample avg dev_std = 0.557)
NEC for r=0.8 all L1 = 0.56 +- 0.190 (in-sample avg dev_std = 0.557)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.405
Model XAI F1 of binarized graphs for r=0.8 =  0.30855
Model XAI WIoU of binarized graphs for r=0.8 =  0.20215000000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.36
NEC for r=0.8 class 0 = 0.247 +- 0.452 (in-sample avg dev_std = 0.447)
NEC for r=0.8 class 1 = 0.228 +- 0.452 (in-sample avg dev_std = 0.447)
NEC for r=0.8 class 2 = 0.172 +- 0.452 (in-sample avg dev_std = 0.447)
NEC for r=0.8 all KL = 0.407 +- 0.452 (in-sample avg dev_std = 0.447)
NEC for r=0.8 all L1 = 0.216 +- 0.274 (in-sample avg dev_std = 0.447)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:01:29 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:01:29 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 183...
[0m[1;37mINFO[0m: [1mCheckpoint 183: 
-----------------------------------
Train ACCURACY: 0.8962
Train Loss: 0.4182
ID Validation ACCURACY: 0.9057
ID Validation Loss: 0.4121
ID Test ACCURACY: 0.8953
ID Test Loss: 0.4527
OOD Validation ACCURACY: 0.9027
OOD Validation Loss: 0.4390
OOD Test ACCURACY: 0.4087
OOD Test Loss: 28.0354

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 129...
[0m[1;37mINFO[0m: [1mCheckpoint 129: 
-----------------------------------
Train ACCURACY: 0.8330
Train Loss: 0.5186
ID Validation ACCURACY: 0.8323
ID Validation Loss: 0.5186
ID Test ACCURACY: 0.8267
ID Test Loss: 0.5417
OOD Validation ACCURACY: 0.9140
OOD Validation Loss: 0.4398
OOD Test ACCURACY: 0.4087
OOD Test Loss: 25.0431

[0m[1;37mINFO[0m: [1mChartInfo 0.8953 0.4087 0.8267 0.4087 0.8323 0.9140[0mGOODMotif2(18000)
Data example from train: Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from train: (tensor([0, 1, 2]), tensor([5925, 6060, 6015]))

Gold ratio (train) =  tensor(0.3124) +- tensor(0.1674)
F1 for r=0.8 = 0.478
WIoU for r=0.8 = 0.654
GOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))

Gold ratio (id_val) =  tensor(0.3098) +- tensor(0.1593)
F1 for r=0.8 = 0.481
WIoU for r=0.8 = 0.660
GOODMotif2(3000)
Data example from val: Data(edge_index=[2, 84], x=[27, 1], node_gt=[27], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=27)
Label distribution from val: (tensor([0, 1, 2]), tensor([1009,  978, 1013]))

Gold ratio (val) =  tensor(0.1589) +- tensor(0.0458)
F1 for r=0.8 = 0.300
WIoU for r=0.8 = 0.800
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.186
WIoU for r=0.8 = 0.101


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.902
Model XAI F1 of binarized graphs for r=0.8 =  0.47845000000000004
Model XAI WIoU of binarized graphs for r=0.8 =  0.6541075000000001
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.576
SUFF++ for r=0.8 class 0 = 0.342 +- 0.329 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.8 class 1 = 0.592 +- 0.329 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.8 class 2 = 0.593 +- 0.329 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.8 all KL = 0.47 +- 0.329 (in-sample avg dev_std = 0.451)
SUFF++ for r=0.8 all L1 = 0.51 +- 0.255 (in-sample avg dev_std = 0.451)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.914
Model XAI F1 of binarized graphs for r=0.8 =  0.4809975
Model XAI WIoU of binarized graphs for r=0.8 =  0.66007125
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.595
SUFF++ for r=0.8 class 0 = 0.339 +- 0.335 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 class 1 = 0.599 +- 0.335 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 class 2 = 0.63 +- 0.335 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 all KL = 0.487 +- 0.335 (in-sample avg dev_std = 0.446)
SUFF++ for r=0.8 all L1 = 0.522 +- 0.259 (in-sample avg dev_std = 0.446)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.908
Model XAI F1 of binarized graphs for r=0.8 =  0.3002225
Model XAI WIoU of binarized graphs for r=0.8 =  0.80019875
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.602
SUFF++ for r=0.8 class 0 = 0.382 +- 0.236 (in-sample avg dev_std = 0.426)
SUFF++ for r=0.8 class 1 = 0.665 +- 0.236 (in-sample avg dev_std = 0.426)
SUFF++ for r=0.8 class 2 = 0.737 +- 0.236 (in-sample avg dev_std = 0.426)
SUFF++ for r=0.8 all KL = 0.64 +- 0.236 (in-sample avg dev_std = 0.426)
SUFF++ for r=0.8 all L1 = 0.594 +- 0.221 (in-sample avg dev_std = 0.426)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.18561999999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.1005675
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.368
SUFF++ for r=0.8 class 0 = 0.899 +- 0.260 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 class 1 = 0.871 +- 0.260 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 class 2 = 0.931 +- 0.260 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all KL = 0.882 +- 0.260 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all L1 = 0.9 +- 0.199 (in-sample avg dev_std = 0.306)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.902
Model XAI F1 of binarized graphs for r=0.8 =  0.47845000000000004
Model XAI WIoU of binarized graphs for r=0.8 =  0.6541075000000001
len(reference) = 800
Effective ratio: 0.811 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.458
NEC for r=0.8 class 0 = 0.636 +- 0.240 (in-sample avg dev_std = 0.677)
NEC for r=0.8 class 1 = 0.582 +- 0.240 (in-sample avg dev_std = 0.677)
NEC for r=0.8 class 2 = 0.616 +- 0.240 (in-sample avg dev_std = 0.677)
NEC for r=0.8 all KL = 0.713 +- 0.240 (in-sample avg dev_std = 0.677)
NEC for r=0.8 all L1 = 0.611 +- 0.158 (in-sample avg dev_std = 0.677)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.914
Model XAI F1 of binarized graphs for r=0.8 =  0.4809975
Model XAI WIoU of binarized graphs for r=0.8 =  0.66007125
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.457
NEC for r=0.8 class 0 = 0.635 +- 0.234 (in-sample avg dev_std = 0.693)
NEC for r=0.8 class 1 = 0.591 +- 0.234 (in-sample avg dev_std = 0.693)
NEC for r=0.8 class 2 = 0.613 +- 0.234 (in-sample avg dev_std = 0.693)
NEC for r=0.8 all KL = 0.72 +- 0.234 (in-sample avg dev_std = 0.693)
NEC for r=0.8 all L1 = 0.613 +- 0.148 (in-sample avg dev_std = 0.693)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.908
Model XAI F1 of binarized graphs for r=0.8 =  0.3002225
Model XAI WIoU of binarized graphs for r=0.8 =  0.80019875
len(reference) = 800
Effective ratio: 0.806 +- 0.005
Model Accuracy over intervened graphs for r=0.8 =  0.496
NEC for r=0.8 class 0 = 0.571 +- 0.226 (in-sample avg dev_std = 0.585)
NEC for r=0.8 class 1 = 0.531 +- 0.226 (in-sample avg dev_std = 0.585)
NEC for r=0.8 class 2 = 0.502 +- 0.226 (in-sample avg dev_std = 0.585)
NEC for r=0.8 all KL = 0.56 +- 0.226 (in-sample avg dev_std = 0.585)
NEC for r=0.8 all L1 = 0.535 +- 0.122 (in-sample avg dev_std = 0.585)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.18561999999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.1005675
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.411
NEC for r=0.8 class 0 = 0.275 +- 0.446 (in-sample avg dev_std = 0.482)
NEC for r=0.8 class 1 = 0.234 +- 0.446 (in-sample avg dev_std = 0.482)
NEC for r=0.8 class 2 = 0.16 +- 0.446 (in-sample avg dev_std = 0.482)
NEC for r=0.8 all KL = 0.419 +- 0.446 (in-sample avg dev_std = 0.482)
NEC for r=0.8 all L1 = 0.222 +- 0.284 (in-sample avg dev_std = 0.482)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.602], 'all_L1': [0.637]}), defaultdict(<class 'list'>, {'all_KL': [0.694], 'all_L1': [0.72]}), defaultdict(<class 'list'>, {'all_KL': [0.623], 'all_L1': [0.64]}), defaultdict(<class 'list'>, {'all_KL': [0.661], 'all_L1': [0.686]}), defaultdict(<class 'list'>, {'all_KL': [0.47], 'all_L1': [0.51]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.723], 'all_L1': [0.612]}), defaultdict(<class 'list'>, {'all_KL': [0.71], 'all_L1': [0.57]}), defaultdict(<class 'list'>, {'all_KL': [0.77], 'all_L1': [0.606]}), defaultdict(<class 'list'>, {'all_KL': [0.774], 'all_L1': [0.611]}), defaultdict(<class 'list'>, {'all_KL': [0.713], 'all_L1': [0.611]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.623], 'all_L1': [0.653]}), defaultdict(<class 'list'>, {'all_KL': [0.706], 'all_L1': [0.727]}), defaultdict(<class 'list'>, {'all_KL': [0.627], 'all_L1': [0.642]}), defaultdict(<class 'list'>, {'all_KL': [0.664], 'all_L1': [0.686]}), defaultdict(<class 'list'>, {'all_KL': [0.487], 'all_L1': [0.522]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.727], 'all_L1': [0.61]}), defaultdict(<class 'list'>, {'all_KL': [0.728], 'all_L1': [0.576]}), defaultdict(<class 'list'>, {'all_KL': [0.779], 'all_L1': [0.604]}), defaultdict(<class 'list'>, {'all_KL': [0.785], 'all_L1': [0.615]}), defaultdict(<class 'list'>, {'all_KL': [0.72], 'all_L1': [0.613]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.599], 'all_L1': [0.574]}), defaultdict(<class 'list'>, {'all_KL': [0.715], 'all_L1': [0.675]}), defaultdict(<class 'list'>, {'all_KL': [0.643], 'all_L1': [0.59]}), defaultdict(<class 'list'>, {'all_KL': [0.599], 'all_L1': [0.567]}), defaultdict(<class 'list'>, {'all_KL': [0.64], 'all_L1': [0.594]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.58], 'all_L1': [0.542]}), defaultdict(<class 'list'>, {'all_KL': [0.591], 'all_L1': [0.538]}), defaultdict(<class 'list'>, {'all_KL': [0.598], 'all_L1': [0.533]}), defaultdict(<class 'list'>, {'all_KL': [0.632], 'all_L1': [0.56]}), defaultdict(<class 'list'>, {'all_KL': [0.56], 'all_L1': [0.535]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.785], 'all_L1': [0.872]}), defaultdict(<class 'list'>, {'all_KL': [0.623], 'all_L1': [0.669]}), defaultdict(<class 'list'>, {'all_KL': [0.922], 'all_L1': [0.904]}), defaultdict(<class 'list'>, {'all_KL': [0.919], 'all_L1': [0.916]}), defaultdict(<class 'list'>, {'all_KL': [0.882], 'all_L1': [0.9]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.393], 'all_L1': [0.221]}), defaultdict(<class 'list'>, {'all_KL': [0.605], 'all_L1': [0.447]}), defaultdict(<class 'list'>, {'all_KL': [0.319], 'all_L1': [0.175]}), defaultdict(<class 'list'>, {'all_KL': [0.407], 'all_L1': [0.216]}), defaultdict(<class 'list'>, {'all_KL': [0.419], 'all_L1': [0.222]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.639 +- 0.071
suff++ class all_KL  =  0.610 +- 0.077
suff++_acc_int  =  0.680 +- 0.060
nec class all_L1  =  0.602 +- 0.016
nec class all_KL  =  0.738 +- 0.028
nec_acc_int  =  0.452 +- 0.009

Eval split id_val
suff++ class all_L1  =  0.646 +- 0.069
suff++ class all_KL  =  0.621 +- 0.074
suff++_acc_int  =  0.689 +- 0.056
nec class all_L1  =  0.604 +- 0.014
nec class all_KL  =  0.748 +- 0.028
nec_acc_int  =  0.450 +- 0.008

Eval split val
suff++ class all_L1  =  0.600 +- 0.039
suff++ class all_KL  =  0.639 +- 0.042
suff++_acc_int  =  0.590 +- 0.064
nec class all_L1  =  0.542 +- 0.010
nec class all_KL  =  0.592 +- 0.024
nec_acc_int  =  0.463 +- 0.042

Eval split test
suff++ class all_L1  =  0.852 +- 0.093
suff++ class all_KL  =  0.826 +- 0.113
suff++_acc_int  =  0.401 +- 0.060
nec class all_L1  =  0.256 +- 0.097
nec class all_KL  =  0.429 +- 0.095
nec_acc_int  =  0.414 +- 0.056


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.620 +- 0.032
Faith. Armon (L1)= 		  =  0.617 +- 0.032
Faith. GMean (L1)= 	  =  0.619 +- 0.032
Faith. Aritm (KL)= 		  =  0.674 +- 0.045
Faith. Armon (KL)= 		  =  0.665 +- 0.053
Faith. GMean (KL)= 	  =  0.670 +- 0.049

Eval split id_val
Faith. Aritm (L1)= 		  =  0.625 +- 0.031
Faith. Armon (L1)= 		  =  0.622 +- 0.030
Faith. GMean (L1)= 	  =  0.623 +- 0.030
Faith. Aritm (KL)= 		  =  0.685 +- 0.044
Faith. Armon (KL)= 		  =  0.677 +- 0.051
Faith. GMean (KL)= 	  =  0.681 +- 0.047

Eval split val
Faith. Aritm (L1)= 		  =  0.571 +- 0.018
Faith. Armon (L1)= 		  =  0.569 +- 0.015
Faith. GMean (L1)= 	  =  0.570 +- 0.017
Faith. Aritm (KL)= 		  =  0.616 +- 0.022
Faith. Armon (KL)= 		  =  0.614 +- 0.020
Faith. GMean (KL)= 	  =  0.615 +- 0.021

Eval split test
Faith. Aritm (L1)= 		  =  0.554 +- 0.010
Faith. Armon (L1)= 		  =  0.378 +- 0.083
Faith. GMean (L1)= 	  =  0.455 +- 0.049
Faith. Aritm (KL)= 		  =  0.627 +- 0.026
Faith. Armon (KL)= 		  =  0.549 +- 0.047
Faith. GMean (KL)= 	  =  0.586 +- 0.031
Computed for split load_split = id



Completed in  0:07:23.511489  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:03:26 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:03:27 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 148...
[0m[1;37mINFO[0m: [1mCheckpoint 148: 
-----------------------------------
Train ACCURACY: 0.9479
Train Loss: 0.0807
ID Validation ACCURACY: 0.8570
ID Validation Loss: 0.6526
ID Test ACCURACY: 0.8504
ID Test Loss: 0.7123
OOD Validation ACCURACY: 0.8377
OOD Validation Loss: 0.8949
OOD Test ACCURACY: 0.7398
OOD Test Loss: 1.5170

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.9165
Train Loss: 0.1472
ID Validation ACCURACY: 0.8142
ID Validation Loss: 0.7736
ID Test ACCURACY: 0.8114
ID Test Loss: 0.8563
OOD Validation ACCURACY: 0.8439
OOD Validation Loss: 0.7952
OOD Test ACCURACY: 0.8108
OOD Test Loss: 0.9714

[0m[1;37mINFO[0m: [1mChartInfo 0.8504 0.7398 0.8114 0.8108 0.8142 0.8439[0mGOODSST2(24744)
Data example from train: Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([10099, 14645]))
[1;34mDEBUG[0m: 04/27/2024 05:03:29 PM : [1mUnbalanced warning for GOODSST2 (train)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/27/2024 05:03:31 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0mF1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.996
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.957
SUFF++ for r=0.8 class 0.0 = 0.838 +- 0.256 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 class 1.0 = 0.989 +- 0.256 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 all KL = 0.841 +- 0.256 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 all L1 = 0.924 +- 0.137 (in-sample avg dev_std = 0.292)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.841
SUFF++ for r=0.8 class 0.0 = 0.839 +- 0.217 (in-sample avg dev_std = 0.281)
SUFF++ for r=0.8 class 1.0 = 0.936 +- 0.217 (in-sample avg dev_std = 0.281)
SUFF++ for r=0.8 all KL = 0.866 +- 0.217 (in-sample avg dev_std = 0.281)
SUFF++ for r=0.8 all L1 = 0.895 +- 0.165 (in-sample avg dev_std = 0.281)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.841
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.813
SUFF++ for r=0.8 class 0.0 = 0.814 +- 0.196 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 class 1.0 = 0.943 +- 0.196 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 all KL = 0.871 +- 0.196 (in-sample avg dev_std = 0.262)
SUFF++ for r=0.8 all L1 = 0.881 +- 0.176 (in-sample avg dev_std = 0.262)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.693
SUFF++ for r=0.8 class 0.0 = 0.776 +- 0.195 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 class 1.0 = 0.963 +- 0.195 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 all KL = 0.879 +- 0.195 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 all L1 = 0.872 +- 0.189 (in-sample avg dev_std = 0.249)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.996
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.98
NEC for r=0.8 class 0.0 = 0.073 +- 0.163 (in-sample avg dev_std = 0.122)
NEC for r=0.8 class 1.0 = 0.012 +- 0.163 (in-sample avg dev_std = 0.122)
NEC for r=0.8 all KL = 0.054 +- 0.163 (in-sample avg dev_std = 0.122)
NEC for r=0.8 all L1 = 0.038 +- 0.121 (in-sample avg dev_std = 0.122)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.862
NEC for r=0.8 class 0.0 = 0.118 +- 0.172 (in-sample avg dev_std = 0.155)
NEC for r=0.8 class 1.0 = 0.05 +- 0.172 (in-sample avg dev_std = 0.155)
NEC for r=0.8 all KL = 0.071 +- 0.172 (in-sample avg dev_std = 0.155)
NEC for r=0.8 all L1 = 0.078 +- 0.156 (in-sample avg dev_std = 0.155)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.841
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.817
NEC for r=0.8 class 0.0 = 0.125 +- 0.139 (in-sample avg dev_std = 0.169)
NEC for r=0.8 class 1.0 = 0.038 +- 0.139 (in-sample avg dev_std = 0.169)
NEC for r=0.8 all KL = 0.061 +- 0.139 (in-sample avg dev_std = 0.169)
NEC for r=0.8 all L1 = 0.079 +- 0.149 (in-sample avg dev_std = 0.169)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.722
NEC for r=0.8 class 0.0 = 0.165 +- 0.149 (in-sample avg dev_std = 0.182)
NEC for r=0.8 class 1.0 = 0.03 +- 0.149 (in-sample avg dev_std = 0.182)
NEC for r=0.8 all KL = 0.071 +- 0.149 (in-sample avg dev_std = 0.182)
NEC for r=0.8 all L1 = 0.096 +- 0.163 (in-sample avg dev_std = 0.182)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:04:36 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:04:37 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:04:38 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 160...
[0m[1;37mINFO[0m: [1mCheckpoint 160: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0768
ID Validation ACCURACY: 0.8587
ID Validation Loss: 0.7200
ID Test ACCURACY: 0.8551
ID Test Loss: 0.8849
OOD Validation ACCURACY: 0.8472
OOD Validation Loss: 1.0741
OOD Test ACCURACY: 0.7967
OOD Test Loss: 1.3148

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 162...
[0m[1;37mINFO[0m: [1mCheckpoint 162: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0760
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.7569
ID Test ACCURACY: 0.8551
ID Test Loss: 0.9395
OOD Validation ACCURACY: 0.8551
OOD Validation Loss: 1.0513
OOD Test ACCURACY: 0.8051
OOD Test Loss: 1.2667

[0m[1;37mINFO[0m: [1mChartInfo 0.8551 0.7967 0.8551 0.8051 0.8581 0.8551[0mGOODSST2(24744)
Data example from train: Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([10099, 14645]))
[1;34mDEBUG[0m: 04/27/2024 05:04:38 PM : [1mUnbalanced warning for GOODSST2 (train)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/27/2024 05:04:41 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0mF1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  1.0
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.955
SUFF++ for r=0.8 class 0.0 = 0.999 +- 0.248 (in-sample avg dev_std = 0.267)
SUFF++ for r=0.8 class 1.0 = 0.87 +- 0.248 (in-sample avg dev_std = 0.267)
SUFF++ for r=0.8 all KL = 0.857 +- 0.248 (in-sample avg dev_std = 0.267)
SUFF++ for r=0.8 all L1 = 0.925 +- 0.148 (in-sample avg dev_std = 0.267)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.85
SUFF++ for r=0.8 class 0.0 = 0.931 +- 0.223 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.8 class 1.0 = 0.884 +- 0.223 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.8 all KL = 0.866 +- 0.223 (in-sample avg dev_std = 0.269)
SUFF++ for r=0.8 all L1 = 0.903 +- 0.173 (in-sample avg dev_std = 0.269)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.86
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.834
SUFF++ for r=0.8 class 0.0 = 0.944 +- 0.191 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.8 class 1.0 = 0.859 +- 0.191 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.8 all KL = 0.887 +- 0.191 (in-sample avg dev_std = 0.241)
SUFF++ for r=0.8 all L1 = 0.9 +- 0.166 (in-sample avg dev_std = 0.241)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.811
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.793
SUFF++ for r=0.8 class 0.0 = 0.919 +- 0.179 (in-sample avg dev_std = 0.247)
SUFF++ for r=0.8 class 1.0 = 0.83 +- 0.179 (in-sample avg dev_std = 0.247)
SUFF++ for r=0.8 all KL = 0.883 +- 0.179 (in-sample avg dev_std = 0.247)
SUFF++ for r=0.8 all L1 = 0.873 +- 0.186 (in-sample avg dev_std = 0.247)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  1.0
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.972
NEC for r=0.8 class 0.0 = 0.0 +- 0.196 (in-sample avg dev_std = 0.176)
NEC for r=0.8 class 1.0 = 0.086 +- 0.196 (in-sample avg dev_std = 0.176)
NEC for r=0.8 all KL = 0.073 +- 0.196 (in-sample avg dev_std = 0.176)
NEC for r=0.8 all L1 = 0.049 +- 0.137 (in-sample avg dev_std = 0.176)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.84
NEC for r=0.8 class 0.0 = 0.06 +- 0.212 (in-sample avg dev_std = 0.198)
NEC for r=0.8 class 1.0 = 0.115 +- 0.212 (in-sample avg dev_std = 0.198)
NEC for r=0.8 all KL = 0.098 +- 0.212 (in-sample avg dev_std = 0.198)
NEC for r=0.8 all L1 = 0.092 +- 0.188 (in-sample avg dev_std = 0.198)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.86
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.849
NEC for r=0.8 class 0.0 = 0.049 +- 0.155 (in-sample avg dev_std = 0.164)
NEC for r=0.8 class 1.0 = 0.101 +- 0.155 (in-sample avg dev_std = 0.164)
NEC for r=0.8 all KL = 0.066 +- 0.155 (in-sample avg dev_std = 0.164)
NEC for r=0.8 all L1 = 0.076 +- 0.154 (in-sample avg dev_std = 0.164)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.811
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.802
NEC for r=0.8 class 0.0 = 0.072 +- 0.176 (in-sample avg dev_std = 0.201)
NEC for r=0.8 class 1.0 = 0.138 +- 0.176 (in-sample avg dev_std = 0.201)
NEC for r=0.8 all KL = 0.087 +- 0.176 (in-sample avg dev_std = 0.201)
NEC for r=0.8 all L1 = 0.106 +- 0.181 (in-sample avg dev_std = 0.201)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:05:47 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 149...
[0m[1;37mINFO[0m: [1mCheckpoint 149: 
-----------------------------------
Train ACCURACY: 0.9479
Train Loss: 0.0798
ID Validation ACCURACY: 0.8600
ID Validation Loss: 0.6948
ID Test ACCURACY: 0.8576
ID Test Loss: 0.7845
OOD Validation ACCURACY: 0.8474
OOD Validation Loss: 1.0437
OOD Test ACCURACY: 0.8045
OOD Test Loss: 1.1397

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 140...
[0m[1;37mINFO[0m: [1mCheckpoint 140: 
-----------------------------------
Train ACCURACY: 0.9461
Train Loss: 0.0840
ID Validation ACCURACY: 0.8459
ID Validation Loss: 0.7816
ID Test ACCURACY: 0.8498
ID Test Loss: 0.8524
OOD Validation ACCURACY: 0.8581
OOD Validation Loss: 0.9137
OOD Test ACCURACY: 0.8097
OOD Test Loss: 1.0383

[0m[1;37mINFO[0m: [1mChartInfo 0.8576 0.8045 0.8498 0.8097 0.8459 0.8581[0mGOODSST2(24744)
Data example from train: Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([10099, 14645]))
[1;34mDEBUG[0m: 04/27/2024 05:05:49 PM : [1mUnbalanced warning for GOODSST2 (train)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/27/2024 05:05:51 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0mF1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  1.0
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.972
SUFF++ for r=0.8 class 0.0 = 0.992 +- 0.249 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.8 class 1.0 = 0.898 +- 0.249 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.8 all KL = 0.852 +- 0.249 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.8 all L1 = 0.938 +- 0.119 (in-sample avg dev_std = 0.256)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.846
SUFF++ for r=0.8 class 0.0 = 0.933 +- 0.251 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 class 1.0 = 0.869 +- 0.251 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all KL = 0.84 +- 0.251 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all L1 = 0.896 +- 0.173 (in-sample avg dev_std = 0.306)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.849
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.836
SUFF++ for r=0.8 class 0.0 = 0.94 +- 0.191 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 class 1.0 = 0.861 +- 0.191 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 all KL = 0.885 +- 0.191 (in-sample avg dev_std = 0.249)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.172 (in-sample avg dev_std = 0.249)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.794
SUFF++ for r=0.8 class 0.0 = 0.922 +- 0.173 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.8 class 1.0 = 0.836 +- 0.173 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.8 all KL = 0.891 +- 0.173 (in-sample avg dev_std = 0.227)
SUFF++ for r=0.8 all L1 = 0.878 +- 0.183 (in-sample avg dev_std = 0.227)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  1.0
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.979
NEC for r=0.8 class 0.0 = 0.003 +- 0.193 (in-sample avg dev_std = 0.147)
NEC for r=0.8 class 1.0 = 0.065 +- 0.193 (in-sample avg dev_std = 0.147)
NEC for r=0.8 all KL = 0.064 +- 0.193 (in-sample avg dev_std = 0.147)
NEC for r=0.8 all L1 = 0.038 +- 0.122 (in-sample avg dev_std = 0.147)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.86
NEC for r=0.8 class 0.0 = 0.065 +- 0.186 (in-sample avg dev_std = 0.193)
NEC for r=0.8 class 1.0 = 0.077 +- 0.186 (in-sample avg dev_std = 0.193)
NEC for r=0.8 all KL = 0.078 +- 0.186 (in-sample avg dev_std = 0.193)
NEC for r=0.8 all L1 = 0.072 +- 0.156 (in-sample avg dev_std = 0.193)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.849
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.849
NEC for r=0.8 class 0.0 = 0.051 +- 0.156 (in-sample avg dev_std = 0.177)
NEC for r=0.8 class 1.0 = 0.089 +- 0.156 (in-sample avg dev_std = 0.177)
NEC for r=0.8 all KL = 0.062 +- 0.156 (in-sample avg dev_std = 0.177)
NEC for r=0.8 all L1 = 0.07 +- 0.148 (in-sample avg dev_std = 0.177)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.811
NEC for r=0.8 class 0.0 = 0.072 +- 0.169 (in-sample avg dev_std = 0.200)
NEC for r=0.8 class 1.0 = 0.134 +- 0.169 (in-sample avg dev_std = 0.200)
NEC for r=0.8 all KL = 0.085 +- 0.169 (in-sample avg dev_std = 0.200)
NEC for r=0.8 all L1 = 0.104 +- 0.172 (in-sample avg dev_std = 0.200)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:06:57 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:06:58 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 113...
[0m[1;37mINFO[0m: [1mCheckpoint 113: 
-----------------------------------
Train ACCURACY: 0.9483
Train Loss: 0.0792
ID Validation ACCURACY: 0.8604
ID Validation Loss: 0.6685
ID Test ACCURACY: 0.8570
ID Test Loss: 0.7609
OOD Validation ACCURACY: 0.8537
OOD Validation Loss: 0.9443
OOD Test ACCURACY: 0.8069
OOD Test Loss: 1.0629

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 114...
[0m[1;37mINFO[0m: [1mCheckpoint 114: 
-----------------------------------
Train ACCURACY: 0.9462
Train Loss: 0.0915
ID Validation ACCURACY: 0.8478
ID Validation Loss: 0.6421
ID Test ACCURACY: 0.8480
ID Test Loss: 0.7499
OOD Validation ACCURACY: 0.8555
OOD Validation Loss: 0.7389
OOD Test ACCURACY: 0.8109
OOD Test Loss: 0.7657

[0m[1;37mINFO[0m: [1mChartInfo 0.8570 0.8069 0.8480 0.8109 0.8478 0.8555[0mGOODSST2(24744)
Data example from train: Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([10099, 14645]))
[1;34mDEBUG[0m: 04/27/2024 05:06:59 PM : [1mUnbalanced warning for GOODSST2 (train)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/27/2024 05:07:01 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0mF1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  1.0
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.977
SUFF++ for r=0.8 class 0.0 = 0.991 +- 0.188 (in-sample avg dev_std = 0.220)
SUFF++ for r=0.8 class 1.0 = 0.909 +- 0.188 (in-sample avg dev_std = 0.220)
SUFF++ for r=0.8 all KL = 0.904 +- 0.188 (in-sample avg dev_std = 0.220)
SUFF++ for r=0.8 all L1 = 0.944 +- 0.108 (in-sample avg dev_std = 0.220)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.847
SUFF++ for r=0.8 class 0.0 = 0.94 +- 0.178 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 class 1.0 = 0.895 +- 0.178 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 all KL = 0.908 +- 0.178 (in-sample avg dev_std = 0.239)
SUFF++ for r=0.8 all L1 = 0.914 +- 0.156 (in-sample avg dev_std = 0.239)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.86
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.842
SUFF++ for r=0.8 class 0.0 = 0.943 +- 0.144 (in-sample avg dev_std = 0.199)
SUFF++ for r=0.8 class 1.0 = 0.893 +- 0.144 (in-sample avg dev_std = 0.199)
SUFF++ for r=0.8 all KL = 0.928 +- 0.144 (in-sample avg dev_std = 0.199)
SUFF++ for r=0.8 all L1 = 0.917 +- 0.148 (in-sample avg dev_std = 0.199)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.824
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.814
SUFF++ for r=0.8 class 0.0 = 0.932 +- 0.129 (in-sample avg dev_std = 0.191)
SUFF++ for r=0.8 class 1.0 = 0.874 +- 0.129 (in-sample avg dev_std = 0.191)
SUFF++ for r=0.8 all KL = 0.93 +- 0.129 (in-sample avg dev_std = 0.191)
SUFF++ for r=0.8 all L1 = 0.902 +- 0.148 (in-sample avg dev_std = 0.191)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  1.0
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.985
NEC for r=0.8 class 0.0 = 0.005 +- 0.133 (in-sample avg dev_std = 0.103)
NEC for r=0.8 class 1.0 = 0.051 +- 0.133 (in-sample avg dev_std = 0.103)
NEC for r=0.8 all KL = 0.039 +- 0.133 (in-sample avg dev_std = 0.103)
NEC for r=0.8 all L1 = 0.032 +- 0.101 (in-sample avg dev_std = 0.103)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.846
NEC for r=0.8 class 0.0 = 0.066 +- 0.190 (in-sample avg dev_std = 0.175)
NEC for r=0.8 class 1.0 = 0.105 +- 0.190 (in-sample avg dev_std = 0.175)
NEC for r=0.8 all KL = 0.081 +- 0.190 (in-sample avg dev_std = 0.175)
NEC for r=0.8 all L1 = 0.089 +- 0.174 (in-sample avg dev_std = 0.175)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.86
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.852
NEC for r=0.8 class 0.0 = 0.058 +- 0.138 (in-sample avg dev_std = 0.166)
NEC for r=0.8 class 1.0 = 0.079 +- 0.138 (in-sample avg dev_std = 0.166)
NEC for r=0.8 all KL = 0.053 +- 0.138 (in-sample avg dev_std = 0.166)
NEC for r=0.8 all L1 = 0.069 +- 0.144 (in-sample avg dev_std = 0.166)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.824
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.825
NEC for r=0.8 class 0.0 = 0.08 +- 0.154 (in-sample avg dev_std = 0.188)
NEC for r=0.8 class 1.0 = 0.117 +- 0.154 (in-sample avg dev_std = 0.188)
NEC for r=0.8 all KL = 0.073 +- 0.154 (in-sample avg dev_std = 0.188)
NEC for r=0.8 all L1 = 0.099 +- 0.164 (in-sample avg dev_std = 0.188)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:08:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 144...
[0m[1;37mINFO[0m: [1mCheckpoint 144: 
-----------------------------------
Train ACCURACY: 0.9443
Train Loss: 0.0872
ID Validation ACCURACY: 0.8447
ID Validation Loss: 0.7583
ID Test ACCURACY: 0.8393
ID Test Loss: 0.8682
OOD Validation ACCURACY: 0.7661
OOD Validation Loss: 1.6477
OOD Test ACCURACY: 0.6583
OOD Test Loss: 3.1226

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 92...
[0m[1;37mINFO[0m: [1mCheckpoint 92: 
-----------------------------------
Train ACCURACY: 0.9332
Train Loss: 0.1117
ID Validation ACCURACY: 0.8306
ID Validation Loss: 0.6694
ID Test ACCURACY: 0.8312
ID Test Loss: 0.7076
OOD Validation ACCURACY: 0.8233
OOD Validation Loss: 0.9547
OOD Test ACCURACY: 0.7755
OOD Test Loss: 1.3760

[0m[1;37mINFO[0m: [1mChartInfo 0.8393 0.6583 0.8312 0.7755 0.8306 0.8233[0mGOODSST2(24744)
Data example from train: Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([10099, 14645]))
[1;34mDEBUG[0m: 04/27/2024 05:08:09 PM : [1mUnbalanced warning for GOODSST2 (train)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 04/27/2024 05:08:12 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0mF1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17206)
Data example from val: Data(x=[8, 768], edge_index=[2, 14], y=[1, 1], idx=[1], sentence_tokens=[8], length=[1], domain_id=[1])
Label distribution from val: (tensor([0., 1.]), tensor([8233, 8973]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.985
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.897
SUFF++ for r=0.8 class 0.0 = 0.687 +- 0.369 (in-sample avg dev_std = 0.406)
SUFF++ for r=0.8 class 1.0 = 0.996 +- 0.369 (in-sample avg dev_std = 0.406)
SUFF++ for r=0.8 all KL = 0.741 +- 0.369 (in-sample avg dev_std = 0.406)
SUFF++ for r=0.8 all L1 = 0.864 +- 0.220 (in-sample avg dev_std = 0.406)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.814
SUFF++ for r=0.8 class 0.0 = 0.753 +- 0.304 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.8 class 1.0 = 0.95 +- 0.304 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.8 all KL = 0.786 +- 0.304 (in-sample avg dev_std = 0.366)
SUFF++ for r=0.8 all L1 = 0.868 +- 0.204 (in-sample avg dev_std = 0.366)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.772
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.716
SUFF++ for r=0.8 class 0.0 = 0.741 +- 0.282 (in-sample avg dev_std = 0.300)
SUFF++ for r=0.8 class 1.0 = 0.97 +- 0.282 (in-sample avg dev_std = 0.300)
SUFF++ for r=0.8 all KL = 0.818 +- 0.282 (in-sample avg dev_std = 0.300)
SUFF++ for r=0.8 all L1 = 0.861 +- 0.222 (in-sample avg dev_std = 0.300)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.617
SUFF++ for r=0.8 class 0.0 = 0.781 +- 0.254 (in-sample avg dev_std = 0.251)
SUFF++ for r=0.8 class 1.0 = 0.979 +- 0.254 (in-sample avg dev_std = 0.251)
SUFF++ for r=0.8 all KL = 0.864 +- 0.254 (in-sample avg dev_std = 0.251)
SUFF++ for r=0.8 all L1 = 0.883 +- 0.212 (in-sample avg dev_std = 0.251)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.985
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 273
Effective ratio: 0.812 +- 0.287
Model Accuracy over intervened graphs for r=0.8 =  0.946
NEC for r=0.8 class 0.0 = 0.163 +- 0.257 (in-sample avg dev_std = 0.198)
NEC for r=0.8 class 1.0 = 0.002 +- 0.257 (in-sample avg dev_std = 0.198)
NEC for r=0.8 all KL = 0.109 +- 0.257 (in-sample avg dev_std = 0.198)
NEC for r=0.8 all L1 = 0.071 +- 0.182 (in-sample avg dev_std = 0.198)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.825
NEC for r=0.8 class 0.0 = 0.174 +- 0.227 (in-sample avg dev_std = 0.199)
NEC for r=0.8 class 1.0 = 0.029 +- 0.227 (in-sample avg dev_std = 0.199)
NEC for r=0.8 all KL = 0.095 +- 0.227 (in-sample avg dev_std = 0.199)
NEC for r=0.8 all L1 = 0.09 +- 0.195 (in-sample avg dev_std = 0.199)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.772
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.824 +- 0.019
Model Accuracy over intervened graphs for r=0.8 =  0.768
NEC for r=0.8 class 0.0 = 0.137 +- 0.160 (in-sample avg dev_std = 0.186)
NEC for r=0.8 class 1.0 = 0.016 +- 0.160 (in-sample avg dev_std = 0.186)
NEC for r=0.8 all KL = 0.065 +- 0.160 (in-sample avg dev_std = 0.186)
NEC for r=0.8 all L1 = 0.074 +- 0.157 (in-sample avg dev_std = 0.186)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.659
NEC for r=0.8 class 0.0 = 0.161 +- 0.189 (in-sample avg dev_std = 0.189)
NEC for r=0.8 class 1.0 = 0.013 +- 0.189 (in-sample avg dev_std = 0.189)
NEC for r=0.8 all KL = 0.08 +- 0.189 (in-sample avg dev_std = 0.189)
NEC for r=0.8 all L1 = 0.084 +- 0.175 (in-sample avg dev_std = 0.189)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.841], 'all_L1': [0.924]}), defaultdict(<class 'list'>, {'all_KL': [0.857], 'all_L1': [0.925]}), defaultdict(<class 'list'>, {'all_KL': [0.852], 'all_L1': [0.938]}), defaultdict(<class 'list'>, {'all_KL': [0.904], 'all_L1': [0.944]}), defaultdict(<class 'list'>, {'all_KL': [0.741], 'all_L1': [0.864]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.054], 'all_L1': [0.038]}), defaultdict(<class 'list'>, {'all_KL': [0.073], 'all_L1': [0.049]}), defaultdict(<class 'list'>, {'all_KL': [0.064], 'all_L1': [0.038]}), defaultdict(<class 'list'>, {'all_KL': [0.039], 'all_L1': [0.032]}), defaultdict(<class 'list'>, {'all_KL': [0.109], 'all_L1': [0.071]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.866], 'all_L1': [0.895]}), defaultdict(<class 'list'>, {'all_KL': [0.866], 'all_L1': [0.903]}), defaultdict(<class 'list'>, {'all_KL': [0.84], 'all_L1': [0.896]}), defaultdict(<class 'list'>, {'all_KL': [0.908], 'all_L1': [0.914]}), defaultdict(<class 'list'>, {'all_KL': [0.786], 'all_L1': [0.868]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.071], 'all_L1': [0.078]}), defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.092]}), defaultdict(<class 'list'>, {'all_KL': [0.078], 'all_L1': [0.072]}), defaultdict(<class 'list'>, {'all_KL': [0.081], 'all_L1': [0.089]}), defaultdict(<class 'list'>, {'all_KL': [0.095], 'all_L1': [0.09]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.871], 'all_L1': [0.881]}), defaultdict(<class 'list'>, {'all_KL': [0.887], 'all_L1': [0.9]}), defaultdict(<class 'list'>, {'all_KL': [0.885], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.928], 'all_L1': [0.917]}), defaultdict(<class 'list'>, {'all_KL': [0.818], 'all_L1': [0.861]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.061], 'all_L1': [0.079]}), defaultdict(<class 'list'>, {'all_KL': [0.066], 'all_L1': [0.076]}), defaultdict(<class 'list'>, {'all_KL': [0.062], 'all_L1': [0.07]}), defaultdict(<class 'list'>, {'all_KL': [0.053], 'all_L1': [0.069]}), defaultdict(<class 'list'>, {'all_KL': [0.065], 'all_L1': [0.074]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.879], 'all_L1': [0.872]}), defaultdict(<class 'list'>, {'all_KL': [0.883], 'all_L1': [0.873]}), defaultdict(<class 'list'>, {'all_KL': [0.891], 'all_L1': [0.878]}), defaultdict(<class 'list'>, {'all_KL': [0.93], 'all_L1': [0.902]}), defaultdict(<class 'list'>, {'all_KL': [0.864], 'all_L1': [0.883]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.071], 'all_L1': [0.096]}), defaultdict(<class 'list'>, {'all_KL': [0.087], 'all_L1': [0.106]}), defaultdict(<class 'list'>, {'all_KL': [0.085], 'all_L1': [0.104]}), defaultdict(<class 'list'>, {'all_KL': [0.073], 'all_L1': [0.099]}), defaultdict(<class 'list'>, {'all_KL': [0.08], 'all_L1': [0.084]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.919 +- 0.029
suff++ class all_KL  =  0.839 +- 0.054
suff++_acc_int  =  0.952 +- 0.028
nec class all_L1  =  0.046 +- 0.014
nec class all_KL  =  0.068 +- 0.023
nec_acc_int  =  0.973 +- 0.014

Eval split id_val
suff++ class all_L1  =  0.895 +- 0.015
suff++ class all_KL  =  0.853 +- 0.040
suff++_acc_int  =  0.840 +- 0.013
nec class all_L1  =  0.084 +- 0.008
nec class all_KL  =  0.085 +- 0.010
nec_acc_int  =  0.847 +- 0.014

Eval split val
suff++ class all_L1  =  0.892 +- 0.019
suff++ class all_KL  =  0.878 +- 0.035
suff++_acc_int  =  0.808 +- 0.047
nec class all_L1  =  0.074 +- 0.004
nec class all_KL  =  0.061 +- 0.005
nec_acc_int  =  0.827 +- 0.032

Eval split test
suff++ class all_L1  =  0.882 +- 0.011
suff++ class all_KL  =  0.889 +- 0.022
suff++_acc_int  =  0.742 +- 0.076
nec class all_L1  =  0.098 +- 0.008
nec class all_KL  =  0.079 +- 0.006
nec_acc_int  =  0.764 +- 0.063


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.482 +- 0.008
Faith. Armon (L1)= 		  =  0.086 +- 0.025
Faith. GMean (L1)= 	  =  0.202 +- 0.026
Faith. Aritm (KL)= 		  =  0.453 +- 0.016
Faith. Armon (KL)= 		  =  0.124 +- 0.039
Faith. GMean (KL)= 	  =  0.234 +- 0.033

Eval split id_val
Faith. Aritm (L1)= 		  =  0.490 +- 0.008
Faith. Armon (L1)= 		  =  0.154 +- 0.013
Faith. GMean (L1)= 	  =  0.274 +- 0.013
Faith. Aritm (KL)= 		  =  0.469 +- 0.019
Faith. Armon (KL)= 		  =  0.154 +- 0.017
Faith. GMean (KL)= 	  =  0.268 +- 0.015

Eval split val
Faith. Aritm (L1)= 		  =  0.483 +- 0.009
Faith. Armon (L1)= 		  =  0.136 +- 0.006
Faith. GMean (L1)= 	  =  0.256 +- 0.005
Faith. Aritm (KL)= 		  =  0.470 +- 0.016
Faith. Armon (KL)= 		  =  0.115 +- 0.008
Faith. GMean (KL)= 	  =  0.232 +- 0.007

Eval split test
Faith. Aritm (L1)= 		  =  0.490 +- 0.006
Faith. Armon (L1)= 		  =  0.176 +- 0.013
Faith. GMean (L1)= 	  =  0.293 +- 0.012
Faith. Aritm (KL)= 		  =  0.484 +- 0.010
Faith. Armon (KL)= 		  =  0.145 +- 0.011
Faith. GMean (KL)= 	  =  0.265 +- 0.010
Computed for split load_split = id



Completed in  0:05:54.011984  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:09:48 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:49 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:50 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:51 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:51 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:53 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:09:54 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 67...
[0m[1;37mINFO[0m: [1mCheckpoint 67: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.7004
ID Validation Loss: 1.4395
ID Test ACCURACY: 0.6498
ID Test Loss: 1.7556
OOD Validation ACCURACY: 0.6269
OOD Validation Loss: 1.7485
OOD Test ACCURACY: 0.5676
OOD Test Loss: 2.4674

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 144...
[0m[1;37mINFO[0m: [1mCheckpoint 144: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6931
ID Validation Loss: 1.6611
ID Test ACCURACY: 0.6588
ID Test Loss: 1.9403
OOD Validation ACCURACY: 0.6409
OOD Validation Loss: 2.1076
OOD Test ACCURACY: 0.5813
OOD Test Loss: 2.4255

[0m[1;37mINFO[0m: [1mChartInfo 0.6498 0.5676 0.6588 0.5813 0.6931 0.6409[0mGOODTwitter(2590)
Data example from train: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from train: (tensor([0, 1, 2]), tensor([ 625, 1283,  682]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.948
SUFF++ for r=0.6 class 0 = 0.879 +- 0.337 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.6 class 1 = 0.914 +- 0.337 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.6 class 2 = 0.877 +- 0.337 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.6 all KL = 0.679 +- 0.337 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.6 all L1 = 0.896 +- 0.125 (in-sample avg dev_std = 0.357)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.658
SUFF++ for r=0.6 class 0 = 0.758 +- 0.255 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.6 class 1 = 0.81 +- 0.255 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.6 class 2 = 0.754 +- 0.255 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.6 all KL = 0.708 +- 0.255 (in-sample avg dev_std = 0.433)
SUFF++ for r=0.6 all L1 = 0.782 +- 0.176 (in-sample avg dev_std = 0.433)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.629
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.627
SUFF++ for r=0.6 class 0 = 0.792 +- 0.193 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 class 1 = 0.814 +- 0.193 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 class 2 = 0.766 +- 0.193 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 all KL = 0.8 +- 0.193 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 all L1 = 0.798 +- 0.182 (in-sample avg dev_std = 0.341)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.57
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.553
SUFF++ for r=0.6 class 0 = 0.817 +- 0.190 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 class 1 = 0.804 +- 0.190 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 class 2 = 0.76 +- 0.190 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 all KL = 0.804 +- 0.190 (in-sample avg dev_std = 0.344)
SUFF++ for r=0.6 all L1 = 0.796 +- 0.178 (in-sample avg dev_std = 0.344)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.994
NEC for r=0.6 class 0 = 0.019 +- 0.140 (in-sample avg dev_std = 0.084)
NEC for r=0.6 class 1 = 0.006 +- 0.140 (in-sample avg dev_std = 0.084)
NEC for r=0.6 class 2 = 0.024 +- 0.140 (in-sample avg dev_std = 0.084)
NEC for r=0.6 all KL = 0.034 +- 0.140 (in-sample avg dev_std = 0.084)
NEC for r=0.6 all L1 = 0.014 +- 0.065 (in-sample avg dev_std = 0.084)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.683
NEC for r=0.6 class 0 = 0.145 +- 0.186 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 1 = 0.116 +- 0.186 (in-sample avg dev_std = 0.208)
NEC for r=0.6 class 2 = 0.176 +- 0.186 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all KL = 0.107 +- 0.186 (in-sample avg dev_std = 0.208)
NEC for r=0.6 all L1 = 0.14 +- 0.192 (in-sample avg dev_std = 0.208)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.629
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.634
NEC for r=0.6 class 0 = 0.198 +- 0.225 (in-sample avg dev_std = 0.241)
NEC for r=0.6 class 1 = 0.181 +- 0.225 (in-sample avg dev_std = 0.241)
NEC for r=0.6 class 2 = 0.213 +- 0.225 (in-sample avg dev_std = 0.241)
NEC for r=0.6 all KL = 0.159 +- 0.225 (in-sample avg dev_std = 0.241)
NEC for r=0.6 all L1 = 0.192 +- 0.219 (in-sample avg dev_std = 0.241)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.57
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.571
NEC for r=0.6 class 0 = 0.179 +- 0.209 (in-sample avg dev_std = 0.231)
NEC for r=0.6 class 1 = 0.172 +- 0.209 (in-sample avg dev_std = 0.231)
NEC for r=0.6 class 2 = 0.219 +- 0.209 (in-sample avg dev_std = 0.231)
NEC for r=0.6 all KL = 0.145 +- 0.209 (in-sample avg dev_std = 0.231)
NEC for r=0.6 all L1 = 0.186 +- 0.213 (in-sample avg dev_std = 0.231)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:11:51 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:51 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:53 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:54 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:54 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:55 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:11:56 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 190...
[0m[1;37mINFO[0m: [1mCheckpoint 190: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6968
ID Validation Loss: 1.8803
ID Test ACCURACY: 0.6552
ID Test Loss: 2.0303
OOD Validation ACCURACY: 0.6415
OOD Validation Loss: 2.1364
OOD Test ACCURACY: 0.5580
OOD Test Loss: 2.5377

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 195...
[0m[1;37mINFO[0m: [1mCheckpoint 195: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6895
ID Validation Loss: 1.7708
ID Test ACCURACY: 0.6625
ID Test Loss: 1.9030
OOD Validation ACCURACY: 0.6471
OOD Validation Loss: 2.0345
OOD Test ACCURACY: 0.5553
OOD Test Loss: 2.4157

[0m[1;37mINFO[0m: [1mChartInfo 0.6552 0.5580 0.6625 0.5553 0.6895 0.6471[0mGOODTwitter(2590)
Data example from train: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from train: (tensor([0, 1, 2]), tensor([ 625, 1283,  682]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.919
SUFF++ for r=0.6 class 0 = 0.814 +- 0.398 (in-sample avg dev_std = 0.439)
SUFF++ for r=0.6 class 1 = 0.968 +- 0.398 (in-sample avg dev_std = 0.439)
SUFF++ for r=0.6 class 2 = 0.713 +- 0.398 (in-sample avg dev_std = 0.439)
SUFF++ for r=0.6 all KL = 0.624 +- 0.398 (in-sample avg dev_std = 0.439)
SUFF++ for r=0.6 all L1 = 0.864 +- 0.170 (in-sample avg dev_std = 0.439)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.693
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.635
SUFF++ for r=0.6 class 0 = 0.723 +- 0.305 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.6 class 1 = 0.868 +- 0.305 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.6 class 2 = 0.72 +- 0.305 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.6 all KL = 0.712 +- 0.305 (in-sample avg dev_std = 0.432)
SUFF++ for r=0.6 all L1 = 0.791 +- 0.208 (in-sample avg dev_std = 0.432)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.645
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.625
SUFF++ for r=0.6 class 0 = 0.76 +- 0.243 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.6 class 1 = 0.873 +- 0.243 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.6 class 2 = 0.78 +- 0.243 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.6 all KL = 0.811 +- 0.243 (in-sample avg dev_std = 0.332)
SUFF++ for r=0.6 all L1 = 0.824 +- 0.202 (in-sample avg dev_std = 0.332)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.566
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.547
SUFF++ for r=0.6 class 0 = 0.767 +- 0.215 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 class 1 = 0.845 +- 0.215 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 class 2 = 0.748 +- 0.215 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 all KL = 0.8 +- 0.215 (in-sample avg dev_std = 0.345)
SUFF++ for r=0.6 all L1 = 0.801 +- 0.195 (in-sample avg dev_std = 0.345)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.994
NEC for r=0.6 class 0 = 0.023 +- 0.148 (in-sample avg dev_std = 0.093)
NEC for r=0.6 class 1 = 0.003 +- 0.148 (in-sample avg dev_std = 0.093)
NEC for r=0.6 class 2 = 0.029 +- 0.148 (in-sample avg dev_std = 0.093)
NEC for r=0.6 all KL = 0.038 +- 0.148 (in-sample avg dev_std = 0.093)
NEC for r=0.6 all L1 = 0.015 +- 0.065 (in-sample avg dev_std = 0.093)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.693
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.663
NEC for r=0.6 class 0 = 0.162 +- 0.202 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 1 = 0.104 +- 0.202 (in-sample avg dev_std = 0.207)
NEC for r=0.6 class 2 = 0.155 +- 0.202 (in-sample avg dev_std = 0.207)
NEC for r=0.6 all KL = 0.113 +- 0.202 (in-sample avg dev_std = 0.207)
NEC for r=0.6 all L1 = 0.133 +- 0.196 (in-sample avg dev_std = 0.207)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.645
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.656
NEC for r=0.6 class 0 = 0.203 +- 0.213 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 1 = 0.124 +- 0.213 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 2 = 0.167 +- 0.213 (in-sample avg dev_std = 0.213)
NEC for r=0.6 all KL = 0.129 +- 0.213 (in-sample avg dev_std = 0.213)
NEC for r=0.6 all L1 = 0.153 +- 0.208 (in-sample avg dev_std = 0.213)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.566
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.56
NEC for r=0.6 class 0 = 0.211 +- 0.213 (in-sample avg dev_std = 0.231)
NEC for r=0.6 class 1 = 0.141 +- 0.213 (in-sample avg dev_std = 0.231)
NEC for r=0.6 class 2 = 0.205 +- 0.213 (in-sample avg dev_std = 0.231)
NEC for r=0.6 all KL = 0.142 +- 0.213 (in-sample avg dev_std = 0.231)
NEC for r=0.6 all L1 = 0.175 +- 0.213 (in-sample avg dev_std = 0.231)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:13:55 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/27/2024 05:13:55 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 05:13:57 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:13:58 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:13:58 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:00 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:14:01 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 127...
[0m[1;37mINFO[0m: [1mCheckpoint 127: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.7004
ID Validation Loss: 2.4023
ID Test ACCURACY: 0.6282
ID Test Loss: 2.7740
OOD Validation ACCURACY: 0.6246
OOD Validation Loss: 2.7762
OOD Test ACCURACY: 0.5635
OOD Test Loss: 3.1692

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 193...
[0m[1;37mINFO[0m: [1mCheckpoint 193: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6679
ID Validation Loss: 2.4808
ID Test ACCURACY: 0.6462
ID Test Loss: 2.5342
OOD Validation ACCURACY: 0.6431
OOD Validation Loss: 2.6516
OOD Test ACCURACY: 0.5765
OOD Test Loss: 2.9538

[0m[1;37mINFO[0m: [1mChartInfo 0.6282 0.5635 0.6462 0.5765 0.6679 0.6431[0mGOODTwitter(2590)
Data example from train: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from train: (tensor([0, 1, 2]), tensor([ 625, 1283,  682]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.986
SUFF++ for r=0.6 class 0 = 0.98 +- 0.252 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 class 1 = 0.964 +- 0.252 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 class 2 = 0.959 +- 0.252 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 all KL = 0.881 +- 0.252 (in-sample avg dev_std = 0.206)
SUFF++ for r=0.6 all L1 = 0.967 +- 0.081 (in-sample avg dev_std = 0.206)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.651
SUFF++ for r=0.6 class 0 = 0.804 +- 0.281 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 class 1 = 0.818 +- 0.281 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 class 2 = 0.744 +- 0.281 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 all KL = 0.729 +- 0.281 (in-sample avg dev_std = 0.413)
SUFF++ for r=0.6 all L1 = 0.794 +- 0.215 (in-sample avg dev_std = 0.413)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.612
SUFF++ for r=0.6 class 0 = 0.801 +- 0.266 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.6 class 1 = 0.808 +- 0.266 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.6 class 2 = 0.771 +- 0.266 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.6 all KL = 0.759 +- 0.266 (in-sample avg dev_std = 0.382)
SUFF++ for r=0.6 all L1 = 0.798 +- 0.221 (in-sample avg dev_std = 0.382)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.56
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.536
SUFF++ for r=0.6 class 0 = 0.795 +- 0.245 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 1 = 0.793 +- 0.245 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 2 = 0.752 +- 0.245 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all KL = 0.759 +- 0.245 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all L1 = 0.784 +- 0.216 (in-sample avg dev_std = 0.371)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.998
NEC for r=0.6 class 0 = 0.006 +- 0.104 (in-sample avg dev_std = 0.070)
NEC for r=0.6 class 1 = 0.005 +- 0.104 (in-sample avg dev_std = 0.070)
NEC for r=0.6 class 2 = 0.007 +- 0.104 (in-sample avg dev_std = 0.070)
NEC for r=0.6 all KL = 0.019 +- 0.104 (in-sample avg dev_std = 0.070)
NEC for r=0.6 all L1 = 0.006 +- 0.034 (in-sample avg dev_std = 0.070)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.668
NEC for r=0.6 class 0 = 0.156 +- 0.230 (in-sample avg dev_std = 0.287)
NEC for r=0.6 class 1 = 0.128 +- 0.230 (in-sample avg dev_std = 0.287)
NEC for r=0.6 class 2 = 0.179 +- 0.230 (in-sample avg dev_std = 0.287)
NEC for r=0.6 all KL = 0.146 +- 0.230 (in-sample avg dev_std = 0.287)
NEC for r=0.6 all L1 = 0.149 +- 0.205 (in-sample avg dev_std = 0.287)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.62
NEC for r=0.6 class 0 = 0.177 +- 0.244 (in-sample avg dev_std = 0.304)
NEC for r=0.6 class 1 = 0.165 +- 0.244 (in-sample avg dev_std = 0.304)
NEC for r=0.6 class 2 = 0.17 +- 0.244 (in-sample avg dev_std = 0.304)
NEC for r=0.6 all KL = 0.176 +- 0.244 (in-sample avg dev_std = 0.304)
NEC for r=0.6 all L1 = 0.169 +- 0.217 (in-sample avg dev_std = 0.304)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.56
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.546
NEC for r=0.6 class 0 = 0.18 +- 0.230 (in-sample avg dev_std = 0.288)
NEC for r=0.6 class 1 = 0.16 +- 0.230 (in-sample avg dev_std = 0.288)
NEC for r=0.6 class 2 = 0.189 +- 0.230 (in-sample avg dev_std = 0.288)
NEC for r=0.6 all KL = 0.161 +- 0.230 (in-sample avg dev_std = 0.288)
NEC for r=0.6 all L1 = 0.172 +- 0.210 (in-sample avg dev_std = 0.288)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:15:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/27/2024 05:15:58 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 05:15:59 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:00 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:00 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:01 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:16:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.7040
ID Validation Loss: 1.6780
ID Test ACCURACY: 0.6408
ID Test Loss: 1.9436
OOD Validation ACCURACY: 0.6353
OOD Validation Loss: 1.9583
OOD Test ACCURACY: 0.5662
OOD Test Loss: 2.4517

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6715
ID Validation Loss: 2.2080
ID Test ACCURACY: 0.6462
ID Test Loss: 2.5187
OOD Validation ACCURACY: 0.6465
OOD Validation Loss: 2.4911
OOD Test ACCURACY: 0.5779
OOD Test Loss: 2.9096

[0m[1;37mINFO[0m: [1mChartInfo 0.6408 0.5662 0.6462 0.5779 0.6715 0.6465[0mGOODTwitter(2590)
Data example from train: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from train: (tensor([0, 1, 2]), tensor([ 625, 1283,  682]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.982
SUFF++ for r=0.6 class 0 = 0.927 +- 0.254 (in-sample avg dev_std = 0.214)
SUFF++ for r=0.6 class 1 = 0.972 +- 0.254 (in-sample avg dev_std = 0.214)
SUFF++ for r=0.6 class 2 = 0.957 +- 0.254 (in-sample avg dev_std = 0.214)
SUFF++ for r=0.6 all KL = 0.858 +- 0.254 (in-sample avg dev_std = 0.214)
SUFF++ for r=0.6 all L1 = 0.957 +- 0.090 (in-sample avg dev_std = 0.214)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.704
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.664
SUFF++ for r=0.6 class 0 = 0.765 +- 0.258 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 1 = 0.803 +- 0.258 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 2 = 0.787 +- 0.258 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all KL = 0.749 +- 0.258 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all L1 = 0.789 +- 0.220 (in-sample avg dev_std = 0.379)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.646
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.628
SUFF++ for r=0.6 class 0 = 0.746 +- 0.251 (in-sample avg dev_std = 0.376)
SUFF++ for r=0.6 class 1 = 0.807 +- 0.251 (in-sample avg dev_std = 0.376)
SUFF++ for r=0.6 class 2 = 0.724 +- 0.251 (in-sample avg dev_std = 0.376)
SUFF++ for r=0.6 all KL = 0.754 +- 0.251 (in-sample avg dev_std = 0.376)
SUFF++ for r=0.6 all L1 = 0.774 +- 0.224 (in-sample avg dev_std = 0.376)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.572
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.548
SUFF++ for r=0.6 class 0 = 0.788 +- 0.234 (in-sample avg dev_std = 0.378)
SUFF++ for r=0.6 class 1 = 0.778 +- 0.234 (in-sample avg dev_std = 0.378)
SUFF++ for r=0.6 class 2 = 0.729 +- 0.234 (in-sample avg dev_std = 0.378)
SUFF++ for r=0.6 all KL = 0.757 +- 0.234 (in-sample avg dev_std = 0.378)
SUFF++ for r=0.6 all L1 = 0.768 +- 0.212 (in-sample avg dev_std = 0.378)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.997
NEC for r=0.6 class 0 = 0.011 +- 0.094 (in-sample avg dev_std = 0.065)
NEC for r=0.6 class 1 = 0.006 +- 0.094 (in-sample avg dev_std = 0.065)
NEC for r=0.6 class 2 = 0.006 +- 0.094 (in-sample avg dev_std = 0.065)
NEC for r=0.6 all KL = 0.02 +- 0.094 (in-sample avg dev_std = 0.065)
NEC for r=0.6 all L1 = 0.007 +- 0.043 (in-sample avg dev_std = 0.065)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.704
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.68
NEC for r=0.6 class 0 = 0.144 +- 0.159 (in-sample avg dev_std = 0.231)
NEC for r=0.6 class 1 = 0.123 +- 0.159 (in-sample avg dev_std = 0.231)
NEC for r=0.6 class 2 = 0.131 +- 0.159 (in-sample avg dev_std = 0.231)
NEC for r=0.6 all KL = 0.099 +- 0.159 (in-sample avg dev_std = 0.231)
NEC for r=0.6 all L1 = 0.131 +- 0.178 (in-sample avg dev_std = 0.231)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.646
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.635
NEC for r=0.6 class 0 = 0.176 +- 0.205 (in-sample avg dev_std = 0.278)
NEC for r=0.6 class 1 = 0.145 +- 0.205 (in-sample avg dev_std = 0.278)
NEC for r=0.6 class 2 = 0.213 +- 0.205 (in-sample avg dev_std = 0.278)
NEC for r=0.6 all KL = 0.142 +- 0.205 (in-sample avg dev_std = 0.278)
NEC for r=0.6 all L1 = 0.167 +- 0.204 (in-sample avg dev_std = 0.278)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.572
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.569
NEC for r=0.6 class 0 = 0.156 +- 0.198 (in-sample avg dev_std = 0.271)
NEC for r=0.6 class 1 = 0.152 +- 0.198 (in-sample avg dev_std = 0.271)
NEC for r=0.6 class 2 = 0.193 +- 0.198 (in-sample avg dev_std = 0.271)
NEC for r=0.6 all KL = 0.133 +- 0.198 (in-sample avg dev_std = 0.271)
NEC for r=0.6 all L1 = 0.163 +- 0.190 (in-sample avg dev_std = 0.271)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:17:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 04/27/2024 05:17:58 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:00 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:00 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:00 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:02 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:18:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 131...
[0m[1;37mINFO[0m: [1mCheckpoint 131: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6841
ID Validation Loss: 2.1540
ID Test ACCURACY: 0.6462
ID Test Loss: 2.4067
OOD Validation ACCURACY: 0.6123
OOD Validation Loss: 2.5559
OOD Test ACCURACY: 0.5511
OOD Test Loss: 3.1148

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 190...
[0m[1;37mINFO[0m: [1mCheckpoint 190: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6643
ID Validation Loss: 2.4438
ID Test ACCURACY: 0.6408
ID Test Loss: 2.7934
OOD Validation ACCURACY: 0.6359
OOD Validation Loss: 2.7242
OOD Test ACCURACY: 0.5704
OOD Test Loss: 3.1535

[0m[1;37mINFO[0m: [1mChartInfo 0.6462 0.5511 0.6408 0.5704 0.6643 0.6359[0mGOODTwitter(2590)
Data example from train: Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
Label distribution from train: (tensor([0, 1, 2]), tensor([ 625, 1283,  682]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(554)
Data example from id_val: Data(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
Label distribution from id_val: (tensor([0, 1, 2]), tensor([138, 264, 152]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1785)
Data example from val: Data(x=[23, 768], edge_index=[2, 44], y=[1], idx=[1], sentence_tokens=[23], length=[1], domain_id=[1], ori_edge_index=[2, 44], node_perm=[23])
Label distribution from val: (tensor([0, 1, 2]), tensor([453, 956, 376]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.983
SUFF++ for r=0.6 class 0 = 0.965 +- 0.268 (in-sample avg dev_std = 0.212)
SUFF++ for r=0.6 class 1 = 0.963 +- 0.268 (in-sample avg dev_std = 0.212)
SUFF++ for r=0.6 class 2 = 0.963 +- 0.268 (in-sample avg dev_std = 0.212)
SUFF++ for r=0.6 all KL = 0.861 +- 0.268 (in-sample avg dev_std = 0.212)
SUFF++ for r=0.6 all L1 = 0.964 +- 0.084 (in-sample avg dev_std = 0.212)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.681
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.646
SUFF++ for r=0.6 class 0 = 0.795 +- 0.293 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 class 1 = 0.817 +- 0.293 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 class 2 = 0.754 +- 0.293 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 all KL = 0.72 +- 0.293 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 all L1 = 0.794 +- 0.224 (in-sample avg dev_std = 0.393)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.594
SUFF++ for r=0.6 class 0 = 0.773 +- 0.259 (in-sample avg dev_std = 0.399)
SUFF++ for r=0.6 class 1 = 0.777 +- 0.259 (in-sample avg dev_std = 0.399)
SUFF++ for r=0.6 class 2 = 0.739 +- 0.259 (in-sample avg dev_std = 0.399)
SUFF++ for r=0.6 all KL = 0.732 +- 0.259 (in-sample avg dev_std = 0.399)
SUFF++ for r=0.6 all L1 = 0.768 +- 0.223 (in-sample avg dev_std = 0.399)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.534
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.531
SUFF++ for r=0.6 class 0 = 0.791 +- 0.250 (in-sample avg dev_std = 0.392)
SUFF++ for r=0.6 class 1 = 0.759 +- 0.250 (in-sample avg dev_std = 0.392)
SUFF++ for r=0.6 class 2 = 0.744 +- 0.250 (in-sample avg dev_std = 0.392)
SUFF++ for r=0.6 all KL = 0.734 +- 0.250 (in-sample avg dev_std = 0.392)
SUFF++ for r=0.6 all L1 = 0.764 +- 0.216 (in-sample avg dev_std = 0.392)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  1.0
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 797
Effective ratio: 0.615 +- 0.014
Model Accuracy over intervened graphs for r=0.6 =  0.997
NEC for r=0.6 class 0 = 0.004 +- 0.098 (in-sample avg dev_std = 0.070)
NEC for r=0.6 class 1 = 0.006 +- 0.098 (in-sample avg dev_std = 0.070)
NEC for r=0.6 class 2 = 0.004 +- 0.098 (in-sample avg dev_std = 0.070)
NEC for r=0.6 all KL = 0.016 +- 0.098 (in-sample avg dev_std = 0.070)
NEC for r=0.6 all L1 = 0.005 +- 0.038 (in-sample avg dev_std = 0.070)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.681
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 548
Effective ratio: 0.617 +- 0.017
Model Accuracy over intervened graphs for r=0.6 =  0.664
NEC for r=0.6 class 0 = 0.148 +- 0.219 (in-sample avg dev_std = 0.266)
NEC for r=0.6 class 1 = 0.12 +- 0.219 (in-sample avg dev_std = 0.266)
NEC for r=0.6 class 2 = 0.164 +- 0.219 (in-sample avg dev_std = 0.266)
NEC for r=0.6 all KL = 0.135 +- 0.219 (in-sample avg dev_std = 0.266)
NEC for r=0.6 all L1 = 0.139 +- 0.193 (in-sample avg dev_std = 0.266)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.612 +- 0.005
Model Accuracy over intervened graphs for r=0.6 =  0.598
NEC for r=0.6 class 0 = 0.175 +- 0.235 (in-sample avg dev_std = 0.312)
NEC for r=0.6 class 1 = 0.183 +- 0.235 (in-sample avg dev_std = 0.312)
NEC for r=0.6 class 2 = 0.211 +- 0.235 (in-sample avg dev_std = 0.312)
NEC for r=0.6 all KL = 0.179 +- 0.235 (in-sample avg dev_std = 0.312)
NEC for r=0.6 all L1 = 0.187 +- 0.214 (in-sample avg dev_std = 0.312)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.534
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.527
NEC for r=0.6 class 0 = 0.146 +- 0.219 (in-sample avg dev_std = 0.290)
NEC for r=0.6 class 1 = 0.187 +- 0.219 (in-sample avg dev_std = 0.290)
NEC for r=0.6 class 2 = 0.196 +- 0.219 (in-sample avg dev_std = 0.290)
NEC for r=0.6 all KL = 0.162 +- 0.219 (in-sample avg dev_std = 0.290)
NEC for r=0.6 all L1 = 0.179 +- 0.200 (in-sample avg dev_std = 0.290)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.679], 'all_L1': [0.896]}), defaultdict(<class 'list'>, {'all_KL': [0.624], 'all_L1': [0.864]}), defaultdict(<class 'list'>, {'all_KL': [0.881], 'all_L1': [0.967]}), defaultdict(<class 'list'>, {'all_KL': [0.858], 'all_L1': [0.957]}), defaultdict(<class 'list'>, {'all_KL': [0.861], 'all_L1': [0.964]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.034], 'all_L1': [0.014]}), defaultdict(<class 'list'>, {'all_KL': [0.038], 'all_L1': [0.015]}), defaultdict(<class 'list'>, {'all_KL': [0.019], 'all_L1': [0.006]}), defaultdict(<class 'list'>, {'all_KL': [0.02], 'all_L1': [0.007]}), defaultdict(<class 'list'>, {'all_KL': [0.016], 'all_L1': [0.005]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.708], 'all_L1': [0.782]}), defaultdict(<class 'list'>, {'all_KL': [0.712], 'all_L1': [0.791]}), defaultdict(<class 'list'>, {'all_KL': [0.729], 'all_L1': [0.794]}), defaultdict(<class 'list'>, {'all_KL': [0.749], 'all_L1': [0.789]}), defaultdict(<class 'list'>, {'all_KL': [0.72], 'all_L1': [0.794]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.107], 'all_L1': [0.14]}), defaultdict(<class 'list'>, {'all_KL': [0.113], 'all_L1': [0.133]}), defaultdict(<class 'list'>, {'all_KL': [0.146], 'all_L1': [0.149]}), defaultdict(<class 'list'>, {'all_KL': [0.099], 'all_L1': [0.131]}), defaultdict(<class 'list'>, {'all_KL': [0.135], 'all_L1': [0.139]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.8], 'all_L1': [0.798]}), defaultdict(<class 'list'>, {'all_KL': [0.811], 'all_L1': [0.824]}), defaultdict(<class 'list'>, {'all_KL': [0.759], 'all_L1': [0.798]}), defaultdict(<class 'list'>, {'all_KL': [0.754], 'all_L1': [0.774]}), defaultdict(<class 'list'>, {'all_KL': [0.732], 'all_L1': [0.768]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.159], 'all_L1': [0.192]}), defaultdict(<class 'list'>, {'all_KL': [0.129], 'all_L1': [0.153]}), defaultdict(<class 'list'>, {'all_KL': [0.176], 'all_L1': [0.169]}), defaultdict(<class 'list'>, {'all_KL': [0.142], 'all_L1': [0.167]}), defaultdict(<class 'list'>, {'all_KL': [0.179], 'all_L1': [0.187]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.804], 'all_L1': [0.796]}), defaultdict(<class 'list'>, {'all_KL': [0.8], 'all_L1': [0.801]}), defaultdict(<class 'list'>, {'all_KL': [0.759], 'all_L1': [0.784]}), defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.768]}), defaultdict(<class 'list'>, {'all_KL': [0.734], 'all_L1': [0.764]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.145], 'all_L1': [0.186]}), defaultdict(<class 'list'>, {'all_KL': [0.142], 'all_L1': [0.175]}), defaultdict(<class 'list'>, {'all_KL': [0.161], 'all_L1': [0.172]}), defaultdict(<class 'list'>, {'all_KL': [0.133], 'all_L1': [0.163]}), defaultdict(<class 'list'>, {'all_KL': [0.162], 'all_L1': [0.179]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.930 +- 0.042
suff++ class all_KL  =  0.781 +- 0.107
suff++_acc_int  =  0.963 +- 0.026
nec class all_L1  =  0.009 +- 0.004
nec class all_KL  =  0.025 +- 0.009
nec_acc_int  =  0.996 +- 0.002

Eval split id_val
suff++ class all_L1  =  0.790 +- 0.004
suff++ class all_KL  =  0.724 +- 0.015
suff++_acc_int  =  0.651 +- 0.010
nec class all_L1  =  0.138 +- 0.006
nec class all_KL  =  0.120 +- 0.018
nec_acc_int  =  0.672 +- 0.008

Eval split val
suff++ class all_L1  =  0.792 +- 0.020
suff++ class all_KL  =  0.771 +- 0.030
suff++_acc_int  =  0.617 +- 0.013
nec class all_L1  =  0.174 +- 0.014
nec class all_KL  =  0.157 +- 0.019
nec_acc_int  =  0.629 +- 0.019

Eval split test
suff++ class all_L1  =  0.783 +- 0.015
suff++ class all_KL  =  0.771 +- 0.027
suff++_acc_int  =  0.543 +- 0.008
nec class all_L1  =  0.175 +- 0.008
nec class all_KL  =  0.149 +- 0.011
nec_acc_int  =  0.555 +- 0.017


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.470 +- 0.019
Faith. Armon (L1)= 		  =  0.019 +- 0.008
Faith. GMean (L1)= 	  =  0.091 +- 0.019
Faith. Aritm (KL)= 		  =  0.403 +- 0.049
Faith. Armon (KL)= 		  =  0.049 +- 0.016
Faith. GMean (KL)= 	  =  0.137 +- 0.014

Eval split id_val
Faith. Aritm (L1)= 		  =  0.464 +- 0.004
Faith. Armon (L1)= 		  =  0.235 +- 0.009
Faith. GMean (L1)= 	  =  0.331 +- 0.008
Faith. Aritm (KL)= 		  =  0.422 +- 0.011
Faith. Armon (KL)= 		  =  0.205 +- 0.026
Faith. GMean (KL)= 	  =  0.294 +- 0.021

Eval split val
Faith. Aritm (L1)= 		  =  0.483 +- 0.008
Faith. Armon (L1)= 		  =  0.284 +- 0.019
Faith. GMean (L1)= 	  =  0.370 +- 0.013
Faith. Aritm (KL)= 		  =  0.464 +- 0.011
Faith. Armon (KL)= 		  =  0.260 +- 0.026
Faith. GMean (KL)= 	  =  0.347 +- 0.018

Eval split test
Faith. Aritm (L1)= 		  =  0.479 +- 0.010
Faith. Armon (L1)= 		  =  0.286 +- 0.011
Faith. GMean (L1)= 	  =  0.370 +- 0.010
Faith. Aritm (KL)= 		  =  0.460 +- 0.012
Faith. Armon (KL)= 		  =  0.249 +- 0.015
Faith. GMean (KL)= 	  =  0.338 +- 0.011
Computed for split load_split = id



Completed in  0:10:12.827745  for CIGAvGIN GOODTwitter/length



DONE CIGA GOODTwitter/length

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:20:32 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:33 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:36 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:39 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:42 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:20:45 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 78...
[0m[1;37mINFO[0m: [1mCheckpoint 78: 
-----------------------------------
Train ROC-AUC: 0.9940
Train Loss: 0.0391
ID Validation ROC-AUC: 0.8450
ID Validation Loss: 0.1821
ID Test ROC-AUC: 0.8077
ID Test Loss: 0.1679
OOD Validation ROC-AUC: 0.6969
OOD Validation Loss: 0.2009
OOD Test ROC-AUC: 0.6673
OOD Test Loss: 0.1473

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 67...
[0m[1;37mINFO[0m: [1mCheckpoint 67: 
-----------------------------------
Train ROC-AUC: 0.9930
Train Loss: 0.0404
ID Validation ROC-AUC: 0.8284
ID Validation Loss: 0.1766
ID Test ROC-AUC: 0.8100
ID Test Loss: 0.1597
OOD Validation ROC-AUC: 0.7746
OOD Validation Loss: 0.1708
OOD Test ROC-AUC: 0.6410
OOD Test Loss: 0.1507

[0m[1;37mINFO[0m: [1mChartInfo 0.8077 0.6673 0.8100 0.6410 0.8284 0.7746[0mGOODHIV(24682)
Data example from train: Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([23758,   924]))
[1;34mDEBUG[0m: 04/27/2024 05:20:46 PM : [1mUnbalanced warning for GOODHIV (train)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([924, 924]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/27/2024 05:20:55 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/27/2024 05:20:59 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/27/2024 05:21:01 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.709
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.561
SUFF++ for r=0.8 class 0.0 = 0.671 +- 0.285 (in-sample avg dev_std = 0.508)
SUFF++ for r=0.8 class 1.0 = 0.603 +- 0.285 (in-sample avg dev_std = 0.508)
SUFF++ for r=0.8 all KL = 0.579 +- 0.285 (in-sample avg dev_std = 0.508)
SUFF++ for r=0.8 all L1 = 0.637 +- 0.223 (in-sample avg dev_std = 0.508)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.737
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.54
SUFF++ for r=0.8 class 0.0 = 0.685 +- 0.283 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.8 class 1.0 = 0.603 +- 0.283 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.8 all KL = 0.562 +- 0.283 (in-sample avg dev_std = 0.524)
SUFF++ for r=0.8 all L1 = 0.644 +- 0.216 (in-sample avg dev_std = 0.524)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.628
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.569
SUFF++ for r=0.8 class 0.0 = 0.755 +- 0.274 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.8 class 1.0 = 0.705 +- 0.274 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.8 all KL = 0.679 +- 0.274 (in-sample avg dev_std = 0.457)
SUFF++ for r=0.8 all L1 = 0.73 +- 0.222 (in-sample avg dev_std = 0.457)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.57
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 161
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.481
SUFF++ for r=0.8 class 0.0 = 0.768 +- 0.265 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.8 class 1.0 = 0.746 +- 0.265 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.8 all KL = 0.732 +- 0.265 (in-sample avg dev_std = 0.391)
SUFF++ for r=0.8 all L1 = 0.757 +- 0.221 (in-sample avg dev_std = 0.391)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.709
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.627
NEC for r=0.8 class 0.0 = 0.283 +- 0.311 (in-sample avg dev_std = 0.276)
NEC for r=0.8 class 1.0 = 0.317 +- 0.311 (in-sample avg dev_std = 0.276)
NEC for r=0.8 all KL = 0.298 +- 0.311 (in-sample avg dev_std = 0.276)
NEC for r=0.8 all L1 = 0.3 +- 0.262 (in-sample avg dev_std = 0.276)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.737
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.642
NEC for r=0.8 class 0.0 = 0.273 +- 0.301 (in-sample avg dev_std = 0.294)
NEC for r=0.8 class 1.0 = 0.296 +- 0.301 (in-sample avg dev_std = 0.294)
NEC for r=0.8 all KL = 0.29 +- 0.301 (in-sample avg dev_std = 0.294)
NEC for r=0.8 all L1 = 0.284 +- 0.255 (in-sample avg dev_std = 0.294)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.628
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.601
NEC for r=0.8 class 0.0 = 0.2 +- 0.244 (in-sample avg dev_std = 0.232)
NEC for r=0.8 class 1.0 = 0.191 +- 0.244 (in-sample avg dev_std = 0.232)
NEC for r=0.8 all KL = 0.183 +- 0.244 (in-sample avg dev_std = 0.232)
NEC for r=0.8 all L1 = 0.196 +- 0.224 (in-sample avg dev_std = 0.232)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.564
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.484
NEC for r=0.8 class 0.0 = 0.232 +- 0.278 (in-sample avg dev_std = 0.300)
NEC for r=0.8 class 1.0 = 0.26 +- 0.278 (in-sample avg dev_std = 0.300)
NEC for r=0.8 all KL = 0.25 +- 0.278 (in-sample avg dev_std = 0.300)
NEC for r=0.8 all L1 = 0.246 +- 0.246 (in-sample avg dev_std = 0.300)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:21:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:45 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:48 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:51 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:54 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:21:57 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 129...
[0m[1;37mINFO[0m: [1mCheckpoint 129: 
-----------------------------------
Train ROC-AUC: 0.9987
Train Loss: 0.0183
ID Validation ROC-AUC: 0.8436
ID Validation Loss: 0.1867
ID Test ROC-AUC: 0.7972
ID Test Loss: 0.1808
OOD Validation ROC-AUC: 0.7113
OOD Validation Loss: 0.2083
OOD Test ROC-AUC: 0.6616
OOD Test Loss: 0.1540

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 11...
[0m[1;37mINFO[0m: [1mCheckpoint 11: 
-----------------------------------
Train ROC-AUC: 0.8665
Train Loss: 0.1133
ID Validation ROC-AUC: 0.8073
ID Validation Loss: 0.1400
ID Test ROC-AUC: 0.8000
ID Test Loss: 0.1208
OOD Validation ROC-AUC: 0.7821
OOD Validation Loss: 0.1218
OOD Test ROC-AUC: 0.6412
OOD Test Loss: 0.0947

[0m[1;37mINFO[0m: [1mChartInfo 0.7972 0.6616 0.8000 0.6412 0.8073 0.7821[0mGOODHIV(24682)
Data example from train: Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([23758,   924]))
[1;34mDEBUG[0m: 04/27/2024 05:21:58 PM : [1mUnbalanced warning for GOODHIV (train)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([924, 924]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/27/2024 05:22:06 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/27/2024 05:22:10 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/27/2024 05:22:12 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.647
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.55
SUFF++ for r=0.8 class 0.0 = 0.881 +- 0.267 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 class 1.0 = 0.764 +- 0.267 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all KL = 0.808 +- 0.267 (in-sample avg dev_std = 0.306)
SUFF++ for r=0.8 all L1 = 0.822 +- 0.236 (in-sample avg dev_std = 0.306)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.685
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.586
SUFF++ for r=0.8 class 0.0 = 0.914 +- 0.243 (in-sample avg dev_std = 0.251)
SUFF++ for r=0.8 class 1.0 = 0.8 +- 0.243 (in-sample avg dev_std = 0.251)
SUFF++ for r=0.8 all KL = 0.845 +- 0.243 (in-sample avg dev_std = 0.251)
SUFF++ for r=0.8 all L1 = 0.857 +- 0.220 (in-sample avg dev_std = 0.251)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.538
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.563
SUFF++ for r=0.8 class 0.0 = 0.868 +- 0.254 (in-sample avg dev_std = 0.279)
SUFF++ for r=0.8 class 1.0 = 0.799 +- 0.254 (in-sample avg dev_std = 0.279)
SUFF++ for r=0.8 all KL = 0.813 +- 0.254 (in-sample avg dev_std = 0.279)
SUFF++ for r=0.8 all L1 = 0.833 +- 0.231 (in-sample avg dev_std = 0.279)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.562
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 161
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.556
SUFF++ for r=0.8 class 0.0 = 0.868 +- 0.250 (in-sample avg dev_std = 0.270)
SUFF++ for r=0.8 class 1.0 = 0.804 +- 0.250 (in-sample avg dev_std = 0.270)
SUFF++ for r=0.8 all KL = 0.828 +- 0.250 (in-sample avg dev_std = 0.270)
SUFF++ for r=0.8 all L1 = 0.836 +- 0.233 (in-sample avg dev_std = 0.270)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.647
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.613
NEC for r=0.8 class 0.0 = 0.196 +- 0.309 (in-sample avg dev_std = 0.323)
NEC for r=0.8 class 1.0 = 0.278 +- 0.309 (in-sample avg dev_std = 0.323)
NEC for r=0.8 all KL = 0.266 +- 0.309 (in-sample avg dev_std = 0.323)
NEC for r=0.8 all L1 = 0.237 +- 0.264 (in-sample avg dev_std = 0.323)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.685
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.648
NEC for r=0.8 class 0.0 = 0.137 +- 0.289 (in-sample avg dev_std = 0.307)
NEC for r=0.8 class 1.0 = 0.241 +- 0.289 (in-sample avg dev_std = 0.307)
NEC for r=0.8 all KL = 0.218 +- 0.289 (in-sample avg dev_std = 0.307)
NEC for r=0.8 all L1 = 0.189 +- 0.235 (in-sample avg dev_std = 0.307)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.538
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.54
NEC for r=0.8 class 0.0 = 0.251 +- 0.327 (in-sample avg dev_std = 0.343)
NEC for r=0.8 class 1.0 = 0.238 +- 0.327 (in-sample avg dev_std = 0.343)
NEC for r=0.8 all KL = 0.288 +- 0.327 (in-sample avg dev_std = 0.343)
NEC for r=0.8 all L1 = 0.244 +- 0.265 (in-sample avg dev_std = 0.343)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.559
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.558
NEC for r=0.8 class 0.0 = 0.114 +- 0.216 (in-sample avg dev_std = 0.278)
NEC for r=0.8 class 1.0 = 0.188 +- 0.216 (in-sample avg dev_std = 0.278)
NEC for r=0.8 all KL = 0.15 +- 0.216 (in-sample avg dev_std = 0.278)
NEC for r=0.8 all L1 = 0.151 +- 0.194 (in-sample avg dev_std = 0.278)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:22:56 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/27/2024 05:22:56 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:22:59 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:02 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:06 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:23:08 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ROC-AUC: 0.9615
Train Loss: 0.0758
ID Validation ROC-AUC: 0.8439
ID Validation Loss: 0.1249
ID Test ROC-AUC: 0.7995
ID Test Loss: 0.1163
OOD Validation ROC-AUC: 0.7437
OOD Validation Loss: 0.1234
OOD Test ROC-AUC: 0.6739
OOD Test Loss: 0.0957

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 13...
[0m[1;37mINFO[0m: [1mCheckpoint 13: 
-----------------------------------
Train ROC-AUC: 0.8809
Train Loss: 0.1125
ID Validation ROC-AUC: 0.8182
ID Validation Loss: 0.1392
ID Test ROC-AUC: 0.7843
ID Test Loss: 0.1272
OOD Validation ROC-AUC: 0.7800
OOD Validation Loss: 0.1249
OOD Test ROC-AUC: 0.6692
OOD Test Loss: 0.0917

[0m[1;37mINFO[0m: [1mChartInfo 0.7995 0.6739 0.7843 0.6692 0.8182 0.7800[0mGOODHIV(24682)
Data example from train: Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([23758,   924]))
[1;34mDEBUG[0m: 04/27/2024 05:23:09 PM : [1mUnbalanced warning for GOODHIV (train)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([924, 924]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/27/2024 05:23:17 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/27/2024 05:23:20 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/27/2024 05:23:24 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.77
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.666
SUFF++ for r=0.8 class 0.0 = 0.919 +- 0.084 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 class 1.0 = 0.845 +- 0.084 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 all KL = 0.929 +- 0.084 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.8 all L1 = 0.882 +- 0.112 (in-sample avg dev_std = 0.156)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.734
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.655
SUFF++ for r=0.8 class 0.0 = 0.929 +- 0.092 (in-sample avg dev_std = 0.157)
SUFF++ for r=0.8 class 1.0 = 0.859 +- 0.092 (in-sample avg dev_std = 0.157)
SUFF++ for r=0.8 all KL = 0.932 +- 0.092 (in-sample avg dev_std = 0.157)
SUFF++ for r=0.8 all L1 = 0.894 +- 0.114 (in-sample avg dev_std = 0.157)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.667
SUFF++ for r=0.8 class 0.0 = 0.923 +- 0.072 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.8 class 1.0 = 0.875 +- 0.072 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.8 all KL = 0.939 +- 0.072 (in-sample avg dev_std = 0.129)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.076 (in-sample avg dev_std = 0.129)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.623
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.568
SUFF++ for r=0.8 class 0.0 = 0.95 +- 0.042 (in-sample avg dev_std = 0.085)
SUFF++ for r=0.8 class 1.0 = 0.902 +- 0.042 (in-sample avg dev_std = 0.085)
SUFF++ for r=0.8 all KL = 0.966 +- 0.042 (in-sample avg dev_std = 0.085)
SUFF++ for r=0.8 all L1 = 0.926 +- 0.081 (in-sample avg dev_std = 0.085)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.77
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.709
NEC for r=0.8 class 0.0 = 0.064 +- 0.079 (in-sample avg dev_std = 0.086)
NEC for r=0.8 class 1.0 = 0.126 +- 0.079 (in-sample avg dev_std = 0.086)
NEC for r=0.8 all KL = 0.045 +- 0.079 (in-sample avg dev_std = 0.086)
NEC for r=0.8 all L1 = 0.095 +- 0.108 (in-sample avg dev_std = 0.086)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.734
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.685
NEC for r=0.8 class 0.0 = 0.057 +- 0.061 (in-sample avg dev_std = 0.086)
NEC for r=0.8 class 1.0 = 0.109 +- 0.061 (in-sample avg dev_std = 0.086)
NEC for r=0.8 all KL = 0.037 +- 0.061 (in-sample avg dev_std = 0.086)
NEC for r=0.8 all L1 = 0.083 +- 0.097 (in-sample avg dev_std = 0.086)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.716
NEC for r=0.8 class 0.0 = 0.056 +- 0.064 (in-sample avg dev_std = 0.076)
NEC for r=0.8 class 1.0 = 0.108 +- 0.064 (in-sample avg dev_std = 0.076)
NEC for r=0.8 all KL = 0.035 +- 0.064 (in-sample avg dev_std = 0.076)
NEC for r=0.8 all L1 = 0.082 +- 0.091 (in-sample avg dev_std = 0.076)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.623
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.612
NEC for r=0.8 class 0.0 = 0.043 +- 0.027 (in-sample avg dev_std = 0.064)
NEC for r=0.8 class 1.0 = 0.078 +- 0.027 (in-sample avg dev_std = 0.064)
NEC for r=0.8 all KL = 0.022 +- 0.027 (in-sample avg dev_std = 0.064)
NEC for r=0.8 all L1 = 0.061 +- 0.063 (in-sample avg dev_std = 0.064)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:24:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:07 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:11 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:14 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:17 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:19 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 143...
[0m[1;37mINFO[0m: [1mCheckpoint 143: 
-----------------------------------
Train ROC-AUC: 0.9983
Train Loss: 0.0242
ID Validation ROC-AUC: 0.8431
ID Validation Loss: 0.1834
ID Test ROC-AUC: 0.8028
ID Test Loss: 0.1734
OOD Validation ROC-AUC: 0.7364
OOD Validation Loss: 0.2070
OOD Test ROC-AUC: 0.6626
OOD Test Loss: 0.1633

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 38...
[0m[1;37mINFO[0m: [1mCheckpoint 38: 
-----------------------------------
Train ROC-AUC: 0.9560
Train Loss: 0.0779
ID Validation ROC-AUC: 0.8151
ID Validation Loss: 0.1518
ID Test ROC-AUC: 0.7777
ID Test Loss: 0.1370
OOD Validation ROC-AUC: 0.7910
OOD Validation Loss: 0.1230
OOD Test ROC-AUC: 0.6634
OOD Test Loss: 0.1055

[0m[1;37mINFO[0m: [1mChartInfo 0.8028 0.6626 0.7777 0.6634 0.8151 0.7910[0mGOODHIV(24682)
Data example from train: Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([23758,   924]))
[1;34mDEBUG[0m: 04/27/2024 05:24:20 PM : [1mUnbalanced warning for GOODHIV (train)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([924, 924]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/27/2024 05:24:28 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/27/2024 05:24:32 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/27/2024 05:24:34 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.701
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.606
SUFF++ for r=0.8 class 0.0 = 0.788 +- 0.299 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 class 1.0 = 0.674 +- 0.299 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 all KL = 0.637 +- 0.299 (in-sample avg dev_std = 0.475)
SUFF++ for r=0.8 all L1 = 0.731 +- 0.225 (in-sample avg dev_std = 0.475)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.669
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.586
SUFF++ for r=0.8 class 0.0 = 0.797 +- 0.300 (in-sample avg dev_std = 0.498)
SUFF++ for r=0.8 class 1.0 = 0.693 +- 0.300 (in-sample avg dev_std = 0.498)
SUFF++ for r=0.8 all KL = 0.631 +- 0.300 (in-sample avg dev_std = 0.498)
SUFF++ for r=0.8 all L1 = 0.745 +- 0.217 (in-sample avg dev_std = 0.498)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.687
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.635
SUFF++ for r=0.8 class 0.0 = 0.856 +- 0.302 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.8 class 1.0 = 0.696 +- 0.302 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.8 all KL = 0.676 +- 0.302 (in-sample avg dev_std = 0.453)
SUFF++ for r=0.8 all L1 = 0.776 +- 0.225 (in-sample avg dev_std = 0.453)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.514
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 161
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.532
SUFF++ for r=0.8 class 0.0 = 0.885 +- 0.233 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 class 1.0 = 0.855 +- 0.233 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 all KL = 0.821 +- 0.233 (in-sample avg dev_std = 0.308)
SUFF++ for r=0.8 all L1 = 0.87 +- 0.171 (in-sample avg dev_std = 0.308)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.701
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.72
NEC for r=0.8 class 0.0 = 0.144 +- 0.272 (in-sample avg dev_std = 0.262)
NEC for r=0.8 class 1.0 = 0.218 +- 0.272 (in-sample avg dev_std = 0.262)
NEC for r=0.8 all KL = 0.2 +- 0.272 (in-sample avg dev_std = 0.262)
NEC for r=0.8 all L1 = 0.181 +- 0.225 (in-sample avg dev_std = 0.262)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.669
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.674
NEC for r=0.8 class 0.0 = 0.141 +- 0.253 (in-sample avg dev_std = 0.279)
NEC for r=0.8 class 1.0 = 0.172 +- 0.253 (in-sample avg dev_std = 0.279)
NEC for r=0.8 all KL = 0.182 +- 0.253 (in-sample avg dev_std = 0.279)
NEC for r=0.8 all L1 = 0.157 +- 0.201 (in-sample avg dev_std = 0.279)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.687
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.693
NEC for r=0.8 class 0.0 = 0.112 +- 0.252 (in-sample avg dev_std = 0.236)
NEC for r=0.8 class 1.0 = 0.172 +- 0.252 (in-sample avg dev_std = 0.236)
NEC for r=0.8 all KL = 0.164 +- 0.252 (in-sample avg dev_std = 0.236)
NEC for r=0.8 all L1 = 0.142 +- 0.208 (in-sample avg dev_std = 0.236)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.511
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.557
NEC for r=0.8 class 0.0 = 0.103 +- 0.221 (in-sample avg dev_std = 0.201)
NEC for r=0.8 class 1.0 = 0.12 +- 0.221 (in-sample avg dev_std = 0.201)
NEC for r=0.8 all KL = 0.128 +- 0.221 (in-sample avg dev_std = 0.201)
NEC for r=0.8 all L1 = 0.111 +- 0.184 (in-sample avg dev_std = 0.201)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:25:18 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODHIV
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:18 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:21 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:24 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:27 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mDataset: {'train': GOODHIV(24682), 'id_val': GOODHIV(4112), 'id_test': GOODHIV(4112), 'val': GOODHIV(4113), 'test': GOODHIV(4108), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1m Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
[0mData(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 42...
[0m[1;37mINFO[0m: [1mCheckpoint 42: 
-----------------------------------
Train ROC-AUC: 0.9711
Train Loss: 0.0868
ID Validation ROC-AUC: 0.8464
ID Validation Loss: 0.1631
ID Test ROC-AUC: 0.8070
ID Test Loss: 0.1620
OOD Validation ROC-AUC: 0.7678
OOD Validation Loss: 0.1715
OOD Test ROC-AUC: 0.7207
OOD Test Loss: 0.1109

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 8...
[0m[1;37mINFO[0m: [1mCheckpoint 8: 
-----------------------------------
Train ROC-AUC: 0.8237
Train Loss: 0.1344
ID Validation ROC-AUC: 0.7836
ID Validation Loss: 0.1594
ID Test ROC-AUC: 0.7505
ID Test Loss: 0.1407
OOD Validation ROC-AUC: 0.7881
OOD Validation Loss: 0.1099
OOD Test ROC-AUC: 0.7056
OOD Test Loss: 0.1049

[0m[1;37mINFO[0m: [1mChartInfo 0.8070 0.7207 0.7505 0.7056 0.7836 0.7881[0mGOODHIV(24682)
Data example from train: Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], smiles='CC1CCCN2C(=O)CN(Cc3ccccc3)CC(=O)N12', idx=[1], scaffold='O=C1CN(Cc2ccccc2)CC(=O)N2CCCCN12', domain_id=[1], env_id=[1])
Label distribution from train: (tensor([0., 1.]), tensor([23758,   924]))
[1;34mDEBUG[0m: 04/27/2024 05:25:30 PM : [1mUnbalanced warning for GOODHIV (train)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([924, 924]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4112)
Data example from id_val: Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], smiles='O=C1CSc2cc([N+](=O)[O-])ccc2N1', idx=[1], scaffold='O=C1CSc2ccccc2N1', domain_id=[1], env_id=[1], ori_edge_index=[2, 30], node_perm=[14])
Label distribution from id_val: (tensor([0., 1.]), tensor([3936,  176]))
[1;34mDEBUG[0m: 04/27/2024 05:25:38 PM : [1mUnbalanced warning for GOODHIV (id_val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([176, 176]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4113)
Data example from val: Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], smiles='OC(c1ccccc1)c1c(-c2ccccc2)[nH]c(-c2ccccc2)c1C(O)c1ccccc1', idx=[1], scaffold='c1ccc(Cc2c(-c3ccccc3)[nH]c(-c3ccccc3)c2Cc2ccccc2)cc1', domain_id=[1], ori_edge_index=[2, 74], node_perm=[33])
Label distribution from val: (tensor([0., 1.]), tensor([3987,  126]))
[1;34mDEBUG[0m: 04/27/2024 05:25:42 PM : [1mUnbalanced warning for GOODHIV (val)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([126, 126]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
GOODHIV(4108)
Data example from test: Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='Nc1ccc2nc3ccc(O)cc3nc2c1.[Na]S[Na]', idx=[1], scaffold='c1ccc2nc3ccccc3nc2c1', domain_id=[1], ori_edge_index=[2, 40], node_perm=[19])
Label distribution from test: (tensor([0., 1.]), tensor([4027,   81]))
[1;34mDEBUG[0m: 04/27/2024 05:25:45 PM : [1mUnbalanced warning for GOODHIV (test)
[0mCreating balanced dataset: (tensor([0., 1.]), tensor([81, 81]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.716
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.64
SUFF++ for r=0.8 class 0.0 = 0.736 +- 0.180 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 class 1.0 = 0.727 +- 0.180 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 all KL = 0.812 +- 0.180 (in-sample avg dev_std = 0.330)
SUFF++ for r=0.8 all L1 = 0.731 +- 0.180 (in-sample avg dev_std = 0.330)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.665
SUFF++ for r=0.8 class 0.0 = 0.716 +- 0.182 (in-sample avg dev_std = 0.346)
SUFF++ for r=0.8 class 1.0 = 0.729 +- 0.182 (in-sample avg dev_std = 0.346)
SUFF++ for r=0.8 all KL = 0.796 +- 0.182 (in-sample avg dev_std = 0.346)
SUFF++ for r=0.8 all L1 = 0.723 +- 0.180 (in-sample avg dev_std = 0.346)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.694
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.694
SUFF++ for r=0.8 class 0.0 = 0.627 +- 0.195 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.8 class 1.0 = 0.795 +- 0.195 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.8 all KL = 0.805 +- 0.195 (in-sample avg dev_std = 0.357)
SUFF++ for r=0.8 all L1 = 0.711 +- 0.196 (in-sample avg dev_std = 0.357)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.663
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 160
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.629
SUFF++ for r=0.8 class 0.0 = 0.744 +- 0.175 (in-sample avg dev_std = 0.329)
SUFF++ for r=0.8 class 1.0 = 0.689 +- 0.175 (in-sample avg dev_std = 0.329)
SUFF++ for r=0.8 all KL = 0.815 +- 0.175 (in-sample avg dev_std = 0.329)
SUFF++ for r=0.8 all L1 = 0.716 +- 0.182 (in-sample avg dev_std = 0.329)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.716
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 799
Effective ratio: 0.808 +- 0.011
Model ROC-AUC over intervened graphs for r=0.8 =  0.67
NEC for r=0.8 class 0.0 = 0.199 +- 0.139 (in-sample avg dev_std = 0.179)
NEC for r=0.8 class 1.0 = 0.193 +- 0.139 (in-sample avg dev_std = 0.179)
NEC for r=0.8 all KL = 0.102 +- 0.139 (in-sample avg dev_std = 0.179)
NEC for r=0.8 all L1 = 0.196 +- 0.177 (in-sample avg dev_std = 0.179)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 352
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.68
NEC for r=0.8 class 0.0 = 0.219 +- 0.159 (in-sample avg dev_std = 0.218)
NEC for r=0.8 class 1.0 = 0.197 +- 0.159 (in-sample avg dev_std = 0.218)
NEC for r=0.8 all KL = 0.123 +- 0.159 (in-sample avg dev_std = 0.218)
NEC for r=0.8 all L1 = 0.208 +- 0.182 (in-sample avg dev_std = 0.218)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.694
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 252
Effective ratio: 0.807 +- 0.006
Model ROC-AUC over intervened graphs for r=0.8 =  0.69
NEC for r=0.8 class 0.0 = 0.319 +- 0.180 (in-sample avg dev_std = 0.234)
NEC for r=0.8 class 1.0 = 0.204 +- 0.180 (in-sample avg dev_std = 0.234)
NEC for r=0.8 all KL = 0.154 +- 0.180 (in-sample avg dev_std = 0.234)
NEC for r=0.8 all L1 = 0.261 +- 0.206 (in-sample avg dev_std = 0.234)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model ROC-AUC of binarized graphs for r=0.8 =  0.664
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 162
Effective ratio: 0.810 +- 0.010
Model ROC-AUC over intervened graphs for r=0.8 =  0.618
NEC for r=0.8 class 0.0 = 0.204 +- 0.155 (in-sample avg dev_std = 0.190)
NEC for r=0.8 class 1.0 = 0.279 +- 0.155 (in-sample avg dev_std = 0.190)
NEC for r=0.8 all KL = 0.125 +- 0.155 (in-sample avg dev_std = 0.190)
NEC for r=0.8 all L1 = 0.242 +- 0.192 (in-sample avg dev_std = 0.190)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.579], 'all_L1': [0.637]}), defaultdict(<class 'list'>, {'all_KL': [0.808], 'all_L1': [0.822]}), defaultdict(<class 'list'>, {'all_KL': [0.929], 'all_L1': [0.882]}), defaultdict(<class 'list'>, {'all_KL': [0.637], 'all_L1': [0.731]}), defaultdict(<class 'list'>, {'all_KL': [0.812], 'all_L1': [0.731]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.298], 'all_L1': [0.3]}), defaultdict(<class 'list'>, {'all_KL': [0.266], 'all_L1': [0.237]}), defaultdict(<class 'list'>, {'all_KL': [0.045], 'all_L1': [0.095]}), defaultdict(<class 'list'>, {'all_KL': [0.2], 'all_L1': [0.181]}), defaultdict(<class 'list'>, {'all_KL': [0.102], 'all_L1': [0.196]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.562], 'all_L1': [0.644]}), defaultdict(<class 'list'>, {'all_KL': [0.845], 'all_L1': [0.857]}), defaultdict(<class 'list'>, {'all_KL': [0.932], 'all_L1': [0.894]}), defaultdict(<class 'list'>, {'all_KL': [0.631], 'all_L1': [0.745]}), defaultdict(<class 'list'>, {'all_KL': [0.796], 'all_L1': [0.723]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.29], 'all_L1': [0.284]}), defaultdict(<class 'list'>, {'all_KL': [0.218], 'all_L1': [0.189]}), defaultdict(<class 'list'>, {'all_KL': [0.037], 'all_L1': [0.083]}), defaultdict(<class 'list'>, {'all_KL': [0.182], 'all_L1': [0.157]}), defaultdict(<class 'list'>, {'all_KL': [0.123], 'all_L1': [0.208]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.679], 'all_L1': [0.73]}), defaultdict(<class 'list'>, {'all_KL': [0.813], 'all_L1': [0.833]}), defaultdict(<class 'list'>, {'all_KL': [0.939], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.676], 'all_L1': [0.776]}), defaultdict(<class 'list'>, {'all_KL': [0.805], 'all_L1': [0.711]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.183], 'all_L1': [0.196]}), defaultdict(<class 'list'>, {'all_KL': [0.288], 'all_L1': [0.244]}), defaultdict(<class 'list'>, {'all_KL': [0.035], 'all_L1': [0.082]}), defaultdict(<class 'list'>, {'all_KL': [0.164], 'all_L1': [0.142]}), defaultdict(<class 'list'>, {'all_KL': [0.154], 'all_L1': [0.261]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.732], 'all_L1': [0.757]}), defaultdict(<class 'list'>, {'all_KL': [0.828], 'all_L1': [0.836]}), defaultdict(<class 'list'>, {'all_KL': [0.966], 'all_L1': [0.926]}), defaultdict(<class 'list'>, {'all_KL': [0.821], 'all_L1': [0.87]}), defaultdict(<class 'list'>, {'all_KL': [0.815], 'all_L1': [0.716]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.25], 'all_L1': [0.246]}), defaultdict(<class 'list'>, {'all_KL': [0.15], 'all_L1': [0.151]}), defaultdict(<class 'list'>, {'all_KL': [0.022], 'all_L1': [0.061]}), defaultdict(<class 'list'>, {'all_KL': [0.128], 'all_L1': [0.111]}), defaultdict(<class 'list'>, {'all_KL': [0.125], 'all_L1': [0.242]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.761 +- 0.084
suff++ class all_KL  =  0.753 +- 0.127
suff++_acc_int  =  0.605 +- 0.044
nec class all_L1  =  0.202 +- 0.067
nec class all_KL  =  0.182 +- 0.096
nec_acc_int  =  0.668 +- 0.043

Eval split id_val
suff++ class all_L1  =  0.773 +- 0.091
suff++ class all_KL  =  0.753 +- 0.137
suff++_acc_int  =  0.607 +- 0.047
nec class all_L1  =  0.184 +- 0.066
nec class all_KL  =  0.170 +- 0.086
nec_acc_int  =  0.666 +- 0.017

Eval split val
suff++ class all_L1  =  0.790 +- 0.069
suff++ class all_KL  =  0.782 +- 0.098
suff++_acc_int  =  0.625 +- 0.052
nec class all_L1  =  0.185 +- 0.066
nec class all_KL  =  0.165 +- 0.081
nec_acc_int  =  0.648 +- 0.067

Eval split test
suff++ class all_L1  =  0.821 +- 0.076
suff++ class all_KL  =  0.832 +- 0.075
suff++_acc_int  =  0.553 +- 0.048
nec class all_L1  =  0.162 +- 0.073
nec class all_KL  =  0.135 +- 0.073
nec_acc_int  =  0.566 +- 0.049


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.481 +- 0.026
Faith. Armon (L1)= 		  =  0.309 +- 0.081
Faith. GMean (L1)= 	  =  0.382 +- 0.056
Faith. Aritm (KL)= 		  =  0.468 +- 0.041
Faith. Armon (KL)= 		  =  0.273 +- 0.123
Faith. GMean (KL)= 	  =  0.346 +- 0.092

Eval split id_val
Faith. Aritm (L1)= 		  =  0.478 +- 0.025
Faith. Armon (L1)= 		  =  0.288 +- 0.080
Faith. GMean (L1)= 	  =  0.366 +- 0.055
Faith. Aritm (KL)= 		  =  0.462 +- 0.044
Faith. Armon (KL)= 		  =  0.259 +- 0.110
Faith. GMean (KL)= 	  =  0.334 +- 0.085

Eval split val
Faith. Aritm (L1)= 		  =  0.487 +- 0.028
Faith. Armon (L1)= 		  =  0.292 +- 0.088
Faith. GMean (L1)= 	  =  0.373 +- 0.065
Faith. Aritm (KL)= 		  =  0.474 +- 0.047
Faith. Armon (KL)= 		  =  0.261 +- 0.114
Faith. GMean (KL)= 	  =  0.341 +- 0.096

Eval split test
Faith. Aritm (L1)= 		  =  0.492 +- 0.007
Faith. Armon (L1)= 		  =  0.260 +- 0.098
Faith. GMean (L1)= 	  =  0.350 +- 0.071
Faith. Aritm (KL)= 		  =  0.484 +- 0.010
Faith. Armon (KL)= 		  =  0.222 +- 0.106
Faith. GMean (KL)= 	  =  0.314 +- 0.093
Computed for split load_split = id



Completed in  0:05:57.451045  for CIGAvGIN GOODHIV/scaffold



DONE CIGA GOODHIV/scaffold

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:27:01 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:27:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.3715
Train Loss: 2.7499
ID Validation ACCURACY: 0.3654
ID Validation Loss: 2.7861
ID Test ACCURACY: 0.3729
ID Test Loss: 2.7814
OOD Validation ACCURACY: 0.3491
OOD Validation Loss: 3.1257
OOD Test ACCURACY: 0.2690
OOD Test Loss: 4.6794

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 85...
[0m[1;37mINFO[0m: [1mCheckpoint 85: 
-----------------------------------
Train ACCURACY: 0.3198
Train Loss: 3.3902
ID Validation ACCURACY: 0.3150
ID Validation Loss: 3.4229
ID Test ACCURACY: 0.3230
ID Test Loss: 3.4163
OOD Validation ACCURACY: 0.3611
OOD Validation Loss: 3.3370
OOD Test ACCURACY: 0.2419
OOD Test Loss: 7.8889

[0m[1;37mINFO[0m: [1mChartInfo 0.3729 0.2690 0.3230 0.2419 0.3150 0.3611[0mGOODCMNIST(42000)
Data example from train: Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
Label distribution from train: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([4156, 4700, 4167, 4257, 4108, 3835, 4128, 4339, 4085, 4225]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.349
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.207
SUFF++ for r=0.6 class 0 = 0.336 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 1 = 0.469 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 2 = 0.294 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 3 = 0.295 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 4 = 0.425 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 5 = 0.329 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 6 = 0.344 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 7 = 0.379 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 8 = 0.334 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 class 9 = 0.448 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 all KL = 0.227 +- 0.231 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.6 all L1 = 0.367 +- 0.135 (in-sample avg dev_std = 0.584)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.381
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.22
SUFF++ for r=0.6 class 0 = 0.335 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 1 = 0.484 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 2 = 0.304 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 3 = 0.295 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 4 = 0.404 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 5 = 0.327 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 6 = 0.382 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 7 = 0.362 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 8 = 0.314 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 class 9 = 0.482 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 all KL = 0.234 +- 0.229 (in-sample avg dev_std = 0.581)
SUFF++ for r=0.6 all L1 = 0.369 +- 0.139 (in-sample avg dev_std = 0.581)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.349
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.207
SUFF++ for r=0.6 class 0 = 0.339 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 1 = 0.793 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 2 = 0.29 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 3 = 0.27 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 4 = 0.367 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 5 = 0.291 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 6 = 0.309 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 7 = 0.34 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 8 = 0.286 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 class 9 = 0.322 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 all KL = 0.188 +- 0.254 (in-sample avg dev_std = 0.625)
SUFF++ for r=0.6 all L1 = 0.368 +- 0.190 (in-sample avg dev_std = 0.625)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.25
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.196
SUFF++ for r=0.6 class 0 = 0.283 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 1 = 0.77 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 2 = 0.243 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 3 = 0.24 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 4 = 0.201 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 5 = 0.258 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 6 = 0.222 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 7 = 0.253 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 8 = 0.221 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 class 9 = 0.225 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 all KL = 0.149 +- 0.277 (in-sample avg dev_std = 0.486)
SUFF++ for r=0.6 all L1 = 0.298 +- 0.208 (in-sample avg dev_std = 0.486)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.349
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.263
NEC for r=0.6 class 0 = 0.238 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 1 = 0.586 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 2 = 0.541 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 3 = 0.538 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 4 = 0.444 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 5 = 0.576 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 6 = 0.522 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 7 = 0.524 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 8 = 0.532 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 class 9 = 0.349 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all KL = 0.439 +- 0.262 (in-sample avg dev_std = 0.261)
NEC for r=0.6 all L1 = 0.486 +- 0.216 (in-sample avg dev_std = 0.261)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.381
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.264
NEC for r=0.6 class 0 = 0.236 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 1 = 0.588 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 2 = 0.569 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 3 = 0.561 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 4 = 0.429 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 5 = 0.58 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 6 = 0.491 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 7 = 0.512 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 8 = 0.564 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 class 9 = 0.339 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 all KL = 0.448 +- 0.265 (in-sample avg dev_std = 0.274)
NEC for r=0.6 all L1 = 0.489 +- 0.211 (in-sample avg dev_std = 0.274)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.349
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.315
NEC for r=0.6 class 0 = 0.225 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 1 = 0.422 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 2 = 0.584 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 3 = 0.593 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 4 = 0.488 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 5 = 0.588 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 6 = 0.554 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 7 = 0.563 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 8 = 0.59 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 class 9 = 0.451 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 all KL = 0.504 +- 0.262 (in-sample avg dev_std = 0.309)
NEC for r=0.6 all L1 = 0.505 +- 0.212 (in-sample avg dev_std = 0.309)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.25
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.232
NEC for r=0.6 class 0 = 0.386 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 1 = 0.249 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 2 = 0.52 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 3 = 0.504 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 4 = 0.381 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 5 = 0.527 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 6 = 0.51 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 7 = 0.559 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 8 = 0.566 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 class 9 = 0.473 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 all KL = 0.427 +- 0.272 (in-sample avg dev_std = 0.272)
NEC for r=0.6 all L1 = 0.465 +- 0.238 (in-sample avg dev_std = 0.272)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:36:12 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:13 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:36:14 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 87...
[0m[1;37mINFO[0m: [1mCheckpoint 87: 
-----------------------------------
Train ACCURACY: 0.7181
Train Loss: 0.8746
ID Validation ACCURACY: 0.6574
ID Validation Loss: 1.2188
ID Test ACCURACY: 0.6584
ID Test Loss: 1.2023
OOD Validation ACCURACY: 0.4703
OOD Validation Loss: 2.8402
OOD Test ACCURACY: 0.1883
OOD Test Loss: 15.9927

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 67...
[0m[1;37mINFO[0m: [1mCheckpoint 67: 
-----------------------------------
Train ACCURACY: 0.6624
Train Loss: 1.0155
ID Validation ACCURACY: 0.6256
ID Validation Loss: 1.1994
ID Test ACCURACY: 0.6141
ID Test Loss: 1.2223
OOD Validation ACCURACY: 0.5277
OOD Validation Loss: 1.8377
OOD Test ACCURACY: 0.2083
OOD Test Loss: 11.1952

[0m[1;37mINFO[0m: [1mChartInfo 0.6584 0.1883 0.6141 0.2083 0.6256 0.5277[0mGOODCMNIST(42000)
Data example from train: Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
Label distribution from train: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([4156, 4700, 4167, 4257, 4108, 3835, 4128, 4339, 4085, 4225]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.661
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.209
SUFF++ for r=0.6 class 0 = 0.229 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 1 = 0.339 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 2 = 0.23 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 3 = 0.198 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 4 = 0.237 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 5 = 0.237 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 6 = 0.2 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 7 = 0.26 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 8 = 0.207 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 class 9 = 0.268 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 all KL = 0.032 +- 0.070 (in-sample avg dev_std = 0.592)
SUFF++ for r=0.6 all L1 = 0.242 +- 0.097 (in-sample avg dev_std = 0.592)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.212
SUFF++ for r=0.6 class 0 = 0.234 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 1 = 0.372 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 2 = 0.214 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 3 = 0.193 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 4 = 0.238 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 5 = 0.24 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 6 = 0.204 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 7 = 0.253 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 8 = 0.197 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 class 9 = 0.267 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 all KL = 0.034 +- 0.073 (in-sample avg dev_std = 0.586)
SUFF++ for r=0.6 all L1 = 0.243 +- 0.105 (in-sample avg dev_std = 0.586)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.477
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.181
SUFF++ for r=0.6 class 0 = 0.247 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 1 = 0.539 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 2 = 0.197 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 3 = 0.183 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 4 = 0.245 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 5 = 0.197 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 6 = 0.218 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 7 = 0.239 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 8 = 0.225 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 class 9 = 0.26 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 all KL = 0.037 +- 0.116 (in-sample avg dev_std = 0.595)
SUFF++ for r=0.6 all L1 = 0.26 +- 0.154 (in-sample avg dev_std = 0.595)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.172
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.156
SUFF++ for r=0.6 class 0 = 0.335 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 1 = 0.604 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 2 = 0.375 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 3 = 0.407 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 4 = 0.482 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 5 = 0.445 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 6 = 0.48 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 7 = 0.544 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 8 = 0.486 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 class 9 = 0.545 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 all KL = 0.205 +- 0.330 (in-sample avg dev_std = 0.682)
SUFF++ for r=0.6 all L1 = 0.471 +- 0.276 (in-sample avg dev_std = 0.682)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.661
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.418
NEC for r=0.6 class 0 = 0.671 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 1 = 0.298 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 2 = 0.568 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 3 = 0.585 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 4 = 0.539 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 5 = 0.597 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 6 = 0.665 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 7 = 0.634 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 8 = 0.515 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 class 9 = 0.634 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 all KL = 0.701 +- 0.252 (in-sample avg dev_std = 0.451)
NEC for r=0.6 all L1 = 0.567 +- 0.206 (in-sample avg dev_std = 0.451)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.429
NEC for r=0.6 class 0 = 0.669 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 1 = 0.271 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 2 = 0.587 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 3 = 0.553 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 4 = 0.516 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 5 = 0.597 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 6 = 0.663 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 7 = 0.64 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 8 = 0.537 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 class 9 = 0.6 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 all KL = 0.689 +- 0.264 (in-sample avg dev_std = 0.440)
NEC for r=0.6 all L1 = 0.559 +- 0.215 (in-sample avg dev_std = 0.440)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.477
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.368
NEC for r=0.6 class 0 = 0.564 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 1 = 0.125 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 2 = 0.608 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 3 = 0.503 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 4 = 0.566 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 5 = 0.611 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 6 = 0.61 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 7 = 0.635 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 8 = 0.445 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 class 9 = 0.588 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 all KL = 0.631 +- 0.311 (in-sample avg dev_std = 0.443)
NEC for r=0.6 all L1 = 0.52 +- 0.251 (in-sample avg dev_std = 0.443)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.172
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.177
NEC for r=0.6 class 0 = 0.35 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 1 = 0.102 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 2 = 0.41 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 3 = 0.394 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 4 = 0.276 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 5 = 0.43 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 6 = 0.25 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 7 = 0.294 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 8 = 0.354 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 class 9 = 0.227 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 all KL = 0.409 +- 0.401 (in-sample avg dev_std = 0.373)
NEC for r=0.6 all L1 = 0.306 +- 0.310 (in-sample avg dev_std = 0.373)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:45:42 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:45:43 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 56...
[0m[1;37mINFO[0m: [1mCheckpoint 56: 
-----------------------------------
Train ACCURACY: 0.4452
Train Loss: 1.8491
ID Validation ACCURACY: 0.4349
ID Validation Loss: 1.9176
ID Test ACCURACY: 0.4390
ID Test Loss: 1.8691
OOD Validation ACCURACY: 0.3544
OOD Validation Loss: 2.4861
OOD Test ACCURACY: 0.2210
OOD Test Loss: 3.7235

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 73...
[0m[1;37mINFO[0m: [1mCheckpoint 73: 
-----------------------------------
Train ACCURACY: 0.4067
Train Loss: 2.3142
ID Validation ACCURACY: 0.3957
ID Validation Loss: 2.4284
ID Test ACCURACY: 0.4073
ID Test Loss: 2.3439
OOD Validation ACCURACY: 0.3826
OOD Validation Loss: 2.4420
OOD Test ACCURACY: 0.2560
OOD Test Loss: 4.6004

[0m[1;37mINFO[0m: [1mChartInfo 0.4390 0.2210 0.4073 0.2560 0.3957 0.3826[0mGOODCMNIST(42000)
Data example from train: Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
Label distribution from train: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([4156, 4700, 4167, 4257, 4108, 3835, 4128, 4339, 4085, 4225]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.439
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.181
SUFF++ for r=0.6 class 0 = 0.242 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 1 = 0.432 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 2 = 0.227 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 3 = 0.249 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 4 = 0.223 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 5 = 0.235 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 6 = 0.243 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 7 = 0.231 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 8 = 0.223 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 class 9 = 0.23 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 all KL = 0.071 +- 0.143 (in-sample avg dev_std = 0.513)
SUFF++ for r=0.6 all L1 = 0.256 +- 0.103 (in-sample avg dev_std = 0.513)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.452
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.19
SUFF++ for r=0.6 class 0 = 0.254 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 1 = 0.453 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 2 = 0.227 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 3 = 0.233 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 4 = 0.224 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 5 = 0.232 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 6 = 0.242 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 7 = 0.25 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 8 = 0.228 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 class 9 = 0.235 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 all KL = 0.078 +- 0.150 (in-sample avg dev_std = 0.518)
SUFF++ for r=0.6 all L1 = 0.261 +- 0.106 (in-sample avg dev_std = 0.518)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.338
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.191
SUFF++ for r=0.6 class 0 = 0.344 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 1 = 0.433 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 2 = 0.285 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 3 = 0.273 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 4 = 0.267 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 5 = 0.297 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 6 = 0.294 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 7 = 0.297 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 8 = 0.258 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 class 9 = 0.287 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 all KL = 0.119 +- 0.165 (in-sample avg dev_std = 0.580)
SUFF++ for r=0.6 all L1 = 0.305 +- 0.101 (in-sample avg dev_std = 0.580)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.199
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.16
SUFF++ for r=0.6 class 0 = 0.295 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 1 = 0.351 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 2 = 0.218 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 3 = 0.214 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 4 = 0.211 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 5 = 0.237 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 6 = 0.227 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 7 = 0.241 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 8 = 0.221 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 class 9 = 0.224 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 all KL = 0.076 +- 0.128 (in-sample avg dev_std = 0.478)
SUFF++ for r=0.6 all L1 = 0.245 +- 0.083 (in-sample avg dev_std = 0.478)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.439
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.34
NEC for r=0.6 class 0 = 0.203 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 1 = 0.5 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 2 = 0.502 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 3 = 0.5 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 4 = 0.439 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 5 = 0.501 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 6 = 0.522 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 7 = 0.53 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 8 = 0.474 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 class 9 = 0.373 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 all KL = 0.367 +- 0.219 (in-sample avg dev_std = 0.270)
NEC for r=0.6 all L1 = 0.455 +- 0.187 (in-sample avg dev_std = 0.270)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.452
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.351
NEC for r=0.6 class 0 = 0.23 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 1 = 0.512 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 2 = 0.534 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 3 = 0.514 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 4 = 0.424 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 5 = 0.509 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 6 = 0.489 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 7 = 0.488 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 8 = 0.439 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 class 9 = 0.366 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 all KL = 0.365 +- 0.219 (in-sample avg dev_std = 0.276)
NEC for r=0.6 all L1 = 0.452 +- 0.183 (in-sample avg dev_std = 0.276)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.338
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.289
NEC for r=0.6 class 0 = 0.143 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 1 = 0.547 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 2 = 0.51 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 3 = 0.505 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 4 = 0.455 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 5 = 0.505 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 6 = 0.478 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 7 = 0.507 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 8 = 0.517 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 class 9 = 0.473 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 all KL = 0.395 +- 0.218 (in-sample avg dev_std = 0.294)
NEC for r=0.6 all L1 = 0.466 +- 0.193 (in-sample avg dev_std = 0.294)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.199
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.192
NEC for r=0.6 class 0 = 0.288 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 1 = 0.36 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 2 = 0.422 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 3 = 0.487 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 4 = 0.413 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 5 = 0.472 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 6 = 0.472 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 7 = 0.494 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 8 = 0.522 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 class 9 = 0.469 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 all KL = 0.348 +- 0.254 (in-sample avg dev_std = 0.283)
NEC for r=0.6 all L1 = 0.438 +- 0.203 (in-sample avg dev_std = 0.283)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 17:55:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:08 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:08 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:08 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 05:55:09 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.6588
Train Loss: 0.9911
ID Validation ACCURACY: 0.6276
ID Validation Loss: 1.1178
ID Test ACCURACY: 0.6277
ID Test Loss: 1.1142
OOD Validation ACCURACY: 0.4983
OOD Validation Loss: 1.7285
OOD Test ACCURACY: 0.2486
OOD Test Loss: 4.9047

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.5989
Train Loss: 1.1783
ID Validation ACCURACY: 0.5699
ID Validation Loss: 1.3077
ID Test ACCURACY: 0.5776
ID Test Loss: 1.2851
OOD Validation ACCURACY: 0.5351
OOD Validation Loss: 1.4928
OOD Test ACCURACY: 0.2547
OOD Test Loss: 5.2471

[0m[1;37mINFO[0m: [1mChartInfo 0.6277 0.2486 0.5776 0.2547 0.5699 0.5351[0mGOODCMNIST(42000)
Data example from train: Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
Label distribution from train: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([4156, 4700, 4167, 4257, 4108, 3835, 4128, 4339, 4085, 4225]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.227
SUFF++ for r=0.6 class 0 = 0.277 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 1 = 0.77 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 2 = 0.223 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 3 = 0.213 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 4 = 0.245 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 5 = 0.222 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 6 = 0.208 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 7 = 0.218 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 8 = 0.233 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 class 9 = 0.244 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all KL = 0.1 +- 0.214 (in-sample avg dev_std = 0.554)
SUFF++ for r=0.6 all L1 = 0.292 +- 0.195 (in-sample avg dev_std = 0.554)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.232
SUFF++ for r=0.6 class 0 = 0.294 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 1 = 0.815 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 2 = 0.206 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 3 = 0.213 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 4 = 0.25 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 5 = 0.235 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 6 = 0.22 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 7 = 0.222 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 8 = 0.222 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 class 9 = 0.242 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 all KL = 0.107 +- 0.224 (in-sample avg dev_std = 0.556)
SUFF++ for r=0.6 all L1 = 0.299 +- 0.205 (in-sample avg dev_std = 0.556)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.507
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.206
SUFF++ for r=0.6 class 0 = 0.303 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 1 = 0.915 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 2 = 0.212 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 3 = 0.218 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 4 = 0.266 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 5 = 0.227 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 6 = 0.238 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 7 = 0.261 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 8 = 0.268 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 class 9 = 0.311 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 all KL = 0.176 +- 0.300 (in-sample avg dev_std = 0.542)
SUFF++ for r=0.6 all L1 = 0.331 +- 0.247 (in-sample avg dev_std = 0.542)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.26
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.185
SUFF++ for r=0.6 class 0 = 0.308 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 1 = 0.894 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 2 = 0.283 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 3 = 0.355 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 4 = 0.458 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 5 = 0.361 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 6 = 0.478 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 7 = 0.456 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 8 = 0.467 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 9 = 0.636 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 all KL = 0.358 +- 0.337 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 all L1 = 0.473 +- 0.283 (in-sample avg dev_std = 0.516)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.611
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.398
NEC for r=0.6 class 0 = 0.61 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 1 = 0.133 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 2 = 0.5 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 3 = 0.544 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 4 = 0.513 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 5 = 0.55 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 6 = 0.662 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 7 = 0.611 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 8 = 0.552 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 class 9 = 0.607 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 all KL = 0.54 +- 0.268 (in-sample avg dev_std = 0.331)
NEC for r=0.6 all L1 = 0.523 +- 0.206 (in-sample avg dev_std = 0.331)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.431
NEC for r=0.6 class 0 = 0.591 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 1 = 0.096 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 2 = 0.519 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 3 = 0.499 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 4 = 0.491 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 5 = 0.552 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 6 = 0.634 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 7 = 0.603 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 8 = 0.564 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 class 9 = 0.612 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 all KL = 0.525 +- 0.270 (in-sample avg dev_std = 0.316)
NEC for r=0.6 all L1 = 0.51 +- 0.214 (in-sample avg dev_std = 0.316)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.507
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.417
NEC for r=0.6 class 0 = 0.418 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 1 = 0.054 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 2 = 0.541 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 3 = 0.552 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 4 = 0.549 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 5 = 0.556 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 6 = 0.576 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 7 = 0.602 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 8 = 0.483 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 class 9 = 0.566 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 all KL = 0.482 +- 0.275 (in-sample avg dev_std = 0.328)
NEC for r=0.6 all L1 = 0.484 +- 0.228 (in-sample avg dev_std = 0.328)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.26
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.247
NEC for r=0.6 class 0 = 0.448 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 1 = 0.049 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 2 = 0.506 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 3 = 0.446 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 4 = 0.35 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 5 = 0.447 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 6 = 0.302 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 7 = 0.39 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 8 = 0.374 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 class 9 = 0.213 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 all KL = 0.318 +- 0.268 (in-sample avg dev_std = 0.284)
NEC for r=0.6 all L1 = 0.35 +- 0.252 (in-sample avg dev_std = 0.284)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 18:04:59 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:00 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:05:01 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 42...
[0m[1;37mINFO[0m: [1mCheckpoint 42: 
-----------------------------------
Train ACCURACY: 0.4206
Train Loss: 1.8923
ID Validation ACCURACY: 0.4203
ID Validation Loss: 1.9484
ID Test ACCURACY: 0.4191
ID Test Loss: 1.8983
OOD Validation ACCURACY: 0.3490
OOD Validation Loss: 2.3843
OOD Test ACCURACY: 0.2689
OOD Test Loss: 2.7813

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.4088
Train Loss: 1.8664
ID Validation ACCURACY: 0.4009
ID Validation Loss: 1.9187
ID Test ACCURACY: 0.4040
ID Test Loss: 1.9019
OOD Validation ACCURACY: 0.3840
OOD Validation Loss: 2.1207
OOD Test ACCURACY: 0.2513
OOD Test Loss: 3.0490

[0m[1;37mINFO[0m: [1mChartInfo 0.4191 0.2689 0.4040 0.2513 0.4009 0.3840[0mGOODCMNIST(42000)
Data example from train: Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
Label distribution from train: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([4156, 4700, 4167, 4257, 4108, 3835, 4128, 4339, 4085, 4225]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from val: Data(x=[75, 3], edge_index=[2, 1422], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1422], node_perm=[75])
Label distribution from val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([670, 794, 681, 716, 699, 616, 708, 732, 655, 729]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.415
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.23
SUFF++ for r=0.6 class 0 = 0.325 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 1 = 0.593 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 2 = 0.274 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 3 = 0.309 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 4 = 0.284 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 5 = 0.286 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 6 = 0.271 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 7 = 0.314 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 8 = 0.299 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 class 9 = 0.285 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 all KL = 0.193 +- 0.208 (in-sample avg dev_std = 0.574)
SUFF++ for r=0.6 all L1 = 0.328 +- 0.137 (in-sample avg dev_std = 0.574)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.431
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.226
SUFF++ for r=0.6 class 0 = 0.328 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 1 = 0.626 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 2 = 0.266 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 3 = 0.295 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 4 = 0.287 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 5 = 0.297 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 6 = 0.282 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 7 = 0.317 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 8 = 0.292 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 class 9 = 0.288 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 all KL = 0.19 +- 0.213 (in-sample avg dev_std = 0.577)
SUFF++ for r=0.6 all L1 = 0.332 +- 0.140 (in-sample avg dev_std = 0.577)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.335
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.195
SUFF++ for r=0.6 class 0 = 0.365 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 1 = 0.615 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 2 = 0.294 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 3 = 0.296 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 4 = 0.289 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 5 = 0.302 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 6 = 0.302 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 7 = 0.309 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 8 = 0.318 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 class 9 = 0.312 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 all KL = 0.196 +- 0.238 (in-sample avg dev_std = 0.626)
SUFF++ for r=0.6 all L1 = 0.344 +- 0.130 (in-sample avg dev_std = 0.626)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.255
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.186
SUFF++ for r=0.6 class 0 = 0.347 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 1 = 0.469 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 2 = 0.248 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 3 = 0.241 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 4 = 0.232 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 5 = 0.265 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 6 = 0.239 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 7 = 0.275 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 8 = 0.238 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 class 9 = 0.264 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 all KL = 0.167 +- 0.210 (in-sample avg dev_std = 0.516)
SUFF++ for r=0.6 all L1 = 0.284 +- 0.113 (in-sample avg dev_std = 0.516)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.415
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.384
NEC for r=0.6 class 0 = 0.221 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 1 = 0.306 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 2 = 0.459 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 3 = 0.445 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 4 = 0.379 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 5 = 0.502 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 6 = 0.51 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 7 = 0.472 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 8 = 0.437 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 class 9 = 0.439 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 all KL = 0.302 +- 0.195 (in-sample avg dev_std = 0.234)
NEC for r=0.6 all L1 = 0.415 +- 0.175 (in-sample avg dev_std = 0.234)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.431
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.4
NEC for r=0.6 class 0 = 0.242 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 1 = 0.317 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 2 = 0.455 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 3 = 0.442 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 4 = 0.386 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 5 = 0.493 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 6 = 0.501 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 7 = 0.467 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 8 = 0.441 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 class 9 = 0.421 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 all KL = 0.307 +- 0.196 (in-sample avg dev_std = 0.242)
NEC for r=0.6 all L1 = 0.415 +- 0.168 (in-sample avg dev_std = 0.242)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.335
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.332
NEC for r=0.6 class 0 = 0.074 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 1 = 0.379 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 2 = 0.43 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 3 = 0.45 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 4 = 0.418 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 5 = 0.431 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 6 = 0.446 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 7 = 0.461 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 8 = 0.407 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 9 = 0.439 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 all KL = 0.297 +- 0.189 (in-sample avg dev_std = 0.262)
NEC for r=0.6 all L1 = 0.395 +- 0.185 (in-sample avg dev_std = 0.262)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.255
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.269
NEC for r=0.6 class 0 = 0.203 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 1 = 0.297 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 2 = 0.494 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 3 = 0.495 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 4 = 0.449 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 5 = 0.472 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 6 = 0.486 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 7 = 0.455 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 8 = 0.485 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 class 9 = 0.428 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 all KL = 0.31 +- 0.202 (in-sample avg dev_std = 0.237)
NEC for r=0.6 all L1 = 0.424 +- 0.173 (in-sample avg dev_std = 0.237)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.227], 'all_L1': [0.367]}), defaultdict(<class 'list'>, {'all_KL': [0.032], 'all_L1': [0.242]}), defaultdict(<class 'list'>, {'all_KL': [0.071], 'all_L1': [0.256]}), defaultdict(<class 'list'>, {'all_KL': [0.1], 'all_L1': [0.292]}), defaultdict(<class 'list'>, {'all_KL': [0.193], 'all_L1': [0.328]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.439], 'all_L1': [0.486]}), defaultdict(<class 'list'>, {'all_KL': [0.701], 'all_L1': [0.567]}), defaultdict(<class 'list'>, {'all_KL': [0.367], 'all_L1': [0.455]}), defaultdict(<class 'list'>, {'all_KL': [0.54], 'all_L1': [0.523]}), defaultdict(<class 'list'>, {'all_KL': [0.302], 'all_L1': [0.415]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.234], 'all_L1': [0.369]}), defaultdict(<class 'list'>, {'all_KL': [0.034], 'all_L1': [0.243]}), defaultdict(<class 'list'>, {'all_KL': [0.078], 'all_L1': [0.261]}), defaultdict(<class 'list'>, {'all_KL': [0.107], 'all_L1': [0.299]}), defaultdict(<class 'list'>, {'all_KL': [0.19], 'all_L1': [0.332]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.448], 'all_L1': [0.489]}), defaultdict(<class 'list'>, {'all_KL': [0.689], 'all_L1': [0.559]}), defaultdict(<class 'list'>, {'all_KL': [0.365], 'all_L1': [0.452]}), defaultdict(<class 'list'>, {'all_KL': [0.525], 'all_L1': [0.51]}), defaultdict(<class 'list'>, {'all_KL': [0.307], 'all_L1': [0.415]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.188], 'all_L1': [0.368]}), defaultdict(<class 'list'>, {'all_KL': [0.037], 'all_L1': [0.26]}), defaultdict(<class 'list'>, {'all_KL': [0.119], 'all_L1': [0.305]}), defaultdict(<class 'list'>, {'all_KL': [0.176], 'all_L1': [0.331]}), defaultdict(<class 'list'>, {'all_KL': [0.196], 'all_L1': [0.344]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.504], 'all_L1': [0.505]}), defaultdict(<class 'list'>, {'all_KL': [0.631], 'all_L1': [0.52]}), defaultdict(<class 'list'>, {'all_KL': [0.395], 'all_L1': [0.466]}), defaultdict(<class 'list'>, {'all_KL': [0.482], 'all_L1': [0.484]}), defaultdict(<class 'list'>, {'all_KL': [0.297], 'all_L1': [0.395]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.149], 'all_L1': [0.298]}), defaultdict(<class 'list'>, {'all_KL': [0.205], 'all_L1': [0.471]}), defaultdict(<class 'list'>, {'all_KL': [0.076], 'all_L1': [0.245]}), defaultdict(<class 'list'>, {'all_KL': [0.358], 'all_L1': [0.473]}), defaultdict(<class 'list'>, {'all_KL': [0.167], 'all_L1': [0.284]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.427], 'all_L1': [0.465]}), defaultdict(<class 'list'>, {'all_KL': [0.409], 'all_L1': [0.306]}), defaultdict(<class 'list'>, {'all_KL': [0.348], 'all_L1': [0.438]}), defaultdict(<class 'list'>, {'all_KL': [0.318], 'all_L1': [0.35]}), defaultdict(<class 'list'>, {'all_KL': [0.31], 'all_L1': [0.424]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.297 +- 0.046
suff++ class all_KL  =  0.125 +- 0.074
suff++_acc_int  =  0.211 +- 0.018
nec class all_L1  =  0.489 +- 0.053
nec class all_KL  =  0.470 +- 0.140
nec_acc_int  =  0.361 +- 0.055

Eval split id_val
suff++ class all_L1  =  0.301 +- 0.046
suff++ class all_KL  =  0.129 +- 0.073
suff++_acc_int  =  0.216 +- 0.014
nec class all_L1  =  0.485 +- 0.049
nec class all_KL  =  0.467 +- 0.133
nec_acc_int  =  0.375 +- 0.063

Eval split val
suff++ class all_L1  =  0.322 +- 0.037
suff++ class all_KL  =  0.143 +- 0.060
suff++_acc_int  =  0.196 +- 0.010
nec class all_L1  =  0.474 +- 0.044
nec class all_KL  =  0.462 +- 0.112
nec_acc_int  =  0.344 +- 0.045

Eval split test
suff++ class all_L1  =  0.354 +- 0.098
suff++ class all_KL  =  0.191 +- 0.093
suff++_acc_int  =  0.177 +- 0.016
nec class all_L1  =  0.397 +- 0.059
nec class all_KL  =  0.362 +- 0.047
nec_acc_int  =  0.223 +- 0.034


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.393 +- 0.026
Faith. Armon (L1)= 		  =  0.365 +- 0.032
Faith. GMean (L1)= 	  =  0.379 +- 0.027
Faith. Aritm (KL)= 		  =  0.297 +- 0.055
Faith. Armon (KL)= 		  =  0.177 +- 0.084
Faith. GMean (KL)= 	  =  0.220 +- 0.060

Eval split id_val
Faith. Aritm (L1)= 		  =  0.393 +- 0.025
Faith. Armon (L1)= 		  =  0.367 +- 0.032
Faith. GMean (L1)= 	  =  0.380 +- 0.027
Faith. Aritm (KL)= 		  =  0.298 +- 0.054
Faith. Armon (KL)= 		  =  0.183 +- 0.084
Faith. GMean (KL)= 	  =  0.225 +- 0.061

Eval split val
Faith. Aritm (L1)= 		  =  0.398 +- 0.023
Faith. Armon (L1)= 		  =  0.380 +- 0.027
Faith. GMean (L1)= 	  =  0.389 +- 0.024
Faith. Aritm (KL)= 		  =  0.303 +- 0.042
Faith. Armon (KL)= 		  =  0.204 +- 0.074
Faith. GMean (KL)= 	  =  0.242 +- 0.055

Eval split test
Faith. Aritm (L1)= 		  =  0.375 +- 0.025
Faith. Armon (L1)= 		  =  0.358 +- 0.030
Faith. GMean (L1)= 	  =  0.367 +- 0.027
Faith. Aritm (KL)= 		  =  0.277 +- 0.046
Faith. Armon (KL)= 		  =  0.235 +- 0.070
Faith. GMean (KL)= 	  =  0.254 +- 0.059
Computed for split load_split = id



Completed in  0:47:25.827267  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 18:14:57 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/27/2024 06:14:58 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 06:15:29 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 06:15:40 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 06:15:51 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:08 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:16:24 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 23...
[0m[1;37mINFO[0m: [1mCheckpoint 23: 
-----------------------------------
Train ROC-AUC: 0.9686
Train Loss: 0.1597
ID Validation ROC-AUC: 0.9236
ID Validation Loss: 0.2366
ID Test ROC-AUC: 0.9233
ID Test Loss: 0.2412
OOD Validation ROC-AUC: 0.6572
OOD Validation Loss: 0.3846
OOD Test ROC-AUC: 0.6967
OOD Test Loss: 0.5574

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 1...
[0m[1;37mINFO[0m: [1mCheckpoint 1: 
-----------------------------------
Train ROC-AUC: 0.8713
Train Loss: 0.2869
ID Validation ROC-AUC: 0.8665
ID Validation Loss: 0.2911
ID Test ROC-AUC: 0.8695
ID Test Loss: 0.2927
OOD Validation ROC-AUC: 0.6861
OOD Validation Loss: 0.2792
OOD Test ROC-AUC: 0.7034
OOD Test Loss: 0.4207

[0m[1;37mINFO[0m: [1mChartInfo 0.9233 0.6967 0.8695 0.7034 0.8665 0.6861[0mLBAPcore(34179)
Data example from train: Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
Label distribution from train: (tensor([0., 1.]), tensor([ 3960, 30219]))
[1;34mDEBUG[0m: 04/27/2024 06:16:25 PM : [1mUnbalanced warning for LBAPcore (train)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/27/2024 06:16:35 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/27/2024 06:16:43 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/27/2024 06:16:51 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.601
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.56
SUFF++ for r=0.6 class 0.0 = 0.829 +- 0.048 (in-sample avg dev_std = 0.144)
SUFF++ for r=0.6 class 1.0 = 0.862 +- 0.048 (in-sample avg dev_std = 0.144)
SUFF++ for r=0.6 all KL = 0.952 +- 0.048 (in-sample avg dev_std = 0.144)
SUFF++ for r=0.6 all L1 = 0.858 +- 0.073 (in-sample avg dev_std = 0.144)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.616
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.601
SUFF++ for r=0.6 class 0.0 = 0.823 +- 0.052 (in-sample avg dev_std = 0.142)
SUFF++ for r=0.6 class 1.0 = 0.868 +- 0.052 (in-sample avg dev_std = 0.142)
SUFF++ for r=0.6 all KL = 0.952 +- 0.052 (in-sample avg dev_std = 0.142)
SUFF++ for r=0.6 all L1 = 0.863 +- 0.073 (in-sample avg dev_std = 0.142)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.495
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 799
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.511
SUFF++ for r=0.6 class 0.0 = 0.83 +- 0.060 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.6 class 1.0 = 0.858 +- 0.060 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.6 all KL = 0.949 +- 0.060 (in-sample avg dev_std = 0.147)
SUFF++ for r=0.6 all L1 = 0.856 +- 0.075 (in-sample avg dev_std = 0.147)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.562
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.555
SUFF++ for r=0.6 class 0.0 = 0.834 +- 0.069 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.6 class 1.0 = 0.848 +- 0.069 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.6 all KL = 0.942 +- 0.069 (in-sample avg dev_std = 0.156)
SUFF++ for r=0.6 all L1 = 0.845 +- 0.080 (in-sample avg dev_std = 0.156)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.601
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.611
NEC for r=0.6 class 0.0 = 0.121 +- 0.029 (in-sample avg dev_std = 0.088)
NEC for r=0.6 class 1.0 = 0.089 +- 0.029 (in-sample avg dev_std = 0.088)
NEC for r=0.6 all KL = 0.02 +- 0.029 (in-sample avg dev_std = 0.088)
NEC for r=0.6 all L1 = 0.093 +- 0.067 (in-sample avg dev_std = 0.088)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.616
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.639
NEC for r=0.6 class 0.0 = 0.13 +- 0.035 (in-sample avg dev_std = 0.087)
NEC for r=0.6 class 1.0 = 0.084 +- 0.035 (in-sample avg dev_std = 0.087)
NEC for r=0.6 all KL = 0.021 +- 0.035 (in-sample avg dev_std = 0.087)
NEC for r=0.6 all L1 = 0.089 +- 0.069 (in-sample avg dev_std = 0.087)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.502
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.542
NEC for r=0.6 class 0.0 = 0.132 +- 0.046 (in-sample avg dev_std = 0.097)
NEC for r=0.6 class 1.0 = 0.101 +- 0.046 (in-sample avg dev_std = 0.097)
NEC for r=0.6 all KL = 0.027 +- 0.046 (in-sample avg dev_std = 0.097)
NEC for r=0.6 all L1 = 0.104 +- 0.075 (in-sample avg dev_std = 0.097)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.562
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.577
NEC for r=0.6 class 0.0 = 0.12 +- 0.040 (in-sample avg dev_std = 0.088)
NEC for r=0.6 class 1.0 = 0.103 +- 0.040 (in-sample avg dev_std = 0.088)
NEC for r=0.6 all KL = 0.025 +- 0.040 (in-sample avg dev_std = 0.088)
NEC for r=0.6 all L1 = 0.106 +- 0.078 (in-sample avg dev_std = 0.088)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 18:18:33 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/27/2024 06:18:34 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:05 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:16 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:27 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:43 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:19:59 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 37...
[0m[1;37mINFO[0m: [1mCheckpoint 37: 
-----------------------------------
Train ROC-AUC: 0.9829
Train Loss: 0.1261
ID Validation ROC-AUC: 0.9196
ID Validation Loss: 0.2917
ID Test ROC-AUC: 0.9170
ID Test Loss: 0.3056
OOD Validation ROC-AUC: 0.6433
OOD Validation Loss: 0.4965
OOD Test ROC-AUC: 0.6785
OOD Test Loss: 0.7235

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8825
Train Loss: 0.2710
ID Validation ROC-AUC: 0.8746
ID Validation Loss: 0.2831
ID Test ROC-AUC: 0.8768
ID Test Loss: 0.2841
OOD Validation ROC-AUC: 0.6840
OOD Validation Loss: 0.3139
OOD Test ROC-AUC: 0.7101
OOD Test Loss: 0.4991

[0m[1;37mINFO[0m: [1mChartInfo 0.9170 0.6785 0.8768 0.7101 0.8746 0.6840[0mLBAPcore(34179)
Data example from train: Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
Label distribution from train: (tensor([0., 1.]), tensor([ 3960, 30219]))
[1;34mDEBUG[0m: 04/27/2024 06:20:00 PM : [1mUnbalanced warning for LBAPcore (train)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/27/2024 06:20:08 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/27/2024 06:20:17 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/27/2024 06:20:25 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.713
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.623
SUFF++ for r=0.6 class 0.0 = 0.695 +- 0.149 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.6 class 1.0 = 0.821 +- 0.149 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.6 all KL = 0.843 +- 0.149 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.6 all L1 = 0.807 +- 0.167 (in-sample avg dev_std = 0.256)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.628
SUFF++ for r=0.6 class 0.0 = 0.705 +- 0.146 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.6 class 1.0 = 0.819 +- 0.146 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.6 all KL = 0.846 +- 0.146 (in-sample avg dev_std = 0.256)
SUFF++ for r=0.6 all L1 = 0.806 +- 0.168 (in-sample avg dev_std = 0.256)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.648
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.61
SUFF++ for r=0.6 class 0.0 = 0.71 +- 0.144 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 class 1.0 = 0.776 +- 0.144 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 all KL = 0.832 +- 0.144 (in-sample avg dev_std = 0.272)
SUFF++ for r=0.6 all L1 = 0.77 +- 0.170 (in-sample avg dev_std = 0.272)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.587
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.565
SUFF++ for r=0.6 class 0.0 = 0.706 +- 0.150 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.6 class 1.0 = 0.771 +- 0.150 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.6 all KL = 0.812 +- 0.150 (in-sample avg dev_std = 0.286)
SUFF++ for r=0.6 all L1 = 0.76 +- 0.164 (in-sample avg dev_std = 0.286)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.713
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.704
NEC for r=0.6 class 0.0 = 0.273 +- 0.135 (in-sample avg dev_std = 0.172)
NEC for r=0.6 class 1.0 = 0.159 +- 0.135 (in-sample avg dev_std = 0.172)
NEC for r=0.6 all KL = 0.112 +- 0.135 (in-sample avg dev_std = 0.172)
NEC for r=0.6 all L1 = 0.173 +- 0.176 (in-sample avg dev_std = 0.172)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.697
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.69
NEC for r=0.6 class 0.0 = 0.256 +- 0.129 (in-sample avg dev_std = 0.173)
NEC for r=0.6 class 1.0 = 0.159 +- 0.129 (in-sample avg dev_std = 0.173)
NEC for r=0.6 all KL = 0.106 +- 0.129 (in-sample avg dev_std = 0.173)
NEC for r=0.6 all L1 = 0.17 +- 0.176 (in-sample avg dev_std = 0.173)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.648
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.634
NEC for r=0.6 class 0.0 = 0.242 +- 0.133 (in-sample avg dev_std = 0.186)
NEC for r=0.6 class 1.0 = 0.201 +- 0.133 (in-sample avg dev_std = 0.186)
NEC for r=0.6 all KL = 0.123 +- 0.133 (in-sample avg dev_std = 0.186)
NEC for r=0.6 all L1 = 0.205 +- 0.178 (in-sample avg dev_std = 0.186)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.587
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.594
NEC for r=0.6 class 0.0 = 0.259 +- 0.132 (in-sample avg dev_std = 0.183)
NEC for r=0.6 class 1.0 = 0.194 +- 0.132 (in-sample avg dev_std = 0.183)
NEC for r=0.6 all KL = 0.126 +- 0.132 (in-sample avg dev_std = 0.183)
NEC for r=0.6 all L1 = 0.205 +- 0.178 (in-sample avg dev_std = 0.183)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 18:22:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/27/2024 06:22:07 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 06:22:39 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 06:22:50 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:01 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:17 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.9540
Train Loss: 0.1749
ID Validation ROC-AUC: 0.9236
ID Validation Loss: 0.2164
ID Test ROC-AUC: 0.9206
ID Test Loss: 0.2244
OOD Validation ROC-AUC: 0.6596
OOD Validation Loss: 0.3480
OOD Test ROC-AUC: 0.6987
OOD Test Loss: 0.4931

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8972
Train Loss: 0.2438
ID Validation ROC-AUC: 0.8845
ID Validation Loss: 0.2539
ID Test ROC-AUC: 0.8865
ID Test Loss: 0.2549
OOD Validation ROC-AUC: 0.6935
OOD Validation Loss: 0.2839
OOD Test ROC-AUC: 0.7162
OOD Test Loss: 0.4332

[0m[1;37mINFO[0m: [1mChartInfo 0.9206 0.6987 0.8865 0.7162 0.8845 0.6935[0mLBAPcore(34179)
Data example from train: Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
Label distribution from train: (tensor([0., 1.]), tensor([ 3960, 30219]))
[1;34mDEBUG[0m: 04/27/2024 06:23:34 PM : [1mUnbalanced warning for LBAPcore (train)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/27/2024 06:23:43 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/27/2024 06:23:51 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/27/2024 06:23:59 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.643
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.594
SUFF++ for r=0.6 class 0.0 = 0.884 +- 0.023 (in-sample avg dev_std = 0.104)
SUFF++ for r=0.6 class 1.0 = 0.885 +- 0.023 (in-sample avg dev_std = 0.104)
SUFF++ for r=0.6 all KL = 0.983 +- 0.023 (in-sample avg dev_std = 0.104)
SUFF++ for r=0.6 all L1 = 0.885 +- 0.046 (in-sample avg dev_std = 0.104)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.677
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.608
SUFF++ for r=0.6 class 0.0 = 0.887 +- 0.020 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.6 class 1.0 = 0.889 +- 0.020 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.6 all KL = 0.985 +- 0.020 (in-sample avg dev_std = 0.099)
SUFF++ for r=0.6 all L1 = 0.889 +- 0.045 (in-sample avg dev_std = 0.099)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.5
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.475
SUFF++ for r=0.6 class 0.0 = 0.897 +- 0.011 (in-sample avg dev_std = 0.089)
SUFF++ for r=0.6 class 1.0 = 0.888 +- 0.011 (in-sample avg dev_std = 0.089)
SUFF++ for r=0.6 all KL = 0.987 +- 0.011 (in-sample avg dev_std = 0.089)
SUFF++ for r=0.6 all L1 = 0.889 +- 0.041 (in-sample avg dev_std = 0.089)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.566
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.549
SUFF++ for r=0.6 class 0.0 = 0.887 +- 0.012 (in-sample avg dev_std = 0.090)
SUFF++ for r=0.6 class 1.0 = 0.889 +- 0.012 (in-sample avg dev_std = 0.090)
SUFF++ for r=0.6 all KL = 0.987 +- 0.012 (in-sample avg dev_std = 0.090)
SUFF++ for r=0.6 all L1 = 0.889 +- 0.043 (in-sample avg dev_std = 0.090)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.643
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.607
NEC for r=0.6 class 0.0 = 0.063 +- 0.018 (in-sample avg dev_std = 0.059)
NEC for r=0.6 class 1.0 = 0.065 +- 0.018 (in-sample avg dev_std = 0.059)
NEC for r=0.6 all KL = 0.006 +- 0.018 (in-sample avg dev_std = 0.059)
NEC for r=0.6 all L1 = 0.065 +- 0.035 (in-sample avg dev_std = 0.059)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.677
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.633
NEC for r=0.6 class 0.0 = 0.066 +- 0.016 (in-sample avg dev_std = 0.057)
NEC for r=0.6 class 1.0 = 0.065 +- 0.016 (in-sample avg dev_std = 0.057)
NEC for r=0.6 all KL = 0.006 +- 0.016 (in-sample avg dev_std = 0.057)
NEC for r=0.6 all L1 = 0.065 +- 0.034 (in-sample avg dev_std = 0.057)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.499
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.495
NEC for r=0.6 class 0.0 = 0.075 +- 0.005 (in-sample avg dev_std = 0.051)
NEC for r=0.6 class 1.0 = 0.063 +- 0.005 (in-sample avg dev_std = 0.051)
NEC for r=0.6 all KL = 0.004 +- 0.005 (in-sample avg dev_std = 0.051)
NEC for r=0.6 all L1 = 0.064 +- 0.032 (in-sample avg dev_std = 0.051)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.566
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.556
NEC for r=0.6 class 0.0 = 0.066 +- 0.008 (in-sample avg dev_std = 0.054)
NEC for r=0.6 class 1.0 = 0.067 +- 0.008 (in-sample avg dev_std = 0.054)
NEC for r=0.6 all KL = 0.005 +- 0.008 (in-sample avg dev_std = 0.054)
NEC for r=0.6 all L1 = 0.067 +- 0.036 (in-sample avg dev_std = 0.054)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 18:25:44 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/27/2024 06:25:44 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 06:26:17 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 06:26:28 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 06:26:39 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 06:26:56 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:27:12 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 31...
[0m[1;37mINFO[0m: [1mCheckpoint 31: 
-----------------------------------
Train ROC-AUC: 0.9785
Train Loss: 0.1297
ID Validation ROC-AUC: 0.9231
ID Validation Loss: 0.2533
ID Test ROC-AUC: 0.9232
ID Test Loss: 0.2591
OOD Validation ROC-AUC: 0.6355
OOD Validation Loss: 0.4365
OOD Test ROC-AUC: 0.6820
OOD Test Loss: 0.6119

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 7...
[0m[1;37mINFO[0m: [1mCheckpoint 7: 
-----------------------------------
Train ROC-AUC: 0.9090
Train Loss: 0.2539
ID Validation ROC-AUC: 0.8928
ID Validation Loss: 0.2745
ID Test ROC-AUC: 0.8905
ID Test Loss: 0.2822
OOD Validation ROC-AUC: 0.6800
OOD Validation Loss: 0.3263
OOD Test ROC-AUC: 0.7103
OOD Test Loss: 0.5223

[0m[1;37mINFO[0m: [1mChartInfo 0.9232 0.6820 0.8905 0.7103 0.8928 0.6800[0mLBAPcore(34179)
Data example from train: Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
Label distribution from train: (tensor([0., 1.]), tensor([ 3960, 30219]))
[1;34mDEBUG[0m: 04/27/2024 06:27:13 PM : [1mUnbalanced warning for LBAPcore (train)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/27/2024 06:27:21 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/27/2024 06:27:30 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/27/2024 06:27:38 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.632
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.624
SUFF++ for r=0.6 class 0.0 = 0.697 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.6 class 1.0 = 0.683 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.6 all KL = 0.809 +- 0.207 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.6 all L1 = 0.684 +- 0.157 (in-sample avg dev_std = 0.326)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.609
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.615
SUFF++ for r=0.6 class 0.0 = 0.692 +- 0.217 (in-sample avg dev_std = 0.342)
SUFF++ for r=0.6 class 1.0 = 0.667 +- 0.217 (in-sample avg dev_std = 0.342)
SUFF++ for r=0.6 all KL = 0.794 +- 0.217 (in-sample avg dev_std = 0.342)
SUFF++ for r=0.6 all L1 = 0.67 +- 0.161 (in-sample avg dev_std = 0.342)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.579
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.581
SUFF++ for r=0.6 class 0.0 = 0.682 +- 0.195 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.6 class 1.0 = 0.667 +- 0.195 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.6 all KL = 0.804 +- 0.195 (in-sample avg dev_std = 0.326)
SUFF++ for r=0.6 all L1 = 0.668 +- 0.160 (in-sample avg dev_std = 0.326)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.515
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.543
SUFF++ for r=0.6 class 0.0 = 0.666 +- 0.215 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 class 1.0 = 0.667 +- 0.215 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 all KL = 0.794 +- 0.215 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.6 all L1 = 0.667 +- 0.163 (in-sample avg dev_std = 0.341)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.632
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.598
NEC for r=0.6 class 0.0 = 0.176 +- 0.129 (in-sample avg dev_std = 0.213)
NEC for r=0.6 class 1.0 = 0.198 +- 0.129 (in-sample avg dev_std = 0.213)
NEC for r=0.6 all KL = 0.085 +- 0.129 (in-sample avg dev_std = 0.213)
NEC for r=0.6 all L1 = 0.195 +- 0.133 (in-sample avg dev_std = 0.213)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.609
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.579
NEC for r=0.6 class 0.0 = 0.194 +- 0.130 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 1.0 = 0.202 +- 0.130 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all KL = 0.088 +- 0.130 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all L1 = 0.201 +- 0.132 (in-sample avg dev_std = 0.214)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.579
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.548
NEC for r=0.6 class 0.0 = 0.233 +- 0.120 (in-sample avg dev_std = 0.215)
NEC for r=0.6 class 1.0 = 0.216 +- 0.120 (in-sample avg dev_std = 0.215)
NEC for r=0.6 all KL = 0.091 +- 0.120 (in-sample avg dev_std = 0.215)
NEC for r=0.6 all L1 = 0.217 +- 0.138 (in-sample avg dev_std = 0.215)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.515
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.505
NEC for r=0.6 class 0.0 = 0.211 +- 0.132 (in-sample avg dev_std = 0.211)
NEC for r=0.6 class 1.0 = 0.205 +- 0.132 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all KL = 0.089 +- 0.132 (in-sample avg dev_std = 0.211)
NEC for r=0.6 all L1 = 0.206 +- 0.140 (in-sample avg dev_std = 0.211)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Apr 27 18:29:21 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 04/27/2024 06:29:21 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 04/27/2024 06:29:54 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:04 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:16 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:32 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 04/27/2024 06:30:48 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 04/27/2024 06:30:49 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 24...
[0m[1;37mINFO[0m: [1mCheckpoint 24: 
-----------------------------------
Train ROC-AUC: 0.9685
Train Loss: 0.1653
ID Validation ROC-AUC: 0.9224
ID Validation Loss: 0.2315
ID Test ROC-AUC: 0.9196
ID Test Loss: 0.2391
OOD Validation ROC-AUC: 0.6434
OOD Validation Loss: 0.3691
OOD Test ROC-AUC: 0.6778
OOD Test Loss: 0.5284

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8955
Train Loss: 0.2439
ID Validation ROC-AUC: 0.8855
ID Validation Loss: 0.2541
ID Test ROC-AUC: 0.8869
ID Test Loss: 0.2554
OOD Validation ROC-AUC: 0.6944
OOD Validation Loss: 0.2933
OOD Test ROC-AUC: 0.7180
OOD Test Loss: 0.4561

[0m[1;37mINFO[0m: [1mChartInfo 0.9196 0.6778 0.8869 0.7180 0.8855 0.6944[0mLBAPcore(34179)
Data example from train: Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
Label distribution from train: (tensor([0., 1.]), tensor([ 3960, 30219]))
[1;34mDEBUG[0m: 04/27/2024 06:30:49 PM : [1mUnbalanced warning for LBAPcore (train)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 04/27/2024 06:30:58 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19028)
Data example from val: Data(x=[23, 39], edge_index=[2, 50], edge_attr=[50, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 50], node_perm=[23])
Label distribution from val: (tensor([0., 1.]), tensor([ 1602, 17426]))
[1;34mDEBUG[0m: 04/27/2024 06:31:06 PM : [1mUnbalanced warning for LBAPcore (val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 04/27/2024 06:31:14 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.347
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.437
SUFF++ for r=0.6 class 0.0 = 0.892 +- 0.010 (in-sample avg dev_std = 0.074)
SUFF++ for r=0.6 class 1.0 = 0.916 +- 0.010 (in-sample avg dev_std = 0.074)
SUFF++ for r=0.6 all KL = 0.992 +- 0.010 (in-sample avg dev_std = 0.074)
SUFF++ for r=0.6 all L1 = 0.913 +- 0.044 (in-sample avg dev_std = 0.074)



--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.32
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.413
SUFF++ for r=0.6 class 0.0 = 0.881 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 class 1.0 = 0.914 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 all KL = 0.991 +- 0.014 (in-sample avg dev_std = 0.077)
SUFF++ for r=0.6 all L1 = 0.91 +- 0.048 (in-sample avg dev_std = 0.077)



--------------------------------------------------


#D#Computing SUFF++ over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.449
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.468
SUFF++ for r=0.6 class 0.0 = 0.898 +- 0.015 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 class 1.0 = 0.91 +- 0.015 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all KL = 0.99 +- 0.015 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all L1 = 0.909 +- 0.051 (in-sample avg dev_std = 0.079)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.388
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.427
SUFF++ for r=0.6 class 0.0 = 0.891 +- 0.010 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 class 1.0 = 0.91 +- 0.010 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all KL = 0.991 +- 0.010 (in-sample avg dev_std = 0.079)
SUFF++ for r=0.6 all L1 = 0.907 +- 0.048 (in-sample avg dev_std = 0.079)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over train across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.347
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.331
NEC for r=0.6 class 0.0 = 0.052 +- 0.003 (in-sample avg dev_std = 0.034)
NEC for r=0.6 class 1.0 = 0.042 +- 0.003 (in-sample avg dev_std = 0.034)
NEC for r=0.6 all KL = 0.002 +- 0.003 (in-sample avg dev_std = 0.034)
NEC for r=0.6 all L1 = 0.043 +- 0.024 (in-sample avg dev_std = 0.034)



--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.32
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.327
NEC for r=0.6 class 0.0 = 0.051 +- 0.003 (in-sample avg dev_std = 0.035)
NEC for r=0.6 class 1.0 = 0.042 +- 0.003 (in-sample avg dev_std = 0.035)
NEC for r=0.6 all KL = 0.002 +- 0.003 (in-sample avg dev_std = 0.035)
NEC for r=0.6 all L1 = 0.043 +- 0.024 (in-sample avg dev_std = 0.035)



--------------------------------------------------


#D#Computing NEC over val across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.449
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.414
NEC for r=0.6 class 0.0 = 0.054 +- 0.005 (in-sample avg dev_std = 0.036)
NEC for r=0.6 class 1.0 = 0.043 +- 0.005 (in-sample avg dev_std = 0.036)
NEC for r=0.6 all KL = 0.002 +- 0.005 (in-sample avg dev_std = 0.036)
NEC for r=0.6 all L1 = 0.044 +- 0.031 (in-sample avg dev_std = 0.036)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.388
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.37
NEC for r=0.6 class 0.0 = 0.054 +- 0.004 (in-sample avg dev_std = 0.037)
NEC for r=0.6 class 1.0 = 0.046 +- 0.004 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all KL = 0.002 +- 0.004 (in-sample avg dev_std = 0.037)
NEC for r=0.6 all L1 = 0.047 +- 0.030 (in-sample avg dev_std = 0.037)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split train
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.952], 'all_L1': [0.858]}), defaultdict(<class 'list'>, {'all_KL': [0.843], 'all_L1': [0.807]}), defaultdict(<class 'list'>, {'all_KL': [0.983], 'all_L1': [0.885]}), defaultdict(<class 'list'>, {'all_KL': [0.809], 'all_L1': [0.684]}), defaultdict(<class 'list'>, {'all_KL': [0.992], 'all_L1': [0.913]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.02], 'all_L1': [0.093]}), defaultdict(<class 'list'>, {'all_KL': [0.112], 'all_L1': [0.173]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.065]}), defaultdict(<class 'list'>, {'all_KL': [0.085], 'all_L1': [0.195]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.043]})]

Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.952], 'all_L1': [0.863]}), defaultdict(<class 'list'>, {'all_KL': [0.846], 'all_L1': [0.806]}), defaultdict(<class 'list'>, {'all_KL': [0.985], 'all_L1': [0.889]}), defaultdict(<class 'list'>, {'all_KL': [0.794], 'all_L1': [0.67]}), defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.91]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.021], 'all_L1': [0.089]}), defaultdict(<class 'list'>, {'all_KL': [0.106], 'all_L1': [0.17]}), defaultdict(<class 'list'>, {'all_KL': [0.006], 'all_L1': [0.065]}), defaultdict(<class 'list'>, {'all_KL': [0.088], 'all_L1': [0.201]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.043]})]

Eval split val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.949], 'all_L1': [0.856]}), defaultdict(<class 'list'>, {'all_KL': [0.832], 'all_L1': [0.77]}), defaultdict(<class 'list'>, {'all_KL': [0.987], 'all_L1': [0.889]}), defaultdict(<class 'list'>, {'all_KL': [0.804], 'all_L1': [0.668]}), defaultdict(<class 'list'>, {'all_KL': [0.99], 'all_L1': [0.909]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.027], 'all_L1': [0.104]}), defaultdict(<class 'list'>, {'all_KL': [0.123], 'all_L1': [0.205]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.064]}), defaultdict(<class 'list'>, {'all_KL': [0.091], 'all_L1': [0.217]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.044]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.942], 'all_L1': [0.845]}), defaultdict(<class 'list'>, {'all_KL': [0.812], 'all_L1': [0.76]}), defaultdict(<class 'list'>, {'all_KL': [0.987], 'all_L1': [0.889]}), defaultdict(<class 'list'>, {'all_KL': [0.794], 'all_L1': [0.667]}), defaultdict(<class 'list'>, {'all_KL': [0.991], 'all_L1': [0.907]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.025], 'all_L1': [0.106]}), defaultdict(<class 'list'>, {'all_KL': [0.126], 'all_L1': [0.205]}), defaultdict(<class 'list'>, {'all_KL': [0.005], 'all_L1': [0.067]}), defaultdict(<class 'list'>, {'all_KL': [0.089], 'all_L1': [0.206]}), defaultdict(<class 'list'>, {'all_KL': [0.002], 'all_L1': [0.047]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split train
suff++ class all_L1  =  0.829 +- 0.081
suff++ class all_KL  =  0.916 +- 0.075
suff++_acc_int  =  0.568 +- 0.069
nec class all_L1  =  0.114 +- 0.060
nec class all_KL  =  0.045 +- 0.045
nec_acc_int  =  0.570 +- 0.126

Eval split id_val
suff++ class all_L1  =  0.828 +- 0.086
suff++ class all_KL  =  0.914 +- 0.079
suff++_acc_int  =  0.573 +- 0.080
nec class all_L1  =  0.114 +- 0.061
nec class all_KL  =  0.045 +- 0.044
nec_acc_int  =  0.574 +- 0.128

Eval split val
suff++ class all_L1  =  0.818 +- 0.089
suff++ class all_KL  =  0.912 +- 0.079
suff++_acc_int  =  0.529 +- 0.057
nec class all_L1  =  0.127 +- 0.072
nec class all_KL  =  0.049 +- 0.049
nec_acc_int  =  0.527 +- 0.072

Eval split test
suff++ class all_L1  =  0.814 +- 0.089
suff++ class all_KL  =  0.905 +- 0.085
suff++_acc_int  =  0.528 +- 0.051
nec class all_L1  =  0.126 +- 0.067
nec class all_KL  =  0.049 +- 0.049
nec_acc_int  =  0.520 +- 0.081


 -------------------------------------------------- 
Computing faithfulness

Eval split train
Faith. Aritm (L1)= 		  =  0.472 +- 0.017
Faith. Armon (L1)= 		  =  0.192 +- 0.088
Faith. GMean (L1)= 	  =  0.292 +- 0.069
Faith. Aritm (KL)= 		  =  0.480 +- 0.018
Faith. Armon (KL)= 		  =  0.081 +- 0.079
Faith. GMean (KL)= 	  =  0.166 +- 0.103

Eval split id_val
Faith. Aritm (L1)= 		  =  0.471 +- 0.018
Faith. Armon (L1)= 		  =  0.191 +- 0.089
Faith. GMean (L1)= 	  =  0.290 +- 0.069
Faith. Aritm (KL)= 		  =  0.479 +- 0.020
Faith. Armon (KL)= 		  =  0.081 +- 0.077
Faith. GMean (KL)= 	  =  0.165 +- 0.101

Eval split val
Faith. Aritm (L1)= 		  =  0.473 +- 0.016
Faith. Armon (L1)= 		  =  0.208 +- 0.101
Faith. GMean (L1)= 	  =  0.303 +- 0.077
Faith. Aritm (KL)= 		  =  0.481 +- 0.018
Faith. Armon (KL)= 		  =  0.088 +- 0.085
Faith. GMean (KL)= 	  =  0.172 +- 0.109

Eval split test
Faith. Aritm (L1)= 		  =  0.470 +- 0.017
Faith. Armon (L1)= 		  =  0.208 +- 0.096
Faith. GMean (L1)= 	  =  0.303 +- 0.072
Faith. Aritm (KL)= 		  =  0.477 +- 0.021
Faith. Armon (KL)= 		  =  0.088 +- 0.086
Faith. GMean (KL)= 	  =  0.171 +- 0.107
Computed for split load_split = id



Completed in  0:18:01.670707  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay
DONE all :)
