nohup: ignoring input
Time to compute metrics for random explanations!
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 11:50:03 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 11:50:03 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 111...
[0m[1;37mINFO[0m: [1mCheckpoint 111: 
-----------------------------------
Train ACCURACY: 0.9205
Train Loss: 0.3805
ID Validation ACCURACY: 0.9270
ID Validation Loss: 0.3724
ID Test ACCURACY: 0.9183
ID Test Loss: 0.4140
OOD Validation ACCURACY: 0.8453
OOD Validation Loss: 0.4964
OOD Test ACCURACY: 0.4087
OOD Test Loss: 18.3446

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 50...
[0m[1;37mINFO[0m: [1mCheckpoint 50: 
-----------------------------------
Train ACCURACY: 0.8979
Train Loss: 0.4880
ID Validation ACCURACY: 0.9050
ID Validation Loss: 0.4614
ID Test ACCURACY: 0.8947
ID Test Loss: 0.5226
OOD Validation ACCURACY: 0.9203
OOD Validation Loss: 0.3953
OOD Test ACCURACY: 0.4087
OOD Test Loss: 28.2468

[0m[1;37mINFO[0m: [1mChartInfo 0.9183 0.4087 0.8947 0.4087 0.9050 0.9203[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.305
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.312
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.243
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.253


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.30534750000000005
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.507
SUFF++ for r=0.8 class 0 = 0.36 +- 0.288 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.8 class 1 = 0.51 +- 0.288 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.8 class 2 = 0.481 +- 0.288 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.8 all KL = 0.369 +- 0.288 (in-sample avg dev_std = 0.584)
SUFF++ for r=0.8 all L1 = 0.45 +- 0.209 (in-sample avg dev_std = 0.584)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24253125
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.381
SUFF++ for r=0.8 class 0 = 0.777 +- 0.386 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 class 1 = 0.809 +- 0.386 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 class 2 = 0.893 +- 0.386 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 all KL = 0.725 +- 0.386 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.8 all L1 = 0.826 +- 0.268 (in-sample avg dev_std = 0.387)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.30534750000000005
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.485
NEC for r=0.8 class 0 = 0.634 +- 0.244 (in-sample avg dev_std = 0.690)
NEC for r=0.8 class 1 = 0.548 +- 0.244 (in-sample avg dev_std = 0.690)
NEC for r=0.8 class 2 = 0.571 +- 0.244 (in-sample avg dev_std = 0.690)
NEC for r=0.8 all KL = 0.698 +- 0.244 (in-sample avg dev_std = 0.690)
NEC for r=0.8 all L1 = 0.584 +- 0.162 (in-sample avg dev_std = 0.690)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24253125
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.398
NEC for r=0.8 class 0 = 0.25 +- 0.421 (in-sample avg dev_std = 0.461)
NEC for r=0.8 class 1 = 0.209 +- 0.421 (in-sample avg dev_std = 0.461)
NEC for r=0.8 class 2 = 0.125 +- 0.421 (in-sample avg dev_std = 0.461)
NEC for r=0.8 all KL = 0.332 +- 0.421 (in-sample avg dev_std = 0.461)
NEC for r=0.8 all L1 = 0.194 +- 0.273 (in-sample avg dev_std = 0.461)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 11:51:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:00 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 29...
[0m[1;37mINFO[0m: [1mCheckpoint 29: 
-----------------------------------
Train ACCURACY: 0.9219
Train Loss: 0.4585
ID Validation ACCURACY: 0.9277
ID Validation Loss: 0.4446
ID Test ACCURACY: 0.9153
ID Test Loss: 0.5136
OOD Validation ACCURACY: 0.9030
OOD Validation Loss: 0.4462
OOD Test ACCURACY: 0.4087
OOD Test Loss: 12.6375

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.9233
Train Loss: 0.4108
ID Validation ACCURACY: 0.9277
ID Validation Loss: 0.3921
ID Test ACCURACY: 0.9180
ID Test Loss: 0.4568
OOD Validation ACCURACY: 0.9187
OOD Validation Loss: 0.3832
OOD Test ACCURACY: 0.4087
OOD Test Loss: 14.2353

[0m[1;37mINFO[0m: [1mChartInfo 0.9153 0.4087 0.9180 0.4087 0.9277 0.9187[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.313
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.314
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.247
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.254


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.935
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3130175
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.544
SUFF++ for r=0.8 class 0 = 0.383 +- 0.304 (in-sample avg dev_std = 0.666)
SUFF++ for r=0.8 class 1 = 0.551 +- 0.304 (in-sample avg dev_std = 0.666)
SUFF++ for r=0.8 class 2 = 0.505 +- 0.304 (in-sample avg dev_std = 0.666)
SUFF++ for r=0.8 all KL = 0.321 +- 0.304 (in-sample avg dev_std = 0.666)
SUFF++ for r=0.8 all L1 = 0.48 +- 0.236 (in-sample avg dev_std = 0.666)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24676875000000004
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.398
SUFF++ for r=0.8 class 0 = 0.735 +- 0.388 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 class 1 = 0.712 +- 0.388 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 class 2 = 0.849 +- 0.388 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 all KL = 0.575 +- 0.388 (in-sample avg dev_std = 0.465)
SUFF++ for r=0.8 all L1 = 0.765 +- 0.256 (in-sample avg dev_std = 0.465)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.935
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3130175
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.485
NEC for r=0.8 class 0 = 0.619 +- 0.245 (in-sample avg dev_std = 0.748)
NEC for r=0.8 class 1 = 0.529 +- 0.245 (in-sample avg dev_std = 0.748)
NEC for r=0.8 class 2 = 0.579 +- 0.245 (in-sample avg dev_std = 0.748)
NEC for r=0.8 all KL = 0.757 +- 0.245 (in-sample avg dev_std = 0.748)
NEC for r=0.8 all L1 = 0.576 +- 0.189 (in-sample avg dev_std = 0.748)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24676875000000004
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.389
NEC for r=0.8 class 0 = 0.302 +- 0.396 (in-sample avg dev_std = 0.542)
NEC for r=0.8 class 1 = 0.331 +- 0.396 (in-sample avg dev_std = 0.542)
NEC for r=0.8 class 2 = 0.188 +- 0.396 (in-sample avg dev_std = 0.542)
NEC for r=0.8 all KL = 0.535 +- 0.396 (in-sample avg dev_std = 0.542)
NEC for r=0.8 all L1 = 0.274 +- 0.260 (in-sample avg dev_std = 0.542)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 11:51:53 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 11:51:53 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8898
Train Loss: 0.5536
ID Validation ACCURACY: 0.9003
ID Validation Loss: 0.5339
ID Test ACCURACY: 0.8880
ID Test Loss: 0.5790
OOD Validation ACCURACY: 0.8117
OOD Validation Loss: 0.5655
OOD Test ACCURACY: 0.4087
OOD Test Loss: 19.5623

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 144...
[0m[1;37mINFO[0m: [1mCheckpoint 144: 
-----------------------------------
Train ACCURACY: 0.8036
Train Loss: 0.5833
ID Validation ACCURACY: 0.8053
ID Validation Loss: 0.5878
ID Test ACCURACY: 0.7930
ID Test Loss: 0.6074
OOD Validation ACCURACY: 0.8780
OOD Validation Loss: 0.4487
OOD Test ACCURACY: 0.4087
OOD Test Loss: 16.8359

[0m[1;37mINFO[0m: [1mChartInfo 0.8880 0.4087 0.7930 0.4087 0.8053 0.8780[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.311
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.313
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.240
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.909
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.31086375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.507
SUFF++ for r=0.8 class 0 = 0.467 +- 0.285 (in-sample avg dev_std = 0.600)
SUFF++ for r=0.8 class 1 = 0.438 +- 0.285 (in-sample avg dev_std = 0.600)
SUFF++ for r=0.8 class 2 = 0.485 +- 0.285 (in-sample avg dev_std = 0.600)
SUFF++ for r=0.8 all KL = 0.375 +- 0.285 (in-sample avg dev_std = 0.600)
SUFF++ for r=0.8 all L1 = 0.463 +- 0.203 (in-sample avg dev_std = 0.600)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.2403775
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.372
SUFF++ for r=0.8 class 0 = 0.795 +- 0.383 (in-sample avg dev_std = 0.363)
SUFF++ for r=0.8 class 1 = 0.808 +- 0.383 (in-sample avg dev_std = 0.363)
SUFF++ for r=0.8 class 2 = 0.891 +- 0.383 (in-sample avg dev_std = 0.363)
SUFF++ for r=0.8 all KL = 0.737 +- 0.383 (in-sample avg dev_std = 0.363)
SUFF++ for r=0.8 all L1 = 0.832 +- 0.266 (in-sample avg dev_std = 0.363)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.909
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.31086375
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.48
NEC for r=0.8 class 0 = 0.539 +- 0.263 (in-sample avg dev_std = 0.686)
NEC for r=0.8 class 1 = 0.576 +- 0.263 (in-sample avg dev_std = 0.686)
NEC for r=0.8 class 2 = 0.584 +- 0.263 (in-sample avg dev_std = 0.686)
NEC for r=0.8 all KL = 0.675 +- 0.263 (in-sample avg dev_std = 0.686)
NEC for r=0.8 all L1 = 0.566 +- 0.168 (in-sample avg dev_std = 0.686)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.404
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.2403775
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.371
NEC for r=0.8 class 0 = 0.239 +- 0.431 (in-sample avg dev_std = 0.440)
NEC for r=0.8 class 1 = 0.228 +- 0.431 (in-sample avg dev_std = 0.440)
NEC for r=0.8 class 2 = 0.14 +- 0.431 (in-sample avg dev_std = 0.440)
NEC for r=0.8 all KL = 0.358 +- 0.431 (in-sample avg dev_std = 0.440)
NEC for r=0.8 all L1 = 0.202 +- 0.276 (in-sample avg dev_std = 0.440)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 11:52:45 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 11:52:45 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 33...
[0m[1;37mINFO[0m: [1mCheckpoint 33: 
-----------------------------------
Train ACCURACY: 0.9238
Train Loss: 0.4617
ID Validation ACCURACY: 0.9273
ID Validation Loss: 0.4236
ID Test ACCURACY: 0.9190
ID Test Loss: 0.4980
OOD Validation ACCURACY: 0.9040
OOD Validation Loss: 0.4178
OOD Test ACCURACY: 0.4087
OOD Test Loss: 18.1921

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 27...
[0m[1;37mINFO[0m: [1mCheckpoint 27: 
-----------------------------------
Train ACCURACY: 0.9162
Train Loss: 0.4235
ID Validation ACCURACY: 0.9237
ID Validation Loss: 0.3927
ID Test ACCURACY: 0.9133
ID Test Loss: 0.4441
OOD Validation ACCURACY: 0.9150
OOD Validation Loss: 0.3759
OOD Test ACCURACY: 0.6387
OOD Test Loss: 3.0586

[0m[1;37mINFO[0m: [1mChartInfo 0.9190 0.4087 0.9133 0.6387 0.9237 0.9150[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.300
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.303
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.243
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.251


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.300225
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.553
SUFF++ for r=0.8 class 0 = 0.387 +- 0.315 (in-sample avg dev_std = 0.657)
SUFF++ for r=0.8 class 1 = 0.632 +- 0.315 (in-sample avg dev_std = 0.657)
SUFF++ for r=0.8 class 2 = 0.426 +- 0.315 (in-sample avg dev_std = 0.657)
SUFF++ for r=0.8 all KL = 0.316 +- 0.315 (in-sample avg dev_std = 0.657)
SUFF++ for r=0.8 all L1 = 0.483 +- 0.252 (in-sample avg dev_std = 0.657)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.405
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24330374999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.392
SUFF++ for r=0.8 class 0 = 0.781 +- 0.392 (in-sample avg dev_std = 0.405)
SUFF++ for r=0.8 class 1 = 0.792 +- 0.392 (in-sample avg dev_std = 0.405)
SUFF++ for r=0.8 class 2 = 0.845 +- 0.392 (in-sample avg dev_std = 0.405)
SUFF++ for r=0.8 all KL = 0.674 +- 0.392 (in-sample avg dev_std = 0.405)
SUFF++ for r=0.8 all L1 = 0.806 +- 0.258 (in-sample avg dev_std = 0.405)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.933
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.300225
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.528
NEC for r=0.8 class 0 = 0.622 +- 0.270 (in-sample avg dev_std = 0.773)
NEC for r=0.8 class 1 = 0.461 +- 0.270 (in-sample avg dev_std = 0.773)
NEC for r=0.8 class 2 = 0.58 +- 0.270 (in-sample avg dev_std = 0.773)
NEC for r=0.8 all KL = 0.761 +- 0.270 (in-sample avg dev_std = 0.773)
NEC for r=0.8 all L1 = 0.554 +- 0.205 (in-sample avg dev_std = 0.773)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.405
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.24330374999999996
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.399
NEC for r=0.8 class 0 = 0.272 +- 0.429 (in-sample avg dev_std = 0.497)
NEC for r=0.8 class 1 = 0.259 +- 0.429 (in-sample avg dev_std = 0.497)
NEC for r=0.8 class 2 = 0.188 +- 0.429 (in-sample avg dev_std = 0.497)
NEC for r=0.8 all KL = 0.434 +- 0.429 (in-sample avg dev_std = 0.497)
NEC for r=0.8 all L1 = 0.239 +- 0.276 (in-sample avg dev_std = 0.497)
model_dirname= repr_CIGAGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 11:53:44 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 11:53:44 AM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 62...
[0m[1;37mINFO[0m: [1mCheckpoint 62: 
-----------------------------------
Train ACCURACY: 0.9232
Train Loss: 0.4686
ID Validation ACCURACY: 0.9290
ID Validation Loss: 0.4482
ID Test ACCURACY: 0.9190
ID Test Loss: 0.5270
OOD Validation ACCURACY: 0.9023
OOD Validation Loss: 0.4335
OOD Test ACCURACY: 0.4097
OOD Test Loss: 11.4881

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 112...
[0m[1;37mINFO[0m: [1mCheckpoint 112: 
-----------------------------------
Train ACCURACY: 0.9212
Train Loss: 0.4121
ID Validation ACCURACY: 0.9280
ID Validation Loss: 0.3988
ID Test ACCURACY: 0.9170
ID Test Loss: 0.4630
OOD Validation ACCURACY: 0.9170
OOD Validation Loss: 0.3851
OOD Test ACCURACY: 0.4087
OOD Test Loss: 36.3874

[0m[1;37mINFO[0m: [1mChartInfo 0.9190 0.4097 0.9170 0.4087 0.9280 0.9170[0mGOODMotif2(3000)
Data example from id_val: Data(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
Label distribution from id_val: (tensor([0, 1, 2]), tensor([1002, 1009,  989]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument lr_filternod in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.310
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.313
GOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))
F1 for r=0.8 = 0.000
WIoU for r=0.8 = 0.243
F1 for r=1.0 = 0.000
WIoU for r=1.0 = 0.252


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.936
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3102825
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.539
SUFF++ for r=0.8 class 0 = 0.367 +- 0.329 (in-sample avg dev_std = 0.620)
SUFF++ for r=0.8 class 1 = 0.658 +- 0.329 (in-sample avg dev_std = 0.620)
SUFF++ for r=0.8 class 2 = 0.436 +- 0.329 (in-sample avg dev_std = 0.620)
SUFF++ for r=0.8 all KL = 0.335 +- 0.329 (in-sample avg dev_std = 0.620)
SUFF++ for r=0.8 all L1 = 0.488 +- 0.265 (in-sample avg dev_std = 0.620)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.405
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.2425225
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.423
SUFF++ for r=0.8 class 0 = 0.755 +- 0.391 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.8 class 1 = 0.673 +- 0.391 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.8 class 2 = 0.797 +- 0.391 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.8 all KL = 0.53 +- 0.391 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.8 all L1 = 0.741 +- 0.266 (in-sample avg dev_std = 0.490)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.936
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.3102825
len(reference) = 800
Effective ratio: 0.812 +- 0.010
Model Accuracy over intervened graphs for r=0.8 =  0.497
NEC for r=0.8 class 0 = 0.636 +- 0.272 (in-sample avg dev_std = 0.742)
NEC for r=0.8 class 1 = 0.421 +- 0.272 (in-sample avg dev_std = 0.742)
NEC for r=0.8 class 2 = 0.616 +- 0.272 (in-sample avg dev_std = 0.742)
NEC for r=0.8 all KL = 0.754 +- 0.272 (in-sample avg dev_std = 0.742)
NEC for r=0.8 all L1 = 0.557 +- 0.221 (in-sample avg dev_std = 0.742)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.405
Model XAI F1 of binarized graphs for r=0.8 =  0.0
Model XAI WIoU of binarized graphs for r=0.8 =  0.2425225
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.413
NEC for r=0.8 class 0 = 0.279 +- 0.416 (in-sample avg dev_std = 0.564)
NEC for r=0.8 class 1 = 0.35 +- 0.416 (in-sample avg dev_std = 0.564)
NEC for r=0.8 class 2 = 0.216 +- 0.416 (in-sample avg dev_std = 0.564)
NEC for r=0.8 all KL = 0.519 +- 0.416 (in-sample avg dev_std = 0.564)
NEC for r=0.8 all L1 = 0.282 +- 0.269 (in-sample avg dev_std = 0.564)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.369], 'all_L1': [0.45]}), defaultdict(<class 'list'>, {'all_KL': [0.321], 'all_L1': [0.48]}), defaultdict(<class 'list'>, {'all_KL': [0.375], 'all_L1': [0.463]}), defaultdict(<class 'list'>, {'all_KL': [0.316], 'all_L1': [0.483]}), defaultdict(<class 'list'>, {'all_KL': [0.335], 'all_L1': [0.488]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.698], 'all_L1': [0.584]}), defaultdict(<class 'list'>, {'all_KL': [0.757], 'all_L1': [0.576]}), defaultdict(<class 'list'>, {'all_KL': [0.675], 'all_L1': [0.566]}), defaultdict(<class 'list'>, {'all_KL': [0.761], 'all_L1': [0.554]}), defaultdict(<class 'list'>, {'all_KL': [0.754], 'all_L1': [0.557]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.725], 'all_L1': [0.826]}), defaultdict(<class 'list'>, {'all_KL': [0.575], 'all_L1': [0.765]}), defaultdict(<class 'list'>, {'all_KL': [0.737], 'all_L1': [0.832]}), defaultdict(<class 'list'>, {'all_KL': [0.674], 'all_L1': [0.806]}), defaultdict(<class 'list'>, {'all_KL': [0.53], 'all_L1': [0.741]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.332], 'all_L1': [0.194]}), defaultdict(<class 'list'>, {'all_KL': [0.535], 'all_L1': [0.274]}), defaultdict(<class 'list'>, {'all_KL': [0.358], 'all_L1': [0.202]}), defaultdict(<class 'list'>, {'all_KL': [0.434], 'all_L1': [0.239]}), defaultdict(<class 'list'>, {'all_KL': [0.519], 'all_L1': [0.282]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.473 +- 0.014
suff++ class all_KL  =  0.343 +- 0.024
suff++_acc_int  =  0.530 +- 0.019
nec class all_L1  =  0.567 +- 0.011
nec class all_KL  =  0.729 +- 0.036
nec_acc_int  =  0.495 +- 0.017

Eval split test
suff++ class all_L1  =  0.794 +- 0.035
suff++ class all_KL  =  0.648 +- 0.082
suff++_acc_int  =  0.393 +- 0.017
nec class all_L1  =  0.238 +- 0.036
nec class all_KL  =  0.436 +- 0.082
nec_acc_int  =  0.394 +- 0.014


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.520 +- 0.005
Faith. Armon (L1)= 		  =  0.516 +- 0.006
Faith. GMean (L1)= 	  =  0.518 +- 0.005
Faith. Aritm (KL)= 		  =  0.536 +- 0.007
Faith. Armon (KL)= 		  =  0.465 +- 0.015
Faith. GMean (KL)= 	  =  0.499 +- 0.007

Eval split test
Faith. Aritm (L1)= 		  =  0.516 +- 0.005
Faith. Armon (L1)= 		  =  0.364 +- 0.039
Faith. GMean (L1)= 	  =  0.433 +- 0.024
Faith. Aritm (KL)= 		  =  0.542 +- 0.013
Faith. Armon (KL)= 		  =  0.509 +- 0.035
Faith. GMean (KL)= 	  =  0.525 +- 0.022
Computed for split load_split = id



Completed in  0:04:36.526048  for CIGAGIN GOODMotif2/basis



Time to compute metrics for random explanations!
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:00:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:20 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:00:21 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 148...
[0m[1;37mINFO[0m: [1mCheckpoint 148: 
-----------------------------------
Train ACCURACY: 0.9479
Train Loss: 0.0807
ID Validation ACCURACY: 0.8570
ID Validation Loss: 0.6526
ID Test ACCURACY: 0.8504
ID Test Loss: 0.7123
OOD Validation ACCURACY: 0.8377
OOD Validation Loss: 0.8949
OOD Test ACCURACY: 0.7398
OOD Test Loss: 1.5170

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.9165
Train Loss: 0.1472
ID Validation ACCURACY: 0.8142
ID Validation Loss: 0.7736
ID Test ACCURACY: 0.8114
ID Test Loss: 0.8563
OOD Validation ACCURACY: 0.8439
OOD Validation Loss: 0.7952
OOD Test ACCURACY: 0.8108
OOD Test Loss: 0.9714

[0m[1;37mINFO[0m: [1mChartInfo 0.8504 0.7398 0.8114 0.8108 0.8142 0.8439[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 12:00:22 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.845
SUFF++ for r=0.8 class 0.0 = 0.816 +- 0.247 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 class 1.0 = 0.934 +- 0.247 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 all KL = 0.834 +- 0.247 (in-sample avg dev_std = 0.322)
SUFF++ for r=0.8 all L1 = 0.885 +- 0.170 (in-sample avg dev_std = 0.322)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.701
SUFF++ for r=0.8 class 0.0 = 0.765 +- 0.204 (in-sample avg dev_std = 0.284)
SUFF++ for r=0.8 class 1.0 = 0.956 +- 0.204 (in-sample avg dev_std = 0.284)
SUFF++ for r=0.8 all KL = 0.864 +- 0.204 (in-sample avg dev_std = 0.284)
SUFF++ for r=0.8 all L1 = 0.863 +- 0.193 (in-sample avg dev_std = 0.284)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.862
NEC for r=0.8 class 0.0 = 0.125 +- 0.171 (in-sample avg dev_std = 0.151)
NEC for r=0.8 class 1.0 = 0.048 +- 0.171 (in-sample avg dev_std = 0.151)
NEC for r=0.8 all KL = 0.072 +- 0.171 (in-sample avg dev_std = 0.151)
NEC for r=0.8 all L1 = 0.081 +- 0.164 (in-sample avg dev_std = 0.151)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.716
NEC for r=0.8 class 0.0 = 0.169 +- 0.154 (in-sample avg dev_std = 0.183)
NEC for r=0.8 class 1.0 = 0.031 +- 0.154 (in-sample avg dev_std = 0.183)
NEC for r=0.8 all KL = 0.073 +- 0.154 (in-sample avg dev_std = 0.183)
NEC for r=0.8 all L1 = 0.098 +- 0.164 (in-sample avg dev_std = 0.183)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:01:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:15 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:15 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 160...
[0m[1;37mINFO[0m: [1mCheckpoint 160: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0768
ID Validation ACCURACY: 0.8587
ID Validation Loss: 0.7200
ID Test ACCURACY: 0.8551
ID Test Loss: 0.8849
OOD Validation ACCURACY: 0.8472
OOD Validation Loss: 1.0741
OOD Test ACCURACY: 0.7967
OOD Test Loss: 1.3148

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 162...
[0m[1;37mINFO[0m: [1mCheckpoint 162: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0760
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.7569
ID Test ACCURACY: 0.8551
ID Test Loss: 0.9395
OOD Validation ACCURACY: 0.8551
OOD Validation Loss: 1.0513
OOD Test ACCURACY: 0.8051
OOD Test Loss: 1.2667

[0m[1;37mINFO[0m: [1mChartInfo 0.8551 0.7967 0.8551 0.8051 0.8581 0.8551[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 12:01:16 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.836
SUFF++ for r=0.8 class 0.0 = 0.924 +- 0.289 (in-sample avg dev_std = 0.349)
SUFF++ for r=0.8 class 1.0 = 0.852 +- 0.289 (in-sample avg dev_std = 0.349)
SUFF++ for r=0.8 all KL = 0.8 +- 0.289 (in-sample avg dev_std = 0.349)
SUFF++ for r=0.8 all L1 = 0.882 +- 0.177 (in-sample avg dev_std = 0.349)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.811
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.786
SUFF++ for r=0.8 class 0.0 = 0.907 +- 0.224 (in-sample avg dev_std = 0.313)
SUFF++ for r=0.8 class 1.0 = 0.794 +- 0.224 (in-sample avg dev_std = 0.313)
SUFF++ for r=0.8 all KL = 0.826 +- 0.224 (in-sample avg dev_std = 0.313)
SUFF++ for r=0.8 all L1 = 0.849 +- 0.196 (in-sample avg dev_std = 0.313)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.845
NEC for r=0.8 class 0.0 = 0.06 +- 0.213 (in-sample avg dev_std = 0.196)
NEC for r=0.8 class 1.0 = 0.116 +- 0.213 (in-sample avg dev_std = 0.196)
NEC for r=0.8 all KL = 0.099 +- 0.213 (in-sample avg dev_std = 0.196)
NEC for r=0.8 all L1 = 0.093 +- 0.194 (in-sample avg dev_std = 0.196)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.811
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.798
NEC for r=0.8 class 0.0 = 0.076 +- 0.179 (in-sample avg dev_std = 0.199)
NEC for r=0.8 class 1.0 = 0.143 +- 0.179 (in-sample avg dev_std = 0.199)
NEC for r=0.8 all KL = 0.093 +- 0.179 (in-sample avg dev_std = 0.199)
NEC for r=0.8 all L1 = 0.111 +- 0.185 (in-sample avg dev_std = 0.199)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:02:01 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 149...
[0m[1;37mINFO[0m: [1mCheckpoint 149: 
-----------------------------------
Train ACCURACY: 0.9479
Train Loss: 0.0798
ID Validation ACCURACY: 0.8600
ID Validation Loss: 0.6948
ID Test ACCURACY: 0.8576
ID Test Loss: 0.7845
OOD Validation ACCURACY: 0.8474
OOD Validation Loss: 1.0437
OOD Test ACCURACY: 0.8045
OOD Test Loss: 1.1397

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 140...
[0m[1;37mINFO[0m: [1mCheckpoint 140: 
-----------------------------------
Train ACCURACY: 0.9461
Train Loss: 0.0840
ID Validation ACCURACY: 0.8459
ID Validation Loss: 0.7816
ID Test ACCURACY: 0.8498
ID Test Loss: 0.8524
OOD Validation ACCURACY: 0.8581
OOD Validation Loss: 0.9137
OOD Test ACCURACY: 0.8097
OOD Test Loss: 1.0383

[0m[1;37mINFO[0m: [1mChartInfo 0.8576 0.8045 0.8498 0.8097 0.8459 0.8581[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 12:02:02 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.843
SUFF++ for r=0.8 class 0.0 = 0.93 +- 0.258 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 class 1.0 = 0.878 +- 0.258 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 all KL = 0.827 +- 0.258 (in-sample avg dev_std = 0.324)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.156 (in-sample avg dev_std = 0.324)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.795
SUFF++ for r=0.8 class 0.0 = 0.902 +- 0.199 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 class 1.0 = 0.818 +- 0.199 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 all KL = 0.846 +- 0.199 (in-sample avg dev_std = 0.292)
SUFF++ for r=0.8 all L1 = 0.859 +- 0.183 (in-sample avg dev_std = 0.292)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.867
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.861
NEC for r=0.8 class 0.0 = 0.065 +- 0.200 (in-sample avg dev_std = 0.200)
NEC for r=0.8 class 1.0 = 0.088 +- 0.200 (in-sample avg dev_std = 0.200)
NEC for r=0.8 all KL = 0.089 +- 0.200 (in-sample avg dev_std = 0.200)
NEC for r=0.8 all L1 = 0.079 +- 0.166 (in-sample avg dev_std = 0.200)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.821
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.805
NEC for r=0.8 class 0.0 = 0.074 +- 0.170 (in-sample avg dev_std = 0.202)
NEC for r=0.8 class 1.0 = 0.139 +- 0.170 (in-sample avg dev_std = 0.202)
NEC for r=0.8 all KL = 0.088 +- 0.170 (in-sample avg dev_std = 0.202)
NEC for r=0.8 all L1 = 0.107 +- 0.179 (in-sample avg dev_std = 0.202)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:02:47 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 113...
[0m[1;37mINFO[0m: [1mCheckpoint 113: 
-----------------------------------
Train ACCURACY: 0.9483
Train Loss: 0.0792
ID Validation ACCURACY: 0.8604
ID Validation Loss: 0.6685
ID Test ACCURACY: 0.8570
ID Test Loss: 0.7609
OOD Validation ACCURACY: 0.8537
OOD Validation Loss: 0.9443
OOD Test ACCURACY: 0.8069
OOD Test Loss: 1.0629

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 114...
[0m[1;37mINFO[0m: [1mCheckpoint 114: 
-----------------------------------
Train ACCURACY: 0.9462
Train Loss: 0.0915
ID Validation ACCURACY: 0.8478
ID Validation Loss: 0.6421
ID Test ACCURACY: 0.8480
ID Test Loss: 0.7499
OOD Validation ACCURACY: 0.8555
OOD Validation Loss: 0.7389
OOD Test ACCURACY: 0.8109
OOD Test Loss: 0.7657

[0m[1;37mINFO[0m: [1mChartInfo 0.8570 0.8069 0.8480 0.8109 0.8478 0.8555[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 12:02:48 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.829
SUFF++ for r=0.8 class 0.0 = 0.913 +- 0.254 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 class 1.0 = 0.835 +- 0.254 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 all KL = 0.808 +- 0.254 (in-sample avg dev_std = 0.369)
SUFF++ for r=0.8 all L1 = 0.868 +- 0.179 (in-sample avg dev_std = 0.369)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.824
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.801
SUFF++ for r=0.8 class 0.0 = 0.902 +- 0.187 (in-sample avg dev_std = 0.299)
SUFF++ for r=0.8 class 1.0 = 0.813 +- 0.187 (in-sample avg dev_std = 0.299)
SUFF++ for r=0.8 all KL = 0.855 +- 0.187 (in-sample avg dev_std = 0.299)
SUFF++ for r=0.8 all L1 = 0.856 +- 0.180 (in-sample avg dev_std = 0.299)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.874
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.845
NEC for r=0.8 class 0.0 = 0.067 +- 0.191 (in-sample avg dev_std = 0.162)
NEC for r=0.8 class 1.0 = 0.107 +- 0.191 (in-sample avg dev_std = 0.162)
NEC for r=0.8 all KL = 0.079 +- 0.191 (in-sample avg dev_std = 0.162)
NEC for r=0.8 all L1 = 0.09 +- 0.182 (in-sample avg dev_std = 0.162)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.824
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.822
NEC for r=0.8 class 0.0 = 0.081 +- 0.148 (in-sample avg dev_std = 0.182)
NEC for r=0.8 class 1.0 = 0.113 +- 0.148 (in-sample avg dev_std = 0.182)
NEC for r=0.8 all KL = 0.07 +- 0.148 (in-sample avg dev_std = 0.182)
NEC for r=0.8 all L1 = 0.098 +- 0.162 (in-sample avg dev_std = 0.182)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:03:34 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:03:35 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:03:36 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 144...
[0m[1;37mINFO[0m: [1mCheckpoint 144: 
-----------------------------------
Train ACCURACY: 0.9443
Train Loss: 0.0872
ID Validation ACCURACY: 0.8447
ID Validation Loss: 0.7583
ID Test ACCURACY: 0.8393
ID Test Loss: 0.8682
OOD Validation ACCURACY: 0.7661
OOD Validation Loss: 1.6477
OOD Test ACCURACY: 0.6583
OOD Test Loss: 3.1226

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 92...
[0m[1;37mINFO[0m: [1mCheckpoint 92: 
-----------------------------------
Train ACCURACY: 0.9332
Train Loss: 0.1117
ID Validation ACCURACY: 0.8306
ID Validation Loss: 0.6694
ID Test ACCURACY: 0.8312
ID Test Loss: 0.7076
OOD Validation ACCURACY: 0.8233
OOD Validation Loss: 0.9547
OOD Test ACCURACY: 0.7755
OOD Test Loss: 1.3760

[0m[1;37mINFO[0m: [1mChartInfo 0.8393 0.6583 0.8312 0.7755 0.8306 0.8233[0mGOODSST2(5301)
Data example from id_val: Data(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
Label distribution from id_val: (tensor([0., 1.]), tensor([2144, 3157]))
[1;34mDEBUG[0m: 09/28/2024 12:03:36 PM : [1mUnbalanced warning for GOODSST2 (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
F1 for r=0.8 = nan
WIoU for r=0.8 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.816
SUFF++ for r=0.8 class 0.0 = 0.784 +- 0.293 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 class 1.0 = 0.954 +- 0.293 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 all KL = 0.817 +- 0.293 (in-sample avg dev_std = 0.341)
SUFF++ for r=0.8 all L1 = 0.883 +- 0.188 (in-sample avg dev_std = 0.341)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.634
SUFF++ for r=0.8 class 0.0 = 0.784 +- 0.250 (in-sample avg dev_std = 0.275)
SUFF++ for r=0.8 class 1.0 = 0.985 +- 0.250 (in-sample avg dev_std = 0.275)
SUFF++ for r=0.8 all KL = 0.869 +- 0.250 (in-sample avg dev_std = 0.275)
SUFF++ for r=0.8 all L1 = 0.888 +- 0.201 (in-sample avg dev_std = 0.275)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.842
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 285
Effective ratio: 0.796 +- 0.301
Model Accuracy over intervened graphs for r=0.8 =  0.827
NEC for r=0.8 class 0.0 = 0.168 +- 0.219 (in-sample avg dev_std = 0.195)
NEC for r=0.8 class 1.0 = 0.029 +- 0.219 (in-sample avg dev_std = 0.195)
NEC for r=0.8 all KL = 0.094 +- 0.219 (in-sample avg dev_std = 0.195)
NEC for r=0.8 all L1 = 0.087 +- 0.190 (in-sample avg dev_std = 0.195)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.67
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.646
NEC for r=0.8 class 0.0 = 0.173 +- 0.198 (in-sample avg dev_std = 0.203)
NEC for r=0.8 class 1.0 = 0.013 +- 0.198 (in-sample avg dev_std = 0.203)
NEC for r=0.8 all KL = 0.087 +- 0.198 (in-sample avg dev_std = 0.203)
NEC for r=0.8 all L1 = 0.09 +- 0.182 (in-sample avg dev_std = 0.203)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.834], 'all_L1': [0.885]}), defaultdict(<class 'list'>, {'all_KL': [0.8], 'all_L1': [0.882]}), defaultdict(<class 'list'>, {'all_KL': [0.827], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.808], 'all_L1': [0.868]}), defaultdict(<class 'list'>, {'all_KL': [0.817], 'all_L1': [0.883]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.072], 'all_L1': [0.081]}), defaultdict(<class 'list'>, {'all_KL': [0.099], 'all_L1': [0.093]}), defaultdict(<class 'list'>, {'all_KL': [0.089], 'all_L1': [0.079]}), defaultdict(<class 'list'>, {'all_KL': [0.079], 'all_L1': [0.09]}), defaultdict(<class 'list'>, {'all_KL': [0.094], 'all_L1': [0.087]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.864], 'all_L1': [0.863]}), defaultdict(<class 'list'>, {'all_KL': [0.826], 'all_L1': [0.849]}), defaultdict(<class 'list'>, {'all_KL': [0.846], 'all_L1': [0.859]}), defaultdict(<class 'list'>, {'all_KL': [0.855], 'all_L1': [0.856]}), defaultdict(<class 'list'>, {'all_KL': [0.869], 'all_L1': [0.888]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.073], 'all_L1': [0.098]}), defaultdict(<class 'list'>, {'all_KL': [0.093], 'all_L1': [0.111]}), defaultdict(<class 'list'>, {'all_KL': [0.088], 'all_L1': [0.107]}), defaultdict(<class 'list'>, {'all_KL': [0.07], 'all_L1': [0.098]}), defaultdict(<class 'list'>, {'all_KL': [0.087], 'all_L1': [0.09]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.883 +- 0.010
suff++ class all_KL  =  0.817 +- 0.012
suff++_acc_int  =  0.834 +- 0.010
nec class all_L1  =  0.086 +- 0.005
nec class all_KL  =  0.087 +- 0.010
nec_acc_int  =  0.848 +- 0.013

Eval split test
suff++ class all_L1  =  0.863 +- 0.013
suff++ class all_KL  =  0.852 +- 0.015
suff++_acc_int  =  0.743 +- 0.066
nec class all_L1  =  0.101 +- 0.007
nec class all_KL  =  0.082 +- 0.009
nec_acc_int  =  0.757 +- 0.066


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.485 +- 0.004
Faith. Armon (L1)= 		  =  0.157 +- 0.009
Faith. GMean (L1)= 	  =  0.275 +- 0.007
Faith. Aritm (KL)= 		  =  0.452 +- 0.005
Faith. Armon (KL)= 		  =  0.156 +- 0.016
Faith. GMean (KL)= 	  =  0.266 +- 0.014

Eval split test
Faith. Aritm (L1)= 		  =  0.482 +- 0.004
Faith. Armon (L1)= 		  =  0.180 +- 0.012
Faith. GMean (L1)= 	  =  0.295 +- 0.009
Faith. Aritm (KL)= 		  =  0.467 +- 0.006
Faith. Armon (KL)= 		  =  0.150 +- 0.015
Faith. GMean (KL)= 	  =  0.264 +- 0.014
Computed for split load_split = id



Completed in  0:04:09.172334  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:04:42 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 12:04:43 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 12:05:20 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 12:05:32 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 12:05:47 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:07 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:06:26 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 23...
[0m[1;37mINFO[0m: [1mCheckpoint 23: 
-----------------------------------
Train ROC-AUC: 0.9686
Train Loss: 0.1597
ID Validation ROC-AUC: 0.9236
ID Validation Loss: 0.2366
ID Test ROC-AUC: 0.9233
ID Test Loss: 0.2412
OOD Validation ROC-AUC: 0.6572
OOD Validation Loss: 0.3846
OOD Test ROC-AUC: 0.6967
OOD Test Loss: 0.5574

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 1...
[0m[1;37mINFO[0m: [1mCheckpoint 1: 
-----------------------------------
Train ROC-AUC: 0.8713
Train Loss: 0.2869
ID Validation ROC-AUC: 0.8665
ID Validation Loss: 0.2911
ID Test ROC-AUC: 0.8695
ID Test Loss: 0.2927
OOD Validation ROC-AUC: 0.6861
OOD Validation Loss: 0.2792
OOD Test ROC-AUC: 0.7034
OOD Test Loss: 0.4207

[0m[1;37mINFO[0m: [1mChartInfo 0.9233 0.6967 0.8695 0.7034 0.8665 0.6861[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 12:06:27 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 12:06:37 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.608
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.626
SUFF++ for r=0.6 class 0.0 = 0.851 +- 0.038 (in-sample avg dev_std = 0.118)
SUFF++ for r=0.6 class 1.0 = 0.885 +- 0.038 (in-sample avg dev_std = 0.118)
SUFF++ for r=0.6 all KL = 0.964 +- 0.038 (in-sample avg dev_std = 0.118)
SUFF++ for r=0.6 all L1 = 0.881 +- 0.071 (in-sample avg dev_std = 0.118)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.563
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.567
SUFF++ for r=0.6 class 0.0 = 0.854 +- 0.052 (in-sample avg dev_std = 0.131)
SUFF++ for r=0.6 class 1.0 = 0.866 +- 0.052 (in-sample avg dev_std = 0.131)
SUFF++ for r=0.6 all KL = 0.957 +- 0.052 (in-sample avg dev_std = 0.131)
SUFF++ for r=0.6 all L1 = 0.864 +- 0.079 (in-sample avg dev_std = 0.131)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.608
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.641
NEC for r=0.6 class 0.0 = 0.148 +- 0.045 (in-sample avg dev_std = 0.105)
NEC for r=0.6 class 1.0 = 0.093 +- 0.045 (in-sample avg dev_std = 0.105)
NEC for r=0.6 all KL = 0.028 +- 0.045 (in-sample avg dev_std = 0.105)
NEC for r=0.6 all L1 = 0.1 +- 0.075 (in-sample avg dev_std = 0.105)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.563
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.575
NEC for r=0.6 class 0.0 = 0.136 +- 0.068 (in-sample avg dev_std = 0.121)
NEC for r=0.6 class 1.0 = 0.12 +- 0.068 (in-sample avg dev_std = 0.121)
NEC for r=0.6 all KL = 0.039 +- 0.068 (in-sample avg dev_std = 0.121)
NEC for r=0.6 all L1 = 0.122 +- 0.092 (in-sample avg dev_std = 0.121)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:07:50 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 12:07:50 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 12:08:30 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 12:08:43 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 12:08:56 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:14 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 37...
[0m[1;37mINFO[0m: [1mCheckpoint 37: 
-----------------------------------
Train ROC-AUC: 0.9829
Train Loss: 0.1261
ID Validation ROC-AUC: 0.9196
ID Validation Loss: 0.2917
ID Test ROC-AUC: 0.9170
ID Test Loss: 0.3056
OOD Validation ROC-AUC: 0.6433
OOD Validation Loss: 0.4965
OOD Test ROC-AUC: 0.6785
OOD Test Loss: 0.7235

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8825
Train Loss: 0.2710
ID Validation ROC-AUC: 0.8746
ID Validation Loss: 0.2831
ID Test ROC-AUC: 0.8768
ID Test Loss: 0.2841
OOD Validation ROC-AUC: 0.6840
OOD Validation Loss: 0.3139
OOD Test ROC-AUC: 0.7101
OOD Test Loss: 0.4991

[0m[1;37mINFO[0m: [1mChartInfo 0.9170 0.6785 0.8768 0.7101 0.8746 0.6840[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 12:09:31 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 12:09:41 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.7
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.638
SUFF++ for r=0.6 class 0.0 = 0.721 +- 0.130 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 class 1.0 = 0.859 +- 0.130 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 all KL = 0.888 +- 0.130 (in-sample avg dev_std = 0.215)
SUFF++ for r=0.6 all L1 = 0.843 +- 0.170 (in-sample avg dev_std = 0.215)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.59
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.565
SUFF++ for r=0.6 class 0.0 = 0.749 +- 0.139 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.6 class 1.0 = 0.813 +- 0.139 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.6 all KL = 0.856 +- 0.139 (in-sample avg dev_std = 0.250)
SUFF++ for r=0.6 all L1 = 0.802 +- 0.176 (in-sample avg dev_std = 0.250)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.7
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.712
NEC for r=0.6 class 0.0 = 0.311 +- 0.175 (in-sample avg dev_std = 0.210)
NEC for r=0.6 class 1.0 = 0.18 +- 0.175 (in-sample avg dev_std = 0.210)
NEC for r=0.6 all KL = 0.151 +- 0.175 (in-sample avg dev_std = 0.210)
NEC for r=0.6 all L1 = 0.195 +- 0.194 (in-sample avg dev_std = 0.210)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.59
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.598
NEC for r=0.6 class 0.0 = 0.294 +- 0.177 (in-sample avg dev_std = 0.225)
NEC for r=0.6 class 1.0 = 0.222 +- 0.177 (in-sample avg dev_std = 0.225)
NEC for r=0.6 all KL = 0.176 +- 0.177 (in-sample avg dev_std = 0.225)
NEC for r=0.6 all L1 = 0.234 +- 0.193 (in-sample avg dev_std = 0.225)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:11:00 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 12:11:00 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 12:11:38 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 12:11:49 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:00 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:16 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ROC-AUC: 0.9540
Train Loss: 0.1749
ID Validation ROC-AUC: 0.9236
ID Validation Loss: 0.2164
ID Test ROC-AUC: 0.9206
ID Test Loss: 0.2244
OOD Validation ROC-AUC: 0.6596
OOD Validation Loss: 0.3480
OOD Test ROC-AUC: 0.6987
OOD Test Loss: 0.4931

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8972
Train Loss: 0.2438
ID Validation ROC-AUC: 0.8845
ID Validation Loss: 0.2539
ID Test ROC-AUC: 0.8865
ID Test Loss: 0.2549
OOD Validation ROC-AUC: 0.6935
OOD Validation Loss: 0.2839
OOD Test ROC-AUC: 0.7162
OOD Test Loss: 0.4332

[0m[1;37mINFO[0m: [1mChartInfo 0.9206 0.6987 0.8865 0.7162 0.8845 0.6935[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 12:12:36 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 12:12:47 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.691
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.623
SUFF++ for r=0.6 class 0.0 = 0.887 +- 0.016 (in-sample avg dev_std = 0.103)
SUFF++ for r=0.6 class 1.0 = 0.89 +- 0.016 (in-sample avg dev_std = 0.103)
SUFF++ for r=0.6 all KL = 0.986 +- 0.016 (in-sample avg dev_std = 0.103)
SUFF++ for r=0.6 all L1 = 0.89 +- 0.043 (in-sample avg dev_std = 0.103)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.582
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.551
SUFF++ for r=0.6 class 0.0 = 0.889 +- 0.013 (in-sample avg dev_std = 0.108)
SUFF++ for r=0.6 class 1.0 = 0.887 +- 0.013 (in-sample avg dev_std = 0.108)
SUFF++ for r=0.6 all KL = 0.986 +- 0.013 (in-sample avg dev_std = 0.108)
SUFF++ for r=0.6 all L1 = 0.887 +- 0.042 (in-sample avg dev_std = 0.108)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.691
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.638
NEC for r=0.6 class 0.0 = 0.084 +- 0.021 (in-sample avg dev_std = 0.071)
NEC for r=0.6 class 1.0 = 0.079 +- 0.021 (in-sample avg dev_std = 0.071)
NEC for r=0.6 all KL = 0.008 +- 0.021 (in-sample avg dev_std = 0.071)
NEC for r=0.6 all L1 = 0.079 +- 0.041 (in-sample avg dev_std = 0.071)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.582
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.572
NEC for r=0.6 class 0.0 = 0.088 +- 0.012 (in-sample avg dev_std = 0.069)
NEC for r=0.6 class 1.0 = 0.083 +- 0.012 (in-sample avg dev_std = 0.069)
NEC for r=0.6 all KL = 0.008 +- 0.012 (in-sample avg dev_std = 0.069)
NEC for r=0.6 all L1 = 0.084 +- 0.047 (in-sample avg dev_std = 0.069)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:14:09 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 12:14:10 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 12:14:43 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 12:14:56 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:08 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:31 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 31...
[0m[1;37mINFO[0m: [1mCheckpoint 31: 
-----------------------------------
Train ROC-AUC: 0.9785
Train Loss: 0.1297
ID Validation ROC-AUC: 0.9231
ID Validation Loss: 0.2533
ID Test ROC-AUC: 0.9232
ID Test Loss: 0.2591
OOD Validation ROC-AUC: 0.6355
OOD Validation Loss: 0.4365
OOD Test ROC-AUC: 0.6820
OOD Test Loss: 0.6119

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 7...
[0m[1;37mINFO[0m: [1mCheckpoint 7: 
-----------------------------------
Train ROC-AUC: 0.9090
Train Loss: 0.2539
ID Validation ROC-AUC: 0.8928
ID Validation Loss: 0.2745
ID Test ROC-AUC: 0.8905
ID Test Loss: 0.2822
OOD Validation ROC-AUC: 0.6800
OOD Validation Loss: 0.3263
OOD Test ROC-AUC: 0.7103
OOD Test Loss: 0.5223

[0m[1;37mINFO[0m: [1mChartInfo 0.9232 0.6820 0.8905 0.7103 0.8928 0.6800[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 12:15:48 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 12:15:59 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.612
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.624
SUFF++ for r=0.6 class 0.0 = 0.677 +- 0.163 (in-sample avg dev_std = 0.380)
SUFF++ for r=0.6 class 1.0 = 0.672 +- 0.163 (in-sample avg dev_std = 0.380)
SUFF++ for r=0.6 all KL = 0.799 +- 0.163 (in-sample avg dev_std = 0.380)
SUFF++ for r=0.6 all L1 = 0.673 +- 0.119 (in-sample avg dev_std = 0.380)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.522
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.549
SUFF++ for r=0.6 class 0.0 = 0.68 +- 0.155 (in-sample avg dev_std = 0.361)
SUFF++ for r=0.6 class 1.0 = 0.681 +- 0.155 (in-sample avg dev_std = 0.361)
SUFF++ for r=0.6 all KL = 0.812 +- 0.155 (in-sample avg dev_std = 0.361)
SUFF++ for r=0.6 all L1 = 0.681 +- 0.121 (in-sample avg dev_std = 0.361)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.612
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.606
NEC for r=0.6 class 0.0 = 0.237 +- 0.185 (in-sample avg dev_std = 0.262)
NEC for r=0.6 class 1.0 = 0.233 +- 0.185 (in-sample avg dev_std = 0.262)
NEC for r=0.6 all KL = 0.127 +- 0.185 (in-sample avg dev_std = 0.262)
NEC for r=0.6 all L1 = 0.234 +- 0.160 (in-sample avg dev_std = 0.262)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.522
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.529
NEC for r=0.6 class 0.0 = 0.248 +- 0.180 (in-sample avg dev_std = 0.255)
NEC for r=0.6 class 1.0 = 0.238 +- 0.180 (in-sample avg dev_std = 0.255)
NEC for r=0.6 all KL = 0.128 +- 0.180 (in-sample avg dev_std = 0.255)
NEC for r=0.6 all L1 = 0.24 +- 0.168 (in-sample avg dev_std = 0.255)
model_dirname= repr_CIGAvGIN_3l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:17:07 2024
[0m[1;37mINFO[0m: [1mLoad Dataset LBAPcore
[0m[1;34mDEBUG[0m: 09/28/2024 12:17:07 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 09/28/2024 12:17:40 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 09/28/2024 12:17:50 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:03 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:19 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mDataset: {'train': LBAPcore(34179), 'id_val': LBAPcore(11314), 'id_test': LBAPcore(11683), 'val': LBAPcore(19028), 'test': LBAPcore(19032), 'task': 'Binary classification', 'metric': 'ROC-AUC'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1m Data(x=[31, 39], edge_index=[2, 68], edge_attr=[68, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 68], node_perm=[31])
[0mData(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:35 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing no mitigation None
[0m[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUsing the fixed _explain_ functionality
[0mmitigation_virtual in vGINMolEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 24...
[0m[1;37mINFO[0m: [1mCheckpoint 24: 
-----------------------------------
Train ROC-AUC: 0.9685
Train Loss: 0.1653
ID Validation ROC-AUC: 0.9224
ID Validation Loss: 0.2315
ID Test ROC-AUC: 0.9196
ID Test Loss: 0.2391
OOD Validation ROC-AUC: 0.6434
OOD Validation Loss: 0.3691
OOD Test ROC-AUC: 0.6778
OOD Test Loss: 0.5284

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 3...
[0m[1;37mINFO[0m: [1mCheckpoint 3: 
-----------------------------------
Train ROC-AUC: 0.8955
Train Loss: 0.2439
ID Validation ROC-AUC: 0.8855
ID Validation Loss: 0.2541
ID Test ROC-AUC: 0.8869
ID Test Loss: 0.2554
OOD Validation ROC-AUC: 0.6944
OOD Validation Loss: 0.2933
OOD Test ROC-AUC: 0.7180
OOD Test Loss: 0.4561

[0m[1;37mINFO[0m: [1mChartInfo 0.9196 0.6778 0.8869 0.7180 0.8855 0.6944[0mLBAPcore(11314)
Data example from id_val: Data(x=[30, 39], edge_index=[2, 66], edge_attr=[66, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 66], node_perm=[30])
Label distribution from id_val: (tensor([0., 1.]), tensor([1318, 9996]))
[1;34mDEBUG[0m: 09/28/2024 12:18:36 PM : [1mUnbalanced warning for LBAPcore (id_val)
[0m/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
LBAPcore(19032)
Data example from test: Data(x=[28, 39], edge_index=[2, 64], edge_attr=[64, 10], y=[1, 1], env_id=[1], ori_edge_index=[2, 64], node_perm=[28])
Label distribution from test: (tensor([0., 1.]), tensor([ 3168, 15864]))
[1;34mDEBUG[0m: 09/28/2024 12:18:45 PM : [1mUnbalanced warning for LBAPcore (test)
[0mF1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.312
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.374
SUFF++ for r=0.6 class 0.0 = 0.858 +- 0.018 (in-sample avg dev_std = 0.101)
SUFF++ for r=0.6 class 1.0 = 0.891 +- 0.018 (in-sample avg dev_std = 0.101)
SUFF++ for r=0.6 all KL = 0.985 +- 0.018 (in-sample avg dev_std = 0.101)
SUFF++ for r=0.6 all L1 = 0.887 +- 0.052 (in-sample avg dev_std = 0.101)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.381
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.404
SUFF++ for r=0.6 class 0.0 = 0.87 +- 0.013 (in-sample avg dev_std = 0.105)
SUFF++ for r=0.6 class 1.0 = 0.884 +- 0.013 (in-sample avg dev_std = 0.105)
SUFF++ for r=0.6 all KL = 0.984 +- 0.013 (in-sample avg dev_std = 0.105)
SUFF++ for r=0.6 all L1 = 0.882 +- 0.049 (in-sample avg dev_std = 0.105)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.312
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.005
Model ROC-AUC over intervened graphs for r=0.6 =  0.321
NEC for r=0.6 class 0.0 = 0.071 +- 0.005 (in-sample avg dev_std = 0.044)
NEC for r=0.6 class 1.0 = 0.056 +- 0.005 (in-sample avg dev_std = 0.044)
NEC for r=0.6 all KL = 0.003 +- 0.005 (in-sample avg dev_std = 0.044)
NEC for r=0.6 all L1 = 0.058 +- 0.031 (in-sample avg dev_std = 0.044)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model ROC-AUC of binarized graphs for r=0.6 =  0.381
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.608 +- 0.006
Model ROC-AUC over intervened graphs for r=0.6 =  0.379
NEC for r=0.6 class 0.0 = 0.072 +- 0.006 (in-sample avg dev_std = 0.048)
NEC for r=0.6 class 1.0 = 0.06 +- 0.006 (in-sample avg dev_std = 0.048)
NEC for r=0.6 all KL = 0.004 +- 0.006 (in-sample avg dev_std = 0.048)
NEC for r=0.6 all L1 = 0.062 +- 0.034 (in-sample avg dev_std = 0.048)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.964], 'all_L1': [0.881]}), defaultdict(<class 'list'>, {'all_KL': [0.888], 'all_L1': [0.843]}), defaultdict(<class 'list'>, {'all_KL': [0.986], 'all_L1': [0.89]}), defaultdict(<class 'list'>, {'all_KL': [0.799], 'all_L1': [0.673]}), defaultdict(<class 'list'>, {'all_KL': [0.985], 'all_L1': [0.887]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.028], 'all_L1': [0.1]}), defaultdict(<class 'list'>, {'all_KL': [0.151], 'all_L1': [0.195]}), defaultdict(<class 'list'>, {'all_KL': [0.008], 'all_L1': [0.079]}), defaultdict(<class 'list'>, {'all_KL': [0.127], 'all_L1': [0.234]}), defaultdict(<class 'list'>, {'all_KL': [0.003], 'all_L1': [0.058]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.957], 'all_L1': [0.864]}), defaultdict(<class 'list'>, {'all_KL': [0.856], 'all_L1': [0.802]}), defaultdict(<class 'list'>, {'all_KL': [0.986], 'all_L1': [0.887]}), defaultdict(<class 'list'>, {'all_KL': [0.812], 'all_L1': [0.681]}), defaultdict(<class 'list'>, {'all_KL': [0.984], 'all_L1': [0.882]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.039], 'all_L1': [0.122]}), defaultdict(<class 'list'>, {'all_KL': [0.176], 'all_L1': [0.234]}), defaultdict(<class 'list'>, {'all_KL': [0.008], 'all_L1': [0.084]}), defaultdict(<class 'list'>, {'all_KL': [0.128], 'all_L1': [0.24]}), defaultdict(<class 'list'>, {'all_KL': [0.004], 'all_L1': [0.062]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.835 +- 0.083
suff++ class all_KL  =  0.924 +- 0.072
suff++_acc_int  =  0.577 +- 0.102
nec class all_L1  =  0.133 +- 0.069
nec class all_KL  =  0.063 +- 0.063
nec_acc_int  =  0.584 +- 0.136

Eval split test
suff++ class all_L1  =  0.823 +- 0.077
suff++ class all_KL  =  0.919 +- 0.072
suff++_acc_int  =  0.527 +- 0.062
nec class all_L1  =  0.148 +- 0.075
nec class all_KL  =  0.071 +- 0.069
nec_acc_int  =  0.531 +- 0.079


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.484 +- 0.022
Faith. Armon (L1)= 		  =  0.220 +- 0.095
Faith. GMean (L1)= 	  =  0.318 +- 0.071
Faith. Aritm (KL)= 		  =  0.494 +- 0.018
Faith. Armon (KL)= 		  =  0.111 +- 0.106
Faith. GMean (KL)= 	  =  0.198 +- 0.124

Eval split test
Faith. Aritm (L1)= 		  =  0.486 +- 0.020
Faith. Armon (L1)= 		  =  0.240 +- 0.102
Faith. GMean (L1)= 	  =  0.334 +- 0.076
Faith. Aritm (KL)= 		  =  0.495 +- 0.015
Faith. Armon (KL)= 		  =  0.122 +- 0.114
Faith. GMean (KL)= 	  =  0.211 +- 0.127
Computed for split load_split = id



Completed in  0:15:15.155259  for CIGAvGIN LBAPcore/assay



DONE CIGA LBAPcore/assay
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:20:10 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:10 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:20:11 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 72...
[0m[1;37mINFO[0m: [1mCheckpoint 72: 
-----------------------------------
Train ACCURACY: 0.3715
Train Loss: 2.7499
ID Validation ACCURACY: 0.3654
ID Validation Loss: 2.7861
ID Test ACCURACY: 0.3729
ID Test Loss: 2.7814
OOD Validation ACCURACY: 0.3491
OOD Validation Loss: 3.1257
OOD Test ACCURACY: 0.2690
OOD Test Loss: 4.6794

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 85...
[0m[1;37mINFO[0m: [1mCheckpoint 85: 
-----------------------------------
Train ACCURACY: 0.3198
Train Loss: 3.3902
ID Validation ACCURACY: 0.3150
ID Validation Loss: 3.4229
ID Test ACCURACY: 0.3230
ID Test Loss: 3.4163
OOD Validation ACCURACY: 0.3611
OOD Validation Loss: 3.3370
OOD Test ACCURACY: 0.2419
OOD Test Loss: 7.8889

[0m[1;37mINFO[0m: [1mChartInfo 0.3729 0.2690 0.3230 0.2419 0.3150 0.3611[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.381
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.197
SUFF++ for r=0.6 class 0 = 0.423 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 1 = 0.341 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 2 = 0.393 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 3 = 0.372 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 4 = 0.514 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 5 = 0.408 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 6 = 0.466 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 7 = 0.448 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 8 = 0.409 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 class 9 = 0.544 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 all KL = 0.396 +- 0.226 (in-sample avg dev_std = 0.490)
SUFF++ for r=0.6 all L1 = 0.43 +- 0.126 (in-sample avg dev_std = 0.490)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.25
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.198
SUFF++ for r=0.6 class 0 = 0.354 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 1 = 0.473 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 2 = 0.296 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 3 = 0.296 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 4 = 0.283 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 5 = 0.303 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 6 = 0.302 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 7 = 0.305 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 8 = 0.305 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 class 9 = 0.295 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 all KL = 0.127 +- 0.138 (in-sample avg dev_std = 0.634)
SUFF++ for r=0.6 all L1 = 0.323 +- 0.097 (in-sample avg dev_std = 0.634)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.381
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.289
NEC for r=0.6 class 0 = 0.162 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 1 = 0.495 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 2 = 0.505 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 3 = 0.513 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 4 = 0.384 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 5 = 0.507 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 6 = 0.426 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 7 = 0.477 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 8 = 0.51 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 class 9 = 0.316 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 all KL = 0.359 +- 0.223 (in-sample avg dev_std = 0.282)
NEC for r=0.6 all L1 = 0.431 +- 0.196 (in-sample avg dev_std = 0.282)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.25
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.269
NEC for r=0.6 class 0 = 0.373 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 1 = 0.182 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 2 = 0.459 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 3 = 0.549 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 4 = 0.53 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 5 = 0.521 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 6 = 0.516 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 7 = 0.545 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 8 = 0.53 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 class 9 = 0.528 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 all KL = 0.429 +- 0.258 (in-sample avg dev_std = 0.318)
NEC for r=0.6 all L1 = 0.468 +- 0.214 (in-sample avg dev_std = 0.318)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:26:15 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:26:17 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 87...
[0m[1;37mINFO[0m: [1mCheckpoint 87: 
-----------------------------------
Train ACCURACY: 0.7181
Train Loss: 0.8746
ID Validation ACCURACY: 0.6574
ID Validation Loss: 1.2188
ID Test ACCURACY: 0.6584
ID Test Loss: 1.2023
OOD Validation ACCURACY: 0.4703
OOD Validation Loss: 2.8402
OOD Test ACCURACY: 0.1883
OOD Test Loss: 15.9927

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 67...
[0m[1;37mINFO[0m: [1mCheckpoint 67: 
-----------------------------------
Train ACCURACY: 0.6624
Train Loss: 1.0155
ID Validation ACCURACY: 0.6256
ID Validation Loss: 1.1994
ID Test ACCURACY: 0.6141
ID Test Loss: 1.2223
OOD Validation ACCURACY: 0.5277
OOD Validation Loss: 1.8377
OOD Test ACCURACY: 0.2083
OOD Test Loss: 11.1952

[0m[1;37mINFO[0m: [1mChartInfo 0.6584 0.1883 0.6141 0.2083 0.6256 0.5277[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.267
SUFF++ for r=0.6 class 0 = 0.247 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 1 = 0.328 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 2 = 0.28 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 3 = 0.242 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 4 = 0.312 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 5 = 0.28 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 6 = 0.248 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 7 = 0.296 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 8 = 0.261 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 class 9 = 0.298 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 all KL = 0.069 +- 0.110 (in-sample avg dev_std = 0.651)
SUFF++ for r=0.6 all L1 = 0.279 +- 0.093 (in-sample avg dev_std = 0.651)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.172
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.165
SUFF++ for r=0.6 class 0 = 0.412 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 1 = 0.63 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 2 = 0.422 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 3 = 0.403 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 4 = 0.502 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 5 = 0.422 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 6 = 0.581 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 7 = 0.527 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 8 = 0.427 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 class 9 = 0.586 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 all KL = 0.179 +- 0.308 (in-sample avg dev_std = 0.719)
SUFF++ for r=0.6 all L1 = 0.492 +- 0.262 (in-sample avg dev_std = 0.719)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.699
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.42
NEC for r=0.6 class 0 = 0.634 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 1 = 0.276 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 2 = 0.579 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 3 = 0.614 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 4 = 0.524 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 5 = 0.606 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 6 = 0.657 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 7 = 0.598 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 8 = 0.55 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 class 9 = 0.569 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 all KL = 0.689 +- 0.261 (in-sample avg dev_std = 0.475)
NEC for r=0.6 all L1 = 0.557 +- 0.211 (in-sample avg dev_std = 0.475)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.172
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.2
NEC for r=0.6 class 0 = 0.386 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 1 = 0.165 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 2 = 0.413 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 3 = 0.439 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 4 = 0.263 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 5 = 0.437 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 6 = 0.27 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 7 = 0.273 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 8 = 0.33 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 class 9 = 0.177 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 all KL = 0.428 +- 0.414 (in-sample avg dev_std = 0.408)
NEC for r=0.6 all L1 = 0.314 +- 0.322 (in-sample avg dev_std = 0.408)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:32:01 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:32:02 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:32:03 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 56...
[0m[1;37mINFO[0m: [1mCheckpoint 56: 
-----------------------------------
Train ACCURACY: 0.4452
Train Loss: 1.8491
ID Validation ACCURACY: 0.4349
ID Validation Loss: 1.9176
ID Test ACCURACY: 0.4390
ID Test Loss: 1.8691
OOD Validation ACCURACY: 0.3544
OOD Validation Loss: 2.4861
OOD Test ACCURACY: 0.2210
OOD Test Loss: 3.7235

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 73...
[0m[1;37mINFO[0m: [1mCheckpoint 73: 
-----------------------------------
Train ACCURACY: 0.4067
Train Loss: 2.3142
ID Validation ACCURACY: 0.3957
ID Validation Loss: 2.4284
ID Test ACCURACY: 0.4073
ID Test Loss: 2.3439
OOD Validation ACCURACY: 0.3826
OOD Validation Loss: 2.4420
OOD Test ACCURACY: 0.2560
OOD Test Loss: 4.6004

[0m[1;37mINFO[0m: [1mChartInfo 0.4390 0.2210 0.4073 0.2560 0.3957 0.3826[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.452
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.246
SUFF++ for r=0.6 class 0 = 0.325 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 1 = 0.557 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 2 = 0.234 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 3 = 0.25 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 4 = 0.254 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 5 = 0.254 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 6 = 0.258 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 7 = 0.257 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 8 = 0.264 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 class 9 = 0.279 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 all KL = 0.09 +- 0.213 (in-sample avg dev_std = 0.697)
SUFF++ for r=0.6 all L1 = 0.297 +- 0.129 (in-sample avg dev_std = 0.697)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.199
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.173
SUFF++ for r=0.6 class 0 = 0.314 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 1 = 0.41 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 2 = 0.272 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 3 = 0.267 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 4 = 0.264 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 5 = 0.272 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 6 = 0.281 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 7 = 0.283 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 8 = 0.281 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 class 9 = 0.275 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 all KL = 0.102 +- 0.169 (in-sample avg dev_std = 0.678)
SUFF++ for r=0.6 all L1 = 0.294 +- 0.075 (in-sample avg dev_std = 0.678)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.452
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.378
NEC for r=0.6 class 0 = 0.252 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 1 = 0.369 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 2 = 0.579 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 3 = 0.538 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 4 = 0.517 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 5 = 0.544 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 6 = 0.507 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 7 = 0.52 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 8 = 0.513 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 class 9 = 0.458 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 all KL = 0.43 +- 0.240 (in-sample avg dev_std = 0.306)
NEC for r=0.6 all L1 = 0.478 +- 0.188 (in-sample avg dev_std = 0.306)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.199
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.225
NEC for r=0.6 class 0 = 0.284 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 1 = 0.374 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 2 = 0.467 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 3 = 0.519 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 4 = 0.512 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 5 = 0.494 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 6 = 0.507 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 7 = 0.511 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 8 = 0.496 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 class 9 = 0.519 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 all KL = 0.387 +- 0.225 (in-sample avg dev_std = 0.305)
NEC for r=0.6 all L1 = 0.466 +- 0.183 (in-sample avg dev_std = 0.305)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:37:57 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:37:58 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 51...
[0m[1;37mINFO[0m: [1mCheckpoint 51: 
-----------------------------------
Train ACCURACY: 0.6588
Train Loss: 0.9911
ID Validation ACCURACY: 0.6276
ID Validation Loss: 1.1178
ID Test ACCURACY: 0.6277
ID Test Loss: 1.1142
OOD Validation ACCURACY: 0.4983
OOD Validation Loss: 1.7285
OOD Test ACCURACY: 0.2486
OOD Test Loss: 4.9047

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 57...
[0m[1;37mINFO[0m: [1mCheckpoint 57: 
-----------------------------------
Train ACCURACY: 0.5989
Train Loss: 1.1783
ID Validation ACCURACY: 0.5699
ID Validation Loss: 1.3077
ID Test ACCURACY: 0.5776
ID Test Loss: 1.2851
OOD Validation ACCURACY: 0.5351
OOD Validation Loss: 1.4928
OOD Test ACCURACY: 0.2547
OOD Test Loss: 5.2471

[0m[1;37mINFO[0m: [1mChartInfo 0.6277 0.2486 0.5776 0.2547 0.5699 0.5351[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.284
SUFF++ for r=0.6 class 0 = 0.284 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 1 = 0.423 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 2 = 0.307 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 3 = 0.344 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 4 = 0.346 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 5 = 0.317 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 6 = 0.275 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 7 = 0.291 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 8 = 0.379 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 class 9 = 0.321 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 all KL = 0.173 +- 0.164 (in-sample avg dev_std = 0.597)
SUFF++ for r=0.6 all L1 = 0.33 +- 0.096 (in-sample avg dev_std = 0.597)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.26
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.202
SUFF++ for r=0.6 class 0 = 0.345 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 1 = 0.848 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 2 = 0.326 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 3 = 0.414 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 4 = 0.469 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 5 = 0.38 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 6 = 0.515 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 7 = 0.482 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 8 = 0.474 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 class 9 = 0.593 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 all KL = 0.344 +- 0.288 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.6 all L1 = 0.488 +- 0.244 (in-sample avg dev_std = 0.559)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.639
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.44
NEC for r=0.6 class 0 = 0.457 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 1 = 0.07 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 2 = 0.561 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 3 = 0.52 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 4 = 0.507 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 5 = 0.551 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 6 = 0.594 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 7 = 0.596 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 8 = 0.525 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 class 9 = 0.559 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 all KL = 0.501 +- 0.257 (in-sample avg dev_std = 0.354)
NEC for r=0.6 all L1 = 0.488 +- 0.211 (in-sample avg dev_std = 0.354)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.26
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.277
NEC for r=0.6 class 0 = 0.428 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 1 = 0.055 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 2 = 0.537 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 3 = 0.429 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 4 = 0.41 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 5 = 0.447 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 6 = 0.375 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 7 = 0.452 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 8 = 0.383 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 class 9 = 0.263 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 all KL = 0.353 +- 0.269 (in-sample avg dev_std = 0.301)
NEC for r=0.6 all L1 = 0.375 +- 0.248 (in-sample avg dev_std = 0.301)
model_dirname= repr_CIGAvGIN_5l_meanpool_0.5dp_mitig_backboneNone_mitig_samplingfeatavgedgeattnmean

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Sat Sep 28 12:44:15 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODCMNIST
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mLoading dataset with permuted nodes
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mDataset: {'train': GOODCMNIST(42000), 'id_val': GOODCMNIST(7000), 'id_test': GOODCMNIST(7000), 'val': GOODCMNIST(7000), 'test': GOODCMNIST(7000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1m Data(x=[75, 3], edge_index=[2, 1309], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1309], node_perm=[75])
[0mData(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mCreating GINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing  3  layers for classifier
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit Backbone:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mCreating vGINFeatExtractor:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit CLF:  5
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit GINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mInit vGINFeatExtractor
[0mUsing BN in BasicEncoder
mitigation_readout =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 09/28/2024 12:44:16 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 09/28/2024 12:44:17 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 42...
[0m[1;37mINFO[0m: [1mCheckpoint 42: 
-----------------------------------
Train ACCURACY: 0.4206
Train Loss: 1.8923
ID Validation ACCURACY: 0.4203
ID Validation Loss: 1.9484
ID Test ACCURACY: 0.4191
ID Test Loss: 1.8983
OOD Validation ACCURACY: 0.3490
OOD Validation Loss: 2.3843
OOD Test ACCURACY: 0.2689
OOD Test Loss: 2.7813

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.4088
Train Loss: 1.8664
ID Validation ACCURACY: 0.4009
ID Validation Loss: 1.9187
ID Test ACCURACY: 0.4040
ID Test Loss: 1.9019
OOD Validation ACCURACY: 0.3840
OOD Validation Loss: 2.1207
OOD Test ACCURACY: 0.2513
OOD Test Loss: 3.0490

[0m[1;37mINFO[0m: [1mChartInfo 0.4191 0.2689 0.4040 0.2513 0.4009 0.3840[0mGOODCMNIST(7000)
Data example from id_val: Data(x=[75, 3], edge_index=[2, 1496], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1496], node_perm=[75])
Label distribution from id_val: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([704, 788, 715, 765, 668, 583, 676, 769, 678, 654]))
/mnt/cimec-storage6/users/steve.azzolin/sedignn/leci_private_fork/GOOD/utils/config_reader.py:166: UserWarning: Argument channel_weight_decay in the chosen config yaml file are not defined in command arguments, which will lead to incomplete code detection and the lack of argument temporary modification by adding command arguments.
  warnings.warn(f'Argument {key} in the chosen config yaml file are not defined in command arguments, '
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan
GOODCMNIST(7000)
Data example from test: Data(x=[75, 3], edge_index=[2, 1431], y=[1], pos=[75, 2], color=[1], env_id=[1], ori_edge_index=[2, 1431], node_perm=[75])
Label distribution from test: (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([716, 781, 751, 697, 673, 616, 676, 705, 705, 680]))
F1 for r=0.6 = nan
WIoU for r=0.6 = nan
F1 for r=1.0 = nan
WIoU for r=1.0 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.431
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.259
SUFF++ for r=0.6 class 0 = 0.34 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 1 = 0.714 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 2 = 0.299 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 3 = 0.334 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 4 = 0.316 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 5 = 0.322 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 6 = 0.293 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 7 = 0.336 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 8 = 0.32 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 class 9 = 0.312 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 all KL = 0.212 +- 0.245 (in-sample avg dev_std = 0.649)
SUFF++ for r=0.6 all L1 = 0.364 +- 0.157 (in-sample avg dev_std = 0.649)



--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.255
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.223
SUFF++ for r=0.6 class 0 = 0.365 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 1 = 0.474 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 2 = 0.343 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 3 = 0.342 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 4 = 0.349 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 5 = 0.334 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 6 = 0.355 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 7 = 0.361 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 8 = 0.348 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 class 9 = 0.374 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 all KL = 0.273 +- 0.172 (in-sample avg dev_std = 0.590)
SUFF++ for r=0.6 all L1 = 0.366 +- 0.076 (in-sample avg dev_std = 0.590)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over id_val across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.431
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.417
NEC for r=0.6 class 0 = 0.157 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 1 = 0.214 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 2 = 0.463 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 3 = 0.459 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 4 = 0.416 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 5 = 0.485 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 6 = 0.488 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 7 = 0.457 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 8 = 0.46 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 9 = 0.44 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 all KL = 0.294 +- 0.191 (in-sample avg dev_std = 0.244)
NEC for r=0.6 all L1 = 0.401 +- 0.181 (in-sample avg dev_std = 0.244)



--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=True)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.255
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.600 +- 0.000
Model Accuracy over intervened graphs for r=0.6 =  0.286
NEC for r=0.6 class 0 = 0.152 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 1 = 0.332 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 2 = 0.44 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 3 = 0.483 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 4 = 0.479 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 5 = 0.462 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 6 = 0.476 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 7 = 0.476 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 8 = 0.459 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 class 9 = 0.439 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 all KL = 0.292 +- 0.175 (in-sample avg dev_std = 0.244)
NEC for r=0.6 all L1 = 0.417 +- 0.165 (in-sample avg dev_std = 0.244)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split id_val
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.396], 'all_L1': [0.43]}), defaultdict(<class 'list'>, {'all_KL': [0.069], 'all_L1': [0.279]}), defaultdict(<class 'list'>, {'all_KL': [0.09], 'all_L1': [0.297]}), defaultdict(<class 'list'>, {'all_KL': [0.173], 'all_L1': [0.33]}), defaultdict(<class 'list'>, {'all_KL': [0.212], 'all_L1': [0.364]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.359], 'all_L1': [0.431]}), defaultdict(<class 'list'>, {'all_KL': [0.689], 'all_L1': [0.557]}), defaultdict(<class 'list'>, {'all_KL': [0.43], 'all_L1': [0.478]}), defaultdict(<class 'list'>, {'all_KL': [0.501], 'all_L1': [0.488]}), defaultdict(<class 'list'>, {'all_KL': [0.294], 'all_L1': [0.401]})]

Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.127], 'all_L1': [0.323]}), defaultdict(<class 'list'>, {'all_KL': [0.179], 'all_L1': [0.492]}), defaultdict(<class 'list'>, {'all_KL': [0.102], 'all_L1': [0.294]}), defaultdict(<class 'list'>, {'all_KL': [0.344], 'all_L1': [0.488]}), defaultdict(<class 'list'>, {'all_KL': [0.273], 'all_L1': [0.366]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.429], 'all_L1': [0.468]}), defaultdict(<class 'list'>, {'all_KL': [0.428], 'all_L1': [0.314]}), defaultdict(<class 'list'>, {'all_KL': [0.387], 'all_L1': [0.466]}), defaultdict(<class 'list'>, {'all_KL': [0.353], 'all_L1': [0.375]}), defaultdict(<class 'list'>, {'all_KL': [0.292], 'all_L1': [0.417]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split id_val
suff++ class all_L1  =  0.340 +- 0.054
suff++ class all_KL  =  0.188 +- 0.116
suff++_acc_int  =  0.251 +- 0.029
nec class all_L1  =  0.471 +- 0.053
nec class all_KL  =  0.455 +- 0.136
nec_acc_int  =  0.389 +- 0.054

Eval split test
suff++ class all_L1  =  0.393 +- 0.083
suff++ class all_KL  =  0.205 +- 0.091
suff++_acc_int  =  0.192 +- 0.021
nec class all_L1  =  0.408 +- 0.058
nec class all_KL  =  0.378 +- 0.051
nec_acc_int  =  0.251 +- 0.033


 -------------------------------------------------- 
Computing faithfulness

Eval split id_val
Faith. Aritm (L1)= 		  =  0.406 +- 0.018
Faith. Armon (L1)= 		  =  0.389 +- 0.023
Faith. GMean (L1)= 	  =  0.397 +- 0.019
Faith. Aritm (KL)= 		  =  0.321 +- 0.055
Faith. Armon (KL)= 		  =  0.231 +- 0.089
Faith. GMean (KL)= 	  =  0.267 +- 0.064

Eval split test
Faith. Aritm (L1)= 		  =  0.400 +- 0.017
Faith. Armon (L1)= 		  =  0.388 +- 0.021
Faith. GMean (L1)= 	  =  0.394 +- 0.019
Faith. Aritm (KL)= 		  =  0.291 +- 0.034
Faith. Armon (KL)= 		  =  0.248 +- 0.066
Faith. GMean (KL)= 	  =  0.268 +- 0.051
Computed for split load_split = id



Completed in  0:29:48.312470  for CIGAvGIN GOODCMNIST/color



DONE CIGA GOODCMNIST/color
DONE all :)
