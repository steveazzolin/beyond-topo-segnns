Time to compute metrics!

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:20:29 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:29 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:45 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:47 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:49 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:51 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:20:53 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9236
Train Loss: 0.3604
ID Validation ACCURACY: 0.9227
ID Validation Loss: 0.3790
ID Test ACCURACY: 0.9237
ID Test Loss: 0.3648
OOD Validation ACCURACY: 0.4100
OOD Validation Loss: 14.0711
OOD Test ACCURACY: 0.6730
OOD Test Loss: 0.9019

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 0...
[0m[1;37mINFO[0m: [1mCheckpoint 0: 
-----------------------------------
Train ACCURACY: 0.8039
Train Loss: 0.6801
ID Validation ACCURACY: 0.7977
ID Validation Loss: 0.7040
ID Test ACCURACY: 0.8167
ID Test Loss: 0.6746
OOD Validation ACCURACY: 0.9300
OOD Validation Loss: 0.4492
OOD Test ACCURACY: 0.3700
OOD Test Loss: 12.3785

[0m[1;37mINFO[0m: [1mChartInfo 0.9237 0.6730 0.8167 0.3700 0.7977 0.9300[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.577
WIoU for r=0.6 = 0.438


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.65
Model XAI F1 of binarized graphs for r=0.6 =  0.577325
Model XAI WIoU of binarized graphs for r=0.6 =  0.43811374999999997
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.498
SUFF++ for r=0.6 class 0 = 0.482 +- 0.251 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 1 = 0.56 +- 0.251 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 class 2 = 0.38 +- 0.251 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 all KL = 0.395 +- 0.251 (in-sample avg dev_std = 0.639)
SUFF++ for r=0.6 all L1 = 0.472 +- 0.166 (in-sample avg dev_std = 0.639)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.65
Model XAI F1 of binarized graphs for r=0.6 =  0.577325
Model XAI WIoU of binarized graphs for r=0.6 =  0.43811374999999997
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.518
NEC for r=0.6 class 0 = 0.542 +- 0.268 (in-sample avg dev_std = 0.521)
NEC for r=0.6 class 1 = 0.383 +- 0.268 (in-sample avg dev_std = 0.521)
NEC for r=0.6 class 2 = 0.526 +- 0.268 (in-sample avg dev_std = 0.521)
NEC for r=0.6 all KL = 0.544 +- 0.268 (in-sample avg dev_std = 0.521)
NEC for r=0.6 all L1 = 0.486 +- 0.194 (in-sample avg dev_std = 0.521)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:21:16 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:16 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:29 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:31 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:33 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:34 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:36 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 38...
[0m[1;37mINFO[0m: [1mCheckpoint 38: 
-----------------------------------
Train ACCURACY: 0.8845
Train Loss: 0.5305
ID Validation ACCURACY: 0.8827
ID Validation Loss: 0.5555
ID Test ACCURACY: 0.8790
ID Test Loss: 0.5276
OOD Validation ACCURACY: 0.4290
OOD Validation Loss: 21.8019
OOD Test ACCURACY: 0.5357
OOD Test Loss: 1.5598

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 91...
[0m[1;37mINFO[0m: [1mCheckpoint 91: 
-----------------------------------
Train ACCURACY: 0.7712
Train Loss: 0.6710
ID Validation ACCURACY: 0.7690
ID Validation Loss: 0.6984
ID Test ACCURACY: 0.7687
ID Test Loss: 0.6839
OOD Validation ACCURACY: 0.7957
OOD Validation Loss: 0.6392
OOD Test ACCURACY: 0.4723
OOD Test Loss: 1.6875

[0m[1;37mINFO[0m: [1mChartInfo 0.8790 0.5357 0.7687 0.4723 0.7690 0.7957[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.607
WIoU for r=0.6 = 0.547


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  0.60715375
Model XAI WIoU of binarized graphs for r=0.6 =  0.54695
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.559
SUFF++ for r=0.6 class 0 = 0.54 +- 0.291 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 1 = 0.526 +- 0.291 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 class 2 = 0.946 +- 0.291 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 all KL = 0.65 +- 0.291 (in-sample avg dev_std = 0.564)
SUFF++ for r=0.6 all L1 = 0.673 +- 0.244 (in-sample avg dev_std = 0.564)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.546
Model XAI F1 of binarized graphs for r=0.6 =  0.60715375
Model XAI WIoU of binarized graphs for r=0.6 =  0.54695
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.442
NEC for r=0.6 class 0 = 0.578 +- 0.314 (in-sample avg dev_std = 0.670)
NEC for r=0.6 class 1 = 0.418 +- 0.314 (in-sample avg dev_std = 0.670)
NEC for r=0.6 class 2 = 0.588 +- 0.314 (in-sample avg dev_std = 0.670)
NEC for r=0.6 all KL = 0.628 +- 0.314 (in-sample avg dev_std = 0.670)
NEC for r=0.6 all L1 = 0.53 +- 0.197 (in-sample avg dev_std = 0.670)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:21:55 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:21:55 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:08 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:10 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:12 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:14 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:16 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 26...
[0m[1;37mINFO[0m: [1mCheckpoint 26: 
-----------------------------------
Train ACCURACY: 0.8712
Train Loss: 0.4857
ID Validation ACCURACY: 0.8610
ID Validation Loss: 0.5151
ID Test ACCURACY: 0.8730
ID Test Loss: 0.4697
OOD Validation ACCURACY: 0.6573
OOD Validation Loss: 2.5816
OOD Test ACCURACY: 0.7480
OOD Test Loss: 0.6393

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 54...
[0m[1;37mINFO[0m: [1mCheckpoint 54: 
-----------------------------------
Train ACCURACY: 0.7893
Train Loss: 0.6406
ID Validation ACCURACY: 0.7840
ID Validation Loss: 0.6862
ID Test ACCURACY: 0.7930
ID Test Loss: 0.6301
OOD Validation ACCURACY: 0.9127
OOD Validation Loss: 0.4331
OOD Test ACCURACY: 0.5323
OOD Test Loss: 1.1321

[0m[1;37mINFO[0m: [1mChartInfo 0.8730 0.7480 0.7930 0.5323 0.7840 0.9127[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.555
WIoU for r=0.6 = 0.504


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.745
Model XAI F1 of binarized graphs for r=0.6 =  0.55481
Model XAI WIoU of binarized graphs for r=0.6 =  0.50397625
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.654
SUFF++ for r=0.6 class 0 = 0.538 +- 0.206 (in-sample avg dev_std = 0.474)
SUFF++ for r=0.6 class 1 = 0.636 +- 0.206 (in-sample avg dev_std = 0.474)
SUFF++ for r=0.6 class 2 = 0.797 +- 0.206 (in-sample avg dev_std = 0.474)
SUFF++ for r=0.6 all KL = 0.712 +- 0.206 (in-sample avg dev_std = 0.474)
SUFF++ for r=0.6 all L1 = 0.658 +- 0.171 (in-sample avg dev_std = 0.474)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.745
Model XAI F1 of binarized graphs for r=0.6 =  0.55481
Model XAI WIoU of binarized graphs for r=0.6 =  0.50397625
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.494
NEC for r=0.6 class 0 = 0.489 +- 0.339 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 1 = 0.225 +- 0.339 (in-sample avg dev_std = 0.503)
NEC for r=0.6 class 2 = 0.668 +- 0.339 (in-sample avg dev_std = 0.503)
NEC for r=0.6 all KL = 0.419 +- 0.339 (in-sample avg dev_std = 0.503)
NEC for r=0.6 all L1 = 0.465 +- 0.226 (in-sample avg dev_std = 0.503)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:22:35 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:35 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:48 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:50 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:52 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:54 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:22:56 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 23...
[0m[1;37mINFO[0m: [1mCheckpoint 23: 
-----------------------------------
Train ACCURACY: 0.9014
Train Loss: 0.4323
ID Validation ACCURACY: 0.9013
ID Validation Loss: 0.4485
ID Test ACCURACY: 0.9030
ID Test Loss: 0.4260
OOD Validation ACCURACY: 0.5967
OOD Validation Loss: 8.9459
OOD Test ACCURACY: 0.7107
OOD Test Loss: 0.7527

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.8482
Train Loss: 0.5161
ID Validation ACCURACY: 0.8487
ID Validation Loss: 0.5316
ID Test ACCURACY: 0.8440
ID Test Loss: 0.5144
OOD Validation ACCURACY: 0.7533
OOD Validation Loss: 1.2198
OOD Test ACCURACY: 0.7353
OOD Test Loss: 0.8736

[0m[1;37mINFO[0m: [1mChartInfo 0.9030 0.7107 0.8440 0.7353 0.8487 0.7533[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.638
WIoU for r=0.6 = 0.597


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.724
Model XAI F1 of binarized graphs for r=0.6 =  0.63822625
Model XAI WIoU of binarized graphs for r=0.6 =  0.59697875
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.634
SUFF++ for r=0.6 class 0 = 0.559 +- 0.233 (in-sample avg dev_std = 0.411)
SUFF++ for r=0.6 class 1 = 0.524 +- 0.233 (in-sample avg dev_std = 0.411)
SUFF++ for r=0.6 class 2 = 0.809 +- 0.233 (in-sample avg dev_std = 0.411)
SUFF++ for r=0.6 all KL = 0.698 +- 0.233 (in-sample avg dev_std = 0.411)
SUFF++ for r=0.6 all L1 = 0.632 +- 0.190 (in-sample avg dev_std = 0.411)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.724
Model XAI F1 of binarized graphs for r=0.6 =  0.63822625
Model XAI WIoU of binarized graphs for r=0.6 =  0.59697875
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.511
NEC for r=0.6 class 0 = 0.495 +- 0.270 (in-sample avg dev_std = 0.474)
NEC for r=0.6 class 1 = 0.329 +- 0.270 (in-sample avg dev_std = 0.474)
NEC for r=0.6 class 2 = 0.57 +- 0.270 (in-sample avg dev_std = 0.474)
NEC for r=0.6 all KL = 0.433 +- 0.270 (in-sample avg dev_std = 0.474)
NEC for r=0.6 all L1 = 0.467 +- 0.171 (in-sample avg dev_std = 0.474)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:23:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:14 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:27 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:29 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:31 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:33 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1m Data(edge_index=[2, 64], x=[18, 1], node_gt=[18], edge_gt=[64], y=[1], env_id=[1], ori_edge_index=[2, 64], node_perm=[18], num_nodes=18)
[0mData(edge_index=[2, 52], x=[19, 1], node_gt=[19], edge_gt=[52], y=[1], env_id=[1], ori_edge_index=[2, 52], node_perm=[19], num_nodes=19)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:23:35 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.9261
Train Loss: 0.3565
ID Validation ACCURACY: 0.9233
ID Validation Loss: 0.3727
ID Test ACCURACY: 0.9250
ID Test Loss: 0.3602
OOD Validation ACCURACY: 0.4207
OOD Validation Loss: 11.5665
OOD Test ACCURACY: 0.5380
OOD Test Loss: 1.3419

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 0...
[0m[1;37mINFO[0m: [1mCheckpoint 0: 
-----------------------------------
Train ACCURACY: 0.8612
Train Loss: 0.6286
ID Validation ACCURACY: 0.8557
ID Validation Loss: 0.6690
ID Test ACCURACY: 0.8683
ID Test Loss: 0.6245
OOD Validation ACCURACY: 0.9297
OOD Validation Loss: 0.5461
OOD Test ACCURACY: 0.3917
OOD Test Loss: 9.1522

[0m[1;37mINFO[0m: [1mChartInfo 0.9250 0.5380 0.8683 0.3917 0.8557 0.9297[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 26], x=[12, 1], node_gt=[12], edge_gt=[26], y=[1], env_id=[1], ori_edge_index=[2, 26], node_perm=[12], num_nodes=12)
Label distribution from test: (tensor([0, 1, 2]), tensor([1017,  962, 1021]))

Gold ratio (test) =  tensor(0.3931) +- tensor(0.0982)
F1 for r=0.6 = 0.464
WIoU for r=0.6 = 0.327


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.504
Model XAI F1 of binarized graphs for r=0.6 =  0.46384749999999997
Model XAI WIoU of binarized graphs for r=0.6 =  0.3265675
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.426
SUFF++ for r=0.6 class 0 = 0.592 +- 0.250 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 1 = 0.8 +- 0.250 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 class 2 = 0.487 +- 0.250 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all KL = 0.631 +- 0.250 (in-sample avg dev_std = 0.379)
SUFF++ for r=0.6 all L1 = 0.623 +- 0.217 (in-sample avg dev_std = 0.379)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.505
Model XAI F1 of binarized graphs for r=0.6 =  0.46384749999999997
Model XAI WIoU of binarized graphs for r=0.6 =  0.3265675
len(reference) = 800
Effective ratio: 0.616 +- 0.010
Model Accuracy over intervened graphs for r=0.6 =  0.594
NEC for r=0.6 class 0 = 0.509 +- 0.245 (in-sample avg dev_std = 0.489)
NEC for r=0.6 class 1 = 0.327 +- 0.245 (in-sample avg dev_std = 0.489)
NEC for r=0.6 class 2 = 0.542 +- 0.245 (in-sample avg dev_std = 0.489)
NEC for r=0.6 all KL = 0.502 +- 0.245 (in-sample avg dev_std = 0.489)
NEC for r=0.6 all L1 = 0.462 +- 0.197 (in-sample avg dev_std = 0.489)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.395], 'all_L1': [0.472]}), defaultdict(<class 'list'>, {'all_KL': [0.65], 'all_L1': [0.673]}), defaultdict(<class 'list'>, {'all_KL': [0.712], 'all_L1': [0.658]}), defaultdict(<class 'list'>, {'all_KL': [0.698], 'all_L1': [0.632]}), defaultdict(<class 'list'>, {'all_KL': [0.631], 'all_L1': [0.623]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.544], 'all_L1': [0.486]}), defaultdict(<class 'list'>, {'all_KL': [0.628], 'all_L1': [0.53]}), defaultdict(<class 'list'>, {'all_KL': [0.419], 'all_L1': [0.465]}), defaultdict(<class 'list'>, {'all_KL': [0.433], 'all_L1': [0.467]}), defaultdict(<class 'list'>, {'all_KL': [0.502], 'all_L1': [0.462]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.612 +- 0.072
suff++ class all_KL  =  0.617 +- 0.115
suff++_acc_int  =  0.554 +- 0.085
nec class all_L1  =  0.482 +- 0.025
nec class all_KL  =  0.505 +- 0.076
nec_acc_int  =  0.512 +- 0.049


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.547 +- 0.040
Faith. Armon (L1)= 		  =  0.537 +- 0.036
Faith. GMean (L1)= 	  =  0.542 +- 0.038
Faith. Aritm (KL)= 		  =  0.561 +- 0.054
Faith. Armon (KL)= 		  =  0.544 +- 0.058
Faith. GMean (KL)= 	  =  0.552 +- 0.056
Computed for split load_split = id



Completed in  0:03:26.671639  for CIGAGIN GOODMotif/basis



DONE CIGA GOODMotif/basis readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:24:04 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:04 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:15 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:17 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:19 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:23 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:24:29 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ACCURACY: 0.8884
Train Loss: 0.4866
ID Validation ACCURACY: 0.8947
ID Validation Loss: 0.4630
ID Test ACCURACY: 0.8917
ID Test Loss: 0.4878
OOD Validation ACCURACY: 0.7910
OOD Validation Loss: 0.6662
OOD Test ACCURACY: 0.5353
OOD Test Loss: 1.1040

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 15...
[0m[1;37mINFO[0m: [1mCheckpoint 15: 
-----------------------------------
Train ACCURACY: 0.8884
Train Loss: 0.4866
ID Validation ACCURACY: 0.8947
ID Validation Loss: 0.4630
ID Test ACCURACY: 0.8917
ID Test Loss: 0.4878
OOD Validation ACCURACY: 0.7910
OOD Validation Loss: 0.6662
OOD Test ACCURACY: 0.5353
OOD Test Loss: 1.1040

[0m[1;37mINFO[0m: [1mChartInfo 0.8917 0.5353 0.8917 0.5353 0.8947 0.7910[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.138
WIoU for r=0.8 = 0.172


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.536
Model XAI F1 of binarized graphs for r=0.8 =  0.1376925
Model XAI WIoU of binarized graphs for r=0.8 =  0.17154125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.53
SUFF++ for r=0.8 class 0 = 0.637 +- 0.184 (in-sample avg dev_std = 0.401)
SUFF++ for r=0.8 class 1 = 0.637 +- 0.184 (in-sample avg dev_std = 0.401)
SUFF++ for r=0.8 class 2 = 0.702 +- 0.184 (in-sample avg dev_std = 0.401)
SUFF++ for r=0.8 all KL = 0.761 +- 0.184 (in-sample avg dev_std = 0.401)
SUFF++ for r=0.8 all L1 = 0.657 +- 0.142 (in-sample avg dev_std = 0.401)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.536
Model XAI F1 of binarized graphs for r=0.8 =  0.1376925
Model XAI WIoU of binarized graphs for r=0.8 =  0.17154125
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.387
NEC for r=0.8 class 0 = 0.42 +- 0.214 (in-sample avg dev_std = 0.207)
NEC for r=0.8 class 1 = 0.395 +- 0.214 (in-sample avg dev_std = 0.207)
NEC for r=0.8 class 2 = 0.383 +- 0.214 (in-sample avg dev_std = 0.207)
NEC for r=0.8 all KL = 0.232 +- 0.214 (in-sample avg dev_std = 0.207)
NEC for r=0.8 all L1 = 0.4 +- 0.168 (in-sample avg dev_std = 0.207)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:25:28 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:28 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:39 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:41 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:43 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:46 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:25:53 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 44...
[0m[1;37mINFO[0m: [1mCheckpoint 44: 
-----------------------------------
Train ACCURACY: 0.8914
Train Loss: 0.5063
ID Validation ACCURACY: 0.8997
ID Validation Loss: 0.4882
ID Test ACCURACY: 0.8967
ID Test Loss: 0.4931
OOD Validation ACCURACY: 0.7990
OOD Validation Loss: 0.6163
OOD Test ACCURACY: 0.5463
OOD Test Loss: 5.1394

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 92...
[0m[1;37mINFO[0m: [1mCheckpoint 92: 
-----------------------------------
Train ACCURACY: 0.8750
Train Loss: 0.5856
ID Validation ACCURACY: 0.8820
ID Validation Loss: 0.5386
ID Test ACCURACY: 0.8793
ID Test Loss: 0.5812
OOD Validation ACCURACY: 0.8227
OOD Validation Loss: 0.6221
OOD Test ACCURACY: 0.5200
OOD Test Loss: 4.5541

[0m[1;37mINFO[0m: [1mChartInfo 0.8967 0.5463 0.8793 0.5200 0.8820 0.8227[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.137
WIoU for r=0.8 = 0.127


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.543
Model XAI F1 of binarized graphs for r=0.8 =  0.13707125
Model XAI WIoU of binarized graphs for r=0.8 =  0.12698874999999998
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.458
SUFF++ for r=0.8 class 0 = 0.615 +- 0.316 (in-sample avg dev_std = 0.530)
SUFF++ for r=0.8 class 1 = 0.559 +- 0.316 (in-sample avg dev_std = 0.530)
SUFF++ for r=0.8 class 2 = 0.601 +- 0.316 (in-sample avg dev_std = 0.530)
SUFF++ for r=0.8 all KL = 0.597 +- 0.316 (in-sample avg dev_std = 0.530)
SUFF++ for r=0.8 all L1 = 0.592 +- 0.122 (in-sample avg dev_std = 0.530)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.543
Model XAI F1 of binarized graphs for r=0.8 =  0.13707125
Model XAI WIoU of binarized graphs for r=0.8 =  0.12698874999999998
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.399
NEC for r=0.8 class 0 = 0.48 +- 0.326 (in-sample avg dev_std = 0.325)
NEC for r=0.8 class 1 = 0.5 +- 0.326 (in-sample avg dev_std = 0.325)
NEC for r=0.8 class 2 = 0.47 +- 0.326 (in-sample avg dev_std = 0.325)
NEC for r=0.8 all KL = 0.388 +- 0.326 (in-sample avg dev_std = 0.325)
NEC for r=0.8 all L1 = 0.484 +- 0.201 (in-sample avg dev_std = 0.325)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:26:50 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:26:50 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:02 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:04 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:06 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:09 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:27:15 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 52...
[0m[1;37mINFO[0m: [1mCheckpoint 52: 
-----------------------------------
Train ACCURACY: 0.8883
Train Loss: 0.4479
ID Validation ACCURACY: 0.8857
ID Validation Loss: 0.4413
ID Test ACCURACY: 0.8940
ID Test Loss: 0.4470
OOD Validation ACCURACY: 0.6337
OOD Validation Loss: 0.7853
OOD Test ACCURACY: 0.3707
OOD Test Loss: 1.3105

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 64...
[0m[1;37mINFO[0m: [1mCheckpoint 64: 
-----------------------------------
Train ACCURACY: 0.8548
Train Loss: 0.5436
ID Validation ACCURACY: 0.8480
ID Validation Loss: 0.5519
ID Test ACCURACY: 0.8573
ID Test Loss: 0.5426
OOD Validation ACCURACY: 0.7287
OOD Validation Loss: 0.9406
OOD Test ACCURACY: 0.3510
OOD Test Loss: 8.4851

[0m[1;37mINFO[0m: [1mChartInfo 0.8940 0.3707 0.8573 0.3510 0.8480 0.7287[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.093
WIoU for r=0.8 = 0.109


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.376
Model XAI F1 of binarized graphs for r=0.8 =  0.09319624999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.10880375
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.387
SUFF++ for r=0.8 class 0 = 0.592 +- 0.206 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 1 = 0.653 +- 0.206 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 class 2 = 0.602 +- 0.206 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all KL = 0.738 +- 0.206 (in-sample avg dev_std = 0.372)
SUFF++ for r=0.8 all L1 = 0.616 +- 0.158 (in-sample avg dev_std = 0.372)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.376
Model XAI F1 of binarized graphs for r=0.8 =  0.09319624999999998
Model XAI WIoU of binarized graphs for r=0.8 =  0.10880375
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.347
NEC for r=0.8 class 0 = 0.454 +- 0.306 (in-sample avg dev_std = 0.267)
NEC for r=0.8 class 1 = 0.453 +- 0.306 (in-sample avg dev_std = 0.267)
NEC for r=0.8 class 2 = 0.448 +- 0.306 (in-sample avg dev_std = 0.267)
NEC for r=0.8 all KL = 0.343 +- 0.306 (in-sample avg dev_std = 0.267)
NEC for r=0.8 all L1 = 0.452 +- 0.239 (in-sample avg dev_std = 0.267)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:28:12 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:12 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:24 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:25 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:27 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:30 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:28:37 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8933
Train Loss: 0.5080
ID Validation ACCURACY: 0.8957
ID Validation Loss: 0.4867
ID Test ACCURACY: 0.8957
ID Test Loss: 0.5108
OOD Validation ACCURACY: 0.8220
OOD Validation Loss: 0.6464
OOD Test ACCURACY: 0.5567
OOD Test Loss: 2.8145

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8933
Train Loss: 0.5080
ID Validation ACCURACY: 0.8957
ID Validation Loss: 0.4867
ID Test ACCURACY: 0.8957
ID Test Loss: 0.5108
OOD Validation ACCURACY: 0.8220
OOD Validation Loss: 0.6464
OOD Test ACCURACY: 0.5567
OOD Test Loss: 2.8145

[0m[1;37mINFO[0m: [1mChartInfo 0.8957 0.5567 0.8957 0.5567 0.8957 0.8220[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.138
WIoU for r=0.8 = 0.185


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.571
Model XAI F1 of binarized graphs for r=0.8 =  0.13754375
Model XAI WIoU of binarized graphs for r=0.8 =  0.18451499999999998
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.49
SUFF++ for r=0.8 class 0 = 0.6 +- 0.307 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.8 class 1 = 0.588 +- 0.307 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.8 class 2 = 0.625 +- 0.307 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.8 all KL = 0.646 +- 0.307 (in-sample avg dev_std = 0.473)
SUFF++ for r=0.8 all L1 = 0.604 +- 0.119 (in-sample avg dev_std = 0.473)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.571
Model XAI F1 of binarized graphs for r=0.8 =  0.13754375
Model XAI WIoU of binarized graphs for r=0.8 =  0.18451499999999998
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.409
NEC for r=0.8 class 0 = 0.465 +- 0.332 (in-sample avg dev_std = 0.422)
NEC for r=0.8 class 1 = 0.49 +- 0.332 (in-sample avg dev_std = 0.422)
NEC for r=0.8 class 2 = 0.49 +- 0.332 (in-sample avg dev_std = 0.422)
NEC for r=0.8 all KL = 0.379 +- 0.332 (in-sample avg dev_std = 0.422)
NEC for r=0.8 all L1 = 0.482 +- 0.182 (in-sample avg dev_std = 0.422)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:29:34 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:34 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:45 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:47 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:49 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:52 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mDataset: {'train': GOODMotif(18000), 'id_val': GOODMotif(3000), 'id_test': GOODMotif(3000), 'val': GOODMotif(3000), 'test': GOODMotif(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1m Data(edge_index=[2, 58], x=[17, 1], node_gt=[17], edge_gt=[58], y=[1], env_id=[1], ori_edge_index=[2, 58], node_perm=[17], num_nodes=17)
[0mData(edge_index=[2, 80], x=[22, 1], node_gt=[22], edge_gt=[80], y=[1], env_id=[1], ori_edge_index=[2, 80], node_perm=[22], num_nodes=22)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:29:59 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 36...
[0m[1;37mINFO[0m: [1mCheckpoint 36: 
-----------------------------------
Train ACCURACY: 0.8711
Train Loss: 0.5166
ID Validation ACCURACY: 0.8697
ID Validation Loss: 0.5030
ID Test ACCURACY: 0.8843
ID Test Loss: 0.5074
OOD Validation ACCURACY: 0.7277
OOD Validation Loss: 0.7653
OOD Test ACCURACY: 0.3917
OOD Test Loss: 1.9534

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 28...
[0m[1;37mINFO[0m: [1mCheckpoint 28: 
-----------------------------------
Train ACCURACY: 0.8627
Train Loss: 0.6287
ID Validation ACCURACY: 0.8663
ID Validation Loss: 0.5823
ID Test ACCURACY: 0.8713
ID Test Loss: 0.6228
OOD Validation ACCURACY: 0.7327
OOD Validation Loss: 0.7117
OOD Test ACCURACY: 0.4613
OOD Test Loss: 1.5428

[0m[1;37mINFO[0m: [1mChartInfo 0.8843 0.3917 0.8713 0.4613 0.8663 0.7327[0mGOODMotif(3000)
Data example from test: Data(edge_index=[2, 462], x=[149, 1], node_gt=[149], edge_gt=[462], y=[1], env_id=[1], ori_edge_index=[2, 462], node_perm=[149], num_nodes=149)
Label distribution from test: (tensor([0, 1, 2]), tensor([1019, 1029,  952]))

Gold ratio (test) =  tensor(0.0605) +- tensor(0.0246)
F1 for r=0.8 = 0.099
WIoU for r=0.8 = 0.109


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.385
Model XAI F1 of binarized graphs for r=0.8 =  0.0994325
Model XAI WIoU of binarized graphs for r=0.8 =  0.10938
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.43
SUFF++ for r=0.8 class 0 = 0.491 +- 0.332 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.8 class 1 = 0.573 +- 0.332 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.8 class 2 = 0.494 +- 0.332 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.8 all KL = 0.609 +- 0.332 (in-sample avg dev_std = 0.309)
SUFF++ for r=0.8 all L1 = 0.52 +- 0.235 (in-sample avg dev_std = 0.309)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.385
Model XAI F1 of binarized graphs for r=0.8 =  0.0994325
Model XAI WIoU of binarized graphs for r=0.8 =  0.10938
len(reference) = 800
Effective ratio: 0.802 +- 0.002
Model Accuracy over intervened graphs for r=0.8 =  0.355
NEC for r=0.8 class 0 = 0.481 +- 0.372 (in-sample avg dev_std = 0.193)
NEC for r=0.8 class 1 = 0.493 +- 0.372 (in-sample avg dev_std = 0.193)
NEC for r=0.8 class 2 = 0.486 +- 0.372 (in-sample avg dev_std = 0.193)
NEC for r=0.8 all KL = 0.416 +- 0.372 (in-sample avg dev_std = 0.193)
NEC for r=0.8 all L1 = 0.487 +- 0.290 (in-sample avg dev_std = 0.193)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.761], 'all_L1': [0.657]}), defaultdict(<class 'list'>, {'all_KL': [0.597], 'all_L1': [0.592]}), defaultdict(<class 'list'>, {'all_KL': [0.738], 'all_L1': [0.616]}), defaultdict(<class 'list'>, {'all_KL': [0.646], 'all_L1': [0.604]}), defaultdict(<class 'list'>, {'all_KL': [0.609], 'all_L1': [0.52]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.232], 'all_L1': [0.4]}), defaultdict(<class 'list'>, {'all_KL': [0.388], 'all_L1': [0.484]}), defaultdict(<class 'list'>, {'all_KL': [0.343], 'all_L1': [0.452]}), defaultdict(<class 'list'>, {'all_KL': [0.379], 'all_L1': [0.482]}), defaultdict(<class 'list'>, {'all_KL': [0.416], 'all_L1': [0.487]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.598 +- 0.045
suff++ class all_KL  =  0.670 +- 0.067
suff++_acc_int  =  0.459 +- 0.049
nec class all_L1  =  0.461 +- 0.033
nec class all_KL  =  0.352 +- 0.064
nec_acc_int  =  0.379 +- 0.024


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.529 +- 0.014
Faith. Armon (L1)= 		  =  0.518 +- 0.016
Faith. GMean (L1)= 	  =  0.524 +- 0.014
Faith. Aritm (KL)= 		  =  0.511 +- 0.017
Faith. Armon (KL)= 		  =  0.453 +- 0.050
Faith. GMean (KL)= 	  =  0.481 +- 0.031
Computed for split load_split = id



Completed in  0:06:53.519881  for CIGAGIN GOODMotif/size



DONE CIGA GOODMotif/size readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:31:14 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:14 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 49...
[0m[1;37mINFO[0m: [1mCheckpoint 49: 
-----------------------------------
Train ACCURACY: 0.9018
Train Loss: 0.4354
ID Validation ACCURACY: 0.9037
ID Validation Loss: 0.4370
ID Test ACCURACY: 0.8970
ID Test Loss: 0.4710
OOD Validation ACCURACY: 0.7420
OOD Validation Loss: 0.6904
OOD Test ACCURACY: 0.4850
OOD Test Loss: 11.6168

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 89...
[0m[1;37mINFO[0m: [1mCheckpoint 89: 
-----------------------------------
Train ACCURACY: 0.8768
Train Loss: 0.4361
ID Validation ACCURACY: 0.8860
ID Validation Loss: 0.4229
ID Test ACCURACY: 0.8727
ID Test Loss: 0.4687
OOD Validation ACCURACY: 0.8807
OOD Validation Loss: 0.4940
OOD Test ACCURACY: 0.4280
OOD Test Loss: 6.8125

[0m[1;37mINFO[0m: [1mChartInfo 0.8970 0.4850 0.8727 0.4280 0.8860 0.8807[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.293
WIoU for r=0.8 = 0.163


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.491
Model XAI F1 of binarized graphs for r=0.8 =  0.29258375000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.16311374999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.423
SUFF++ for r=0.8 class 0 = 0.731 +- 0.369 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 class 1 = 0.781 +- 0.369 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 class 2 = 0.698 +- 0.369 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 all KL = 0.621 +- 0.369 (in-sample avg dev_std = 0.416)
SUFF++ for r=0.8 all L1 = 0.737 +- 0.279 (in-sample avg dev_std = 0.416)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.491
Model XAI F1 of binarized graphs for r=0.8 =  0.29258375000000003
Model XAI WIoU of binarized graphs for r=0.8 =  0.16311374999999997
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.419
NEC for r=0.8 class 0 = 0.428 +- 0.360 (in-sample avg dev_std = 0.552)
NEC for r=0.8 class 1 = 0.231 +- 0.360 (in-sample avg dev_std = 0.552)
NEC for r=0.8 class 2 = 0.49 +- 0.360 (in-sample avg dev_std = 0.552)
NEC for r=0.8 all KL = 0.587 +- 0.360 (in-sample avg dev_std = 0.552)
NEC for r=0.8 all L1 = 0.381 +- 0.276 (in-sample avg dev_std = 0.552)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:31:37 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:37 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 13...
[0m[1;37mINFO[0m: [1mCheckpoint 13: 
-----------------------------------
Train ACCURACY: 0.9209
Train Loss: 0.4610
ID Validation ACCURACY: 0.9260
ID Validation Loss: 0.4490
ID Test ACCURACY: 0.9150
ID Test Loss: 0.5098
OOD Validation ACCURACY: 0.8717
OOD Validation Loss: 0.5431
OOD Test ACCURACY: 0.3343
OOD Test Loss: 29.2798

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 71...
[0m[1;37mINFO[0m: [1mCheckpoint 71: 
-----------------------------------
Train ACCURACY: 0.8146
Train Loss: 0.6065
ID Validation ACCURACY: 0.8143
ID Validation Loss: 0.6034
ID Test ACCURACY: 0.8157
ID Test Loss: 0.6423
OOD Validation ACCURACY: 0.9270
OOD Validation Loss: 0.4652
OOD Test ACCURACY: 0.3343
OOD Test Loss: 40.5826

[0m[1;37mINFO[0m: [1mChartInfo 0.9150 0.3343 0.8157 0.3343 0.8143 0.9270[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.346
WIoU for r=0.8 = 0.209


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.3457124999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.20945375
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.337
SUFF++ for r=0.8 class 0 = 0.993 +- 0.088 (in-sample avg dev_std = 0.096)
SUFF++ for r=0.8 class 1 = 0.955 +- 0.088 (in-sample avg dev_std = 0.096)
SUFF++ for r=0.8 class 2 = 0.999 +- 0.088 (in-sample avg dev_std = 0.096)
SUFF++ for r=0.8 all KL = 0.971 +- 0.088 (in-sample avg dev_std = 0.096)
SUFF++ for r=0.8 all L1 = 0.982 +- 0.055 (in-sample avg dev_std = 0.096)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.3457124999999999
Model XAI WIoU of binarized graphs for r=0.8 =  0.20945375
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.397
NEC for r=0.8 class 0 = 0.242 +- 0.462 (in-sample avg dev_std = 0.517)
NEC for r=0.8 class 1 = 0.253 +- 0.462 (in-sample avg dev_std = 0.517)
NEC for r=0.8 class 2 = 0.117 +- 0.462 (in-sample avg dev_std = 0.517)
NEC for r=0.8 all KL = 0.364 +- 0.462 (in-sample avg dev_std = 0.517)
NEC for r=0.8 all L1 = 0.204 +- 0.281 (in-sample avg dev_std = 0.517)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:31:58 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:31:58 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 48...
[0m[1;37mINFO[0m: [1mCheckpoint 48: 
-----------------------------------
Train ACCURACY: 0.9083
Train Loss: 0.4178
ID Validation ACCURACY: 0.9150
ID Validation Loss: 0.3921
ID Test ACCURACY: 0.9070
ID Test Loss: 0.4430
OOD Validation ACCURACY: 0.7490
OOD Validation Loss: 0.6150
OOD Test ACCURACY: 0.4973
OOD Test Loss: 4.7022

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 93...
[0m[1;37mINFO[0m: [1mCheckpoint 93: 
-----------------------------------
Train ACCURACY: 0.8751
Train Loss: 0.4864
ID Validation ACCURACY: 0.8857
ID Validation Loss: 0.4608
ID Test ACCURACY: 0.8740
ID Test Loss: 0.5012
OOD Validation ACCURACY: 0.9083
OOD Validation Loss: 0.4673
OOD Test ACCURACY: 0.5683
OOD Test Loss: 3.0991

[0m[1;37mINFO[0m: [1mChartInfo 0.9070 0.4973 0.8740 0.5683 0.8857 0.9083[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.331
WIoU for r=0.8 = 0.202


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.504
Model XAI F1 of binarized graphs for r=0.8 =  0.33074624999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.20179624999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.488
SUFF++ for r=0.8 class 0 = 0.571 +- 0.348 (in-sample avg dev_std = 0.502)
SUFF++ for r=0.8 class 1 = 0.755 +- 0.348 (in-sample avg dev_std = 0.502)
SUFF++ for r=0.8 class 2 = 0.711 +- 0.348 (in-sample avg dev_std = 0.502)
SUFF++ for r=0.8 all KL = 0.523 +- 0.348 (in-sample avg dev_std = 0.502)
SUFF++ for r=0.8 all L1 = 0.681 +- 0.223 (in-sample avg dev_std = 0.502)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.504
Model XAI F1 of binarized graphs for r=0.8 =  0.33074624999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.20179624999999998
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.526
NEC for r=0.8 class 0 = 0.435 +- 0.296 (in-sample avg dev_std = 0.600)
NEC for r=0.8 class 1 = 0.479 +- 0.296 (in-sample avg dev_std = 0.600)
NEC for r=0.8 class 2 = 0.339 +- 0.296 (in-sample avg dev_std = 0.600)
NEC for r=0.8 all KL = 0.668 +- 0.296 (in-sample avg dev_std = 0.600)
NEC for r=0.8 all L1 = 0.418 +- 0.180 (in-sample avg dev_std = 0.600)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:32:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:19 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 140...
[0m[1;37mINFO[0m: [1mCheckpoint 140: 
-----------------------------------
Train ACCURACY: 0.9114
Train Loss: 0.3792
ID Validation ACCURACY: 0.9143
ID Validation Loss: 0.3758
ID Test ACCURACY: 0.9077
ID Test Loss: 0.4035
OOD Validation ACCURACY: 0.8203
OOD Validation Loss: 0.5376
OOD Test ACCURACY: 0.5747
OOD Test Loss: 2.8057

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.8786
Train Loss: 0.4845
ID Validation ACCURACY: 0.8843
ID Validation Loss: 0.4701
ID Test ACCURACY: 0.8797
ID Test Loss: 0.5157
OOD Validation ACCURACY: 0.8787
OOD Validation Loss: 0.4998
OOD Test ACCURACY: 0.5490
OOD Test Loss: 1.6271

[0m[1;37mINFO[0m: [1mChartInfo 0.9077 0.5747 0.8797 0.5490 0.8843 0.8787[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.409
WIoU for r=0.8 = 0.346


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.582
Model XAI F1 of binarized graphs for r=0.8 =  0.40937874999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.346325
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.502
SUFF++ for r=0.8 class 0 = 0.338 +- 0.297 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.8 class 1 = 0.447 +- 0.297 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.8 class 2 = 0.478 +- 0.297 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.8 all KL = 0.234 +- 0.297 (in-sample avg dev_std = 0.559)
SUFF++ for r=0.8 all L1 = 0.422 +- 0.220 (in-sample avg dev_std = 0.559)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.582
Model XAI F1 of binarized graphs for r=0.8 =  0.40937874999999996
Model XAI WIoU of binarized graphs for r=0.8 =  0.346325
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.437
NEC for r=0.8 class 0 = 0.547 +- 0.277 (in-sample avg dev_std = 0.575)
NEC for r=0.8 class 1 = 0.498 +- 0.277 (in-sample avg dev_std = 0.575)
NEC for r=0.8 class 2 = 0.581 +- 0.277 (in-sample avg dev_std = 0.575)
NEC for r=0.8 all KL = 0.624 +- 0.277 (in-sample avg dev_std = 0.575)
NEC for r=0.8 all L1 = 0.541 +- 0.198 (in-sample avg dev_std = 0.575)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:32:40 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODMotif2
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mDataset: {'train': GOODMotif2(18000), 'id_val': GOODMotif2(3000), 'id_test': GOODMotif2(3000), 'val': GOODMotif2(3000), 'test': GOODMotif2(3000), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1m Data(edge_index=[2, 84], x=[29, 1], node_gt=[29], edge_gt=[84], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[0mData(edge_index=[2, 86], x=[29, 1], node_gt=[29], edge_gt=[86], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=29)
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:32:40 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 19...
[0m[1;37mINFO[0m: [1mCheckpoint 19: 
-----------------------------------
Train ACCURACY: 0.9057
Train Loss: 0.4458
ID Validation ACCURACY: 0.9100
ID Validation Loss: 0.4449
ID Test ACCURACY: 0.9023
ID Test Loss: 0.4743
OOD Validation ACCURACY: 0.6823
OOD Validation Loss: 0.7176
OOD Test ACCURACY: 0.3343
OOD Test Loss: 11.3225

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 14...
[0m[1;37mINFO[0m: [1mCheckpoint 14: 
-----------------------------------
Train ACCURACY: 0.8665
Train Loss: 0.5395
ID Validation ACCURACY: 0.8680
ID Validation Loss: 0.5306
ID Test ACCURACY: 0.8620
ID Test Loss: 0.5767
OOD Validation ACCURACY: 0.8170
OOD Validation Loss: 0.6338
OOD Test ACCURACY: 0.4653
OOD Test Loss: 8.5253

[0m[1;37mINFO[0m: [1mChartInfo 0.9023 0.3343 0.8620 0.4653 0.8680 0.8170[0mGOODMotif2(3000)
Data example from test: Data(edge_index=[2, 70], x=[20, 1], node_gt=[20], edge_gt=[70], basis_id=[1], motif_id=[1], y=[1], env_id=[1], num_nodes=20)
Label distribution from test: (tensor([0, 1, 2]), tensor([ 971, 1026, 1003]))

Gold ratio (test) =  tensor(0.2515) +- tensor(0.1001)
F1 for r=0.8 = 0.293
WIoU for r=0.8 = 0.174


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.29305375
Model XAI WIoU of binarized graphs for r=0.8 =  0.17390250000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.344
SUFF++ for r=0.8 class 0 = 0.824 +- 0.234 (in-sample avg dev_std = 0.268)
SUFF++ for r=0.8 class 1 = 0.914 +- 0.234 (in-sample avg dev_std = 0.268)
SUFF++ for r=0.8 class 2 = 0.925 +- 0.234 (in-sample avg dev_std = 0.268)
SUFF++ for r=0.8 all KL = 0.835 +- 0.234 (in-sample avg dev_std = 0.268)
SUFF++ for r=0.8 all L1 = 0.888 +- 0.149 (in-sample avg dev_std = 0.268)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.334
Model XAI F1 of binarized graphs for r=0.8 =  0.29305375
Model XAI WIoU of binarized graphs for r=0.8 =  0.17390250000000002
len(reference) = 800
Effective ratio: 0.806 +- 0.006
Model Accuracy over intervened graphs for r=0.8 =  0.413
NEC for r=0.8 class 0 = 0.221 +- 0.339 (in-sample avg dev_std = 0.411)
NEC for r=0.8 class 1 = 0.308 +- 0.339 (in-sample avg dev_std = 0.411)
NEC for r=0.8 class 2 = 0.17 +- 0.339 (in-sample avg dev_std = 0.411)
NEC for r=0.8 all KL = 0.378 +- 0.339 (in-sample avg dev_std = 0.411)
NEC for r=0.8 all L1 = 0.234 +- 0.230 (in-sample avg dev_std = 0.411)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.621], 'all_L1': [0.737]}), defaultdict(<class 'list'>, {'all_KL': [0.971], 'all_L1': [0.982]}), defaultdict(<class 'list'>, {'all_KL': [0.523], 'all_L1': [0.681]}), defaultdict(<class 'list'>, {'all_KL': [0.234], 'all_L1': [0.422]}), defaultdict(<class 'list'>, {'all_KL': [0.835], 'all_L1': [0.888]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.587], 'all_L1': [0.381]}), defaultdict(<class 'list'>, {'all_KL': [0.364], 'all_L1': [0.204]}), defaultdict(<class 'list'>, {'all_KL': [0.668], 'all_L1': [0.418]}), defaultdict(<class 'list'>, {'all_KL': [0.624], 'all_L1': [0.541]}), defaultdict(<class 'list'>, {'all_KL': [0.378], 'all_L1': [0.234]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.742 +- 0.192
suff++ class all_KL  =  0.637 +- 0.256
suff++_acc_int  =  0.419 +- 0.069
nec class all_L1  =  0.356 +- 0.124
nec class all_KL  =  0.524 +- 0.128
nec_acc_int  =  0.438 +- 0.046


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.549 +- 0.037
Faith. Armon (L1)= 		  =  0.441 +- 0.073
Faith. GMean (L1)= 	  =  0.489 +- 0.036
Faith. Aritm (KL)= 		  =  0.581 +- 0.080
Faith. Armon (KL)= 		  =  0.516 +- 0.093
Faith. GMean (KL)= 	  =  0.547 +- 0.083
Computed for split load_split = id



Completed in  0:01:49.543575  for CIGAGIN GOODMotif2/basis



DONE CIGA GOODMotif2/basis readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:33:19 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:21 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 174...
[0m[1;37mINFO[0m: [1mCheckpoint 174: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0751
ID Validation ACCURACY: 0.8570
ID Validation Loss: 1.0776
ID Test ACCURACY: 0.8538
ID Test Loss: 1.1832
OOD Validation ACCURACY: 0.8127
OOD Validation Loss: 1.9445
OOD Test ACCURACY: 0.7096
OOD Test Loss: 3.2829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 66...
[0m[1;37mINFO[0m: [1mCheckpoint 66: 
-----------------------------------
Train ACCURACY: 0.9462
Train Loss: 0.0859
ID Validation ACCURACY: 0.8468
ID Validation Loss: 0.5292
ID Test ACCURACY: 0.8487
ID Test Loss: 0.5857
OOD Validation ACCURACY: 0.8539
OOD Validation Loss: 0.6808
OOD Test ACCURACY: 0.8169
OOD Test Loss: 0.7504

[0m[1;37mINFO[0m: [1mChartInfo 0.8538 0.7096 0.8487 0.8169 0.8468 0.8539[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.671
SUFF++ for r=0.8 class 0.0 = 0.782 +- 0.284 (in-sample avg dev_std = 0.298)
SUFF++ for r=0.8 class 1.0 = 0.972 +- 0.284 (in-sample avg dev_std = 0.298)
SUFF++ for r=0.8 all KL = 0.836 +- 0.284 (in-sample avg dev_std = 0.298)
SUFF++ for r=0.8 all L1 = 0.88 +- 0.215 (in-sample avg dev_std = 0.298)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.715
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.705
NEC for r=0.8 class 0.0 = 0.173 +- 0.240 (in-sample avg dev_std = 0.246)
NEC for r=0.8 class 1.0 = 0.023 +- 0.240 (in-sample avg dev_std = 0.246)
NEC for r=0.8 all KL = 0.109 +- 0.240 (in-sample avg dev_std = 0.246)
NEC for r=0.8 all L1 = 0.095 +- 0.200 (in-sample avg dev_std = 0.246)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:33:53 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:33:54 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 193...
[0m[1;37mINFO[0m: [1mCheckpoint 193: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8621
ID Validation Loss: 1.0883
ID Test ACCURACY: 0.8557
ID Test Loss: 1.1549
OOD Validation ACCURACY: 0.8230
OOD Validation Loss: 1.8365
OOD Test ACCURACY: 0.7082
OOD Test Loss: 3.4909

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 125...
[0m[1;37mINFO[0m: [1mCheckpoint 125: 
-----------------------------------
Train ACCURACY: 0.9435
Train Loss: 0.0905
ID Validation ACCURACY: 0.8459
ID Validation Loss: 0.6688
ID Test ACCURACY: 0.8398
ID Test Loss: 0.7578
OOD Validation ACCURACY: 0.8575
OOD Validation Loss: 0.7711
OOD Test ACCURACY: 0.8105
OOD Test Loss: 0.8099

[0m[1;37mINFO[0m: [1mChartInfo 0.8557 0.7082 0.8398 0.8105 0.8459 0.8575[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.672
SUFF++ for r=0.8 class 0.0 = 0.793 +- 0.269 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 class 1.0 = 0.974 +- 0.269 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 all KL = 0.851 +- 0.269 (in-sample avg dev_std = 0.283)
SUFF++ for r=0.8 all L1 = 0.887 +- 0.213 (in-sample avg dev_std = 0.283)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.714
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.714
NEC for r=0.8 class 0.0 = 0.156 +- 0.216 (in-sample avg dev_std = 0.237)
NEC for r=0.8 class 1.0 = 0.023 +- 0.216 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all KL = 0.097 +- 0.216 (in-sample avg dev_std = 0.237)
NEC for r=0.8 all L1 = 0.088 +- 0.185 (in-sample avg dev_std = 0.237)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:34:24 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:25 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0773
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.5674
ID Test ACCURACY: 0.8602
ID Test Loss: 0.6145
OOD Validation ACCURACY: 0.8564
OOD Validation Loss: 0.7958
OOD Test ACCURACY: 0.8034
OOD Test Loss: 0.9273

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 95...
[0m[1;37mINFO[0m: [1mCheckpoint 95: 
-----------------------------------
Train ACCURACY: 0.9487
Train Loss: 0.0773
ID Validation ACCURACY: 0.8581
ID Validation Loss: 0.5674
ID Test ACCURACY: 0.8602
ID Test Loss: 0.6145
OOD Validation ACCURACY: 0.8564
OOD Validation Loss: 0.7958
OOD Test ACCURACY: 0.8034
OOD Test Loss: 0.9273

[0m[1;37mINFO[0m: [1mChartInfo 0.8602 0.8034 0.8602 0.8034 0.8581 0.8564[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.81
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.799
SUFF++ for r=0.8 class 0.0 = 0.877 +- 0.164 (in-sample avg dev_std = 0.224)
SUFF++ for r=0.8 class 1.0 = 0.919 +- 0.164 (in-sample avg dev_std = 0.224)
SUFF++ for r=0.8 all KL = 0.908 +- 0.164 (in-sample avg dev_std = 0.224)
SUFF++ for r=0.8 all L1 = 0.899 +- 0.161 (in-sample avg dev_std = 0.224)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.81
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.808
NEC for r=0.8 class 0.0 = 0.098 +- 0.151 (in-sample avg dev_std = 0.181)
NEC for r=0.8 class 1.0 = 0.079 +- 0.151 (in-sample avg dev_std = 0.181)
NEC for r=0.8 all KL = 0.07 +- 0.151 (in-sample avg dev_std = 0.181)
NEC for r=0.8 all L1 = 0.088 +- 0.156 (in-sample avg dev_std = 0.181)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:34:55 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:34:56 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 192...
[0m[1;37mINFO[0m: [1mCheckpoint 192: 
-----------------------------------
Train ACCURACY: 0.9489
Train Loss: 0.0750
ID Validation ACCURACY: 0.8600
ID Validation Loss: 1.1678
ID Test ACCURACY: 0.8613
ID Test Loss: 1.2094
OOD Validation ACCURACY: 0.8373
OOD Validation Loss: 1.8807
OOD Test ACCURACY: 0.7372
OOD Test Loss: 3.1834

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 145...
[0m[1;37mINFO[0m: [1mCheckpoint 145: 
-----------------------------------
Train ACCURACY: 0.9451
Train Loss: 0.0856
ID Validation ACCURACY: 0.8491
ID Validation Loss: 0.7959
ID Test ACCURACY: 0.8461
ID Test Loss: 0.9122
OOD Validation ACCURACY: 0.8587
OOD Validation Loss: 0.9326
OOD Test ACCURACY: 0.8134
OOD Test Loss: 1.1555

[0m[1;37mINFO[0m: [1mChartInfo 0.8613 0.7372 0.8461 0.8134 0.8491 0.8587[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.711
SUFF++ for r=0.8 class 0.0 = 0.813 +- 0.274 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.8 class 1.0 = 0.981 +- 0.274 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.8 all KL = 0.851 +- 0.274 (in-sample avg dev_std = 0.296)
SUFF++ for r=0.8 all L1 = 0.9 +- 0.189 (in-sample avg dev_std = 0.296)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.741
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.74
NEC for r=0.8 class 0.0 = 0.143 +- 0.223 (in-sample avg dev_std = 0.222)
NEC for r=0.8 class 1.0 = 0.022 +- 0.223 (in-sample avg dev_std = 0.222)
NEC for r=0.8 all KL = 0.098 +- 0.223 (in-sample avg dev_std = 0.222)
NEC for r=0.8 all L1 = 0.081 +- 0.177 (in-sample avg dev_std = 0.222)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:35:26 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODSST2
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mDataset: {'train': GOODSST2(24744), 'id_val': GOODSST2(5301), 'id_test': GOODSST2(5301), 'val': GOODSST2(17206), 'test': GOODSST2(17490), 'task': 'Binary classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1m Data(x=[6, 768], edge_index=[2, 10], y=[1, 1], idx=[1], sentence_tokens=[6], length=[1], domain_id=[1], env_id=[1])
[0mData(x=[3, 768], edge_index=[2, 4], y=[1, 1], idx=[1], sentence_tokens=[3], length=[1], domain_id=[1], env_id=[1])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:35:28 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 135...
[0m[1;37mINFO[0m: [1mCheckpoint 135: 
-----------------------------------
Train ACCURACY: 0.9480
Train Loss: 0.0788
ID Validation ACCURACY: 0.8570
ID Validation Loss: 0.7811
ID Test ACCURACY: 0.8510
ID Test Loss: 0.7955
OOD Validation ACCURACY: 0.8215
OOD Validation Loss: 1.3083
OOD Test ACCURACY: 0.7393
OOD Test Loss: 2.3604

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 12...
[0m[1;37mINFO[0m: [1mCheckpoint 12: 
-----------------------------------
Train ACCURACY: 0.8942
Train Loss: 0.1948
ID Validation ACCURACY: 0.8127
ID Validation Loss: 0.4206
ID Test ACCURACY: 0.8148
ID Test Loss: 0.4362
OOD Validation ACCURACY: 0.8560
OOD Validation Loss: 0.4250
OOD Test ACCURACY: 0.8085
OOD Test Loss: 0.5168

[0m[1;37mINFO[0m: [1mChartInfo 0.8510 0.7393 0.8148 0.8085 0.8127 0.8560[0mGOODSST2(17490)
Data example from test: Data(x=[15, 768], edge_index=[2, 28], y=[1, 1], idx=[1], sentence_tokens=[15], length=[1], domain_id=[1])
Label distribution from test: (tensor([0., 1.]), tensor([8463, 9027]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.8 = nan
WIoU for r=0.8 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.748
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.711
SUFF++ for r=0.8 class 0.0 = 0.798 +- 0.213 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 class 1.0 = 0.974 +- 0.213 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 all KL = 0.879 +- 0.213 (in-sample avg dev_std = 0.253)
SUFF++ for r=0.8 all L1 = 0.889 +- 0.187 (in-sample avg dev_std = 0.253)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.8


Computing with masking

Model Accuracy of binarized graphs for r=0.8 =  0.748
Model XAI F1 of binarized graphs for r=0.8 =  nan
Model XAI WIoU of binarized graphs for r=0.8 =  nan
len(reference) = 800
Effective ratio: 0.810 +- 0.008
Model Accuracy over intervened graphs for r=0.8 =  0.747
NEC for r=0.8 class 0.0 = 0.162 +- 0.177 (in-sample avg dev_std = 0.191)
NEC for r=0.8 class 1.0 = 0.023 +- 0.177 (in-sample avg dev_std = 0.191)
NEC for r=0.8 all KL = 0.082 +- 0.177 (in-sample avg dev_std = 0.191)
NEC for r=0.8 all L1 = 0.09 +- 0.171 (in-sample avg dev_std = 0.191)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.836], 'all_L1': [0.88]}), defaultdict(<class 'list'>, {'all_KL': [0.851], 'all_L1': [0.887]}), defaultdict(<class 'list'>, {'all_KL': [0.908], 'all_L1': [0.899]}), defaultdict(<class 'list'>, {'all_KL': [0.851], 'all_L1': [0.9]}), defaultdict(<class 'list'>, {'all_KL': [0.879], 'all_L1': [0.889]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.109], 'all_L1': [0.095]}), defaultdict(<class 'list'>, {'all_KL': [0.097], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.07], 'all_L1': [0.088]}), defaultdict(<class 'list'>, {'all_KL': [0.098], 'all_L1': [0.081]}), defaultdict(<class 'list'>, {'all_KL': [0.082], 'all_L1': [0.09]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.891 +- 0.008
suff++ class all_KL  =  0.865 +- 0.026
suff++_acc_int  =  0.713 +- 0.047
nec class all_L1  =  0.088 +- 0.004
nec class all_KL  =  0.091 +- 0.014
nec_acc_int  =  0.743 +- 0.036


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.490 +- 0.002
Faith. Armon (L1)= 		  =  0.161 +- 0.007
Faith. GMean (L1)= 	  =  0.281 +- 0.006
Faith. Aritm (KL)= 		  =  0.478 +- 0.006
Faith. Armon (KL)= 		  =  0.165 +- 0.022
Faith. GMean (KL)= 	  =  0.280 +- 0.017
Computed for split load_split = id



Completed in  0:02:40.459127  for CIGAvGIN GOODSST2/length



DONE CIGA GOODSST2/length readout

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:36:17 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:17 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:18 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:19 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:19 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:21 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:36:22 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 62...
[0m[1;37mINFO[0m: [1mCheckpoint 62: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.7058
ID Validation Loss: 1.4156
ID Test ACCURACY: 0.6715
ID Test Loss: 1.6499
OOD Validation ACCURACY: 0.6258
OOD Validation Loss: 1.7398
OOD Test ACCURACY: 0.5635
OOD Test Loss: 2.2884

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 56...
[0m[1;37mINFO[0m: [1mCheckpoint 56: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0002
ID Validation ACCURACY: 0.6841
ID Validation Loss: 1.4339
ID Test ACCURACY: 0.6715
ID Test Loss: 1.5868
OOD Validation ACCURACY: 0.6331
OOD Validation Loss: 1.6768
OOD Test ACCURACY: 0.5553
OOD Test Loss: 2.2147

[0m[1;37mINFO[0m: [1mChartInfo 0.6715 0.5635 0.6715 0.5553 0.6841 0.6331[0mGOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 1 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.559
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.539
SUFF++ for r=0.6 class 0 = 0.783 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 1 = 0.794 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 class 2 = 0.733 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all KL = 0.775 +- 0.207 (in-sample avg dev_std = 0.371)
SUFF++ for r=0.6 all L1 = 0.776 +- 0.181 (in-sample avg dev_std = 0.371)


Evaluating NEC for seed 1 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.559
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.557
NEC for r=0.6 class 0 = 0.189 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 1 = 0.161 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 class 2 = 0.223 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all KL = 0.136 +- 0.208 (in-sample avg dev_std = 0.214)
NEC for r=0.6 all L1 = 0.183 +- 0.211 (in-sample avg dev_std = 0.214)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:37:02 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:02 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:04 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:04 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:05 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:06 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:07 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 103...
[0m[1;37mINFO[0m: [1mCheckpoint 103: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0001
ID Validation ACCURACY: 0.6986
ID Validation Loss: 1.6698
ID Test ACCURACY: 0.6444
ID Test Loss: 1.7400
OOD Validation ACCURACY: 0.6129
OOD Validation Loss: 1.8537
OOD Test ACCURACY: 0.5367
OOD Test Loss: 2.1560

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 86...
[0m[1;37mINFO[0m: [1mCheckpoint 86: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0002
ID Validation ACCURACY: 0.6823
ID Validation Loss: 1.5415
ID Test ACCURACY: 0.6751
ID Test Loss: 1.6717
OOD Validation ACCURACY: 0.6381
OOD Validation Loss: 1.7231
OOD Test ACCURACY: 0.5628
OOD Test Loss: 2.0130

[0m[1;37mINFO[0m: [1mChartInfo 0.6444 0.5367 0.6751 0.5628 0.6823 0.6381[0mGOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 2 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.526
SUFF++ for r=0.6 class 0 = 0.743 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 class 1 = 0.76 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 class 2 = 0.73 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 all KL = 0.752 +- 0.194 (in-sample avg dev_std = 0.393)
SUFF++ for r=0.6 all L1 = 0.748 +- 0.169 (in-sample avg dev_std = 0.393)


Evaluating NEC for seed 2 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.523
NEC for r=0.6 class 0 = 0.194 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 class 1 = 0.201 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 class 2 = 0.218 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 all KL = 0.14 +- 0.184 (in-sample avg dev_std = 0.217)
NEC for r=0.6 all L1 = 0.203 +- 0.207 (in-sample avg dev_std = 0.217)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:37:46 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:46 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:48 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:49 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:49 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:50 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:37:51 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 125...
[0m[1;37mINFO[0m: [1mCheckpoint 125: 
-----------------------------------
Train ACCURACY: 0.9996
Train Loss: 0.0013
ID Validation ACCURACY: 0.6986
ID Validation Loss: 1.7716
ID Test ACCURACY: 0.6390
ID Test Loss: 1.9140
OOD Validation ACCURACY: 0.6392
OOD Validation Loss: 1.9480
OOD Test ACCURACY: 0.5470
OOD Test Loss: 2.3429

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 181...
[0m[1;37mINFO[0m: [1mCheckpoint 181: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6625
ID Validation Loss: 1.8607
ID Test ACCURACY: 0.6462
ID Test Loss: 1.9297
OOD Validation ACCURACY: 0.6538
OOD Validation Loss: 1.9479
OOD Test ACCURACY: 0.5518
OOD Test Loss: 2.4294

[0m[1;37mINFO[0m: [1mChartInfo 0.6390 0.5470 0.6462 0.5518 0.6625 0.6538[0mGOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 3 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.532
SUFF++ for r=0.6 class 0 = 0.78 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 class 1 = 0.812 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 class 2 = 0.755 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 all KL = 0.792 +- 0.203 (in-sample avg dev_std = 0.351)
SUFF++ for r=0.6 all L1 = 0.79 +- 0.185 (in-sample avg dev_std = 0.351)


Evaluating NEC for seed 3 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.53
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.531
NEC for r=0.6 class 0 = 0.174 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 1 = 0.15 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 class 2 = 0.179 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all KL = 0.113 +- 0.171 (in-sample avg dev_std = 0.204)
NEC for r=0.6 all L1 = 0.163 +- 0.192 (in-sample avg dev_std = 0.204)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:38:31 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:31 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:32 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:33 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:33 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:35 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:38:36 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 34...
[0m[1;37mINFO[0m: [1mCheckpoint 34: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6986
ID Validation Loss: 1.7703
ID Test ACCURACY: 0.6534
ID Test Loss: 1.9993
OOD Validation ACCURACY: 0.6202
OOD Validation Loss: 2.0138
OOD Test ACCURACY: 0.5607
OOD Test Loss: 2.5829

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 30...
[0m[1;37mINFO[0m: [1mCheckpoint 30: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0000
ID Validation ACCURACY: 0.6877
ID Validation Loss: 1.7754
ID Test ACCURACY: 0.6498
ID Test Loss: 1.9549
OOD Validation ACCURACY: 0.6308
OOD Validation Loss: 1.9916
OOD Test ACCURACY: 0.5697
OOD Test Loss: 2.4454

[0m[1;37mINFO[0m: [1mChartInfo 0.6534 0.5607 0.6498 0.5697 0.6877 0.6308[0mGOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 4 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.565
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.55
SUFF++ for r=0.6 class 0 = 0.774 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 class 1 = 0.761 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 class 2 = 0.727 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 all KL = 0.743 +- 0.233 (in-sample avg dev_std = 0.386)
SUFF++ for r=0.6 all L1 = 0.756 +- 0.214 (in-sample avg dev_std = 0.386)


Evaluating NEC for seed 4 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.565
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.551
NEC for r=0.6 class 0 = 0.168 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 class 1 = 0.168 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 class 2 = 0.211 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 all KL = 0.144 +- 0.193 (in-sample avg dev_std = 0.278)
NEC for r=0.6 all L1 = 0.178 +- 0.195 (in-sample avg dev_std = 0.278)

[1;37mINFO[0m: [1m-----------------------------------
    Task: test
Thu May  9 16:39:15 2024
[0m[1;37mINFO[0m: [1mLoad Dataset GOODTwitter
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:15 PM : [1mPermuting node indices to remove explanation bias for train
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:17 PM : [1mPermuting node indices to remove explanation bias for id_val
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:18 PM : [1mPermuting node indices to remove explanation bias for id_test
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:18 PM : [1mPermuting node indices to remove explanation bias for val
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:20 PM : [1mPermuting node indices to remove explanation bias for test
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mDataset: {'train': GOODTwitter(2590), 'id_val': GOODTwitter(554), 'id_test': GOODTwitter(554), 'val': GOODTwitter(1785), 'test': GOODTwitter(1457), 'task': 'Multi-label classification', 'metric': 'Accuracy'}
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1m Data(x=[7, 768], edge_index=[2, 12], y=[1], idx=[1], sentence_tokens=[7], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 12], node_perm=[7])
[0mData(x=[10, 768], edge_index=[2, 18], y=[1], idx=[1], sentence_tokens=[10], length=[1], domain_id=[1], env_id=[1], ori_edge_index=[2, 18], node_perm=[10])
[1;37mINFO[0m: [1mLoading model...
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1m Init CIGAGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mCreating GINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing  1  layers for classifier
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  weighted
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing feature sampling = feat
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit CIGAvGIN
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit Backbone:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mCreating vGINFeatExtractor:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit CLF:  3
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit GINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mInit vGINFeatExtractor
[0mmitigation_readout =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing the fixed _explain_ functionality
[0m[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mUsing no mitigation (None)
[0mmitigation_virtual in vGINEncoder =  None
[1;34mDEBUG[0m: 05/09/2024 04:39:22 PM : [1mConfig model and output the best checkpoint info...
[0m[1;37mINFO[0m: [1mLoading best In-Domain Checkpoint 32...
[0m[1;37mINFO[0m: [1mCheckpoint 32: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0005
ID Validation ACCURACY: 0.6895
ID Validation Loss: 1.4368
ID Test ACCURACY: 0.6408
ID Test Loss: 1.6540
OOD Validation ACCURACY: 0.6028
OOD Validation Loss: 1.6340
OOD Test ACCURACY: 0.5470
OOD Test Loss: 2.1702

[0m[1;37mINFO[0m: [1mLoading best Out-of-Domain Checkpoint 50...
[0m[1;37mINFO[0m: [1mCheckpoint 50: 
-----------------------------------
Train ACCURACY: 1.0000
Train Loss: 0.0003
ID Validation ACCURACY: 0.6769
ID Validation Loss: 1.5279
ID Test ACCURACY: 0.6354
ID Test Loss: 1.7317
OOD Validation ACCURACY: 0.6409
OOD Validation Loss: 1.6703
OOD Test ACCURACY: 0.5786
OOD Test Loss: 1.9127

[0m[1;37mINFO[0m: [1mChartInfo 0.6408 0.5470 0.6354 0.5786 0.6769 0.6409[0mGOODTwitter(1457)
Data example from test: Data(x=[28, 768], edge_index=[2, 54], y=[1], idx=[1], sentence_tokens=[28], length=[1], domain_id=[1], ori_edge_index=[2, 54], node_perm=[28])
Label distribution from test: (tensor([0, 1, 2]), tensor([379, 719, 359]))
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/mnt/cimec-storage6/users/steve.azzolin/venv/leci/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
F1 for r=0.6 = nan
WIoU for r=0.6 = nan


Evaluating SUFF++ for seed 5 with load_split id




--------------------------------------------------


#D#Computing SUFF++ over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.549
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.529
SUFF++ for r=0.6 class 0 = 0.732 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 class 1 = 0.736 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 class 2 = 0.743 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 all KL = 0.752 +- 0.178 (in-sample avg dev_std = 0.387)
SUFF++ for r=0.6 all L1 = 0.737 +- 0.163 (in-sample avg dev_std = 0.387)


Evaluating NEC for seed 5 with load_split id




--------------------------------------------------


#D#Computing NEC over test across ratios (random_expl=False)


ratio=0.6


Computing with masking

Model Accuracy of binarized graphs for r=0.6 =  0.549
Model XAI F1 of binarized graphs for r=0.6 =  nan
Model XAI WIoU of binarized graphs for r=0.6 =  nan
len(reference) = 800
Effective ratio: 0.607 +- 0.004
Model Accuracy over intervened graphs for r=0.6 =  0.533
NEC for r=0.6 class 0 = 0.193 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 class 1 = 0.222 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 class 2 = 0.229 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 all KL = 0.149 +- 0.191 (in-sample avg dev_std = 0.221)
NEC for r=0.6 all L1 = 0.216 +- 0.207 (in-sample avg dev_std = 0.221)


 -------------------------------------------------- 
Printing evaluation results for load_split id



Eval split test
suff++ = [defaultdict(<class 'list'>, {'all_KL': [0.775], 'all_L1': [0.776]}), defaultdict(<class 'list'>, {'all_KL': [0.752], 'all_L1': [0.748]}), defaultdict(<class 'list'>, {'all_KL': [0.792], 'all_L1': [0.79]}), defaultdict(<class 'list'>, {'all_KL': [0.743], 'all_L1': [0.756]}), defaultdict(<class 'list'>, {'all_KL': [0.752], 'all_L1': [0.737]})]
nec = [defaultdict(<class 'list'>, {'all_KL': [0.136], 'all_L1': [0.183]}), defaultdict(<class 'list'>, {'all_KL': [0.14], 'all_L1': [0.203]}), defaultdict(<class 'list'>, {'all_KL': [0.113], 'all_L1': [0.163]}), defaultdict(<class 'list'>, {'all_KL': [0.144], 'all_L1': [0.178]}), defaultdict(<class 'list'>, {'all_KL': [0.149], 'all_L1': [0.216]})]


 -------------------------------------------------- 
Printing evaluation averaged per seed

Eval split test
suff++ class all_L1  =  0.761 +- 0.019
suff++ class all_KL  =  0.763 +- 0.018
suff++_acc_int  =  0.535 +- 0.009
nec class all_L1  =  0.189 +- 0.019
nec class all_KL  =  0.136 +- 0.012
nec_acc_int  =  0.539 +- 0.013


 -------------------------------------------------- 
Computing faithfulness

Eval split test
Faith. Aritm (L1)= 		  =  0.475 +- 0.004
Faith. Armon (L1)= 		  =  0.302 +- 0.023
Faith. GMean (L1)= 	  =  0.378 +- 0.015
Faith. Aritm (KL)= 		  =  0.450 +- 0.004
Faith. Armon (KL)= 		  =  0.231 +- 0.018
Faith. GMean (KL)= 	  =  0.322 +- 0.012
Computed for split load_split = id



Completed in  0:03:46.263344  for CIGAvGIN GOODTwitter/length



DONE CIGA GOODTwitter/length readout
DONE all :)
